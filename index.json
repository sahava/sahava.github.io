[
{
	"uri": "https://www.simoahava.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/personal/",
	"title": "Personal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/post/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/simmer/",
	"title": "simmer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/personal/simmer-is-live/",
	"title": "Simmer Is Live",
	"tags": ["google tag manager", "simmer"],
	"description": "Our new brand and online course platform, Simmer, is live!",
	"content": "This will likely be the shortest blog article I have ever written, but I have just one short thing to say:\nSIMMER IS LIVE!\nSimmer is a new online course platform, where we bring technical marketing courses to your computer (or mobile device) screen with a straightforward, task-based approach.\nOur first class is aptly Server-side Tagging In Google Tag Manager, and it is open for enrollment right now, all the way until March 14th.\nYou can find the Simmer website at https://www.teamsimmer.com/, and you can enroll to the Server-side Tagging course at https://www.teamsimmer.com/courses/server-side-tagging/.\nI hope to see you in our community!\n"
},
{
	"uri": "https://www.simoahava.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/analytics/",
	"title": "Analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/core-web-vitals/",
	"title": "core web vitals",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/custom-templates/",
	"title": "custom templates",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/ga4/",
	"title": "GA4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/google-analytics-4/",
	"title": "google analytics 4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-core-web-vitals-in-ga4-with-google-tag-manager/",
	"title": "Track Core Web Vitals In GA4 With Google Tag Manager",
	"tags": ["google tag manager", "core web vitals", "custom templates", "google analytics 4", "ga4"],
	"description": "A guide for measuring Core Web Vitals (LCP, FID, and CLS) in Google Analytics 4 using Google Tag Manager and a custom template from the template gallery.",
	"content": "Core Web Vitals is described on the dedicated web.dev resource as (emphasis mine):\n \u0026ldquo;Core Web Vitals are the subset of Web Vitals that apply to all pages, should be measured by all site owners, and will be surfaced across all Google tools.\u0026rdquo;\n  Recommended Core Web Vitals thresholds - from https://web.dev/vitals/  The Core Web Vitals measurement as suggested by Google are:\n Largest Contentful Paint (LCP), which measures the time it took to load the largest image or text block in the viewport. This should optimally be below 2500 milliseconds. First Input Delay (FID), which measures the time it took for the browser to react to user interaction (e.g. click or tap). This should optimally be below 100 milliseconds. Cumulative Layout Shift (CLS), which measures the extent of unexpected shifts in layout during the lifetime of the page. This should optimally have a score of less than 0.1.  Tracking Core Web Vitals has already been explored quite thoroughly in articles such as this TagManagerItalia piece, but I wanted to contribute to the discussion with something unique.\nIn this article, I\u0026rsquo;ll show you how to use a new custom template to set up the Core Web Vitals tracking without having to use Custom HTML tags. I\u0026rsquo;ll also show how to use this template\u0026rsquo;s output to send the data to Google Analytics 4.\n Huge thanks to Philip Walton of Google for clarifying many of the things in this article. Philip is the author behind the Web Vitals content on web.dev among other things, so do yourself a favor and subscribe to his content on Twitter, GitHub, and on his blog.\n Get the template To load the template, go to the Google Tag Manager UI, and click the Templates navigation.\n  Next, click the Search Gallery button in the Tag Templates area.\n  In the overlay, search for the template titled Core Web Vitals, select it, and click the Add to workspace button, following any prompts to get the job done.\n  Once the template has been added to your workspace, you can open the Tags view from the navigation and click to create a new tag.\n  Choose the new Core Web Vitals custom template for the tag.\n  The tag itself doesn\u0026rsquo;t take any customization - it will just work (see the template description for more details).\nAdd the All Pages trigger to it. If you only want it to work on certain pages, use a Page View trigger with a suitable trigger condition instead.\n  Save the tag when ready.\nTest it Next, go to preview mode by clicking the Preview button in the user interface.\nOnce browsing your site in preview mode, reload the page and wait for it to completely load (some Core Web Vitals do not function if the page loads in the background). Click around a bit for good measure.\n In fact, if you want to collect FID there must be a user interaction registered on the page. Interacting with the page will also have the effect of finalizing the LCP report, as the resolving of the largest element in the viewport stops as soon as a user interaction is received.\n When you switch to the preview mode tab, you should ideally see three coreWebVitals events in the list. Sometimes you\u0026rsquo;ll only see two, for example if FID didn\u0026rsquo;t register. Sometimes you\u0026rsquo;ll see more than three, for example if the metrics are re-evaluated before a user interaction is received or the tab/browser is backgrounded.\n The best way to ensure all three core web vitals are reported is to allow the page to load completely in the foreground, click around a bit, and then change the tab to Preview mode.\n   Select one of the events and expand the API Call view. You should see something like this:\n  The dataLayer object This is what the dataLayer object contains:\n   Key Sample value Description     event 'coreWebVitals' The event name – always coreWebVitals.   webVitalsMeasurement.name 'FID' One of LCP, FID, CLS.   webVitalsMeasurement.id 'v1-123123123-234234234' A unique ID per measurement per page. Can be used to associate multiple CLS measurements with a single page load.   webVitalsMeasurement.value 123.55 Value in milliseconds (for LCP and FID), or the score for CLS.   webVitalsMeasurement.delta 17.77 Delta to the previous measurement on the same page (relevant for CLS).   webVitalsMeasurement.valueRounded 124 Value rounded to the nearest integer. CLS score is first multiplied by 1000 to get a meaningful number.   webVitalsMeasurement.deltaRounded 18 Delta rounded to the nearest integer. CLS score is first multiplied by 1000 to get a meaningful number.    Create the Google Tag Manager trigger and variables To use these values in Google Tag Manager, you\u0026rsquo;ll first need to create a Custom Event trigger and one Data Layer variable for each of the webVitalsMeasurement keys you want to measure.\nThe Custom Event trigger The Custom Event trigger should look like this:\n  Make sure you set coreWebVitals as the value of the Event Name field.\nThe Data Layer variables Next, create these six Data Layer variables, using the image below as a model.\n     Variable name Data Layer Variable Name     DLV - webVitalsMeasurement.name webVitalsMeasurement.name   DLV - webVitalsMeasurement.id webVitalsMeasurement.id   DLV - webVitalsMeasurement.value webVitalsMeasurement.value   DLV - webVitalsMeasurement.delta webVitalsMeasurement.delta   DLV - webVitalsMeasurement.valueRounded webVitalsMeasurement.valueRounded   DLV - webVitalsMeasurement.deltaRounded webVitalsMeasurement.deltaRounded    Send the data to Universal Analytics To send the data to Universal Analytics, I recommend you follow this excellent guide on Tag Manager Italia: How To Track Core Web Vitals With Google Tag Manager.\nNOTE! One thing you\u0026rsquo;ll need to do is set the transport field in your Universal Analytics tag to beacon:\n  The beacon field ensures that the tag uses the Beacon API. The Beacon API is designed to \u0026ldquo;protect\u0026rdquo; asynchronous requests so that they complete even if the page has unloaded in the process. This is important for some of the Core Web Vitals measurements, which might not report a value until the user is about to leave the page.\n Note that GA4 does not require you to explicitly set anything for the hits to be sent with the Beacon API. It defaults to using navigator.sendBeacon() in all request.\n One other thing to keep in mind is that Universal Analytics only accepts integers into Event value and Custom Metric fields. For this reason, you\u0026rsquo;ll need to use the DLV - webVitalsMeasurement.valueRounded and DLV - webVitalsMeasurement.deltaRounded variables instead of the raw values.\nSend the data to Google Analytics 4 To send the data to Google Analytics 4, you\u0026rsquo;ll need to use an Event tag, so go ahead and create one.\nHow you set the event name and properties is completely up to you, but this is how I built my event tag:\n  I set the Event Name to the metric name that is collected (LCP, FID, or CLS). Remember that GA4 is moving to a more semantic approach in event naming, where the event name should reflect what happened rather than a generic label such as what Event Category used to be in Universal Analytics.\nI chose to send the four \u0026ldquo;raw\u0026rdquo; parameters only, as GA4 can process floating-point values where Universal Analytics couldn\u0026rsquo;t. The four parameter names I chose are:\n web_vitals_measurement_name for the metric name. web_vitals_measurement_id for the metric ID. web_vitals_measurement_value for the raw metric value. value for the raw metric delta.   I\u0026rsquo;m setting value to the metric delta because value is a recommended parameter for the main value of the event. I opted to use delta simply because it makes sense for CLS measurement. If the metric value was sent as the event value instead, then an aggregate of CLS would report inflated values as it would sum the totals rather than the deltas.\n Make sure the tag fires on the Custom Event trigger you created before.\nOnce you\u0026rsquo;ve saved the tag, you can test the setup again. Reset preview mode by clicking the Preview button, and load a page on the site. You should see your GA4 tag being fired, and if you open its contents you should see valid values being sent to GA4.\n  Remember that GA4 can be debugged in its own preview mode interface by clicking the respective measurement ID in the top bar of the preview mode interface. Here you can select one of the events, such as CLS, to see exactly what was sent to GA4 in the HTTP request!\n  Don\u0026rsquo;t forget to check DebugView in the Google Analytics 4 interface as well!\n  Finally, there\u0026rsquo;s nothing as satisfying as watching the data flow into Google BigQuery.\n  Summary I hope this article has been useful to you!\nI particularly hope that the custom template introduced here will help you deploy Core Web Vitals tracking, as you can avoid working with pesky Custom HTML tags.\nThe template always fetches the latest version of the web-vitals library, which means that if a breaking change is introduced the template will break as well. I don\u0026rsquo;t think this is very realistic, but even if it does happen, I\u0026rsquo;ll be sure to fix the template as soon as this problem emerges.\nTracking Core Web Vitals makes a lot of sense, especially in Google Analytics 4 with its more liberal quotas and value type restrictions. Being on top of things when it comes to site performance is just so very important in this day and age, and everything starts with good data.\nPlease let me know in the comments if you have questions about this solution!\n"
},
{
	"uri": "https://www.simoahava.com/gtmtips/write-client-id-other-gtag-fields-datalayer/",
	"title": "#GTMTips: Write Client ID And Other GTAG Fields Into dataLayer",
	"tags": ["google tag manager", "gtmtips", "google analytics 4", "custom templates"],
	"description": "Use a new custom template to collect fields like Client ID and Session ID into dataLayer, and send them to Google Analytics 4 as custom parameters.",
	"content": "One of the big omissions, at least for now, in Google Analytics 4 is the customTask. It is unfortunate, but no such mechanism exists in the client-side SDKs.\nThis means that you won\u0026rsquo;t be able to do all the magical things that customTask enables in Universal Analytics. One of the biggest headaches is how to collect extremely useful fields such as the Client ID, as these are not available by default in the Google Analytics 4 reporting interface.\nWell, with a new custom tag template you can now fetch these values from GTAG, write them into dataLayer, and utilize them in Google Analytics 4 as custom parameters, for example.\nIt\u0026rsquo;s not as easy to pull off as it was with customTask, but it does give you an additional bonus: since the data is written into dataLayer, it can be used with other tags as well and not just Google\u0026rsquo;s scripts.\nTip 121: Write the Client ID and other GTAG fields into dataLayer    For any of this to work, you must have Google Analytics 4 (or Ads / Floodlight) collection implemented on the page, preferably via Google Tag Manager using the default templates.\n Fetch the template and create a tag First, go to Templates in the Google Tag Manager UI, and search for GTAG GET API in the template gallery. Once you find the template, click Add to workspace to install the template.\nNext, go to Tags and create a new tag. Use the GTAG GET API template as the tag type. This is what you should see on the screen now:\n  Configure the tag Set the GA4 Measurement ID in the appropriate field. If you want to collect GCLID for Ads or Floodlight tags, use the relevant measurement ID instead.\nNext, select the fields whose values will be fetched. Client ID (the unique browser identifier that GA4 uses) is a no-brainer, and even though Session ID is collected automatically to GA4, you might still want to send it to other vendors.\nIf you\u0026rsquo;ve set custom fields using the Fields to Set option in your Configuration tag, you can also add those fields into the Custom Fields To Get table.\nThis is what a configured tag might look like:\n  Create the tag sequence Next, and this is important, you need to add this tag in a tag sequence, so that it fires after your GA4 Configuration tag (or Event tag, if you\u0026rsquo;re not using the Configuration tag).\n It\u0026rsquo;s absolutely vital you do it this way. It\u0026rsquo;s the only way you ensure that the API is ready to accept requests.\n To add the tag in the sequence, open your Google Analytics 4: Configuration tag, and scroll to Advanced Settings -\u0026gt; Tag Sequencing.\nCheck the box next to Fire a tag after \u0026hellip; fires, and select the tag you created from the custom template in the drop-down list. This is what it would look like:\n  By using a tag sequence, you\u0026rsquo;re ensuring that the API isn\u0026rsquo;t called until after gtag() has loaded and initialized on the page.\nTest If you now go to Preview mode and test your setup, you should see a new Data Layer event named gtagApiGet in the event list.\n  If you don\u0026rsquo;t see it, it\u0026rsquo;s most likely because you don\u0026rsquo;t have a GA4 configuration on the page, or it\u0026rsquo;s because the API tag fired before gtag() had time to load.\nCreate the trigger and variables To make use of this information, you need to create a Custom Event trigger for the gtagApiGet event as well as Data Layer variables for each of the fields whose value you want to utilize.\n Custom Event trigger for gtagApiGet   Data Layer variable for gtagApiResult.client_id  Create a new Google Analytics 4 event tag Finally, if you want to send these values to Google Analytics 4, you can. You need to choose whether to use a user property or custom event dimensions. For user-scoped fields such as the Client ID, the former makes more sense, and for event-specific fields the latter might be more useful.\nUnfortunately, GA4 in all its wisdom forces any numerical fields to be converted into number types, so a Client ID is rewritten as a double. This means it becomes illegible as there are too many decimals and the result is truncated.\nI\u0026rsquo;ve solved this for now by adding a simple . at the end of the Client ID in the event tag itself (see below). This is easy enough to overlook in the reports or clean up in the BigQuery export. If you have another solution for this, please add it to the comments of this article!\nThis is what a finalized event tag might look like:\n  You can always utilize DebugView to validate the setup.\n  As you can see, the little . at the end of the Client ID is there to make sure GA4 doesn\u0026rsquo;t convert this string into a number.\nSummary I miss customTask. So much. I really hope Google will introduce something similar to gtag(), but I\u0026rsquo;m not optimistic. Thus far many of the things we\u0026rsquo;ve previously done in the client have been moved to the Google Analytics 4 admin (cross-domain tracking, for example).\nI get the feeling that Google wants to simplify the implementation part as much as possible, which makes a lot of sense. But it wouldn\u0026rsquo;t hurt to have customization options for power users.\nIn this article I showed you how to fetch GA4-specific fields using an API of gtag(). The API itself is a bit awkward to use as it\u0026rsquo;s asynchronous. This means that it won\u0026rsquo;t work in a variable as variables rely on synchronous execution.\nThe silver lining is that the data is now available in dataLayer for all your tags, and not just those running on the Google stack.\nI hope you found this useful - please let me know in the comments if there\u0026rsquo;s something unclear about how the template or the .get() API works!\n"
},
{
	"uri": "https://www.simoahava.com/tags/gtmtips/",
	"title": "gtmtips",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/core-web-vitals/",
	"title": "Core Web Vitals - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Core Web Vitals custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Vendor documentation   Gallery entry   GitHub repo    Description This tag adds measurement handlers for three Core Web Vitals: Largest Contentful Paint (LCP), First Input Delay (FID), and Cumulative Layout Shift (CLS).\nEach measurement is pushed into dataLayer as soon as it is available.\n Note that not every page load will register all three milestones. Read the documentation linked to above to understand why.\n Instructions The template itself has no configurable fields - it will simply work as soon as you create a tag with it.\nSet the tag to fire on the All Pages trigger. If you only want to measure some pages, use a Page View trigger with a suitable trigger condition instead.\nThe tag writes the following objects into dataLayer for each time they are measured (CLS, for example, can be measured more than once per page).\n{ event: \u0026#39;coreWebVitals\u0026#39;, webVitalsMeasurement: { name: \u0026#39;LCP\u0026#39;, id: \u0026#39;v1-123123123-123123123\u0026#39;, value: 123.55, delta: 13.68, valueRounded: 124, deltaRounded: 14 } }  You can create a Custom Event trigger for coreWebVitals to fire your tag when this dataLayer.push() happens.\nYou need to create Data Layer variables for all the webVitalsMeasurement.* variable names that you want to utilize in your tags.\nIt\u0026rsquo;s recommended to use the Beacon API when sending the Core Web Vitals data to the endpoint, as some of the measurements might happen upon page unload.\n Google Analytics 4 uses the Beacon API automatically, so you don\u0026rsquo;t need to do anything manually.\n Release notes    Date Changeset     21 January 2021 Add rounded integers to the object for\teasier implementation.   21 January 2021 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/page/",
	"title": "Pages",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/create-css-path-variable-for-click-element/",
	"title": "Create A CSS Path Variable For Click Element",
	"tags": ["google tag manager", "custom javascript", "css", "preview mode"],
	"description": "How to build a CSS path variable in Google Tag Manager, which mimics the path string automatically displayed for Click Element while in Preview mode.",
	"content": "At one point in the turbulent year of 2020, you might have gasped in surprise when looking at the preview interface of Google Tag Manager. No, I\u0026rsquo;m not talking about the new preview mode interface.\nInstead, I\u0026rsquo;m referring to how the Click Element and Form Element built-in variables would now display a CSS path string rather than the expected [object HTMLDivElement] (or equivalent).\n  There was good and bad in this update.\nThe good thing was that the Click Element variable was now much more informative in Preview mode. Honestly, there\u0026rsquo;s not much you can do with the object element string in terms of debugging. Being able to see the CSS path gives us more clues on how the clicked element was reached.\nThe bad thing was that the preview mode output is very misleading. It claims now that Click Element returns a string and a CSS path. It does neither. It still returns an HTML element which you can use with your CSS selector predicates and your custom DOM traversal scripts.\n Just to reiterate: The behavior of Click Element has not changed. The only thing that\u0026rsquo;s changed is how the Click Element variable is displayed in the Preview interface.\n I would have expected Google to keep the Click Element as it was and instead introduce a new built-in variable called Click CSS Path or something. This would have avoided the ambiguity while still adding value to the variables in the container.\nWell, I want to show you how you can create your own CSS path variable. The variable uses the exact same method that GTM uses for determining the CSS path, so it should always match what you see in the Preview interface.\nThe CSS Path variable In Google Tag Manager, go to Variables and click the New button to create a new user-defined variable.\n  Next, choose Custom JavaScript variable as the type.\n Unfortunately, custom templates do not (yet) support DOM traversal or handling HTML elements, so you need to use a Custom JavaScript variable for this.\n Copy-paste the following into the code editor:\nfunction() { // Build a CSS path for the clicked element  var originalEl = {{Click Element}}; var el = originalEl; if (el instanceof Node) { // Build the list of elements along the path  var elList = []; do { if (el instanceof Element) { var classString = el.classList ? [].slice.call(el.classList).join(\u0026#39;.\u0026#39;) : \u0026#39;\u0026#39;; var elementName = (el.tagName ? el.tagName.toLowerCase() : \u0026#39;\u0026#39;) + (classString ? \u0026#39;.\u0026#39; + classString : \u0026#39;\u0026#39;) + (el.id ? \u0026#39;#\u0026#39; + el.id : \u0026#39;\u0026#39;); if (elementName) elList.unshift(elementName); } el = el.parentNode } while (el != null); // Get the stringified element object name  var objString = originalEl.toString().match(/\\[object (\\w+)\\]/); var elementType = objString ? objString[1] : originalEl.toString(); var cssString = elList.join(\u0026#39; \u0026gt; \u0026#39;); // Return the CSS path as a string, prefixed with the element object name  return cssString ? elementType + \u0026#39;: \u0026#39; + cssString : elementType; } }  Save it and you\u0026rsquo;re good to go!\nThe script first builds the list of elements on the click path, all the way to the highest node in the document (html).\nIt does this by checking what the tagName of the element is, and then adding class and/or ID modifiers to it. Then, it moves to the next element up the DOM tree.\nOnce the list is built, the script finally takes the stringified element object name itself (e.g. HTMLDivElement) and adds that as the prefix to the CSS path string.\nA sample return value of the script could be something like this:\n'HTMLDivElement: html \u0026gt; body \u0026gt; div#blog \u0026gt; div.hasCoverMetaIn#main'\nThis means that the clicked element is an HTMLDivElement (\u0026lt;div\u0026gt;), with a single class of hasCoverMetaIn and the ID main. The element has three ancestors: a \u0026lt;div id=\u0026quot;blog\u0026quot;\u0026gt;, the \u0026lt;body\u0026gt; tag, and finally the \u0026lt;html\u0026gt; node itself.\nSummary What is this script useful for? Good question!\nIt\u0026rsquo;s not very useful in a trigger, as you can use the matches CSS selector predicate for a more robust approach to identifying elements in the DOM tree.\nHowever, it might be interesting metadata to send to your analytics tool of choice with the click event. Sometimes sending the click path will help uncover missing bits of information that you might not have considered sending in the first place.\nFor example, you might find out that your Click trigger has been too lax, collecting clicks from elements you never intended it to do. You can check the CSS path to see which \u0026ldquo;rogue elements\u0026rdquo; are receiving clicks, after which you can calibrate the Click trigger itself to be more precise.\nAlso, I miss writing articles like this, where I look at some nifty customization that is achievable with JavaScript.\nI hope you enjoyed the article! If you can think of a use case for the CSS path variable, please let us know in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/tags/css/",
	"title": "css",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/custom-javascript/",
	"title": "custom javascript",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/preview-mode/",
	"title": "preview mode",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/gtag-get-api/",
	"title": "GTAG GET API - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The GTAG GET API custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Vendor documentation   Gallery entry   GitHub repo    Description This tag writes the values for the selected default fields (Client ID, Session ID, GCLID) as well as any custom fields set with the Google Analytics 4 template into dataLayer.\nIt utilizes the asynchronous .get() API of Google Site Tag.\nSee the official documentation for a summary of the API.\nInstructions Create a tag with the GTAG GET API template, and choose all the fields you want it to write into dataLayer.\nThen, and this is important, you have to set the tag in a tag sequence to fire after (as a Cleanup Tag) the Google Analytics 4: Configuration tag (or an Event tag if you don\u0026rsquo;t use a Configuration tag).\n This tag must fire after a Google Analytics 4 tag has completely fired, or else the gtag() GET API will not do anything.\n   When the tag fires, it launches a number of calls to gtag() GET API - one for each field it has to resolve. Once all the fields have been resolved, a dataLayer.push() is done with the following contents:\n{ \u0026#34;event\u0026#34;: \u0026#34;gtagApiGet\u0026#34;, \u0026#34;gtagApiResult\u0026#34;: { \u0026#34;client_id\u0026#34;: \u0026#34;123123123.123123123\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;1608578585\u0026#34;, ... } } After this, you can create a Custom Event trigger for the gtagApiGet event and Data Layer variables for the individual field values in the gtagApiResult object.\n    Release notes    Date Changeset     21 December 2020 Remove console logging.   21 December 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/newsletter/",
	"title": "The Simmer Newsletter",
	"tags": [],
	"description": "",
	"content": "For years, I\u0026rsquo;ve resisted the urge to start a newsletter. My reasoning was always very simple: I want this site to be about knowledge sharing and nothing else. Building a mailing list can really get in the way. I\u0026rsquo;m sure you\u0026rsquo;ve noticed it with your favorite content creators. You can practically pinpoint the moment in time when it became more about followers and less about content.\nI\u0026rsquo;m also personally allergic to websites that relentlessly try to get you to sign up or to buy something, and I\u0026rsquo;ve been proud that this blog has never been about pushing a product, service, or agenda.\nHowever\u0026hellip;\n I\u0026rsquo;m going to bend my principles a little from now on. With my business partner and wife, Mari Ahava, we\u0026rsquo;ve created a new brand called Simmer, and we\u0026rsquo;re furiously proud of it.\nAs a brand, Simmer is an online platform through which we\u0026rsquo;ll offer courses, webinars, and other content designed for anyone interested in the technical side of marketing. There will be both a paid offering as well as free goodies.\nBut as a newsletter Simmer is so much more. In essence, it\u0026rsquo;s an extension of my blog. It will contain short-form, exclusive content from yours truly, and I\u0026rsquo;ll make sure it\u0026rsquo;s going to be worth your while.\n I will not stop writing on this blog, and I will promise you that the blog will stay on topic, will continue to be completely free, and you can enjoy the content with no strings attached whatsoever.\n In the newsletter, I will share tips, tricks, and actionable and topical content from the world of technical digital marketing. The premise is the same as with this blog: I want to explain complicated topics in an understandable manner.\nThe newsletter will also include bits and pieces about the Simmer platform such as upcoming courses and related offers, but the Simmer Newsletter\u0026rsquo;s main point of focus will be knowledge sharing, in all shapes and sizes.\nThe newsletter is going to be the fastest way to get notified on all the new developments in the world of online analytics and tag management, on latest additions to this blog, on the latest tools and services I\u0026rsquo;ve created, and on the courses (and offers!) we\u0026rsquo;ll release in Simmer.\nSo, while waiting for the website and the first courses to be launched (early 2021), please subscribe and enjoy even more free content from yours truly!\nWe won\u0026rsquo;t spam you - I promise.\n "
},
{
	"uri": "https://www.simoahava.com/custom-templates/transaction-id-logger/",
	"title": "Transaction ID Logger - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Transaction ID Logger custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Gallery entry   GitHub repo      Description This tag fires after all tags that fire on the same trigger event have completed execution. It stores the Transaction ID provided in the tag into a cookie and/or localStorage entry named __tid_gtm. The value is appended to an array, and this array can be read with the Transaction ID Reader variable template.\nThe purpose of this setup is to prevent collection of duplicate transactions or purchase conversions.\nInstructions Provide the Transaction ID as a variable into the appropriate field. Typically this is a Data Layer variable from your Enhanced Ecommerce or GA4 dataLayer objects.\nNext, you can choose whether to store the ID in a cookie, a localStorage entry, or both. In either case, the name used to store the ID is __tid_gtm. The item is appended to an array, so you might end up with an array that contains multiple Transaction IDs, if you\u0026rsquo;ve made multiple purchases on the site.\nRelease notes    Date Changeset     11 December 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/transaction-id-reader/",
	"title": "Transaction ID Reader - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The Transaction ID Reader custom variable template is a variable template for Google Tag Manager\u0026rsquo;s community template gallery. It is designed to work together with the Transaction ID Logger tag template.\n   Resource     Gallery entry   GitHub repo    Description When you create a variable with this template, you need to configure two settings.\nFirst, you need to provide the Transaction ID that will be checked against the stored IDs. Typically this would be a Data Layer Variable that returns the ID from an Enhanced Ecommerce object or from a GA4 dataLayer.\nWhen the variable is resolved, it looks for a browser cookie and/or a localStorage entry named __tid_gtm. This is created by the Transaction ID Logger tag template.\nThe entry contains an array of all the transaction IDs recorded by the Transaction ID Logger tag.\nThe variable checks if the Transaction ID the user provided is found within this array, and if a match is made, it returns true. If no match is made, the variable returns false.\nInstead of a boolean return value, you can also choose to have the variable return the full array of stored Transaction IDs, in case you want to process this information for some other reason.\nYou can use the variable to conditionally block purchase tags from firing again if the user returns to the purchase page after the purchase has already been registered.\nThis is an alternative solution to my Universal Analytics\u0026rsquo; customTask deduper, as platforms like GA4 and Facebook do not support customTask.\nRelease notes    Date Changeset     11 December 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/eec-products-ga4-items/",
	"title": "EEC Products -&gt; GA4 Items - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The EEC Products -\u0026gt; GA4 Items custom variable template is a variable template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Gallery entry   GitHub repo    Description This template has four main options.\n Map ecommerce object automatically - the template looks into the latest ecommerce object pushed into dataLayer, and takes its products, impressions, or promotions array (whichever is found). It returns the array mapped into the format required by the items parameter in GA4 ecommerce. Map products array - Select a variable from the drop-down which returns a valid Enhanced Ecommerce products array. The variable will be mapped into the format required by the items parameter. Map impressions array - Select a variable from the drop-down which returns a valid Enhanced Ecommerce impressions array. The variable will be mapped into the format required by the items parameter. Map promotions array - Select a variable from the drop-down which returns a valid Enhanced Ecommerce promotions array. The variable will be mapped into the format required by the items parameter.  The items are mapped as closely as possible, using the following pairs:\n   EEC parameter GA4 parameter Description     id item_id or promotion_id The ID of the product, impression, or promotion.   name item_name or promotion_name The name of the product, impression, or promotion.   brand item_brand The brand of the product or impression.   variant item_variant The variant of the product or impression.   category item_category([2-5]) The category of the product or impression, automatically split into item_category to item_category5.   price price The price of the product or impression.   quantity quantity The quantity of the product or impression.   list item_list_name The product list of the product or impression - taken from either the product/impression object if available, or from actionField if present there.   position index or creative_slot The index of the impression or the slot of the promotion.   creative creative_name The name of the promotion creative.    In addition to this, you can map product-scoped custom dimensions and metrics to proper item parameter names. To create a mapping, you need to provide the index number of the custom dimension or metric (e.g. the 1 in dimension1) in the first field, and then an item parameter name in the second. To map a custom metric in index 13 to an item parameter named quantity_in_stock, you\u0026rsquo;d create the mapping like this:\n  All custom dimensions and metrics that are not mapped will be added into the item object with their original names (so dimension17, metric23 for example).\nRelease notes    Date Changeset     9 December 2020 Fix quantity to not send NaN if source is not a valid number.   28 November 2020 Fix item_categoryX to not have the second underscore. The official documentation is wrong.   23 November 2020 Automatically convert quantity to integer, as any other number format breaks item revenue.   21 October 2020 Add option to map custom dimensions and metrics to proper item parameter names.   17 October 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-web-container-client-server-side-tagging/",
	"title": "New Google Tag Manager Web Container Client For Server Side Tagging",
	"tags": ["google tag manager", "server-side tagging"],
	"description": "Introducing the new",
	"content": "With the introduction of server-side tagging in Google Tag Manager, the variety of things you can do with your own server-side proxy is mind-boggling:\n Reduce client-side bloat by consolidating data streams and distributing them to vendor endpoints server-side. Improve data security by adding safeguards and validations to prevent harmful data from being sent to vendor endpoints. Enrich data server-side, by combining the incoming data stream with data from APIs and data stores that you own and control. Remove third-party downloads from the browser by proxying them via the server-side container.  This last point is what this article expands on. If you check out this video (jumps straight to the relevant part), you can see how it\u0026rsquo;s possible to already proxy popular libraries like analytics.js and gtag.js.\nIn essence, the server-side container downloads the library from Google and then serves it to the browser. This way the library download happens in first-party context (assuming you\u0026rsquo;ve configured the endpoint to run on your own domain namespace), and you can freely adjust things like cache headers.\n  With a recent update, Google Tag Manager\u0026rsquo;s server container now allows you to proxy a Google Tag Manager web container as well.\nHow it works The Google Tag Manager: Web Container client template comes pre-built in Server containers. To activate it, you need to create a new client with it.\nBrowse to Clients in the Google Tag Manager UI and click to create a new client.\n  Click the Client Configuration box to open a selector. Find the Google Tag Manager: Web Container client template, and click to choose it.\n  To configure the client, you basically need to list the Google Tag Manager Container IDs that should be proxied via this client.\nTo add a new ID, simply click Add Container ID, and add it to the list.\n Since the Allowed Container IDs field can take variables as well, you could theoretically create a variable that fetches the container ID from the request URL directly (i.e. anything after ?id= in https://server-container.com/gtm.js?id=GTM-ABCDEFG). However, I think it\u0026rsquo;s a good idea to not create a system like that to avoid the endpoint being spammed with requests for containers that are unrelated to your setup.\n   Once you Save this client, your Server container has now been enabled to respond to requests for Google Tag Manager web containers. A regular request for a web container would look like this:\nhttps://www.googletagmanager.com/gtm.js?id=GTM-ABCDEFG\nWith the client you configured, you can now issue the request like this:\nhttps://your.server.container.com/gtm.js?ID=GTM-ABCDEFG\nYour server-side endpoint will return the Google Tag Manager library in the exact same format that the regular snippet would, with the difference that it\u0026rsquo;s served from your endpoint and not Google\u0026rsquo;s CDN.\nUpdate the snippet You will, of course, need to update your Google Tag Manager container snippet as a result.\n NOTE! The update below applies to environments as well, as the new client template has been designed to work with environments, too.\n This is what a regular Google Tag Manager container snippet looks like:\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;https://www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-PZ7GMV9\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; You need to change the string 'https://www.googletagmanager.com/gtm.js?id=' to correspond with the domain you\u0026rsquo;ve created for your server-side endpoint. If your container serves from https://sgtm.simoahava.com, the updated snippet would look like this:\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;https://sgtm.simoahava.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-PZ7GMV9\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; Once you\u0026rsquo;ve updated the snippet, you can take the Server container to debug mode, and see if it responds to requests for the GTM container:\n  You need to also check where the request for the gtm.js library is sent using your browser\u0026rsquo;s network tools:\n  And, of course, make sure that all your tags work as they should by using whatever debugging tools you have at your disposal.\nThings to note Currently, zone containers will still load from googletagmanager.com, but this will likely be fixed shortly to load these also from your server-side endpoint.\nWhen you choose to serve the Google Tag Manager container from your own first-party domain and you\u0026rsquo;re also sending data to a Universal Analytics and/or GA4 client, you need to make sure you enable the gtag paths in these clients. This is because once you move to loading the Google Tag Manager container from your first-party domain, any gtag.js libraries that are loaded via this container will also need to be loaded from your first-party domain. So make sure you check the relevant options if you\u0026rsquo;re using these two clients.\n    Final thing to note is that the \u0026lt;noscript\u0026gt; snippet can also be modified to load the iframe from your first-party endpoint. The only caveat is that the HTML page in the iframe will continue loading items from googletagmanager.com.\n In case you\u0026rsquo;re wondering, yes: search console verification WILL work with your new first-party \u0026lt;noscript\u0026gt; snippet as well!\n Summary Even though you absolutely don\u0026rsquo;t have to use a web container when working with server-side tagging, I think that many people do and will continue doing so because of the ease that Google Tag Manager brings to tagging a website.\nBeing able to proxy the Google Tag Manager container introduces obvious benefits for moving third-party scripts to a first-party context.\nYou are, of course, technically still downloading vendor JavaScript from their CDN. However, this download happens in the server container, and the browser will only be privy to a request-response pathway with your server container and not the vendor endpoint itself.\nIntroducing the Google Tag Manager container proxy to your Server container might have an impact on the costs associated with your server-side tagging setup, so be sure to monitor the billing in your Google Cloud Platform dashboard.\nLet me know in the comments what you think about this new addition to the server-side tagging built-in client mix!\n"
},
{
	"uri": "https://www.simoahava.com/tags/server-side-tagging/",
	"title": "server-side tagging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/intelligent-tracking-prevention/",
	"title": "intelligent tracking prevention",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/itp/",
	"title": "itp",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/privacy/itp-debug-mode-in-safari/",
	"title": "ITP Debug Mode in Safari",
	"tags": ["intelligent tracking prevention", "itp", "safari"],
	"description": "",
	"content": "Intelligent Tracking Prevention is the name of the tracking prevention mechanism implemented in WebKit and enabled in Safari and all major browsers running on the iOS platform. I\u0026rsquo;ve written about it on this blog, and the CookieStatus.com resource is something you should bookmark for further reading.\nThe purpose of ITP is to prevent tracking tools\u0026rsquo; access to data stored and processed in the browser. This involves things like blocking all third-party cookies and restricting the lifetime of first-party cookies.\n  In this article, I want to show you how to use the ITP Debug Mode. It\u0026rsquo;s a console logger that outputs all the actions taken by Intelligent Tracking Prevention on the user\u0026rsquo;s device or browser.\nA key thing to remember about ITP is that it\u0026rsquo;s algorithmic and on-device. This means that the domains \u0026ldquo;flagged\u0026rdquo; by ITP as having tracking capabilities will change from one device to the next. This is why the ITP Debug Mode is so valuable - it\u0026rsquo;s the only way to have insight into what ITP is actually doing on the current device.\nSafari did release the privacy report feature a while back. However, it\u0026rsquo;s designed to scare the user and not educate them. It uses misleading language which can lead to a critical misunderstanding of how ITP actually works. I recommend treating the feature as nothing else but Safari self-promotion.\nHow to enable ITP Debug Mode in Safari Unfortunately, ITP Debug Mode only works in the desktop version of Safari. Even if you tether your iOS device and use the web inspector tool to review what the iOS browser is doing, the debug logging doesn\u0026rsquo;t reach this interface.\nTo enable ITP Debug Mode in desktop Safari, you first need to reveal the Develop menu in the toolbar. To do this, click Safari -\u0026gt; Preferences, and open the Advanced tab.\n  Here, check the Show Develop menu in menu bar.\n This is a good feature to keep enabled at all times - there\u0026rsquo;s a lot of valuable insight to be gained through the developer tools of a browser.\n Next, in the new Develop menu, choose Enable Intelligent Tracking Prevention Debug Mode.\n  Finally, from the same Develop menu, click Show JavaScript Console to open the console in the browser viewport.\n  If the option to show the console isn\u0026rsquo;t available, you need to browse to a website first.\nOnce the console is open, by browsing to a handful of websites you should start seeing ITP-related logging in the console.\nWhat is logged The inventory of items logged into the console is pretty vast, and it\u0026rsquo;s also quite poorly documented. Nevertheless, here are some of the debug messages you can expect to find.\n   Debug message Description     Applying cross-site tracking restrictions to: [list_of_domains] ITP has flagged the listed domains as having cross-site tracking capabilities and has applied its restrictions to them.   About to remove data records for: [list_of_domains] It has been more than 7 days since a meaningful interaction in first-party context with the listed domains, so Safari deletes either all stored data on these domains or all but cookies (if cookies were set with valid Set-Cookie headers).   Messages pertaining to the Storage Access API. There are many debug messages about the Storage Access API, such as when access is granted, when access couldn\u0026rsquo;t be granted, and when access was denied.   Scheduled domain to have its cookies set to SameSite=Strict. ITP has detected that a domain is participating in bounce tracking and has set its cookies to SameSite=Strict as a consequence.   Capped the expiry of third-party CNAME cloaked cookie named cookie_name. If CNAME cloaking is detected, then cookies set with Set-Cookie headers are set to expire 7 days.     Domains flagged by ITP are reset by deleting the browser history. This nulls the list of domains flagged by ITP.\n Applying cross-site tracking restrictions to\u0026hellip; This debug message means that ITP has flagged the domains in the list as having cross-site tracking capabilities. It doesn\u0026rsquo;t mean they do cross-site tracking, but it means they have the capability to do it.\nTo be listed among these domains typically requires that the domain be requested from enough unrelated sites using HTTP requests. For example, loading the same pixel from google-analytics.com on three unrelated sites (e.g. sitea.com, siteb.com, and sitec.com) will have ITP flag google-analytics.com as a (potential) tracking domain.\n  The main impact of this is on first-party storage, as traffic from a flagged domain can result in first-party cookies being restricted to a lifetime of just 24 hours. There is some impact on the referrer string as well.\nNotably, Safari doesn\u0026rsquo;t use ITP to block third-party cookies anymore. Instead, all third-party cookies are blocked without exception on Safari, and the site needs to use the Storage Access API to gain access.\nAbout to remove data records for\u0026hellip; Safari removes all script-writable storage (localStorage, indexedDB etc.) from a site if the site hasn\u0026rsquo;t been interacted with in first-party context for the last 7 days.\nThis means that the user has to actually visit the site and perform a meaningful interaction (e.g. click, scroll) to have the timer reset.\nJavaScript cookies can have their expiration reset with the document.cookie API, but the maximum expiration is always enforced at 7 days by Safari.\n    Safari also removes all website data (including cookies set with HTTP headers) if the domain has been classified by ITP AND if the domain hasn\u0026rsquo;t been interacted with in first-party context in the last 30 days.\n Thanks to Antoine Bourlon for the clarification above.\n Storage Access API Interactions with the Storage Access API have their own debug logging.\n  For example, when prompting for Storage Access, a message like this is shown if the embedded frame requests access to the parent page\u0026rsquo;s storage:\nAbout to ask the user whether they want to grant storage access to sub_frame_domain under top_frame_domain or not.\nSimilarly, errors are output into the console as well.\n  Bounce tracking protection with a SameSite=Strict jail This feature is special in that it\u0026rsquo;s not currently implemented, even though the protection mechanism exists.\nBased on this article, the restriction has been created to cater to specific, listed domains that participate in such a scheme. Currently, the list is empty.\nThe idea is that if bounce trackers are detected, then domains to which the request is redirected are stripped of their capability to leverage cross-site cookies.\nIt remains to be seen whether this gets more attention in the future. It\u0026rsquo;s likely, considering how prevalent bounce tracking is.\nCNAME cloaking mitigation CNAME cloaking refers to a practice where a tracking domain is masked behind a first-party domain owned by the site that wants to collect data for the tracker.\nFor example, instead of a site sending data directly to https://www.tracker.com, they proxy it through https://track.site.com. This subdomain would actually be a CNAME record to www.tracker.com.\nThis has been a fairly popular way of overcoming ITP\u0026rsquo;s restrictions on first-party cookies. However, with the recent versions of Safari and iOS/iPadOS, WebKit has begun the task to invalidate this practice.\n   There\u0026rsquo;s a bug in WebKit where the cookie name isn\u0026rsquo;t exposed. A fix is forthcoming.\n ITP Debug m´Mode will tell you if the current domain is trying to set cookies using a CNAME record to a cross-site origin, and whether it has taken appropriate action.\nSummary Intelligent Tracking Prevention is a complex machine, so getting familiar with the accompanying debug output is very useful if you want to know more about the inner workings of the mechanism.\nITP is also constantly evolving, and the log output will be constantly updated to match the latest capabilities of WebKit\u0026rsquo;s tracking preventions.\nUnfortunately, it\u0026rsquo;s not currently possible to enable ITP Debug Mode for iOS Safari via USB tether, even if all other capabilities of the web inspector are at your disposal. Hopefully, the mobile browser will have debugging capabilities at some point in the future, but based on discussions with WebKit engineers it doesn\u0026rsquo;t seem likely in the short term.\nLet me know in the comments if you have questions about ITP Debug Mode or about ITP in general!\n"
},
{
	"uri": "https://www.simoahava.com/tags/safari/",
	"title": "safari",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/events/",
	"title": "events",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/implementation-guide-events-google-analytics-4/",
	"title": "Implementation Guide For Events In Google Analytics 4",
	"tags": ["google tag manager", "GA4", "google analytics 4", "events"],
	"description": "",
	"content": "On the surface, tracking events in Google Analytics 4 (GA4) is fairly simple. Events are, after all, pretty much the only thing you can collect in GA4.\nIt\u0026rsquo;s easy to get tied down with endless comparisons to Universal Analytics, though. While I\u0026rsquo;m steadfastly opposed to the idea that GA4 should resemble Universal Analytics, it\u0026rsquo;s still important to cleanse the palate and approach GA4\u0026rsquo;s event tracking with an open mind.\nThere are some comparisons that can be drawn between the new and the old, but what GA4 might lack in some features and use cases, it more than makes up for this with a more flexible data structure.\nIn this implementation guide, I\u0026rsquo;ll take a look at the structure, composition, implementation, and evaluation of Google Analytics 4 events.\n  For more details on how events work in Google Analytics 4, do check out the following excellent resources:\n  Official support documentation\n  Official developer documentation\n  Digital Debrief by Krista Seiden\n  Charles Farina\u0026rsquo;s Blog\n  Ken Williams\u0026rsquo; blog\n  These resources outmatch anything I could create by a wide margin, so make sure you bookmark them and follow them religiously!\n This guide will be focused on data collection for the web, mainly using Google Tag Manager. While mobile app events and gtag.js implementations will be discussed in passing, I have chosen this particular focus due to the overwhelming popularity of Google Tag Manager as the primary means for collecting data to Google Analytics.\n Structure of an event An event hit, when sent to GA4, comprises an event name and parameters.\nFurthermore, parameters can be divided into automatically collected parameters and custom parameters.\nFor example, here is an event named page_view, sent with a bunch of parameters to GA4:\n  There are a handful of special (reserved) parameters collected for measurement, such as:\n tid - Measurement ID sr - Screen resolution _dbg - Debug mode active seg - User is considered engaged for this session   There are more details about these automatically collected parameters in a later chapter.\n I\u0026rsquo;m also sending a couple of custom parameters:\n  epn.readibility_median_grade - A custom parameter sent as a number.\n  epn.readability_reading_time - A custom parameter sent as a number.\n  The prefix epn means a number parameter, whereas the prefix ep means a text parameter.\nHere\u0026rsquo;s an example of an event that sends text parameters and user properties (prefixed with up):\n  Note that GA4 batches events, so if you fire more than one event in a very short time window, the events will be sent together in a single hit:\n  The hit above sent two events: a page_view event and a scroll event, both with some custom parameters.\nIn this article, I\u0026rsquo;ll help you come to terms with the terminology used above.\nEvent tag in Google Tag Manager While Google Analytics 4 implementation via Google Tag Manager is still somewhat in beta (even if GA4 itself isn\u0026rsquo;t), it\u0026rsquo;s still more than possible to set an entire GA4 implementation up with GTM.\nGTM has two tag types for collecting data to Google Analytics 4.\n  Google Analytics: GA4 Configuration - Recommended for GA4 data collection. Establishes the base settings and is used as the underlying configuration for event tags.\n  Google Analytics: GA4 Event - Tag type used for sending events to GA4.\n    In order to send your custom event hits to GA4, you need to establish a configuration first. You can check out my original guide for some (slightly outdated) tips on how to do this. See also the chapter below for more details on how the configuration tag and event tags interact.\nWithin the GA4 Event tag type, you have some fields and settings you can configure the tag with.\n     Setting Description     Configuration Tag If you want to use a configuration tag, you can select it here. Alternatively, you can ignore the configuration tag and just set a measurement ID manually.   Event Name The name of the event as will be shown in GA4 reports. The recommended format is snake_case. Try to avoid using the same names as automatically collected events unless you know what you\u0026rsquo;re doing.   Event Parameters Add any parameters you want to send with the event to GA4. If you use a configuration tag, then any parameters set in that will also be applied to this hit (with the values fixed to what they were when the config tag was fired). Remember that if you want to use parameters in reports, you need to register them as custom dimensions.   User Properties Add any user properties you want to set with this event. User properties are similar to user-scoped custom dimensions in Universal Analytics: they apply to all future hits from this user, until a new value is set for each. You do need to register them in GA4 for them to be available in reports.    Once you\u0026rsquo;ve configured the event name and the parameters to your liking, all you need to do is add a trigger to the tag and start collecting the data.\n If you\u0026rsquo;re using Google Tag Manager\u0026rsquo;s Preview mode to test your tag with (as you should), the hit will automatically have the _dbg parameter set, and you\u0026rsquo;ll see it in DebugView.\n Quotas and limits Unfortunately, GA4 events have some arbitrary, and frankly very annoying, quotas and limits that you need to be aware of. See this article for the full list.\nFor example, you should avoid collecting more than 500 unique event names in GA4. Even though this limit doesn\u0026rsquo;t seem to be technically enforced (you can collect 500+ unique events), you\u0026rsquo;re still in violation of a collection limit, and Google can start enforcing this limit at any time.\nSimilarly, while you can collect more than 25 parameters in a single event tag, only 25 would be passed on to reporting.\n Note that even if you go over the quotas in event parameters and custom dimensions, they will still be passed through to the BigQuery export, which makes the BigQuery export a super valuable thing to set up in every single GA4 implementation!\n These quotas and limits will most likely change as the platform matures, so I recommend keeping the documentation bookmarked so that you aren\u0026rsquo;t blindsided by changes that could impact your data quality.\nEnhanced measurement Enhanced measurement is really neat. It takes a leaf out of autotrack.js\u0026rsquo; book, and with a few flavors from Google Tag Manager\u0026rsquo;s triggers, and bakes it all into a simple, automated tracking tool orchestrated by gtag.js itself.\nConfused? Sorry about that. In short, enhanced measurement tracks stuff automatically just by adding the configuration tag to the page.\nEnhanced measurement is controlled from the GA4 admin settings.\n  If you enable enhanced measurement, the following events will be tracked automatically (assuming you don\u0026rsquo;t disable them) - see the official documentation for more details.\n page_view - sent with the page load as well as browser history events with single-page apps. scroll - sent when the user scrolls to the bottom of the page. click - sent when the user clicks an outbound link. view_search_results - sent when the user loads a page with a configured search query parameter. video_start, video_progress, video_complete - sent automatically when the user starts to view a video, progresses past predefined percentages of the total video length, and reaches the end of the video. file_download - sent when the user clicks a link to a file with one of the predefined extensions.  Note that as these are, for all intents and purposes, automatically collected events, you can\u0026rsquo;t change or modify the parameters sent with these hits. You can add parameters to them by setting them globally in a configuration tag, but check out this later chapter for some serious caveats to setting event parameters in a configuration tag!\n You can always opt-out of enhanced measurement tracking by modifying the GA4 data stream settings. For example, I\u0026rsquo;m not a huge fan of the scroll event, so I never enable it - I\u0026rsquo;d rather have full control over something as potentially spammy as scroll tracking.\n BigQuery export and GA4 events If you\u0026rsquo;ve been reading my blog, you might have noticed that I\u0026rsquo;m a huge fan of Google BigQuery.\nOne of the main reasons to stop reading this right now and go and implement GA4 is because it comes with a free BigQuery export of all your GA4 data!\n BigQuery itself has, naturally, costs associated with usage, but you don\u0026rsquo;t have to pay Google Analytics for the privilege of accessing raw data in the export.\n   For the purposes of event tracking, the BigQuery export has a huge, huge benefit.\n Quotas that would apply to events in reports do not apply to the data exported to BigQuery!\n In other words, if you\u0026rsquo;ve reached the maximum number of custom dimensions, for example, all the custom parameters that you send to GA4 will still be exported into BigQuery.\n  This means that you might end up with two different approaches to the data collected by a single platform.\nFirst is the data that\u0026rsquo;s surfaced in the reporting UI. This is your everyday data - subject to quotas and limitations, but easy to manipulate and explore for useful insights.\nThe second is the raw data in BigQuery. This is the data set that you can take, sculpt, engineer, extract, and manipulate however you wish. It lets you break free of the prescriptive schema imposed by GA4, and you can start to truly unravel the wonders of the event stream model.\nIf you\u0026rsquo;re curious about how BigQuery and GA4 play ball together, takes a look at this article that Pawel Kapuscinski and I wrote!\nEvent types Considering that GA4 is basically an event stream model, it\u0026rsquo;s not surprising that events themselves are categorized in a number of different ways.\nThere are the automatically collected events, which GA4, one way or the other, collects without needing manual tagging.\nThen there are a bunch of recommended events, which Google strongly recommends you to use (if they are applicable for your measurement needs).\nFinally, if there are no automatically collected events or recommended events that suit your use case, you can always use custom events.\nAutomatically collected events Automatically collected events are enumerated in this document. Both app (Firebase SDK) and web (Global Site Tag / Google Tag Manager) automatically collect some events, and if you\u0026rsquo;ve enabled enhanced measurement for the web stream, even more events are collected without the need to manually tag the site.\nJust to reiterate - an event is an automatically collected event when it doesn\u0026rsquo;t require adding a manual tag or code snippet for the data collection. Some automatically collected events collect additional parameters that help flesh out the event metadata.\nHere are the automatically collected events for the web:\n   Event name Description Trigger     click Outbound link collection (when enabled via enhanced measurement). Click on a link that leads out of the current domain.   file_download File download tracking (when enabled via enhanced measurement). Click on a link that downloads a file with one of the predefined extensions.   first_visit First visit from a given client / user ID. When an event is collected from a new client or user ID.   page_view When a page is loaded or a browser history event happens (when enabled via enhanced measurement). Page load or history event (pushState, replaceState, popstate).   scroll Scroll tracking (when enabled via enhanced measurement). Scroll to the bottom of the page.   session_start An event starts a new session. 30 minutes have expired since the last event sent by the current user.   user_engagement The session is considered an engaged session. Active engagement with the website for more than 10 seconds, or a conversion event collected, or two or more pageviews collected.   video_complete YouTube video was watched to completion (when enabled via enhanced measurement). Video ends.   video_progress YouTube video is watched past specific progress milestones (when enabled via enhanced measurement). Video progresses past 10%, 25%, 50%, and 75%.   video_start YouTube video watching begins (when enabled via enhanced measurement). Video starts.   view_search_results A search results page is viewed (when enabled via enhanced measurement). A page is loaded with search query parameters in the URL.    As you can see, many of the events listed have to do with enhanced measurement. However, there are some key events that are always collected, and interestingly they might not actually have their own hit types at all. Instead, they are derived from other events that are collected on the site.\nFor example, let\u0026rsquo;s say you collect a page_view event from a user who hasn\u0026rsquo;t visited your site before, and this page_view is the first hit they send. This is what you\u0026rsquo;ll see in the request:\n  The _fv parameter denotes that this is a first visit event (the user is a new user), and the _ss parameter means that this event started a new session. When you look at the data in GA4, this one single page_view event actually splits into three separate GA4 events!\n  If you look at the request again, you\u0026rsquo;ll see that the seg parameter has the value 0. This means that the session is not considered engaged. When the user then sends another page view before the session timeout (30 minutes), this is what the parameter value becomes in the new page view event:\n  On the web, user_engagement is collected in milliseconds, with each event that classifies as an engagement event sending the current engaged time with the _et parameter. This parameter value isn\u0026rsquo;t surfaced in GA4 events, but it is available in BigQuery:\n  For a platform that celebrates not being bound to an arbitrary concept of a session, it sure does seem like it hasn\u0026rsquo;t shrugged off this arbitrariness entirely. Time will tell how these events will properly translate to the reports.\nHaving said that, the idea that a session is just an annotation (the _ss parameter and the dedicated events) rather than a key aggregation (as in Universal Analytics) is inspiring. It\u0026rsquo;s much easier to reconstruct your own concept of what a meaningful aggregation metric is.\n See this article by Jules Stuifbergen for an example of how you can build your own session approach using BigQuery and the raw data export from GA4.\n Recommended events Recommended events are a curiosity, mainly because at the time of writing they have very little functional use in GA4.\nGoogle has a bunch of support pages focused on telling you what semantics to use when collecting events for specific segments and industries.\nFor example, when the user logs in, Google instructs you to use an event with the name login and a parameter with the name method.\n  Why? Who knows. There\u0026rsquo;s no benefit in the current events to collecting an event with the name login as opposed to, for example, user_login. However, it\u0026rsquo;s a recommended event, so it has to carry some weight, right?\nWell, again, time will tell. I\u0026rsquo;m assuming some of these events get collected into their own dedicated reports. Hopefully, some will even be exempt from current quotas, ecommerce events in particular.\n In fact, ecommerce events are an exception to the rant above - they do have functional weight as they populate the monetization reports. Take a look at my guide for ecommerce implementation in GA4 for more information.\n For now, if you want to add a new data collection tag to the site, take a look at the list of recommended events and see if you find one that fits your use case. If you do, use that, and try to use the recommended parameters as well. There\u0026rsquo;s no harm in doing so, and at best you\u0026rsquo;ll enable some cool functionality that Google will release in the near future.\nHowever, if you don\u0026rsquo;t find a recommended event that fits your use case perfectly, don\u0026rsquo;t waste time trying to shoehorn a square piece into a round hole. Use custom events instead.\nCustom events Once you\u0026rsquo;ve exhausted the list of automatically collected and recommended events, and you can\u0026rsquo;t find one that fits your tracking need, you can always send whatever event name you wish. Just note that there are some reserved names.\nCustom events are super powerful, even if Google is oddly diminutive about their use.\n  For some reason, Google really wants you to use anything but custom events. I don\u0026rsquo;t really agree with this. Custom events are your chance to choose how GA4 will serve your organization. Even if the data isn\u0026rsquo;t surfaced in most of the standard reports (odd argument because custom events are certainly available in all the key parts of the reporting UI), they will be available in BigQuery.\nNo matter how long they\u0026rsquo;ve been in the game, I don\u0026rsquo;t think Google can or should prescribe how you are to do event tracking on any given website. Thus, take their recommended events with a grain of salt, especially since they have little functional impact today, and use custom events at your leisure.\nFamiliarize yourself with BigQuery - it really is the key to making the most of GA4. I love the analysis hub, and I\u0026rsquo;m sure I\u0026rsquo;ll use the data API religiously once it has a stable release, but nothing - nothing - will replace BigQuery as my analysis tool of choice.\nEvent parameters As mentioned above, events comprise an event name and parameters.\nParameters are further split into special parameters (reserved names that contain technical details about the hit), custom parameters (text and number parameters), and user properties.\nSpecial parameters These parameters are collected with every single event, regardless of if it\u0026rsquo;s collected automatically or tagged manually:\n ul or language, which collects the browser language (e.g. en-us). dl or page_location, which collects the current URL. dr or page_referrer, which collects the referrer URL (or empty string if not available). dt or page_title, which collects the page title. sr or screen_resolution, which collects the screen resolution.  There are additional \u0026ldquo;special\u0026rdquo; parameters that are collected in some cases. These include (but are not limited to):\n cid - Client ID; the cookie identifier that helps GA4 recognize repeat visits from the same device. uid - User ID; an identifier you can set manually based on an authentication token, for example. This helps you unite cross-platform and cross-device browsing under a single login identifier. sid - Session ID. sct - Session count; how many sessions have been collected from the current user. NOTE! As this is collected with a client-side cookie that is not related to the Client ID cookie, it\u0026rsquo;s possible this number doesn\u0026rsquo;t actually reflect reality, and a BigQuery analysis might return different results, for example. seg - Session engaged; if the current session is considered \u0026ldquo;engaged\u0026rdquo;. _fv - First visit; if the current hit is the first hit collected from this user. _ss - Session start; if the current hit started a new session.  Custom parameters You can set custom parameters by adding them manually to your tags. Automatically collected events can collect some custom parameters automatically as well.\nCustom parameters come in two flavors:\n Text parameters, when the value set to this parameter is not a number (ep. prefix in the hit). Number parameters, when the value set to this parameter is a number (epn. prefix in the hit).  The main difference is that text parameters can be used as custom dimensions and number parameters can be used as custom metrics.\nTo set parameters, you add them into the event tag like this:\n  Here I\u0026rsquo;ve configured the two variables ({{Readability - Get median grade}} and {{Readability - Get reading time}}) to return numbers, so they get automatically converted to number parameters. The user_level is cast into a string by Google Tag Manager (as it\u0026rsquo;s hard-coded into the tag field), and thus it\u0026rsquo;s treated as a text parameter.\n  The only caveat to setting event parameters in the tag is that event parameter names cannot begin with google_, ga_, or firebase_.\nWhen you want to use event parameters for analysis, they need to be registered as custom dimensions and metrics first.\nIf you have the BigQuery export enabled, all event parameters will be exported to BQ whether they\u0026rsquo;re registered as custom dimensions / metrics or not.\nUser properties User properties can be set with events as well. A user property acts similarly to Universal Analytics\u0026rsquo; user-scoped custom dimensions, with the main difference that they apply from the hit they were set onwards rather than the session* they were set as in Universal Analytics.\nUser properties need to be registered in the GA4 user interface for them to be available in reports.\nTo set a user property, simply add it to the tag in the correct place:\n  You\u0026rsquo;ll see that it\u0026rsquo;s included in the hit with the up. prefix.\n  There are some user property names that cannot be used as they are reserved. Check the list here.\nConfiguration tag and events When you create an event tag in Google Tag Manager, you have the option of choosing a configuration tag to set things up for the event hit.\n  The configuration tag is, surprise surprise, the same as the config command with the global site tag (gtag.js).\nYou can use it to establish a shared configuration for your event hits, and you can even use it to set persistent event parameters.\nThe most common use case for the configuration tag is for configuring the GA4 implementation. You can add fields that configure how cookies are set or to which endpoint to send the hits (useful if you want to collect the data with a server container).\nHowever, you can also set persistent events and user properties with the configuration tag as well. Any field names that you set that are not reserved (e.g. cookie_domain) will be treated as event parameters, and they will be included with every single event that uses this configuration tag.\n  Note that there\u0026rsquo;s one potentially pretty devastating caveat with setting persistent event parameters and user properties with the configuration tag:\nThe values will be fixed to their initial values and will NOT be updated with every event.\nIn other words, when you set an event parameter or property in the configuration tag, it will be sent with every single event that uses the configuration tag. However, the value will always be whatever it was when the configuration tag fired.\nIt\u0026rsquo;s thus not a useful way to set dynamic values like event timestamps or similar. Instead, it should be used to set parameters that are unlikely to change from one event to the next.\nEcommerce events Ecommerce events are, at the time of writing, part of a very special and unique cast: they are recommended events that actually do something.\nThe typical structure of an ecommerce event is a prescribed event name (e.g. add_to_cart) together with an items array that reflects the items or products that were involved in the action.\nIf you want to start tagging your site for ecommerce data collection, I\u0026rsquo;d like to direct you to my comprehensive implementation guide for GA4 ecommerce.\nOne curiosity about ecommerce events is that the items structure is an actual array. Typically, event parameters are primitive values such as strings or numbers.\nThe items array is a refreshing change and hopefully, hopefully, a sign that GA4 will start opening up the rigid data structure inherited from Universal Analytics, where the only multi-dimensional values you can collect are ecommerce objects.\n  I mean, that syntax of turning the array of objects into this weird, proprietary string format doesn\u0026rsquo;t make very much sense either, but at least you can send something other than just primitive values to GA4.\nI really hope they\u0026rsquo;ll extend the data model to accept JSON values, for example. Being able to send an array or object as an event parameter and then having it be automatically parsed for the reporting UI (and nested for BigQuery) would be an absolutely incredible feature to have in GA4. Something that\u0026rsquo;s been missing from Google\u0026rsquo;s analytics tools since day one.\nDebugging events For debugging the event implementation, there are three tools I use.\n  The new Tag Assistant preview mode for making sure that the event fields are populated correctly.\n  The web browser\u0026rsquo;s developer tools for making sure the hit payload contains all the correct values.\n  GA4\u0026rsquo;s DebugView for making sure the data ends up in GA4 in the correct format.\n   If you\u0026rsquo;re a fan of browser extensions, I recommend David Vallejo\u0026rsquo;s excellent GTM/GA Debug which comes equipped with full GA4 debugging support!\n Tag Assistant Preview When you\u0026rsquo;re debugging your GA4 setup with Tag Assistant, you have two ways of validating your setup.\nFirst, you can debug the GTM container where you\u0026rsquo;ve implemented the GA4 tags. Choose the trigger event (e.g. DOM Ready), and make sure all the fields in the event tag look right.\n  You can also select the GA4 measurement ID to see if the hit actually dispatched.\n  Note how the page_view event name is translated to Page View in the Tag Assistant view. Unfortunately, the debug experience for a GA4 tracker is still a bit unwieldy, and you just have to know what event names to look for. Luckily it\u0026rsquo;s usually easy to click through the handful of available messages to find the one where your hit was sent.\nWhen debugging with Tag Assistant, your focus should be on verifying that all the fields and parameters resolve to their correct values. If something is off, you know it needs to be fixed in GTM.\nDeveloper tools Even though the new GA4 measurement ID debug view shows you the hit was dispatched with certain values, I still like to do network request debugging using the browser\u0026rsquo;s developer tools.\nIn GA4, the request is sent to a /collect endpoint just as with Universal Analytics. However, you\u0026rsquo;ll be able to identify a GA4 payload by the v=2 parameter as well as the GA4 measurement ID sent with the tid parameter.\n  Check the requests, make sure they return a 204 status code.\n GA4 requests return a 204 status because the content type for GA4 requests is text, but the response is always empty. It\u0026rsquo;s still a successful request!\n Then go through the parameters and make sure all the values look good. This is what the actual request to GA4 will be, so if something is off at this point you\u0026rsquo;ll know you need to fix it at its source (e.g. GTM).\n  DebugView Finally, if you\u0026rsquo;re previewing a Google Tag Manager container, the hit will be automatically sent to GA4\u0026rsquo;s DebugView. If you want to manually enable DebugView for other hits (e.g. when not previewing a container), you can always set the debug_mode field to any value (though true probably makes most semantic sense).\n  When the request includes the debug mode parameter (_dbg), you\u0026rsquo;ll find a real-time data stream of similar hits in the DebugView report in GA4.\n  I can\u0026rsquo;t emphasize enough how stupendously amazing this feature is. To be honest, it\u0026rsquo;s my favorite part of GA4\u0026rsquo;s reporting interface!\nA real-time stream of debug events, with all the parameters available for perusal, and you can even see how some automatically collected events are generated on the fly!\nSo, as the last step of debugging your event implementation, make sure DebugView agrees with what you think the event should look like. If everything looks good, the next step is to locate the data in your standard reports (might take some time to reach those).\nIf something looks off in DebugView, the issue might be at the source (something was wrong with GTM after all), or you could have messed something with GA4\u0026rsquo;s data stream settings.\nDebugging is an acquired skill - it\u0026rsquo;s not easy to do because you need to know what you\u0026rsquo;re looking for before you can flag it as a potential issue. However, what matters is consistency and the discipline to approach debugging with an end-to-end process rather than just cherry-picking individual parts of the event implementation for analysis.\nIt doesn\u0026rsquo;t make sense to only look at your GTM setup if the issue can also be in your GA4 settings or vice versa.\nSummary I hope this guide has been useful! Setting up event tracking with GA4 can be difficult at first because there\u0026rsquo;s a lot of baggage inherited from Universal Analytics.\nI want to share something that I\u0026rsquo;ve mentioned numerous times in trainings and discussions when GA4 comes up:\nI don\u0026rsquo;t want GA4 to repeat Universal Analytics\u0026rsquo; mistakes!\nSo many people are holding off with trying GA4 out because they want it to have everything that Universal Analytics had first. What\u0026rsquo;s the point? Universal Analytics\u0026rsquo; data model was far from perfect. Sessions, sampling, limited custom dimensions, views, filters, client IDs, just to name a few.\nI really hope that GA4 finds a way out of the shackles imposed upon it by virtue of being an analytics platform created by the same company that also created Google Analytics.\nGA4\u0026rsquo;s data model is strictly event-driven, even if there are whiffs of sessions and users here and there. The hit-stream vibes are strong with this one, which might make it difficult to adjust after the awkward hierarchical event model of Universal Analytics.\nWith this guide, my purpose has been to show how you can approach implementing event tracking with Google Analytics 4. Remember that GA4 is still in its infancy - there\u0026rsquo;s likely to be lots and lots of feature releases between now and soon. You\u0026rsquo;ll be disappointed by some and elated by others, but one thing\u0026rsquo;s for certain: it\u0026rsquo;s a brave new world for Google\u0026rsquo;s analytics tools. If they don\u0026rsquo;t come with guns blazing out of the gate, they risk losing foothold in a world where viable analytics platforms are a dime a dozen.\nI\u0026rsquo;m excited at the prospects of all the things you can do with GA4\u0026rsquo;s event stream model, but I\u0026rsquo;m also a bit worried about how strong the pull of Universal Analytics will be.\nWhat do you think about GA4\u0026rsquo;s event tracking capabilities? Or, if you wish, feel free to unload your thoughts on GA4 in general.\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/facebook-pixel/",
	"title": "Facebook Pixel - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Facebook Pixel custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo      Description This template implements the Facebook Pixel on the website. You can use it to load the SDK, initiate the pixel(s), and to send custom and standard events to Facebook with any custom properties and user attributes you wish. The template should be equipped to handle all the functionality of the pixel. Please let me know in the comments if there are features that you think are missing!\nInstructions Here are the features of the pixel, together with some detail on how they work.\nIn general, when the tag fires, it goes through all the Pixel IDs added to the tag. For each one, it first checks if the pixel has been initialized for the ID. If it hasn\u0026rsquo;t, it proceeds to initialize it (along with any parameters that can be set with the initialization call). Then, the tag proceeds to do whatever you\u0026rsquo;ve configured it to do. It will send an event with any properties you have configured, and you can also use the tag to disable things like automatic history event tracking.\n NOTE! The pixel DOES NOT automatically send the PageView tracking hit similar to how the Facebook Pixel snippet does. You will need to create a PageView tag to send the page view to Facebook.\n Facebook Pixel ID(s) The first field requires you to add your Facebook Pixel ID. You can add more than one ID by separating each with a comma, e.g. 123456789,234567890.\nIf you add more than one ID, then the tag will initialize (if necessary) and send the hit to all the pixel IDs listed in the field.\nEnhanced Ecommerce dataLayer integration If you check this button, then the tag will look in dataLayer for the most recent object with the ecommerce key. If this object has one of the following properties: detail, add, checkout, or purchase, the tag will automatically derive the following information from the object to be sent with the tag.\n If the dataLayer does not have an ecommerce object, or if the ecommerce object doesn\u0026rsquo;t have one of detail, add, checkout, purchase, or if the ecommerce object doesn\u0026rsquo;t have a products array, the tag will end with a failure.\n Event Name is mapped like this:\n  An ecommerce.detail object sets event name as ViewContent.\n  An ecommerce.add object sets event name as AddToCart.\n  An ecommerce.checkout object sets event name as InitiateCheckout.\n  An ecommerce.purchase object sets event name as Purchase.\n  Object Properties are mapped like this:\n  A contents parameter is created from the products array, and it is populated with the id and quantity of each product in the latter.\n  content_type is set to product.\n  currency is derived from ecommerce.currencyCode, or set to USD by default.\n  num_items is used in InitiateCheckout and Purchase and is aggregated from the total quantity of all products in the products array.\n  value is calculated by multiplying the price and quantity of each product in the products array, and summing everything up.\n  Here\u0026rsquo;s an example of how an Enhanced Ecommerce object gets mapped into Facebook object properties:\n{ ecommerce: { currencyCode: \u0026#39;EUR\u0026#39;, checkout: { actionField: { step: 1 }, products: [{ id: \u0026#39;firstProduct\u0026#39;, name: \u0026#39;firstProductName\u0026#39;, price: \u0026#39;3.15\u0026#39;, quantity: 2, category: \u0026#39;prods\u0026#39; },{ id: \u0026#39;secondProduct\u0026#39;, name: \u0026#39;secondProductName\u0026#39;, price: \u0026#39;7.35\u0026#39;, quantity: 3, category: \u0026#39;prods\u0026#39; }] } } } // BECOMES  { content_type: \u0026#39;product\u0026#39;, currency: \u0026#39;EUR\u0026#39;, num_items: 5, contents: [{\u0026#34;id\u0026#34;:\u0026#34;firstProduct\u0026#34;,\u0026#34;quantity\u0026#34;:2},{\u0026#34;id\u0026#34;:\u0026#34;secondProduct\u0026#34;,\u0026#34;quantity\u0026#34;:3}], value: 28.35 }   NOTE! You can use the Object Properties settings (see below) to override these individual properties, if you are unhappy with how some parts of the automatic integration work.\n Event Name If you haven\u0026rsquo;t selected the Enhanced Ecommerce integration, you can choose an event name from the list of standard events. You can also specify a custom event name, or you can use a Google Tag Manager variable to provide the event name dynamically.\nConsent Granted (GDPR) If this field gets the value false (most commonly through a Google Tag Manager variable), the pixel will download the SDK, but it will not initialize nor send any hits to Facebook.\nEnable Advanced Matching If you check this box, then a new group called Customer Information Data Parameters appears.\nData Processing Options You can add supported data processing options to this field. They are evaluated on a tag-by-tag basis, and are set for all pixel IDs of any given tag.\nCustomer Information Data Parameters You can use this to set pre-defined parameters for Facebook\u0026rsquo;s Advanced Matching feature.\nObject Properties You have three options for adding object properties to the hit.\nIf you checked the Enhanced Ecommerce integration, some properties will be automatically populated.\nYou can also load a properties object using a Google Tag Manager variable that returns an object with key-value pairs that will then be added to the hit.\nFinally, you can add properties manually using the table in the tag. Each row represents one property.\nYou can add pre-defined properties and custom properties. Read this for more information.\n NOTE! If you define properties using multiple sources (e.g. Enhanced Ecommerce integration AND variable AND the table), then conflicts are resolved in the order of table \u0026gt; variable \u0026gt; Enhanced Ecommerce. In other words, if you set content_type in the table, it will override content_type set in a variable or the Enhanced Ecommerce integration.\n More Settings You can check the Disable Automatic Configuration to prevent the pixel from automatically listening to clicks or collecting page metadata.\nYou can check the Disable History Event Tracking to prevent the pixel from automatically tracking pushState and replaceState history events.\nYou can add an Event ID to deduplicate hits sent from the website and via server-side tracking.\nRelease notes    Date Changeset     11 November 2020 Added Event ID\tto pixel fields\tfor server-side\tdeduplication.   23 July 2020 Fix to prevent template save if Advanced Matching list enabled but empty.   6 July 2020 Remove deprecated User Properties. Add Data Processing Options. Fix bug with empty Advanced Matching List.   23 April 2020 Add unit tests. Add Enhanced Ecommerce integration.   16 April 2020 Add option to disable automatic tracking of history events.   1 April 2020 Fix \u0026ldquo;country\u0026rdquo; in Advanced Matching to use correct key.   19 October 2019 Fix object property bug.   17 October 2019 Fix object merging. Update event name selection to allow using a variable.   12 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/audienceproject-userreport/",
	"title": "AudienceProject UserReport - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The AudienceProject UserReport custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description This template loads the UserReport SDK on the site, and it also initializes the global _urq queue for passing section pageviews and other items.\nYou can use the template to track section views by choosing the respective option from the drop-down menu, and then adding the Section ID (either directly or via a Google Tag Manager variable) to the field that appears.\nRelease notes    Date Changeset     5 November 2020 Add additional functionality to Anonymous Measurement.   9 September 2020 Added Anonymous Measurement option.   29 April 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/bulk-actions-google-tag-manager-triggers/",
	"title": "Bulk Actions With Google Tag Manager Triggers",
	"tags": ["google tag manager"],
	"description": "A walkthrough of Google Tag Manager bulk actions for triggers and tags. You can edit triggers and extensions for selected tags in bulk.",
	"content": "The release of bulk actions in the Google Tag Manager user interface was very welcome. For years, GTM users had been struggling with a somewhat crippled workflow of item-by-item management.\nThis release is even more impressive with the most recent update to it: bulk actions with TRIGGERS.\n  You can now select multiple tags and attach one or more triggers (or exceptions) to them. Or, conversely, you can use the feature to remove triggers (or exceptions) from tags.\nThis is very useful when you have 200 tags that require an intricate trigger and exception setup to accommodate a consent management platform, for example.\nBe sure to check out the official documentation behind this link as well!\nHow it works First, enter the Tags section of your Google Tag Manager. Check the box next to all the tags you want to select for updating triggers with and click the funky trigger icon in the table header.\n  An overlay opens. If your screen is wide enough, you\u0026rsquo;ll be able to see the items you selected, but there\u0026rsquo;s also a visual reminder of how many tags\u0026rsquo; triggers you are about to edit.\n  In the overlay, you\u0026rsquo;ll see all the triggers and exceptions that exist in the selected tags. The icons at the end of each row reveal a lot about the state of the bulk action:\n  A dark grey box with a minus symbol means that the trigger or exception was found in one or more tags but not all of them. If you click the checkbox, it will turn into an empty box. If you click it then again, it will turn into the blue checkmark box.\nA blue checkmark box means that the trigger or exception will be applied to all the selected tags. If the box already had a blue checkmark when the overlay first opened, it means that the trigger or exception was already attached to all the selected tags. Clicking the box will turn it into an empty box.\nAn empty box means that the trigger or exception will be removed from all the selected tags. This is the default setting for a trigger or exception that was added via the bulk action overlay. If you click an empty box, it will turn into a blue checkmark box.\nIn other words, when you click the checkboxes next to a trigger or exception, you will toggle between add to all selected tags or remove from all selected tags.\n Selections in the overlay.   Result of saving selections.  When you are ready, click the Save button in the top right corner of the overlay.\nTrigger selection and creation flows In the bulk trigger action overlay, if you click the blue plus symbol in the corner of either the triggers list or the exceptions list, you\u0026rsquo;ll enter the familiar flow of trigger or exception selection.\n  You can even progress deeper down the rabbit hole by clicking a similar blue plus symbol in the trigger selection overlay to enter the trigger creation flow.\n  In other words, you can manipulate the entire trigger and exception setup for the selected tags - all the way from creating the necessary triggers and/or exceptions to adding them to the tags.\nSummary Having the ability to bulk edit triggers and exceptions will be a time-saver for many.\nManaging an ever-growing container, especially one with heaps of technical debt, is easily deprioritized when the admin has to decide between refactoring and adding new stuff in.\nWith bulk actions, refactoring and improving the consistency of a container has become a lot easier to do. Combined with partial exports and a powerful RESTful API, your tools for managing complex multi-container setups are in pretty good shape.\nThere\u0026rsquo;s still a lot that can be added to bulk actions (roll changes back, rename items, edit individual fields\u0026hellip;) - I hope we\u0026rsquo;ll keep on seeing nice, incremental updates like this in the future.\nWhat do you think is still missing from bulk actions?\n"
},
{
	"uri": "https://www.simoahava.com/tags/app+web/",
	"title": "app+web",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/ecommerce/",
	"title": "ecommerce",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-4-ecommerce-guide-google-tag-manager/",
	"title": "Google Analytics 4: Ecommerce Guide For Google Tag Manager",
	"tags": ["google tag manager", "app+web", "google analytics 4", "ecommerce"],
	"description": "Guide to implementing Google Analytics 4 Ecommerce measurement using Google Tag Manager.",
	"content": "This is an implementation guide for Google Analytics 4. The guide is aimed at Google Tag Manager users and has been designed to complement the official documentation.\n  A thing to keep in mind is that Google Analytics 4 (GA4) is still very new. The Google Tag Manager integration is still in its infancy, and implementation places a lot of responsibility on accurate tagging and proper dataLayer instrumentation.\nIn this guide, I\u0026rsquo;ll explain the new data model from an implementation perspective, and walk you through the nuts and bolts on how to get the different parts of GA4\u0026rsquo;s Ecommerce machine to work in unison.\nMake sure you follow the official documentation as well:\n Google Analytics 4 data schema Google Analytics 4 instructions for Google Tag Manager implementation    For help with analysis and what to do once the data hits the reports, make sure to follow Krista Seiden\u0026rsquo;s blog, where she\u0026rsquo;s already added a walkthrough of the Ecommerce reports in Google Analytics 4.\nOther great resources include Charles Farina\u0026rsquo;s blog and Ken William\u0026rsquo;s Google Analytics 4 resource.\nUniversal Analytics vs. Google Analytics 4 It\u0026rsquo;s only fair to start with a comparison between Universal Analytics\u0026rsquo; Enhanced Ecommerce data model and the one put forth by GA4. They\u0026rsquo;re similar in many places, but also have some key differences that will be covered here.\nAvailable actions Let\u0026rsquo;s begin with the types of events you can collect. They represent the discrete funnel steps that the data model uses to reflect the funnels in your website.\nThe GA4 action in the table below represents the Event name you need to configure in your site\u0026rsquo;s code or Google Tag Manager tags when building the respective Ecommerce hit to GA4.\nThe Corresponding Enhanced Ecommerce action column refers to what the analogy in Enhanced Ecommerce would be. It\u0026rsquo;s not just an analogy, though, as there\u0026rsquo;s a migration path available when upgrading from Google Analytics Enhanced Ecommerce to GA4 Ecommerce.\n   GA4 action Corresponding Enhanced Ecommerce action Description     view_promotion promoView View of promotions.   select_promotion promoClick Click on a promotion.   view_item_list impressions View of item impressions in a list.   select_item click Click on an item in a list.   view_item detail View item details.   add_to_cart add Add item(s) to cart.   add_to_wishlist N/A Add item(s) to a wishlist.   remove_from_cart remove Remove item(s) from the cart.   view_cart N/A View the contents of the shopping cart.   begin_checkout checkout* Initiate the checkout process.   add_shipping_info checkout_option* Add shipping info during the checkout flow.   add_payment_info checkout_option* Add payment info during the checkout flow.   purchase purchase Purchase items that were checked out.   refund refund Refund one or more items.    * The checkout funnel is collected a bit differently with Google Analytics 4. There are no steps anymore - rather, GA4 offers you the (presumably) most popular checkout steps of adding shipping and payment info to the purchase.\nThe data model includes some welcome additions that were missing from Universal Analytics, namely add_to_wishlist and view_cart.\nProduct data The data model for products has changed somewhat as well. The concept of a product list has been widened to include a list identifier, and multi-level product categories have been expanded to their individual parameters.\n   GA4 parameter Corresponding Enhanced Ecommerce parameter Description     item_id id ID / SKU of the product.   item_name name Name of the product.   item_list_name list Product list name.   item_list_id N/A Product list identifier.   index position Product position in the list.   item_brand brand Product brand.   item_category category Product category top-level.   item_category2 category Product category 2nd level (or alternative).   item_category3 category Product category 3rd level (or alternative).   item_category4 category Product category 4th level (or alternative).   item_category5 category Product category 5th level (or alternative).   item_variant variant Item variant name or description.   affiliation N/A The store affiliation for this event.   discount N/A Any discount associated with this product.   coupon coupon Coupon associated with this product.   price price Price of this product.   currency N/A Currency of the price that is collected.   quantity quantity Quantity of the item. Must be an integer.    As with Enhanced Ecommerce, either the item_id or item_name is always required when products are dealt with. The other parameters might have varying levels of importance, but those two are the only ones that are required.\nSome parameters such as affiliation and currency are brought from the action-level in Enhanced Ecommerce to product-level in Google Analytics 4. It\u0026rsquo;s an interesting choice, as it might not reflect reality to specify these on a product-by-product basis, but it\u0026rsquo;s never a bad thing to have as many parameters as possible available.\nPromotion data With GA4, it\u0026rsquo;s still possible to collect information about promotions. As with Enhanced Ecommerce, they\u0026rsquo;re a bit of an odd duck when it comes to the data model, as they describe banners and other promotions that might not be actually related to any specific products.\n   GA4 parameter Corresponding Enhanced Ecommerce parameter Description     promotion_id id ID of the promotion.   promotion_name name Name of the promotion.   creative_name creative Name of the creative associated with the promotion.   creative_slot position Name of the slot where the creative was shown.   location_id N/A Where the promotion was located on the page when it was shown.    As with Enhanced Ecommerce, either the promotion_id or promotion_name is always required when promotions are sent.\nAction data The importance of different action parameters varies greatly by whatever action is being measured. Broadly speaking, action data refers to parameters the describe the action itself rather than the products within.\n   GA4 parameter Corresponding Enhanced Ecommerce parameter Description     transaction_id id Unique ID for the transaction. Required for purchase and refund events.   affiliation affiliation The store or affiliation where the purchase occurred.   value revenue The value associated with the event.   currency currencyCode Local currency of the collected price. Required for purchase events.   tax tax How much tax is included in the total revenue of the purchase.   shipping shipping Shipping costs included in the total revenue of the purchase.   items products/impressions/promotions Products associated with the event.   shipping_tier option The shipping tier used with add_shipping_info.   payment_type option The payment method sent with add_payment_info.   coupon coupon Coupon associated with the event.   promotion_id N/A ID of a promotion associated with the event.   promotion_name N/A Name of a promotion associated with the event.   creative_name N/A Name of a promotion creative associated with the event.   creative_slot N/A Name of the creative slot associated with the event.   location_id N/A ID of the creative location on the page.   item_list_name N/A Name of the list associated with the event.   item_list_id N/A ID of the list associated with the event.    Parity issues It looks like there are certain aspects of Enhanced Ecommerce that have not been transported to GA4 yet.\nThese are things that I hope (and expect) to see in the product before long, at which point I will naturally update this article accordingly.\nProduct List Attribution Product list attribution is a staple of Enhanced Ecommerce. It\u0026rsquo;s extremely useful, as it lets you build the dataLayer so that product list information does not need to be persisted from action to action.\nThere\u0026rsquo;s no evidence that a similar attribution setup works with GA4 right now. Granted, attribution is part of a data model that might simply not be exposed in GA4 Ecommerce reports yet, so this is something that is likely to be fixed in the near future. It would be odd to have product lists as a data type without them actually having any functionality.\n  Note that the GA4 data model allows you to send things like item_list_name and promotion_id in any Ecommerce object to GA4. You can add them into an items array to indicate that specific items are in this current action due to an interaction with a list or promotion earlier in the session. Or you can add them as regular parameters to the event tag itself, in case the current Ecommerce action took place in a specific item list or within some promotion context.\nProduct-scoped Custom Dimensions and Metrics Universal Analytics lets us add additional metadata to each product using product-scoped custom dimensions and metrics. This is a vital part of dimension widening that any modern online store desperately needs.\n Custom item parameters, where k0 is the parameter name prefix, and v0 is the value prefix.  In GA4, you can add any parameters you wish into the items array, and they will be transported to GA4, but even if you register these as custom definitions it doesn\u0026rsquo;t seem like they\u0026rsquo;re available in the Ecommerce reports themselves.\nCheckout measurement As mentioned above, checkout tracking has certainly changed between Enhanced Ecommerce and GA4.\nWhere Enhanced Ecommerce introduces a combination of funnel steps (e.g. view cart, shipping, payment) and checkout options (e.g. shipping method, payment method), GA4 only has the begin_checkout event for establishing entrance into the checkout flow.\nThere are no additional steps. Rather, you can use actions such as view_cart, add_shipping_info, and add_payment_info to flesh out the funnel.\nIt would be cool to be able to specify a more complete funnel in whatever way I want, and it would also be amazing if we could have multiple different checkout funnels in the data schema.\nBut on the other hand, I don\u0026rsquo;t think this is a very big omission currently. I wouldn\u0026rsquo;t be too surprised if this isn\u0026rsquo;t expanded to have full parity with Enhanced Ecommerce.\nImplementation with Google Tag Manager When implementing GA4 Ecommerce using Google Tag Manager, the beta-ness of the feature really strikes you hard. The only available event tag type is a generic chassis used for sending any type of event. There\u0026rsquo;s no built-in Ecommerce integration, as you\u0026rsquo;ll need to populate the parameters manually.\n  It would be so much simpler to just have a single tag for Ecommerce events, which automatically collects the action parameters and items from the dataLayer, depending on which event name is included in the dataLayer object.\nI\u0026rsquo;m sure we\u0026rsquo;ll see something like this before the tags are out of beta, though.\nAn additional hiccup is the Custom JavaScript variable (or custom template) option, which we\u0026rsquo;ve been spoiled with when using Enhanced Ecommerce.\nEven though you can use a Custom JavaScript variable (or a custom template) to map the items array however you wish, you\u0026rsquo;re still forced to add all the other parameters as individual fields into the tag.\nI hope we get an option similar to Enhanced Ecommerce, where the entire event payload could be fetched from a variable. That way we could have a really dynamic setup and minimize the number of tags we\u0026rsquo;re using. Not just for Ecommerce but for all GA4 data collection!\nMigrating from Enhanced Ecommerce Before you start rewriting your dataLayer objects, do note that Google Analytics 4 Ecommerce does support the Enhanced Ecommerce data model - at least to some extent.\nThe main path of migration is that you do not have to provide an items object in GA4 syntax. Instead, you can set the value of the items parameter in the tag to an impressions or promotions or products variable in your Enhanced Ecommerce object. GA4 will be able to map the parameter names in the Enhanced Ecommerce products object to their corresponding parameters in the Google Analytics 4 schema.\nIf you read through the comparison chapters above, you\u0026rsquo;ll remember that there are some distinct differences between these data models. For example, there is no item_list_id analogy in Enhanced Ecommerce, and the product category is split into five different levels rather than the slash-separated single parameter in Enhanced Ecommerce.\n Note! You do not have to follow the migration path. Considering all the new things that GA4 data model enables, it might make sense to upgrade from Enhanced Ecommerce to GA4 and not resort to using the \u0026ldquo;old\u0026rdquo; data schema in your setup.\n I\u0026rsquo;ve built a custom variable template that you can also use to map an Enhanced Ecommerce products (or impressions or promotions) object to its GA4 counterpart. It has some additional bells and whistles, such as automatic parsing of a multi-level product category into the five separate item_category parameters, and a table that lets you map Enhanced Ecommerce\u0026rsquo;s product-scoped custom dimensions to custom item parameters in the GA4 items objects.\nEvent names One of the key features of Google Analytics 4 is how it utilizes the event name to carry quite a bit of semantic weight. To see the list of event names associated with Ecommerce collection, see the chapter Available actions above.\n  When building the tags to send the Ecommerce data to GA4, you need to make sure that the Event name field is always set to the correct value. A typo or a mismatch between the event name and the parameter map will lead to data quality issues that are impossible to fix retroactively.\nSetting the fields To create an Ecommerce tag in Google Tag Manager, you need to create a new Google Analytics 4 Event tag.\nThe first thing you need to establish is what its Configuration tag is. For more information about this see this article.\nNext, you need to set the Event name. Whether you use a variable for this field or hard-code it, you need to be mindful of what the reserved event names for GA4 are. As you go through the chapters below, each sub-chapter devoted to an Ecommerce action will include details on what the event name should be for that particular action.\n  Individual parameters are set by adding them to the Event Parameters table. For the most part, you\u0026rsquo;ll only need to add the items parameter, but some actions (such as purchase) require additional fields to be set.\n If you use a Data Layer variable to pull in an items array, for example, do be mindful of recursive merge. You might want to engineer the setup so that you always use Version 1 of the Data Layer variable to avoid values \u0026ldquo;leaking\u0026rdquo; from one object to another.\n Trigger As before, make sure you use a trigger on the tag that fires when the relevant Ecommerce object is pushed into dataLayer.\n  This means that you\u0026rsquo;ll want to engineer the dataLayer so that each Ecommerce event is its own, distinct push, and each object has the event key. This way you can create a *Custom Event trigger for this event value in order to fire the tag at the exact right moment.\nTips for server-side tagging One possible path of migration from Enhanced Ecommerce to GA4 is to handle the logic in your server-side tagging setup.\nObviously, it\u0026rsquo;s not a suitable option for everyone (as there are costs involved), but it would provide a pretty sleek path to having the single Universal Analytics stream sent to the server-side container be fanned out to not just the Universal Analytics endpoint but also to GA4.\nWhen you send a Universal Analytics Enhanced Ecommerce event to a server container, the built-in Universal Analytics client chops it up and outputs an event data object like this:\n  As you can see, the model chops up the Enhanced Ecommerce payload in the Measurement Protocol request and outputs the values into standard event parameters. Notably, product-scoped custom dimensions are not mapped to the items array.\nYou could thus fire a GA4 tag in the server container with the Universal Analytics client, and it will send a well-formed event to GA4. You\u0026rsquo;d just need to edit the event_name first, as page_view would not be the correct name for a purchase. You\u0026rsquo;ll also need to add the correct Measurement ID.\n  The challenge is, again, creating a custom mapping of the items array, as the Server container does not currently let you run JavaScript variables.\nYou could build a custom tag template in server-side tagging, but it might be better to just wait for Google to improve the migration path.\nAction reference The following chapters outline how to instrument the dataLayer and the Google Tag Manager tags to measure Ecommerce actions in GA4.\nIncluded with each action chapter is an example of how a migration setup from Universal Analytics to GA4 might work.\n I want to reiterate that instead of using the Universal Analytics dataLayer, you should either engineer or plan to engineer the dataLayer so it\u0026rsquo;s compatible with the new actions and item parameters that GA4 offers.\n view_promotion A view_promotion event is sent when the user sees one or more promotion items on the page. Promotions are a different data model from the rest of GA4 Ecommerce, as they do not necessarily involve products at all, but rather banners, campaigns, and such.\nData Layer composition A sample dataLayer object for the view_promotion event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;view_promotion\u0026#39;, ecommerce: { items: [{ promotion_id: \u0026#39;sc2021\u0026#39;, promotion_name: \u0026#39;summer_campaign_2021\u0026#39;, creative_name: \u0026#39;family_in_bathing_suits_1\u0026#39;, creative_slot: \u0026#39;featured_items\u0026#39;, location_id: \u0026#39;hero_banner\u0026#39; },{ promotion_id: \u0026#39;wc2020\u0026#39;, promotion_name: \u0026#39;winter_campaign_2020\u0026#39;, creative_name: \u0026#39;family_in_winter_clothes_1\u0026#39;, creative_slot: \u0026#39;featured_items_2\u0026#39;, location_id: \u0026#39;hero_banner\u0026#39; }] } });  Required parameters are either promotion_id or promotion_name.\n The example in the documentation shows also item parameters set in the promotion view object. This is something you might consider adding if the promotion is clearly related to an item or a specific set of items. The current suite of Ecommerce reports doesn\u0026rsquo;t seem to have any options for viewing items in a promotion, but the export to BigQuery would include this information.\n Migration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce promotions array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.promoView.promotions Data Layer Version: Version 2  Then you can add it to your GA4 view_promotion event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce promotions object automatically to the format required by view_promotion.\n Tag configuration The tag for view_promotion would look like this:\n  Make sure you set the Event Name field to view_promotion.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.promoView.promotions if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be view_promotion).\nselect_promotion The select_promotion event is sent when the user clicks or selects one of the promotions they viewed.\nData Layer composition A sample dataLayer object for the select_promotion event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;select_promotion\u0026#39;, ecommerce: { items: [{ promotion_id: \u0026#39;sc2021\u0026#39;, promotion_name: \u0026#39;summer_campaign_2021\u0026#39;, creative_name: \u0026#39;family_in_bathing_suits_1\u0026#39;, creative_slot: \u0026#39;featured_items\u0026#39;, location_id: \u0026#39;hero_banner\u0026#39; }] } });  Required parameters are either promotion_id or promotion_name.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce promotions array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.promoClick.promotions Data Layer Version: Version 2  Then you can add it to your GA4 select_promotion event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce promotions object automatically to the format required by select_promotion.\n Tag configuration The tag for select_promotion would look like this:\n  Make sure you set the Event Name field to select_promotion.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.promoClick.promotions if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be select_promotion).\nview_item_list The view_item_list event is sent when the user sees a list of items in a dedicated product list. A product list could be something like search results, featured items, upsell items, and so forth.\nData Layer composition A sample dataLayer object for the view_item_list event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;view_item_list\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, item_list_name: \u0026#39;Featured items\u0026#39;, item_list_id: \u0026#39;FI1\u0026#39;, index: 1, quantity: \u0026#39;1\u0026#39; },{ item_name: \u0026#39;Swedish regular parka\u0026#39;, item_id: \u0026#39;sp2323\u0026#39;, price: \u0026#39;92.00\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Black\u0026#39;, item_list_name: \u0026#39;Featured items\u0026#39;, item_list_id: \u0026#39;FI1\u0026#39;, index: 2, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id. Note that you should always try to populate the item_list_name field so that you know which list was viewed.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce impressions array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.impressions Data Layer Version: Version 2  Then you can add it to your GA4 view_item_list event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce impressions object automatically to the format required by view_item_list.\n Note! If you\u0026rsquo;re bundling impressions with some other Enhanced Ecommerce action (as you might commonly do), the custom variable template will not really work, as it always looks for the products array first.\nTag configuration The tag for view_item_list would look like this:\n  Make sure you set the Event Name field to view_item_list.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.impressions if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be view_item_list).\nselect_item The select_item event is sent when the user actually clicks or selects an item after viewing the item\u0026rsquo;s impression(s) in a list.\nData Layer composition A sample dataLayer object for the select_item event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;select_item\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, item_list_name: \u0026#39;Featured items\u0026#39;, item_list_id: \u0026#39;FI1\u0026#39;, index: 1, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id. Note that you should always try to populate the item_list_name field so that you know which list was the target of the click.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce products array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.click.products Data Layer Version: Version 2  Then you can add it to your GA4 select_item event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by select_item.\n Tag configuration The tag for select_item would look like this:\n  Make sure you set the Event Name field to select_item.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.click.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be select_item).\nview_item The view_item event is sent when the user views the details of any given product.\nData Layer composition A sample dataLayer object for the view_item event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;view_item\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce products array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.detail.products Data Layer Version: Version 2  Then you can add it to your GA4 view_item event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by view_item.\n Tag configuration The tag for view_item would look like this:\n  Make sure you set the Event Name field to view_item.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.detail.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.   You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be view_item).\nadd_to_cart The add_to_cart event is sent when the user adds a product to the cart, either by adding a new product to the cart or by increasing the quantity of an existing item in the cart.\nData Layer composition A sample dataLayer object for the add_to_cart event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;add_to_cart\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id. Set quantity to reflect the number of items added to the cart in this action.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce products array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.add.products Data Layer Version: Version 2  Then you can add it to your GA4 add_to_cart event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by add_to_cart.\n Tag configuration The tag for add_to_cart would look like this:\n  Make sure you set the Event Name field to add_to_cart.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.add.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be add_to_cart).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n add_to_wishlist The add_to_wishlist event is sent when the user adds a product to a wishlist.\n This does not have an analogy in Enhanced Ecommerce. It\u0026rsquo;s a completely new Ecommerce event type.\n Data Layer composition A sample dataLayer object for the add_to_wishlist event could look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;add_to_wishlist\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nTag configuration The tag for add_to_wishlist would look like this:\n  Make sure you set the Event Name field to add_to_wishlist.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be ecommerce.items if you\u0026rsquo;re following the steps outlined here.\nRemember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be add_to_wishlist).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n remove_from_cart The remove_from_cart event is sent when the user removes something from the cart or decreases the quantity of an item in the cart.\nData Layer composition A sample dataLayer object for the remove_from_cart event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;remove_from_cart\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce products array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.remove.products Data Layer Version: Version 2  Then you can add it to your GA4 remove_from_cart event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by remove_from_cart.\n Tag configuration The tag for remove_from_cart would look like this:\n  Make sure you set the Event Name field to remove_from_cart.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.remove.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be remove_from_cart).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n view_cart The view_cart event is sent when the user views the cart contents.\n This does not have an analogy in Enhanced Ecommerce. It\u0026rsquo;s a completely new Ecommerce event type.\n Data Layer composition A sample dataLayer object for the view_cart event could look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;view_cart\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;3\u0026#39; },{ item_name: \u0026#39;Parka stain removal\u0026#39;, item_id: \u0026#39;psr1332\u0026#39;, price: \u0026#39;5.99\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Utility\u0026#39;, item_category3: \u0026#39;Care product\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nTag configuration The tag for view_cart would look like this:\n  Make sure you set the Event Name field to view_cart.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be ecommerce.items if you\u0026rsquo;re following the steps outlined here.\nRemember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be view_cart).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n begin_checkout The begin_checkout event is sent when the user starts the checkout flow.\nData Layer composition A sample dataLayer object for the begin_checkout event could look like this (adapted from the official documentation):\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;begin_checkout\u0026#39;, ecommerce: { items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;3\u0026#39; },{ item_name: \u0026#39;Parka stain removal\u0026#39;, item_id: \u0026#39;psr1332\u0026#39;, price: \u0026#39;5.99\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Utility\u0026#39;, item_category3: \u0026#39;Care product\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are either item_name or item_id.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce If you want to use the Enhanced Ecommerce products array instead of creating a new items object, you need to create the following variable:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.checkout.products Data Layer Version: Version 2  Then you can add it to your GA4 begin_checkout event tag as the value of the items field (see the next chapter).\n You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by begin_checkout.\n Tag configuration The tag for begin_checkout would look like this:\n  Make sure you set the Event Name field to begin_checkout.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.checkout.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be begin_checkout).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n add_shipping_info The add_shipping_info event is sent when the user has selected a shipping method.\n While Enhanced Ecommerce doesn\u0026rsquo;t have a direct analogy to add_shipping_info, most implementations have shipping method selection encoded in a checkout_option object.\n Data Layer composition A sample dataLayer object for the add_shipping_info event could look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;add_shipping_info\u0026#39;, ecommerce: { shipping_tier: \u0026#39;FedEx\u0026#39;, items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;3\u0026#39; },{ item_name: \u0026#39;Parka stain removal\u0026#39;, item_id: \u0026#39;psr1332\u0026#39;, price: \u0026#39;5.99\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Utility\u0026#39;, item_category3: \u0026#39;Care product\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameter is just shipping_tier.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce You can migrate from Enhanced Ecommerce by grabbing the shipping method from the ecommerce.checkout.option or ecommerce.checkout_option.option Data Layer variables, depending on how you\u0026rsquo;ve setup shipping method selection.\nYou\u0026rsquo;ll need to make sure that the trigger that fires this GA4 tag corresponds with the correct option item pushed into dataLayer, so that you don\u0026rsquo;t inadvertently fire this tag when the payment method was selected, for example.\nIf you want to send the items array as well (recommended), you can use a Data Layer variable like this:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.checkout.products Data Layer Version: Version 2  Tag configuration The tag for add_shipping_info would look like this:\n  Make sure you set the Event Name field to add_shipping_info.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.checkout.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Make sure you point the shipping_tier parameter to a Data Layer variable that contains the shipping_tier value. This would be:\n ecommerce.shipping_tier if you\u0026rsquo;re following the steps outlined here. ecommerce.checkout.option or ecommerce.checkout_option.option if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be add_shipping_info). If you\u0026rsquo;re migrating from Enhanced Ecommerce, make sure the trigger fires when the shipping method has been selected, and not for any other checkout option you might have.\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n add_payment_info The add_payment_info event is sent when the user has selected a payment method.\n While Enhanced Ecommerce doesn\u0026rsquo;t have a direct analogy to add_payment_info, most implementations have payment method selection encoded in a checkout_option object.\n Data Layer composition A sample dataLayer object for the add_payment_info event could look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;add_payment_info\u0026#39;, ecommerce: { payment_type: \u0026#39;COD\u0026#39;, items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;3\u0026#39; },{ item_name: \u0026#39;Parka stain removal\u0026#39;, item_id: \u0026#39;psr1332\u0026#39;, price: \u0026#39;5.99\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Utility\u0026#39;, item_category3: \u0026#39;Care product\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameter is just payment_type.\nYou can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce You can migrate from Enhanced Ecommerce by grabbing the payment method from the ecommerce.checkout.option or ecommerce.checkout_option.option Data Layer variables, depending on how you\u0026rsquo;ve setup payment method selection.\nYou\u0026rsquo;ll need to make sure that the trigger that fires this GA4 tag corresponds with the correct option item pushed into dataLayer, so that you don\u0026rsquo;t inadvertently fire this tag when the shipping method was selected, for example.\nIf you want to send the items array as well (recommended), you can use a Data Layer variable like this:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.checkout.products Data Layer Version: Version 2  Tag configuration The tag for add_payment_info would look like this:\n  Make sure you set the Event Name field to add_payment_info.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.checkout.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Make sure you point the payment_type parameter to a Data Layer variable that contains the payment_tpye value. This would be:\n ecommerce.payment_type if you\u0026rsquo;re following the steps outlined here. ecommerce.checkout.option or ecommerce.checkout_option.option if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Remember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be add_payment_info). If you\u0026rsquo;re migrating from Enhanced Ecommerce, make sure the trigger fires when the payment method has been selected, and not for any other checkout option you might have.\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n purchase The purchase event is sent when the user makes a purchase on the site.\n Make sure you configure your website so that the purchase data is only collected once. The best way to do this is to invalidate the thank you page once the user has visited it so that they can\u0026rsquo;t revisit it by reloading the page, using the browser history, or hitting the back button after moving to some other page. Duplicate transactions are things you need to avoid at all costs.\n Data Layer composition A sample dataLayer object for the purchase event could look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;purchase\u0026#39;, ecommerce: { currency: \u0026#39;EUR\u0026#39;, value: 109.29, tax: 7.18, shipping: 10.00, affiliation: \u0026#39;My Parka Store\u0026#39;, transaction_id: \u0026#39;p115-20202000\u0026#39;, coupon: \u0026#39;free_back_rub\u0026#39;, items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;3\u0026#39; },{ item_name: \u0026#39;Parka stain removal\u0026#39;, item_id: \u0026#39;psr1332\u0026#39;, price: \u0026#39;5.99\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Utility\u0026#39;, item_category3: \u0026#39;Care product\u0026#39;, quantity: \u0026#39;1\u0026#39;, item_coupon: \u0026#39;cheap_stain_removal\u0026#39; }] } });  Required parameters are transaction_id and currency.\n Make sure quantity is passed as an integer. A floating point value will cause item revenue to be incorrectly reported in the Monetization reports!\n You can also add promotion and item list parameters (e.g. promotion_name or item_list_name) to the items in the items array if you want to add credit to a promotion or an item list for the current action.\nMigration from Enhanced Ecommerce You can grab all the required parameters from the Enhanced Ecommerce purchase event. You need to create Data Layer Variables (Version 2) for each. Here are the Data Layer Variable Names you need to configure in the variables:\n   GA4 parameter Data Layer Variable Name in EEC     currency ecommerce.currencyCode   value ecommerce.purchase.actionField.revenue   tax ecommerce.purchase.actionField.tax   shipping ecommerce.purchase.actionField.shipping   affiliation ecommerce.purchase.actionField.affiliation   transaction_id ecommerce.purchase.actionField.id   coupon ecommerce.purchase.actionField.coupon   items ecommerce.purchase.products     You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by purchase.\n Tag configuration The tag for purchase would look like this:\n  Make sure you set the Event Name field to purchase.\nMake sure you point the items parameter to a Data Layer variable that contains the items array. This would be:\n ecommerce.items if you\u0026rsquo;re following the steps outlined here. ecommerce.purchase.products if you\u0026rsquo;ve chosen to migrate from Enhanced Ecommerce.  Make sure you point all the other parameters to their counterparts in the ecommerce array. If you\u0026rsquo;re migrating from Enhanced Ecommerce you\u0026rsquo;d need to set them accordingly.\nRemember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be purchase).\n You can add additional parameters (in addition to items) such as promotion_name or item_list_name if you want to associate this event with a promotion or an item list, respectively.\n refund There are two types of refund events you can send: full refund (the entire transaction is refunded) and partial refund (only some items were refunded).\nData Layer composition For a full refund, this is what a sample dataLayer object would look like:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;refund\u0026#39;, ecommerce: { transaction_id: \u0026#39;p115-20202000\u0026#39; } });  Required parameter is transaction_id.\nFor a partial refund, this is what a sample dataLayer object would look like:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;refund\u0026#39;, ecommerce: { transaction_id: \u0026#39;p115-20202000\u0026#39;, items: [{ item_name: \u0026#39;Finnish magical parka\u0026#39;, item_id: \u0026#39;mp1122\u0026#39;, price: \u0026#39;31.10\u0026#39;, item_brand: \u0026#39;PARKA4LIFE\u0026#39;, item_category: \u0026#39;Apparel\u0026#39;, item_category2: \u0026#39;Coats\u0026#39;, item_category3: \u0026#39;Parkas\u0026#39;, item_category4: \u0026#39;Unisex\u0026#39;, item_variant: \u0026#39;Navy blue\u0026#39;, quantity: \u0026#39;1\u0026#39; }] } });  Required parameters are transaction_id, and for each item that is refunded, item_id and quantity refunded.\nMigration from Enhanced Ecommerce To migrate from an Enhanced Ecommerce setup, you\u0026rsquo;d need to create a Data Layer variable for the transaction ID and another one for the products that were refunded.\nThe Data Layer variable for transaction ID looks like this:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.refund.actionField.id Data Layer Version: Version 2  The Data Layer variable for the refunded products looks like this:\n Type: Data Layer Variable Data Layer Variable name: ecommerce.refund.products Data Layer Version: Version 2   You can also use my custom variable template to convert an Enhanced Ecommerce products object automatically to the format required by refund.\n Tag configuration The tag for refund would look like this:\n  Make sure you set the Event Name field to refund.\nIf you\u0026rsquo;re sending a full refund, you only need to configure the transaction_id field. It should point to either ecommerce.transaction_id or, if you\u0026rsquo;re migrating from Enhanced Ecommerce, to ecommerce.refund.actionField.id.\nIf you\u0026rsquo;re sending a partial refund, you\u0026rsquo;ll also need to configure the items field to point to either the ecommerce.items variable or the ecommerce.refund.products variable, with the latter used in case you\u0026rsquo;re migrating from Enhanced Ecommerce.\nRemember also to set the trigger to a Custom Event trigger, set to the value of the event key in the object that was pushed to dataLayer (in the example above, it would be refund).\nSummary We all knew that Google Analytics 4 properties would include Ecommerce capabilities. I think we were all surprised by how beta they still were by the time GA4 was released out of beta.\nIt doesn\u0026rsquo;t really help that the Google Tag Manager tags are still lagging behind in development. There\u0026rsquo;s a lot of manual work involved in building an Ecommerce setup, and while we\u0026rsquo;ve been treated with a migration option from Universal Analytics\u0026rsquo; Enhanced Ecommerce, it doesn\u0026rsquo;t necessarily make sense to use.\nThe new, gtag.js-based format is where all the cool new stuff will be introduced. If you choose complacency over the effort of properly moving to the new setup, you might end up with a suboptimal Ecommerce setup for future iterations of GA4\u0026rsquo;s Ecommerce reports.\nHaving said that, it makes sense to implement GA4 Ecommerce as soon as possible. You don\u0026rsquo;t have to abandon your Universal Analytics setup, but do proceed with either a migration or a completely new dataLayer setup for GA4 as well.\nYou\u0026rsquo;ll want to start collecting that GA4 data as soon as possible to prepare for the eventual shift from Universal Analytics to Google Analytics 4. At that point, the more historical data you have, the better you\u0026rsquo;ll know how the new paradigm works for your organization.\nI know absolute use case parity with Enhanced Ecommerce isn\u0026rsquo;t something GA4 Ecommerce should even pursue, but we\u0026rsquo;re still missing things like item-scoped custom dimensions and easily managed item list and promotion attribution from the mix.\nThe Google Tag Manager tags are also missing a few key pieces, such as an option to pull in all Ecommerce data directly from the dataLayer or even from a Custom JavaScript variable (or custom template).\nUntil we have these things, it\u0026rsquo;s best to treat GA4 Ecommerce as being in rough beta, even if that label has officially been dropped.\nWhat are you most looking forward to about the GA4\u0026rsquo;s Ecommerce reports? What do you expect the Google Tag Manager integration to look like?\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/consent-mode/",
	"title": "Consent Mode (Google tags) - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Consent Mode (Google tags) custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo      Description This tag lets you deploy the Consent Mode functionality offered by Google for controlling the storage settings for their tags and pixels.\nInstructions Please follow the official vendor documentation for information on how to configure the tag.\nThe tag should fire before any other tag in the container, so that the default settings will take effect. This means that you should also deploy a tag built with this template with the default command.\nThe URL Passthrough option isn\u0026rsquo;t really relevant for Google Tag Manager, as you are instructed to use Conversion Linker instead. However, it\u0026rsquo;s in the template in case you have other implementations using gtag.js.\nRelease notes    Date Changeset     17 October 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/new-preview-mode-google-tag-manager/",
	"title": "Tag Assistant Preview Mode In Google Tag Manager",
	"tags": ["google tag manager", "preview mode"],
	"description": "A guide to the new Tag Assistant preview experience in Google Tag Manager. Not only is it detached from the page being previewed, it also works in first-party context, meaning it&#39;s not crippled in browsers that do not support third-party cookies.",
	"content": " Last updated 21 October 2020: Clarified some technical details about the new Preview process.\n Your favorite tagging platform, Google Tag Manager, now comes equipped with a completely revamped preview mode experience.\nGone are the days of having to minimize the debug pane to prevent it from hogging up screen real estate on the website.\nGone are the days of having to use browser extensions to see what happened in GTM on previous page loads.\nGone are the days of trying to squint really hard at the illegible debug pane while in mobile emulation mode.\n  The new preview mode is now integrated into the Google Tag Assistant ecosystem. If this makes you nervous, I don\u0026rsquo;t blame you. Over the years, Google Tag Assistant has been lagging behind the development of Google\u0026rsquo;s tagging tools.\nHowever, with the release of Google Tag Assistant recordings a few years back, and now with the reimagined preview experience, Google Tag Assistant has suddenly jumped to the foreground when discussing which tools to most rely on for debugging the tags on a given website.\nIn this article, I\u0026rsquo;ll walk you through the ins and outs of this new Tag Assistant. It\u0026rsquo;s not quite perfect yet - there are still some rough edges. But it\u0026rsquo;s obvious that Google is pushing a lot of resources to improve the debugging experience, as data quality issues that could be nipped at the bud at collection time are still prevalent across tagging setups.\nVideo Check out the video below for a walkthrough of the new features.\n  If you want to watch the video in YouTube, please follow this link.\nComparison: old vs. new There are obviously some pretty big differences between how preview mode used to be done vs. how it\u0026rsquo;s done with Tag Assistant.\n   Preview mode Tag Assistant     Embedded on the page in an \u0026lt;iframe\u0026gt;. Contained in its own window/tab.   Practically impossible to debug mobile browsers. Mobile debugging is trivial thanks to the detached preview interface.   State is managed with a third-party cookie. State is managed with first-party storage (cookie and localStorage).   Shows dataLayer interactions for the current page only. Shows all API interactions (gtag.js, dataLayer) for all the pages navigated to within the previewed site.   Designed specifically to be used with web containers. Now under the Tag Assistant suite - unifies design with server-side tagging.   Requires authentication (e.g. through a shared preview link). Debugging Google Tag Manager does require authentication, but gtag.js API calls and hits can be debugged without.    If you also have the Google Tag Assistant browser extension installed and active, some additional features are enabled for debugging.\nThe following chapters detail the key features of the new Tag Assistant in Google Tag Manager.\nKey features Tag Assistant preview interface Instead of a preview pane as before, we now have Tag Assistant\u0026rsquo;s preview interface.\n  This bears a striking resemblance to the preview mode in server-side tagging, and no, this isn\u0026rsquo;t a coincidence.\nThe Tag Assistant browser tab is connected to a single debug window (the page / site being previewed). If you have the Tag Assistant extension, you can debug multiple windows in the same Tag Assistant preview interface.\nTag Assistant collects information across pages. So even if you reload the debug window or if you navigate from one page to another, Tag Assistant will show all interactions collected from all these pages. This makes parsing dataLayer history much easier than before.\n  If your site is running both gtag (e.g. via Google Analytics 4) and dataLayer APIs, you can select the different containers from the drop-down menu in the header.\n  Depending on the APIs the library uses, you\u0026rsquo;ll might see different configurations in the preview interface itself. You can jump to the chapter on gtag.js integration to see how that approach changes what you see in the preview interface.\nApart from these changes, the actual capabilities of Tag Assistant are very similar to what we had before. You can see the API interactions (gtag() or dataLayer.push()) in the left-hand side navigation.\nIf you select an API call, you can see the contents of that call as well as the Tags (or Hits), Variables, Data Layer contents, and Errors caused by that message.\n  And similarly, if you open a tag, you can see the settings configured for the tag, as well as details about its triggers and exceptions:\n  The user experience has changed a little and will likely require some getting used to.\nStart preview To start previewing a container, you\u0026rsquo;ll do exactly as you\u0026rsquo;ve done before. You can click the Preview button in the Google Tag Manager user interface to preview the current workspace draft.\n Preview the workspace draft.  You can also go to the Versions tab, choose a version, click the overflow menu, and then Preview to start previewing a version instead.\n Preview a version.  Once you click the preview link, a new browser tab will open with the new Tag Assistant interface. It will also automatically pop up an overlay, where you need to input a URL that Tag Assistant will next open the channel for.\n  Unless your site is allergic to arbitrary URL parameters, keep the Include debug signal in the URL option checked. This makes establishing the channel far more reliable on most modern browsers.\nOnce you click Start, Tag Assistant will first look for a tab or window that you might have previously opened from Tag Assistant and re-establish the preview channel with it. If no such tab or window is found, a new debug window is opened with the URL you configured.\nIn the debug window, you should see a new preview badge appear in the lower corner of the page.\n  The preview badge will look for a channel with Tag Assistant, and if one is found, all Google Tag Manager and gtag.js interactions that happen in the debug window will be relayed to Tag Assistant.\nIf no channel is found, the preview badge will show an error (No debugger found). You can initialize a new Tag Assistant preview process by clicking the Open debugger button in the badge.\nShare preview Sharing Tag Assistant access is a bit of a work-in-progress for now.\n Note! This flow will be more polished in the near future. I\u0026rsquo;ll update the article accordingly when the sharing functionality is improved. In the meantime, the methods shown below will do the trick.\n To share the current workspace, you can copy the Tag Assistant URL that is opened when you click the Preview button in Google Tag Manager. Pass this URL to another user and they, too, can enable Tag Assistant\u0026rsquo;s preview mode in their own browser.\n  To additionally add a URL to this link, you can append \u0026amp;url=https://page/to/debug to this shared link.\nIf you want to share preview for a version, you\u0026rsquo;d do exactly as you\u0026rsquo;ve done before. Go to the Versions tab in the Google Tag Manager user interface, choose the version whose preview you want to share, and then click the Share Preview link in the overflow menu.\n  Share this URL with someone else and they\u0026rsquo;ll be able to establish Tag Assistant\u0026rsquo;s preview mode with the version you shared.\nQuit preview If the preview connection is alive, meaning the debug window is sending data to Tag Assistant, you can quit preview mode in one of two ways:\n Click the X in the top-left corner of a Tag Assistant tab. Close the preview badge while browsing the site in a debug window.    When you click to stop debugging via Tag Assistant, you can choose whether to keep the domain enabled for debugging or not. If you choose to not keep the domain enabled, Tag Assistant will signal the site to delete the first-party __TAG_ASSISTANT cookie and sever the connection until a new one is established via Tag Assistant.\nIf you do choose to keep the connection alive, it just means that the first-party cookie is not deleted. As long as you don\u0026rsquo;t close the debug window, you can always re-establish preview mode through Tag Assistant.\n  If you try to exit preview mode via Tag Assistant and there is no active connection with a debug window, then the first-party cookie is not deleted. In this scenario, you need to visit the site and close the preview badge to clear the first-party cookie.\nGoogle Tag Assistant extension There\u0026rsquo;s an issue with some browsers (e.g. Chrome) that requires Tag Assistant to open the debug window in a new browser window rather than a browser tab.\nSpecifically, if the user types something into the URL bar of the debug window, the window.postMessage channel is severed, and debugging will no longer work even if the user returns to the site being debugged.\nFor this reason, Tag Assistant opens the debug site in a browser window, as the pop-up allows it to make the URL bar inaccessible. The other benefit of having a dedicated browser window is that you have a dedicated browser window just for debugging a site. This makes it less likely that you\u0026rsquo;ll unintentionally sever the connection.\nWhile this makes debugging simple, it might not be the preferred flow for some users. For instance, it\u0026rsquo;s not possible to do (Google Chrome) mobile emulation in a browser window like this, and utilizing developer tools can be a bit annoying as they are opened in yet another window by default.\nLuckily, if you use the Google Tag Assistant browser extension, this limitation is lifted.\n Using the Tag Assistant extension is not required but is recommended.\n If the browser has the extension, Tag Assistant will detect it and automatically use it to enhance the preview experience. The following features are enabled as a result.\n You can debug multiple windows in a single Tag Assistant instance. You can debug content in iframes. You can open debug windows in tabs instead of pop-up windows.  The Tag Assistant extension integration works automatically. As long as you have Tag Assistant enabled in your browser, the preview interface will make note of this and apply the enhancements automatically.\nThe most significant enhancement is, of course, that the debug window is opened in a browser tab rather than a window itself.\n  Running in a browser tab also enables debugging via mobile emulation.\n  None of these enhancements really degrades the experience at all (I wouldn\u0026rsquo;t call them enhancements if they did), so using the Tag Assistant extension when working with the new preview experience is practically mandatory, in my opinion.\ngtag.js integration Tag Assistant can automatically debug all the hits collected with the Global Site Tag.\nIn this case, the Preview interface simply shows the gtag() API calls and all the hits dispatched by gtag.js. Because of this, you can use the Tag Assistant debugger for gtag.js hits without having to authenticate or anything.\nFor example, a simple way to initiate a new Tag Assistant preview channel for a site running gtag.js is to load a page on the site with gtm_debug=x in the URL. This pops up the preview badge, where you can then click Open debugger to start a new preview instance.\n  After adding the site to the list of domains to debug, a new browser tab is opened with the debug window, and you can proceed to use Tag Assistant to debug all the gtag() API calls and hits that happen in the debug window.\n  API calls made with gtag() appear in the list of messages, and you can see what was actually called by expanding the API call itself.\n  The Hits sent will display all the calls to Google servers that were initiated with these API calls. You can see the list of parameters and values (in human-readable format) by clicking one the sent hits.\n  This is very interesting, as you can now use Tag Assistant\u0026rsquo;s preview interface for debugging sites for where you don\u0026rsquo;t actually have access to Google\u0026rsquo;s tools. In this way, the Tag Assistant interface is acting more like your typical developer tools, except it can show you more details about the hits in human-readable format than what you\u0026rsquo;d get by just looking at the network requests.\nIt\u0026rsquo;s a nice enhancement to have, and being able to quickly switch between debugging a Google Tag Manager container vs. the gtag.js implementation is very useful, indeed.\nTechnical details For Tag Assistant to be able to preview a Google Tag Manager container, a version has to be published first. It\u0026rsquo;s no longer to possible to preview a container that doesn\u0026rsquo;t have a published version.\nWhen the site loads a container that is in preview mode, it first loads the container itself (this is why it requires a published version), after which the browser loads a special debug version of the container:\n  To determine if a window should be in debug mode, Tag Assistant uses one or more of three signals to establish whether or not a debug window should be enabled for previewing:\n The gtm_debug=x parameter is in the page URL (controlled by the Include debug signal in the URL checkbox). The referrer is https://tagassistant.google.com. A first-party cookie (__TAG_ASSISTANT) is set on the domain being previewed.  (1) and (2) will cause the __TAG_ASSISTANT first-party cookie to be written on the site. All three will result in the preview badge appearing in the corner of the page.\n Remember to add tagassistant.google.com to your Google Analytics referral exclusion list to avoid preview mode starting a new session.\n When the badge is displayed, the page will look for an open channel to a Tag Assistant preview interface. As long as the site has the first-party cookie, the badge will display and actively search for and interact with a Tag Assistant interface.\nMoving to first-party storage is a huge benefit in today\u0026rsquo;s third-party-cookie-blocking browser world.\nWith GTM\u0026rsquo;s old preview mode, state was maintained with a third-party cookie, which meant that the user had to implement awkward and time-consuming hacks to make preview mode work properly on Safari and even on Google Chrome\u0026rsquo;s Incognito mode.\n Third-party cookies in the old Preview mode.  With the new Tag Assistant, state is no longer managed with third-party storage. Instead, Tag Assistant now utilizes two components to make the transmission of information from the debug window to Tag Assistant work:\n A first-party cookie named __TAG_ASSISTANT tells the Google Tag Manager and gtag.js libraries to actively look for an open channel to Tag Assistant. If a channel is found or established, information is transmitted with window.postMessage.   Tag Assistant itself maintains the list of domains available for debugging with window.localStorage.\n The window.postMessage channel is created when the person doing the previewing clicks to connect a window to Tag Assistant:\n  In other words - for Tag Assistant to be able to listen for events from a debug window, that debug window needs to have been opened via the Tag Assistant interface.\n  If you try to open a browser tab for a page that has the __TAG_ASSISTANT cookie but has not been opened via Tag Assistant, you\u0026rsquo;ll see the page search for an open preview mode channel, only to report that no such channel was found.\n  If Tag Assistant is connected to a window or tab, you\u0026rsquo;ll see the tab icon reflect this. The Tag Assistant tab will show a pulsating tag icon pointing to the right. If you\u0026rsquo;ve also got a debug window running in a tab, this tab will show a tag icon pointing to the left, together with a color indicator that is used to identify the hits from this debug window in Tag Assistant.\n  It\u0026rsquo;s a lot to digest, but the important part to understand is that everything works with first-party storage now. This makes previewing far more reliable than before.\nThe other thing to remember is that a window can only be debugged if it was initially opened via Tag Assistant. So if you find Tag Assistant not working or the preview badge in the debug window not finding a connection, you\u0026rsquo;ll need to go back to Tag Assistant and connect a new debug window.\nThings to look forward to This is a fairly large feature update to Google Tag Manager, and it will take some time for the dust to settle.\nAs everything runs in Tag Assistant now, there are some interesting possibilities ahead.\n  It would be great to see end-to-end debugging with Tag Assistant, where the Google Analytics integration could be included in the debug process. That way you could see the full funnel of data from a dataLayer.push (or gtag() call) all the way to the data being processed in Universal Analytics or Google Analytics 4.\n  Similar to server-side tagging, being able to see the outgoing request summaries in Tag Assistant would be nice. Now we need to use developer tools or browser extensions to achieve the same.\n  This might sound blasphemous, but it wouldn\u0026rsquo;t hurt to still have access to the old debug pane. Don\u0026rsquo;t get me wrong - having the debugger in its own window is an excellent evolution, but there might be times where I want to have everything tucked neatly on the page I\u0026rsquo;m debugging, rather than having to shuffle between two different windows (multi-screen office setups win here big time).\n  There are lots of possibilities here. I suspect most of the feature releases in the near future will revolve around making Tag Assistant a more unified experience across all Google products.\nSummary The renewed preview experience in Google Tag Manager is a lot to digest. The docked preview mode has been a staple of Google Tag Manager, for good and bad, so this is in many ways a very turbulent evolution.\nJust imagine all the screenshots that bloggers and course instructors need to redo because of this!\nIn my book, this is a very welcome change. Being able to separate the debugger from the \u0026hellip; debuggee \u0026hellip; has to be one of the most requested features for Google Tag Manager.\nThe integration with the Tag Assistant extension is also an excellent idea, although I\u0026rsquo;m a bit worried a lot of the best features will be hidden behind this dependency. Even though it\u0026rsquo;s optional, the ability to debug multiple windows, to open the debugger in a tab rather than a window, and to easily use the debugger in mobile emulation mode render the extension practically mandatory.\nI\u0026rsquo;m particularly enamored with the move away from third-party cookies to first-party storage. Not only does this make debugging more robust across modern browsers, it also makes it more resilient against browser extensions that specifically target third-party cookies.\nI can\u0026rsquo;t wait to see what the next iterations of the revamped Tag Assistant preview experience introduce in their wake!\nWhat do you think about this change? What would you like to see in future versions of Tag Assistant?\n"
},
{
	"uri": "https://www.simoahava.com/tags/consent-mode/",
	"title": "consent mode",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/consent-mode-google-tags/",
	"title": "Consent Mode For Google Tags",
	"tags": ["google tag manager", "google analytics", "privacy", "consent mode"],
	"description": "Overview of the new Consent Mode for managing storage access and data processing rules for Google&#39;s tags.",
	"content": "Not too long ago, Google announced a new consent mode for Google tags. It allows you to build a mechanism where Google\u0026rsquo;s tags parse, react, and respond to the consent status of your site visitors.\n Consent Mode with a custom Google Tag Manager template  In short, consent mode is a beta feature, which lets you determine whether or not Google\u0026rsquo;s advertising tags (Ads and Floodlight) and analytics tags (Universal Analytics, App + Web) can utilize browser storage when sending pings to Google\u0026rsquo;s servers.\nThis is (somewhat) in line with, for example, the General Data Protection Regulation (GDPR) and the ePrivacy Directive (ePD) in the European Union, which restrict the storage and processing of personal data (GDPR) and the storage of other data (ePD) with regard to data subjects.\nMuch of this guide revolves around how to set this up in Google Tag Manager, but the overview of the feature applies to implementation using the Global Site Tag (gtag.js) as well.\n Disclaimer: This article is a technical overview of how consent mode works. You need to align the implementation with whatever laws and regulations apply to your organization and the users that visit your site(s).\n Make sure you read the official support documentation as well as the developer guide to get up to speed.\nHow consent mode works First of all, consent mode is not a consent management platform. For consent mode to be effective, you need to already have a system or solution in place for collecting and resolving the user\u0026rsquo;s consent with regard to the data being collected on your site.\nConsent mode has two variants. There are the default settings the page will use while waiting for the user\u0026rsquo;s consent status to be resolved. Then there are the updated settings that are sent as soon as consent has been resolved.\nDefault settings should be established as early as possible in the page load. Ideally, you\u0026rsquo;d add a new gtag snippet to the top of the page template, and establish the default consent settings in that snippet.\n Note! You do not have to be using gtag.js to deploy consent mode. The API just happens to be the global gtag() method, but it works fine with Google Tag Manager\u0026rsquo;s tags.\n When Google\u0026rsquo;s tags fire, they will check the consent status from the default settings and act accordingly.\nOnce you\u0026rsquo;ve established the user\u0026rsquo;s consent, you can call the update command. This lets you change (or keep) the status of advertising storage and of analytics storage.\nThe gtag() call As mentioned above, consent mode utilizes the gtag() API. The Google tags firing on the site will automatically read the consent status from this API when determining whether to read or write cookies and how to compile the network request to Google\u0026rsquo;s servers.\nHere\u0026rsquo;s what the default snippet would look like. Remember, ideally this would be positioned at the top of the page. You can also fire it through Google Tag Manager using a Custom HTML tag or a custom template, but in that case you must make sure that it fires before any Google tags fire.\nwindow.dataLayer = window.dataLayer || []; function gtag() { window.dataLayer.push(arguments); } gtag(\u0026#39;consent\u0026#39;, \u0026#39;default\u0026#39;, { ad_storage: \u0026#39;denied\u0026#39;, analytics_storage: \u0026#39;denied\u0026#39;, wait_for_update: 500, region: [\u0026#39;US-CA\u0026#39;, \u0026#39;FI\u0026#39;] });  The gtag() command above sets default consent settings for visitors from California (US-CA) and Finland (FI) to deny access to advertising and analytics storage. For these users, only cookieless pings will be sent to Google servers. There\u0026rsquo;s also a wait time of 500 milliseconds before any Google tags are allowed to fire, to give time for the update command to be invoked.\n The region parameter is only relevant in the default command, as by the time you run update you should know whether to enable or disable storage for the current user regardless of where they are from.\n Once you\u0026rsquo;ve established the user\u0026rsquo;s consent choices, you can execute the update command. In the following example, the user has given storage access for analytics tags but has denied access to storage for advertising scripts.\ngtag(\u0026#39;consent\u0026#39;, \u0026#39;update\u0026#39;, { ad_storage: \u0026#39;denied\u0026#39;, analytics_storage: \u0026#39;granted\u0026#39; });  You don\u0026rsquo;t have to provide ad_storage again, as it was already denied in the default settings. However, I recommend to explicitly include them in the update command as well to make sure mistakes don\u0026rsquo;t lead to storage being used when the user hasn\u0026rsquo;t opted in to such a thing.\nRestrict advertising storage To restrict advertising storage, you need to call the default or update command and set the value of ad_storage to 'denied' in the consent object.\nWhen advertising storage is restricted, here\u0026rsquo;s what happens:\n No new advertising cookies are written. This means that if the URL has the gclid from a Google search ad click, for example, it will not be written in a first-party cookie. No existing advertising cookies may be read. If there already was a cookie with a click identifier, this would not be used by or sent with the tags. Third-party cookies are solely used for spam and fraud detection. If there already are cookies written on, for example, doubleclick.net, they would be included in the request (assuming the browser doesn\u0026rsquo;t block third-party cookies), but Google says it will only use them for spam and fraud detection. Google Analytics will not read or write advertising cookies, nor will it use them for Google Signals. IP addresses are used solely for geolocation. They are not collected by Google Ads or Floodlight tags. Google Analytics does collect them, but you can opt in to IP anonymization. The full page URL is collected with possible click identifiers. If the current page URL has click identifiers, they are sent to the advertising and analytics platforms with the current page URL.  Redact advertising data You can increase the fidelity of advertising storage restrictions by additionally calling the following gtag() API:\ngtag(\u0026#39;set\u0026#39;, \u0026#39;ads_data_redaction\u0026#39;, true);     This only works when you set ad_storage to denied.\n When you do this, the following additional restrictions will apply:\n Advertising hits will be routed through a new cookieless domain. Instead of routing them to doubleclick.net or google.com, the requests are sent to googleadsyndication.com, which would not have any (advertising-related) cookies set on it by Google. Ad click identifiers will be redacted from advertising requests and consent pings. Page URLs with click identifiers will be redacted. Note that Google Analytics will still collect the full page URL with click identifiers in place.    Restrict analytics storage If you choose to restrict analytics storage by setting the value of analytics_storage to denied in the consent object, Google Analytics will not be able to read or write first-party cookies.\nThis means that all hits that fire before the user reloads or navigates to another page will have the same client identifier (because it\u0026rsquo;s stored in a global variable). However, as soon as the user navigates away from the current page or reloads the page, a new client identifier will be sent with the hits on this new page load.\nIn other words, Google Analytics will not read or write the _ga cookie (even if one already exists). Instead, it will use an ephemeral identifier that exists solely for the duration of the current page load.\nImportantly, restricting analytics STORAGE will not prevent hits to Google Analytics from being sent. The consent mode setting solely prevents Google Analytics from storing data in or reading data from browser cookies (or other storage).\nHowever, these cookieless hits are sent to Google Analytics with a new gcs parameter, which includes the consent status of the hit. The cookieless hits are thus identified by Google Analytics and for now are not collected or exposed in reporting at all.\nMost likely Google will at some point build actionable data out of the cookieless data set as well, perhaps after applying extensive modelling to make it align with the data that was collected with storage consent.\nWhat happens when consent is updated If you send the update command, you will be able to either deny or grant advertising and analytics storage access to all Google tags regardless of region.\n  If you deny storage access, then any tags that fire after the update call will be restricted as described in the previous chapters.\nIf you grant storage access, then tags will have full access to storage. If you grant access to advertising storage, then any ads data redactions you might have established will no longer apply.\nIn addition to this - as soon as you run the update command and grant access to advertising storage, all advertising pings that might have already fired without storage access will fire again. Thus cookies will be written and read, and the requests will be routed through domains that can carry third-party cookies as well.\nURL passthrough If you\u0026rsquo;re using gtag.js, you can toggle URL passthrough on. It checks the current page URL for advertising parameters (gclid, dclid, gclsrc, _gl), and adds them to all internal link URLs the user might be clicking through.\nIt\u0026rsquo;s a useful feature to have in case consent is granted outside the landing page, but it does lead to URL pollution and can mess up sites where functionality is based on a strict set of URL parameters (especially beyond obvious landing pages).\nThe command looks like this:\ngtag(\u0026#39;set\u0026#39;, \u0026#39;url_passthrough\u0026#39;, true);  For this to work, the site must load a gtag.js library, so just using the gtag() API will not work.\nIf you\u0026rsquo;re using Google Tag Manager, you can achieve the same functionality by using the cross-domain linking capabilities of the Conversion Linker tag.\nGoogle Tag Manager support As you might have surmised, for now the Google Tag Manager support for consent mode is a bit flaky. Being forced to use the gtag() API is a bit backwards, considering we already have a global message queue named dataLayer at our disposal.\nI\u0026rsquo;m absolutely positive that a more tightly-knit integration with GTM is on the way. I\u0026rsquo;m particularly hoping to see some or all of the following features:\n Consent status exposed in dataLayer. Right now the gtag() consent commands are not available in dataLayer, making it impossible to check advertising and analytics storage status in case one wanted to apply them to other, non-Google tags as well. Pre-load trigger. This is something I\u0026rsquo;ve been wanting to see for years. It would be so useful to have a trigger event for which all other Google Tag Manager events have to wait to complete before being initialized. It would be the perfect place for a consent mode tag, as all the other tags in the container would not be able to fire until the pre-load event has been completed. First-class consent awareness. With this I mean it should be possible to have tags be consent-aware without having to build elaborate trigger setups. One of the most difficult things to set up when using GTM with a consent management platform is the conditional firing of tags depending on consent status. Consent-aware APIs for templates. A logical extension of consent mode is that it should apply to non-Google tags as well. A perfect place for this would be in custom templates, where consent mode status could be applied to the APIs and network requests generated by custom templates as well.  Summary First of all, remember that consent mode is a beta feature. It\u0026rsquo;s likely to change before it\u0026rsquo;s officially out of beta (at which point I\u0026rsquo;ll update this article accordingly).\nIn addition to a better Google Tag Manager integration, I\u0026rsquo;m also looking forward to more consent options, as limiting just \u0026ldquo;advertising storage\u0026rdquo; and \u0026ldquo;analytics storage\u0026rdquo; doesn\u0026rsquo;t really cut it when it comes to the granularity of how users should be able to decide how their data is collected and processed.\nNevertheless, consent mode gives you a nice way to control storage especially with advertising tags. You\u0026rsquo;ve already been able to control Google Analytics storage with the storage tracker setting, but now it\u0026rsquo;s also configurable via this gtag() API.\n The main difference between consent mode and using the storage field to block analytics tags from having access to storage is that consent mode will not surface the data in reports whereas setting storage to none will.\n It\u0026rsquo;s important to note that consent mode mostly applies to storage access and doesn\u0026rsquo;t actually block any requests. I\u0026rsquo;m certain there will be debate about whether this is enough or not. However, especially in the European Union the laws seem to be rather clear that as long as the nothing is stored (or read from storage), and as long as only ephemeral identifiers that can in no way be linked back to a natural person are used, this approach should be good enough.\nYou will need to trust Google on the ad_storage bit, though. Google promises they won\u0026rsquo;t use third-party cookies for other than spam and fraud detection, but this is difficult to audit.\nSimilarly, consent mode only covers the identifiers that we know about. There might be data leaks, intentional and unintentional, in requests to Google that allow far more data controlling options than the user has opted in to.\nStill, I think any effort taken by Google to improve security and privacy control over their services is welcome. It\u0026rsquo;s just important that this doesn\u0026rsquo;t lead to complacency, though. Just because Google offers it natively doesn\u0026rsquo;t mean you can abandon due diligence in making sure that consent (and any other legitimate purposes) are observed and periodically audited.\n"
},
{
	"uri": "https://www.simoahava.com/tags/google-analytics/",
	"title": "google analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/privacy/",
	"title": "privacy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/adform-tracking-point/",
	"title": "Adform Tracking Point - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The AdForm Tracking Point custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description This template implements the AdForm Tracking Point on the website. You can use it to load the SDK, initiate the tracking point, and to send page- and order-level data to AdForm. The template can pull product data directly from the dataLayer if you have Enhanced Ecommerce purchase tracking already implemented.\nInstructions First choose whether to load the tracking points with JavaScript or as an iframe.\nIn the Tracking ID field, add your AdForm tracking point ID.\nYou can add a custom divider (| by default) and a custom page name (derived from the URL path by default) in the settings. See also this help article.\nUnder Order variables, you can add any number of standard and custom variables to flesh out the metadata you want to send to AdForm.\nUnder Products, you can choose whether to add product-level data to the hit as well. The data can be pulled from a Google Tag Manager variable, or it can be automatically mapped from your Enhanced Ecommerce purchase object, which needs to exist in the dataLayer when the AdForm tag fires.\nRelease notes    Date Changeset     1 October 2020 Update script download URL.   2 October 2019 Add icon, update description.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/search-bulk-actions-google-tag-manager/",
	"title": "Search And Bulk Actions In Google Tag Manager",
	"tags": ["google tag manager"],
	"description": "This article explores how the search function can be used to find data deep inside Google Tag Manager tags within the UI. The article will also show how bulk actions work.",
	"content": "It\u0026rsquo;s lovely to see small, incremental, quality-of-life improvements to Google Tag Manager amid the behemothian feature releases such as custom templates in 2019 and server-side tagging in 2020.\nThis time around, we\u0026rsquo;ll take a look at the upgraded search functionality of the Google Tag Manager user interface, which makes search an actually useful tool. In addition to this, we\u0026rsquo;ll take a look at one of the most requested features in the history of Google Tag Manager, which we finally have access to: bulk actions!\n  Both are relatively small updates compared to what\u0026rsquo;s been coming into Google Tag Manager over the past year or so, but that doesn\u0026rsquo;t diminish their importance one bit.\nNEW: Bulk actions One of the pain points of the Google Tag Manager UI has been the lack of bulk actions. If you wanted to delete multiple tags, you had to open each tag separately and then click to delete them.\n  With bulk actions, this process has improved quite a bit. Take a look:\n  Bulk actions are available for tags, triggers, variables, clients, zones, and templates. The actions you can do depend on where you are in the UI.\n   Asset Bulk actions available     Tags Pause/unpause, Move to Folder, Delete   Triggers Move to Folder, Delete   Variables Move to Folder, Delete   Clients Move to Folder, Delete   Zones Delete   Templates Delete    If you try to delete an item that has references, such as a variable that is referenced from a tag, you will see a warning:\n  It takes a few clicks, but to proceed you basically have to remove all references to the item before retrying the deletion. Easiest way is to open the item itself (by e.g. clicking its name in the overlay), and then clicking through the links in the References to this \u0026lt;type\u0026gt; section. Remove the links, save everything, and then retry deletion.\n  Yes, the workflow could be a bit smoother.\nAlso, I would love to see the Abandon Changes option as a bulk action, allowing you to roll back changes in bulk, especially in the Overview screen of GTM (where no bulk actions are currently available).\nAll in all, a very welcome addition to Google Tag Manager\u0026rsquo;s user interface.\nNEW: Advanced search Until now, the \u0026ldquo;search\u0026rdquo; functionality in GTM has been limited to searching by item name and/or type and filtering tables by the available columns, like this:\n  Well, we finally have a proper search feature, which digs into the actual settings and fields in the items as well, rather than just the metadata.\n  For example, in the image above I\u0026rsquo;ve searched all references to customTask within the container. I get four results:\n  GAS - Local settings - a Google Analytics Settings variable that has a set field named customTask.\n  JS - customTask - Local settings - a Custom JavaScript variable that has the term in its name.\n  UA - Page View - a tag where a field named customTask is set.\n  UA - Page View - SGTM - a tag where a field name customTask is set.\n  As you can see, not only does the search look at the metadata (name, type), it also drills into the actual fields and settings the user has configured!\nThe cool thing is that it also indexes Custom HTML tags and Custom JavaScript variables based on the JavaScript code. So searching for console returns this result:\n  That particular Custom HTML tag utilizes the console.log() method.\nWith templates, the search can also drill down to template name and description, as well as any field names in the template.\nSummary Nothing pleases me more than to see such a huge and popular tool like Google Tag Manager get these small, incremental improvements that directly address pain points and pet peeves of the community.\nIt must be difficult to prioritize these smaller updates, especially since there\u0026rsquo;s a pull to work on the larger and more impactful stuff.\nRemember to use the Send Feedback feature in the Google Tag Manager user interface (you can find it in the overflow menu in the top right corner of the page) if you have feature requests. The feedback is monitored by the team and is likely used to prioritize the feature backlog as well.\n"
},
{
	"uri": "https://www.simoahava.com/tags/false/",
	"title": "false",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/privacy/intelligent-tracking-prevention-ios-14-ipados-14-safari-14/",
	"title": "Intelligent Tracking Prevention In iOS 14, iPadOS 14, And Safari 14",
	"tags": ["intelligent tracking prevention", "webkit", "itp", "safari", "false"],
	"description": "",
	"content": "Apple\u0026rsquo;s annual Worldwide Developer Conference in late June this year included a couple of big announcements around Apple\u0026rsquo;s approach to privacy in their software.\nThe new Privacy Report in Safari 14 (on all platforms) uses DuckDuckGo\u0026rsquo;s tracker radar list to detail which of the most prominent tracking-capable domains have been flagged by Intelligent Tracking Prevention (ITP) in the user\u0026rsquo;s browser.\nApple also announced that the WKWebView class, which all iOS and iPadOS (the operating systems for iPhones and iPads, respectively) must use, will include WebKit\u0026rsquo;s ITP mechanisms on by default. The list of major browsers running on these operating systems includes Brave, Chrome, Edge, Firefox, and Safari.\n  The release date for iOS 14, iPadOS 14, and Safari 14 was announced at the Apple Event on September 15, 2020, and all developers working on the Apple stack groaned in unison when they learned that the new operating systems would be pushed out the following day, September 16.\nQueue a mad scramble to test the app builds against the latest versions of the build tools (released just 24 hours before the operating systems were updated), and the latest set of App Store guidelines (updated a week before).\nMaybe a bit longer lead time next time, please Apple?\nIn this article I\u0026rsquo;ll go over these changes, exploring their impact especially on analytics and digital marketing.\nAlso, I recommend you bookmark CookieStatus.com - a community-led initiative to maintain an up-to-date information resource on the current status of browser (and app) tracking protection mechanisms.\n CookieStatus.com  The Privacy Report Let\u0026rsquo;s tackle the easy one first.\nFor the first time, WebKit\u0026rsquo;s tracking prevention measures are exposed to the user (beyond enabling the Intelligent Tracking Prevention debug mode).\n  You can look at the Privacy Report for any site by clicking the small shield icon next to the address bar. If you want to see more details, click the (i) button.\n  The first thing to note is the terminology.\n domain.com was prevented from profiling you across N websites.\n What does that mean? It means that the Safari browser has detected HTTP requests to the listed domains, and that the listed domains are found in DuckDuckGo\u0026rsquo;s Tracker Radar lists.\nTo put it in another way - if the website is making requests to domains in DDG\u0026rsquo;s Tracker Radar list, then those domains will be listed in the Privacy Report.\nThe funky thing is that these domains might not actually have been flagged by Intelligent Tracking Prevention yet.\nWebKit\u0026rsquo;s ITP is algorithmic and on-device. The decision of whether or not a domain should be \u0026ldquo;flagged\u0026rdquo; as having tracking capabilities is done based on the user\u0026rsquo;s browsing behavior and not against a domain blocklist.\nSo the Privacy Report is a bit misleading.\nThe Privacy Report means, quite simply, that WebKit\u0026rsquo;s global tracking protections, such as truncating all cross-site referrers and blocking all cookie access in third-party context have been applied to all the cross-site HTTP requests sent from the site, including but not limited to those shown in the Privacy Report.\nThe purpose of this approach is without a doubt to just show how the biggest trackers on the web have been prevented from cross-site tracking, but the measures are not limited to just these domains. Nor are WebKit\u0026rsquo;s ITP measures applied to these domains automatically (I repeat: WebKit does not use blocklists - it classifies domains algorithmically).\nIf this is difficult to follow, I don\u0026rsquo;t blame you. I\u0026rsquo;m worried this Privacy Report only serves to confuddle and obfuscate rather than to illuminate and educate. Case in point: When the release was foreshadowed in WWDC, it led to a tidal wave of misinformation spreading on the web. This prompted me to write an article on the topic in an effort to stem the tide.\nLet\u0026rsquo;s recap this feature as clearly as possible:\n The Privacy Report is available in the Safari 14 browser across Apple\u0026rsquo;s operating systems (macOS, iOS, iPadOS). It uses DuckDuckGo\u0026rsquo;s Tracker Radar list to enumerate which known tracking-capable domains have been receiving HTTP requests from the sites the user has visited. The report highlights how some of the most prominent tracking domains (e.g. facebook.com and doubleclick.net) have been prevented from accessing the user\u0026rsquo;s browser storage, among other things. Since WebKit blocks all access to cookies in third-party context, the full list of \u0026ldquo;prevented\u0026rdquo; domains comprises all the cross-site requests done from the sites the user visits, not just those listed in the Privacy Report. If a domain is listed in the report, it does not mean that the domain has been flagged by WebKit\u0026rsquo;s Intelligent Tracking Prevention. This classification is still algorithmic and still based on the sites the user visits, and what types of cross-site requests these sites do. Finally, Safari does not block requests - it strips them of the capability to access cookies or parse referrer headers, etc.  I recommend you visit the Safari page on CookieStatus.com for a more detailed walkthrough of what WebKit does by default, and what is behind Intelligent Tracking Prevention\u0026rsquo;s flags.\nTracking prevention in all iOS and iPad browsers The more interesting, and perhaps more convoluted, update was that Apple is updating the WKWebView class. Per the App Store guidelines, all web browsers running on iOS must implement this class, though officially there\u0026rsquo;s a transition period from the deprecated UIWebView to WKWebView which lasts until December 2020.\nAnd what\u0026rsquo;s the update? Well, nothing more and nothing less than that all WebKit\u0026rsquo;s Intelligent Tracking Preventions are on by default in all browsers running WKWebView in iOS 14 and iPad 14.\nAt the time of writing this, all browsers apart from Brave have updated to the latest OS requirements, and Brave should follow up with a new build very shortly.\nThe main change can be found in Settings for each browser app. This is what Firefox looks like:\n  Across all iOS and iPadOS browsers, the new setting \u0026ldquo;Allow Cross-Website Tracking\u0026rdquo; is toggled off. This means that all these browsers are now implementing the full scale of WebKit\u0026rsquo;s Intelligent Tracking Prevention mechanisms.\nThese include, among others:\n  Full third-party cookie blocking. All cookie access in third-party context is blocked. There are no exceptions. Storage access can only be granted through the Storage Access API.\n  All cross-site referrers are downgraded to just the origin by default (https://www.domain.com).\n  All cookies written with JavaScript will have their expiration capped at a maximum of 7 days from the time the cookie is (re)written.\n  Algorithmic classification of domains the browser communicates with. The classifier detects if the sites the user visits communicate with cross-site origins to a point where the classifier deems these domains to have cross-site tracking capabilities. At this point, additional restrictions that apply to classified domains kick in:\n4.1. All storage on these domains is purged after 30 days of the user not directly interacting (i.e. in first-party context) with the classified domain.\n4.2. If the classified domain sends traffic to other sites, appending parameters into the URLs (such as a Google ad click), then any JavaScript cookies written on the sites the links lead to will have their expiration capped at 24 hours.\n4.3. If the classified domain sends traffic to other sites, and the classified domain has URL parameters (or fragments) in the URL, the document.referrer string on the target site will be truncated to just eTLD+1 (so https://www.simoahava.com/some-page becomes https://simoahava.com/).\n   Note! At the time of writing, there seems to be a bug with the implementation across iOS browsers, and not all these mechanisms are in effect even if the \u0026ldquo;Allow Cross-Website Tracking\u0026rdquo; toggle is left to its default position of OFF.\n This is \u0026hellip; pretty big. The development of these web browsers is now intrinsically linked to the evolution of WebKit\u0026rsquo;s tracking prevention mechanisms. For example, when the upcoming CNAME cloaking mitigation sees daylight, it will be applied to all iOS / iPadOS browsers, and not just Safari as before.\nImpact #1: Cross-site targeting and profiling As third-party cookies are now flushed out of the mobile operating systems, it means that any cross-site tracking scheme that relies exclusively on these is dead in the water. Google\u0026rsquo;s DoubleClick network, for example, will no longer be able to build a cross-site profile of web users based on the sites they visit, as they will no longer be able to associate a cookie identifier to these hits.\nIt\u0026rsquo;s unlikely that ad tech vendors have the gall to use the Storage Access API to ask permission of the user to track them across sites.\nVendors are, naturally, busy at figuring out workarounds. Those that own an identity platform (e.g. Facebook), have for long been moving cross-site tracking away from third-party context, and others will likely follow suit.\nReliance on fingerprinting will likely increase, even though these measures are addressed by WebKit as well.\nThe cat-and-mouse game continues.\nImpact #2: First-party analytics, optimization, personalization Services that run in first-party context are not without impact either.\nWebKit restricts the lifetime of JavaScript cookies to a maximum of 7 days, with the limit set to 24 hours in some instances.\nThis can have an impact on the ratio of \u0026ldquo;new\u0026rdquo; and \u0026ldquo;returning\u0026rdquo; users in analytics tools, and the likelihood of the same individual being included in different experiment groups increases, for example.\nThere is a known mitigation for this, which does not go against WebKit\u0026rsquo;s policies: sites can recycle cookies so that they are set in HTTP headers instead.\n From https://webkit.org/blog/8613/intelligent-tracking-prevention-2-1/  Impact #3: Referrer truncation WebKit\u0026rsquo;s approach to referrers is similar to strict-origin-when-cross-origin, except this is not a \u0026ldquo;default\u0026rdquo; referrer policy (it\u0026rsquo;s always on), and it\u0026rsquo;s more like strict-origin-when-cross-site.\nIn other words, when the website makes a cross-site request (e.g. https://www.simoahava.com/some-page/ to https://www.google-analytics.com/collect), the referrer visible to google-analytics.com will be just the origin: https://www.simoahava.com/.\nIf the website makes a cross-origin (but same-site) request, the referrer string will be untouched.\nAdditionally, if the website is classified by ITP as having cross-site tracking capabilities and it has query parameters (or fragments) in its URL, then any site it sends traffic to will have the document.referrer string truncated to only eTLD+1 (so https://www.simoahava.com/some-page/?key=value will show up as https://simoahava.com if the domain has been classified by ITP).\nTruncating the referrer like this has obvious impacts for analytics, for example, as understanding what sites and pages send you traffic has been a staple of web analytics for a long time.\nNew App Store Review guidelines Apple also updated its App Store review guidelines a week before iOS 14 and iPad 14 were released. I recommend checking out Cory Underwood\u0026rsquo;s overview of the topic - the changes can be quite impactful for apps that also collect data from users.\nApps will basically have to:\n  Disclose in detail what type of data collection goes on.\n  Provide an opt-in mechanism to the collection of user and usage data.\n  Not put up consent walls (allow the user to access content only if they give consent to tracking).\n  Implement an opt-out mechanism as well, where if the user withdraws consent, their data should be purged.\n  There are echoes of GDPR and CCPA here, with the exception that Apple is a private company and not a legislative body. They have far more coverage than the aforementioned legal frameworks, and as these guidelines have a direct financial impact on organizations (loss of revenue if apps are removed from the store), they will likely inspire far more and faster action than any laws or regulations.\nSummary iOS 14, iPadOS 14, and Safari 14, are major releases at least when it comes to privacy protections in software running on Apple\u0026rsquo;s operating systems.\nBrowsers running on iOS and iPadOS now must implement WebKit\u0026rsquo;s ITP mechanisms, which, given iPhone and iPad market share, can have a resounding impact on organizations relying on data collection and sharing.\nWhat\u0026rsquo;s imperative now is that each organization starts benchmarking and modeling the impact of third-party cookie blocking and first-party cookie restrictions on their own data. Please try to avoid contributing to the FUD with knee-jerk reactions such as \u0026ldquo;DATA IS DEAD\u0026rdquo; or conjuring doomsday predictions based on circular reasoning.\nThe only thing that panic serves is the rapid spread of misinformation. And the only thing that misinformation feeds is diverting attention away from what WebKit is doing with these tracking prevention policies: eradicating cross-site tracking vectors from software and services running on the Apple stack.\nPlease let me know in the comments if something was unclear. Note that the releases are still quite fresh, and testing them due to bugs might lead to inconsistent results.\nBe sure to monitor CookieStatus.com as well. If you have information that\u0026rsquo;s missing from the service, please submit an issue about it.\n"
},
{
	"uri": "https://www.simoahava.com/categories/privacy/",
	"title": "Privacy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/webkit/",
	"title": "webkit",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/container-export/",
	"title": "container export",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/partial-container-export-google-tag-manager/",
	"title": "Partial Container Export In Google Tag Manager",
	"tags": ["google tag manager", "container export"],
	"description": "Showcasing the partial container export feature in Google Tag Manager. Partial export lets you choose only a subset of the container to export as a JSON file for importing into other containers.",
	"content": "Exporting a container in Google Tag Manager can have many purposes. From backing up Google Tag Manager versions to creating and distributing repositories of useful container snapshots, the container export is one of the most useful non-tagging-related tools that the GTM user interface offers.\nHowever, one big misgiving in the feature (until now) has been that exporting just a subset of the container version or workspace has been impossible. It\u0026rsquo;s always the entire container or nothing. If you wanted to remove parts of the export before importing it to another container, you had to do JSON surgery to the contents, removing all the unnecessary or sensitive items before sharing the export file.\nWell, Google Tag Manager has now introduced partial export functionality into the container export feature.\n  Partial export allows you to select only the items you want. It also warns you if you are about to export items without all the references included, offering an easy Add All option to add all the dependencies in one go.\nHow it works Click over to Admin in the container, and select Export Container from the list of options under the container.\n  In the first overlay that appears, you can choose a workspace or a version to export the information from.\n Currently, there\u0026rsquo;s no way to mix-and-match - you can only select items from a single workspace or container version.\n   In the next overlay (the actual export view), you have the following options at your disposal.\n  You can select or deselect single items one by one.\n  You can select or deselect all items in one go.\n  You can choose to Retain folder structure. If items are in folders, then the respective folders are exported as well, and the links between items and their folders are retained.\n  You can search through the items for better accessibility.\n  You can open details for all items (apart from built-in variables).\n  You can preview what the export JSON would look like with the current selections.\n  You can export (download) the JSON compiled from your selections.\n  You can add all the missing dependencies in one go.\n  You can add missing dependencies item by item.\n    To operate the tool is dead simple. Just select the item(s) that you want to include in the export, and then click either Preview JSON or Export. The first lets you take a look at the JSON before downloading it, the second downloads it immediately.\nDependency graph If you try to Preview or Export the JSON, and you\u0026rsquo;ve selected items that have dependencies (e.g. linked triggers, variable references, tags in a sequence), and you haven\u0026rsquo;t selected all the dependencies, you\u0026rsquo;ll see the following warning:\n  If you choose Export (or Preview if that was your original action), you\u0026rsquo;ll proceed with the action regardless of the missing references.\nIf you select Cancel, a new warning about missing references appears in the overlay, and you\u0026rsquo;ll also see a new column in the table, which lets you sort items by the missing dependency warning.\n Note that the dependency graph is only shown for items you tried to export or preview, not for all items in the version or workspace. You need to actually select items and try to preview or export them to trigger the warning and display the missing references.\n   If you click the warning triangle in the respective column of the table for an item with missing references, a popup will open that shows you what those references are and lets you select all of them in one go.\n  This is a pretty neat way to make sure that you don\u0026rsquo;t unintentionally export a subset of the container but end up missing many of the bits and pieces required to make the selected items work.\nSummary As far as quality-of-life improvements go, this is pretty good.\nSelective export of a Google Tag Manager container is vital for many workflows out there, as it allows you to do things like keep the staging and live environment in sync, compile a repository of your favorite combinations, and make sure that no sensitive information is accidentally shared when exporting and importing container JSONs.\nThere are a couple of features that might be useful down the line, such as being able to mix-and-match across container versions and workspaces, and being able to review dependencies before running the export.\nBut as it stands, I think it\u0026rsquo;s a useful feature that should make many Google Tag Manager users very happy indeed.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/container-notifications-google-tag-manager/",
	"title": "#GTMTips: Container Notifications In Google Tag Manager",
	"tags": ["google tag manager", "gtmtips"],
	"description": "Introducing the Container Notifications feature in Google Tag Manager. Receive an email whenever a key action (e.g. container publish) takes place in Google Tag Manager.",
	"content": "A recent update to Google Tag Manager introduced container notifications.\nBy subscribing to container notifications, your Google Tag Manager login email address can be configured to receive an email for some of the key workflows in Google Tag Manager: containers getting published, and containers being submitted for approval (Tag Manager 360 only), for example.\nBe sure to check out the official help center article about container notifications.\nIn this article, I\u0026rsquo;ll walk you through the feature and share a couple of tips on how to make it even more useful!\nTip 120: Container notifications in Google Tag Manager   You\u0026rsquo;ll find container notifications for an individual container under Admin:\n  You can also set default notification settings in your User Settings.\nClick the overflow menu in the top-right corner of the GTM UI (the menu with the three vertical dots) and choose User Settings. Scroll down to the Default Container Notifications. Whatever you set here will be on by default in all the containers you manage.\n  When you set Default Container Notifications, they will apply to all containers you have access to until you go to each individual container, change the value of a notification setting, and Save it.\n A bit confusingly, the settings for an individual container do not tell you whether or not the current settings override or inherit the default container notification setting.\n Notification triggers To receive an email notification, you can configure the following events to trigger one:\n1. A container version is published The Always option sends an email whenever a version is published into any environment.\nThe Live Environment only option sends an email only when a version is published into the live environment.\n2. A container version is created The Always option sends an email whenever a container version is created (but not published).\n3. A workspace is sent for approval  Tag Manager 360 only.\n When an approval request is submitted for a workspace, you can configure your email address to receive a notification.\nThe Only when I am tagged as an approver will do exactly what it says. If you have been designated as an approver for an approval request, you will receive the email.\nAlways will send you an email whenever an approval request is submitted.\n4. A workspace is withdrawn  Tag Manager 360 only.\n The Only when I am tagged as an approver option configures your email to receive a notification when an approval request where you have been designated as an approver is withdrawn (i.e. deleted).\nAlways will send you an email when an approval request is withdrawn.\n5. A workspace is sent back  Tag Manager 360 only.\n The Only for approvals I requested option sends you an email when an approval request that you submitted gets sent back by an approver because it needs more work.\nAlways will send you an email when an approval request gets sent back by an approver.\nThe email When you do receive an email notification for a container version, it looks like this:\n  In addition to the information on what was added, updated, or deleted in the version, there\u0026rsquo;s a link directly to the version information (See the version details).\nThere are also direct links (well hidden in the footer) to unsubscribe from this category of notifications or to jump directly to container notification settings.\nAn email for approval requests is a bit plainer. Here\u0026rsquo;s a sample notification for when your approval is requested:\n  Use Zapier for even more flexibility You can connect a trigger service like Zapier to create even more elaborate notifications out of your container notification emails.\n  Above is an example of what a Gmail -\u0026gt; Slack Zap looks like. The Zap is configured to search for Version and published from emails sent from the container notification service. When such an email is received, the Zap sends a Slack message with the email subject to the channel.\n   Note that Gmail only works with Slack for company email addresses @mycompany.com. The integration is not available for e.g. @gmail.com or other commercial addresses.\n Zapier has a gazillion apps for integrating into the workflow. Take a look at the offering here.\nSummary The notification feature is a welcome one. Being notified of changes to a container has been a very popular feature request, and it even prompted me to write a custom Slack integration guide, which instructs how to build a simple Google Cloud messaging system.\nHaving this feature natively in the Google Tag Manager user interface is great. It does exactly what it should, but this doesn\u0026rsquo;t mean it couldn\u0026rsquo;t be better.\nI\u0026rsquo;d like to see the option of adding other email addresses that have access to the account rather than just the one you\u0026rsquo;re logged in with. It would also be cool if there were even more triggers for the notification, such as workspaces being deleted, user permissions being modified, new containers created, and so forth. But, I\u0026rsquo;m certainly happy with the feature as it stands today.\nLet me know in the comments what you think about this UX upgrade to Google Tag Manager!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/utilize-app-engine-headers-server-side-tagging/",
	"title": "#GTMTips: Utilize App Engine Headers In Server-side Tagging",
	"tags": ["google tag manager", "gtmtips", "server-side tagging"],
	"description": "App Engine automatically geolocates the IP addresses of the incoming requests, and assigns the values to HTTP headers in the requests. You can pull these values from the headers and utilize them in your Server container tags.",
	"content": "When you create a Server container in Google Tag Manager, GTM creates an App Engine deployment in the Google Cloud Platform for you.\nApp Engine is a managed serverless platform, which basically means it\u0026rsquo;s a (set of) virtual machine(s) running in the cloud, with some extra bells and whistles added to make managing it easier.\nA potentially useful thing that App Engine does is decorate all incoming HTTP requests with some HTTP headers that can be used in the app. These headers include geolocation headers, based on the IP address of the machine that sent the hits.\nAs with geolocation always, accuracy varies wildly, but App Engine will make a best-effort attempt to determine the country, city, region, and latitude/longitude of the city where the IP address was geolocated to.\nIn this article, I\u0026rsquo;ll show you how to grab these header values and pass them to the tags firing in the Server container.\n This article was inspired and influenced by discussions with the Google engineers and developers working on Server-side tagging, Adam Halbardier in particular.\n Tip 119: Use App Engine\u0026rsquo;s headers to geolocate the request source   The initiative to write this article came while working with the ip_override and user_agent overrides in another article. As it turns out, App + Web does not currently support overriding a user\u0026rsquo;s IP address in the outgoing request to Google servers, which means that App + Web sees all requests as originating from your Server container. This is good for privacy but bad for geolocation.\nLuckily, you can utilize these request headers to parse what App Engine has determined to be the geographical location (down to the accuracy of the city) of the incoming request:\n  X-Appengine-Country - ISO 3166-1 alpha-2 country code, such as FI for Finland and US for the USA.\n  X-Appengine-Region - ISO 3166-2 region code whose value and encoding depends on the country. For example, my region is Uusimaa, and the region code is 18. A request from California would show up with region code ca.\n  X-Appengine-City - Name of the city (if available) where the request originated from. There is no canonical or standardized list of values here.\n  X-Appengine-Citylatlong - Comma-separated latitude and longitude of the city where the request originated from. For Espoo (my city), it would look like this: 60.204813,24.652052.\n  To grab these, you need to create new User-defined variables in the Server container, where each variable is of type Request Header, and set to the value of one of these headers. Like so:\n  You can then add them to your tags as overridden fields. Here\u0026rsquo;s an example where I enhance my server-side App + Web tag.\n  And this is what it looks like in App + Web.\n   You can also add geolocation information to your Universal Analytics tags by utilizing the \u0026amp;geoId Measurement Protocol parameter. This way you could geolocate the user without Google collecting the IP address. However, you would need to map the geolocation data provided by App Engine to the criteria ID parameter Google Analytics uses, which is a bit of a chore without a Lookup Table variable.\n Anyway, I have no doubt that proper IP override will be introduced in App + Web tags soon, but you can continue sending this information as metadata, as it might be useful to see if App Engine\u0026rsquo;s and App + Web\u0026rsquo;s geolocation results differ.\n NOTE! I don\u0026rsquo;t know how App Engine resolves geolocation, nor do I know how App + Web does it either. Most likely they use a proprietary service, but there is a notable lack of documentation about this. This document merely states that the WHOIS database is not used for geolocation.\n Let me know what you think about this simple trick in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/get-true-ip-anonymization-server-side-tagging/",
	"title": "#GTMTips: Get True IP Anonymization With Server-side Tagging",
	"tags": ["google tag manager", "gtmtips", "server-side tagging"],
	"description": "With Server-side tagging, you can get unprecendented amount of control over the hits passed through Google Tag Manager to third-party platforms like Google Analytics and Facebook. In this article, I&#39;ll show you how to eliminate all traces of the user&#39;s actual IP address and User Agent from the outgoing hit to Google Analytics.",
	"content": "Since the release of Server-side tagging in Google Tag Manager, I\u0026rsquo;ve jumped at every opportunity to celebrate the tools it provides for improving end-user privacy and data security.\nOne of the biggest benefits is obfuscation-by-default. Since all hits are passed through the server-side proxy, the default view for any third-party tool (such as Google Analytics) is that of the server in the Google Cloud rather than the browser and device with which the user was browsing the site.\nIn other words, by using the Server container as a proxy, you are \u0026ldquo;hiding\u0026rdquo; the actual user, as all hits seem to come from the virtual machine rather than the user\u0026rsquo;s browser.\nHowever, the Universal Analytics tag in the Server container copies the user\u0026rsquo;s IP address and User Agent into the outgoing Measurement Protocol request using the \u0026amp;uip and \u0026amp;ua parameters. This means that even though the outgoing HTTP request to Google Analytics originates from the virtual machine in Google Cloud, the Google Analytics data payload is updated to reference the user\u0026rsquo;s actual device and IP address.\nIn this article, I\u0026rsquo;ll show you how to override this behavior and prevent Universal Analytics from doing this copy-paste operation.\n Thanks to Adam Halbardier of the Google team working on Server-side tagging for this tip!\n Tip 118: Remove IP address and User Agent overrides from Universal Analytics tags   It\u0026rsquo;s very easy to accomplish this. In the Universal Analytics tag that fires in the Server container, you need to do the following.\nFirst, check Enable overriding settings in this tag. This allows you to set individual fields in the tag which override those set by the Client.\n  Then, expand Override Fields to Set, and add the following two fields without any values (leave the value fields empty).\nuser_agent\nip_override\n NOTE! These fields are not the same as analytics.js fields. They are fields established in the event model used in Server-side tagging. There is still a lack of exhaustive documentation about what fields are available, but digging into Debug mode in the Server container and exploring the Event Data tab will get you a long way.\n By setting the fields empty, you are essentially preventing the tag from setting those fields with values from the incoming request, i.e. the user\u0026rsquo;s actual browser and device.\nThis is enough to prevent the override, and you can guarantee the actual user\u0026rsquo;s IP address and User Agent string are not passed to Google Analytics in any shape or size.\nBut what about if you want variable behavior. Prevent the override in some cases, but allow it in others?\nWell, while we\u0026rsquo;re waiting for a Lookup Table variable type to be added to the Server container, there\u0026rsquo;s no way to set variable values for these fields based on, for example, the existence of the \u0026amp;aip (Anonymize IP) parameter.\nSo if you wanted to allow the IP and User Agent transfer except when the event data has anonymize_ip set to true (this is what the \u0026amp;aip parameter is turned into in the event object), you\u0026rsquo;d need two tags.\nOne sends the data to Google Analytics without the overrides (i.e. default Universal Analytics tag behavior without any overridden settings), and fires when an Event Data variable set for the key anonymize_ip is not true.\n    The other tag sends the data to Google Analytics following the instructions in this article. It needs to have the ip_override and user_agent fields set to blank values. The trigger for it is the reverse of the one for the other tag, so you need to check if anonymize_ip is actually true.\n  Yes, a Lookup Table variable would make this much smoother as you\u0026rsquo;d need just one tag and one trigger and set the value of ip_override and user_agent to blank or default values depending on whether the IP should be anonymized or not.\nI hope you find this tip useful. At the very least, it should reinforce the notion that Google Tag Manager\u0026rsquo;s Server-side Tagging can be used to improve the prospects of user anonymization and obfuscation of personal data.\n"
},
{
	"uri": "https://www.simoahava.com/tags/client-id/",
	"title": "client id",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/fpid-cookie-google-analytics-server-side-tagging/",
	"title": "The FPID Cookie For Google Analytics In Server-side Tagging",
	"tags": ["google tag manager", "server-side tagging", "universal analytics", "client id"],
	"description": "The new FPID HttpOnly cookie is used by Google Analytics to replace the script-readable _ga cookie. It requires a Server-side tagging setup to work. In this article, I&#39;ll show you how FPID works, and how to set it up.",
	"content": "With Server-side tagging, the developer community has a chance to vastly improve the data collection capabilities of Google\u0026rsquo;s analytics platforms (Universal Analytics and App+Web). The ability to build our own templates is particularly potent with a Server container.\nHowever, it\u0026rsquo;s not as if Google themselves are just sitting idly by and seeing what the community can come up with.\nIn the built-in Universal Analytics Client template in a Server container, there\u0026rsquo;s an option to migrate to a Server Managed Client ID.\n  When using this particular Client for proxying Google Analytics requests, the Server container introduces a new cookie that is only accessible to the webserver and not to browser JavaScript. This cookie is named FPID (First Party Identifier) by default. The value stored in FPID will be used for setting the Client ID in the request to Google\u0026rsquo;s servers.\nIn this article, I\u0026rsquo;ll walk you through how this works and what its implications are for data collection and security.\nHow it works Be sure to check out the main guide for an introduction to Server-side tagging. An understanding of how Clients and tags work will make following this article much easier.\nGoogle Analytics uses the \u0026amp;cid URL parameter in its data collection HTTP request to pass the Client ID from the browser (or device) to Google Analytics\u0026rsquo; servers. This Client ID is persisted in a first-party cookie written (and read) with JavaScript.\nWith FPID, Google is moving away from the JavaScript-accessible cookie to an HTTP-set one, which is further secured with the HttpOnly flag. The new setting in the Universal Analytics Client gives you a couple of approaches to how to migrate (or not).\nJavaScript Managed - business as usual When you set the configuration in the Client to JavaScript Managed, the Client will read the incoming request as usual, use the \u0026amp;cid parameter from the request to set the Client ID in the outgoing request to Google Analytics, and then not really do anything else. So if you don\u0026rsquo;t want to use this new way of storing the client identifier, you need to set the option to JavaScript Managed.\n  Server Managed - new HTTP cookie However, when you set it to Server Managed, the Client will now parse the identifier from a new cookie and prefer that to what the browser (or device) sends as the value of the \u0026amp;cid parameter. The new cookie is written in the Set-Cookie HTTP header in the response back to the browser or other network source.\nIn other words, without making adjustments (see below), the Server container will generate a new FPID cookie (if one doesn\u0026rsquo;t already exist) and use that to populate the Client ID of the outgoing request to Google Analytics servers.\nThe cookie is set with the HttpOnly flag, which means it is not accessible to browser JavaScript. Only the webserver can read the value of the cookie. This mitigates against cross-site tracking, as first-party JavaScript-readable cookies are often repurposed for building cross-site profiles.\n  Setting the cookie in the HTTP response also makes it a bit more resilient against browsers with measures in place to reduce the utility of JavaScript cookies (see e.g. Apple\u0026rsquo;s Intelligent Tracking Prevention).\nMigrating from JavaScript Managed to Server Managed To ensure that the Server container doesn\u0026rsquo;t just start creating new Google Analytics users en masse, you can select the Migrate from JavaScript Managed Client ID option.\nWith this option, the Universal Analytics Client will continue using the original JavaScript Managed Client ID value until such a time that the _ga cookie is deleted or the Client ID is reset. At that point, the system will migrate to the new Server Managed option stored in the FPID cookie.\nHere\u0026rsquo;s what it does in detail:\n IF the incoming HTTP request doesn\u0026rsquo;t have the FPID cookie but does have the \u0026amp;cid parameter set in the request, a new FPID cookie is created with a hash from the value of the \u0026amp;cid parameter in the incoming request. This \u0026amp;cid value is passed through to Google Analytics as the Client ID, thus not resetting the user. IF the incoming HTTP request has both the FPID cookie and the \u0026amp;cid parameter AND the FPID hash has been generated from this precise \u0026amp;cid parameter, the value of the \u0026amp;cid in the incoming request is passed through to Google Analytics as the Client ID, thus not resetting the user. IF the incoming HTTP request has the FPID cookie and either doesn\u0026rsquo;t have the \u0026amp;cid parameter OR the values differ, then the hash stored in FPID is sent to Google Analytics as the Client ID. This technically \u0026ldquo;resets\u0026rdquo; the user, but since the \u0026amp;cid already has a different value than what the FPID was originally derived from, the user would have been reset anyway.     Note that the FPID hash generated from the \u0026amp;cid value also includes a server-side seed, making it impossible to deduce the FPID value from that stored in the _ga cookie using client-side code.\n If these hits sent by the Server container are collected in a new Google Analytics property, it makes no sense to enable the migration option, as there would be no pre-existing users. Just use the Server Managed option without the migration selection checked.\nOn the other hand, if you start with a Server Managed setup but then want to switch to the migration flow, perhaps because you decide to start collecting to your main Google Analytics property instead of a test property, you can enable the migration option. However, you\u0026rsquo;ll first want to rename the FPID cookie or else the value stored in FPID from the original setup will be used instead of the Client ID of the incoming request. Renaming the FPID cookie essentially resets it.\nCaveat 1: Multiple Google Analytics cookies One problem that arises with the Server Managed FPID cookie is when your site\u0026rsquo;s trackers use different Client IDs. This is quite common, especially with cross-domain tracking, where the roll-up cookie is kept separate from the regular Google Analytics cookie to avoid cross-domain tracking from overwriting the Client ID for trackers that don\u0026rsquo;t want to use cross-domain tracking.\n  There is no support for multiple Client IDs in the Server Managed FPID option, so if you don\u0026rsquo;t want the Server Managed option to break your multi-cookie setup, you need to hold off until a solution is released.\nCaveat 2: Cross-domain tracking Similarly, because FPID is HttpOnly, it doesn\u0026rsquo;t lend itself to cross-domain tracking. Cross-domain tracking is enabled with client-side code, and the HttpOnly flag makes it impossible for client-side code to access the Client ID for cross-domain link decoration.\nThere is very likely a feature being designed to support cross-domain tracking, but until such a feature is released, you should hold off from running with the Server Managed option.\nCaveat 3: No cookieless option Client-side Google Analytics can be used without cookies. This is a viable option in the EU if the user hasn\u0026rsquo;t given consent for storing or persisting any data in the browser or device.\nUnfortunately, the Server Managed FPID cookie doesn\u0026rsquo;t currently have a way to comply with this wish. Incoming HTTP requests claimed by the Client with the Server Managed option activated generate the FPID cookie in all cases.\nAgain, if this is a deal-breaker for you, you\u0026rsquo;ll need to wait for the Client to support a cookieless option.\nSummary If Google Analytics engineers were given a chance to redesign how GA persists the client identifier, they would build GA with the FPID (or something similar) and steer far away from JavaScript cookies.\nCookies are notoriously tricky to get right, but the fact is that the closer they are to client-side access, the less secure they are. Even though the Google Analytics identifier doesn\u0026rsquo;t have any personal data encoded within, nor can it be used as an access key for any authentication systems, it\u0026rsquo;s still a vector for cross-site tracking.\nThe Google Analytics cookie is a persistent first-party identifier that can be repurposed for cross-site tracking in cookie syncing setups, for example.\nBy moving the identifier to an HttpOnly cookie, the identifier is protected from misuse.\nGive FPID a spin - you can use the JavaScript Managed option for best compatibility. Just be mindful of the caveats listed in this article.\n"
},
{
	"uri": "https://www.simoahava.com/tags/universal-analytics/",
	"title": "universal analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/build-custom-universal-analytics-client-server-side-tagging/",
	"title": "#GTMTips: Build A Custom Universal Analytics Client For Server-side Tagging",
	"tags": ["google tag manager", "gtmtips", "server-side tagging", "cookie", "universal analytics"],
	"description": "Build a new Universal Analytics Client to be used with Google Tag Manager&#39;s Server container. The Client sends the request to Google and rewrites the _ga cookie in a Set-Cookie response header.",
	"content": "You can utilize Server-side tagging in Google Tag Manager to build your own custom Universal Analytics proxy.\nThis proxy comes in the shape of a new Client custom template, which takes the incoming /collect requests and sends them to Google Analytics. While doing so, it also returns the _ga cookie in a Set-Cookie header, thus preventing Safari\u0026rsquo;s Intelligent Tracking Prevention from capping its expiration to just 7 days.\n You might also be interested in reading what Google\u0026rsquo;s own solution is for migrating from JavaScript cookies to those set in HTTP headers.\n I\u0026rsquo;ve also created a video that is similar though a bit more advanced than this written tutorial.\n NOTE! The video has one important omission. When creating the Client template, make sure to update the \u0026ldquo;Sends HTTP Requests\u0026rdquo; permission to include \u0026ldquo;Allow Google Domains\u0026rdquo;. Otherwise the proxying of analytics.js doesn\u0026rsquo;t work.\n   If the video doesn\u0026rsquo;t work, you can watch it here.\nTip 117: Build a Universal Analytics Client   To get things working, you need a couple of things:\n A fully functional Server-side tagging setup. A Client running in the Server container (we\u0026rsquo;ll build this now). A Universal Analytics tag in the Server container (I\u0026rsquo;ll instruct this as well). Modifications to all the Universal Analytics tags firing in the web container (yes, I\u0026rsquo;ll help with this as well).  You will use regular Universal Analytics tags in the web container, but you\u0026rsquo;ll set their Transport URL field to point to your Server endpoint. These tags will only be used for sending the data to the Server container rather than directly to Google Analytics.\n  Next, all the Universal Analytics tags in the web container will need the following modification in their Fields to Set (using a Google Analytics Settings variable is recommended).\n  This field prevents the Universal Analytics JavaScript from refreshing the _ga cookie. You want to do this because the whole purpose of the Set-Cookie header we\u0026rsquo;ll configure in the Server container is to prevent the web browser from setting the _ga cookie with JavaScript. All JavaScript-set cookies are prevented by Safari from having a longer expiration than 7 days.\n Remember, this field must be set in all Universal Analytics tags. Even a single tag without this will rewrite the cookie when it fires and thus will negate any benefit you derived from the custom Client.\n Finally, the proxy itself.\nBuild the Client To build the proxy, you need a new Client template, built in the Server container. In the Server container UI, browse to Templates and click to create a new Client template.\n  In the first tab, give the template a name and a brief description. If you want, you can also add an icon image.\nFor this particular template, you can skip the Fields tab. We don\u0026rsquo;t need the Client to support any customization.\nIn Code, copy-paste the following:\nconst claimRequest = require(\u0026#39;claimRequest\u0026#39;); const extractEventsFromMpv1 = require(\u0026#39;extractEventsFromMpv1\u0026#39;); const getCookie = require(\u0026#39;getCookieValues\u0026#39;); const getRemoteAddress = require(\u0026#39;getRemoteAddress\u0026#39;); const getRequestHeader = require(\u0026#39;getRequestHeader\u0026#39;); const isRequestMpv1 = require(\u0026#39;isRequestMpv1\u0026#39;); const returnResponse = require(\u0026#39;returnResponse\u0026#39;); const runContainer = require(\u0026#39;runContainer\u0026#39;); const setCookie = require(\u0026#39;setCookie\u0026#39;); const setPixelResponse = require(\u0026#39;setPixelResponse\u0026#39;); const setResponseHeader = require(\u0026#39;setResponseHeader\u0026#39;); // Get User-Agent and IP from incoming request const ua = getRequestHeader(\u0026#39;user-agent\u0026#39;); const ip = getRemoteAddress(); // Check if request is Measurement Protocol if (isRequestMpv1()) { // Claim the request  claimRequest(); const events = extractEventsFromMpv1(); const max = events.length - 1; events.forEach((event, i) =\u0026gt; { // Unless the event had IP and user-agent overrides, manually  // override them with the IP and user-agent from the request  // That way the GA collect call will appear to have originated  // from the user\u0026#39;s browser / device.  if(!event.ip_override \u0026amp;\u0026amp; ip) event.ip_override = ip; if(!event.user_agent \u0026amp;\u0026amp; ua) event.user_agent = ua; // Pass the event to a virtual container  runContainer(event, () =\u0026gt; { if (i === max) { // Rewrite the _ga cookie to avoid Safari expiration.  const ga = getCookie(\u0026#39;_ga\u0026#39;); if (ga \u0026amp;\u0026amp; ga.length) { setCookie(\u0026#39;_ga\u0026#39;, ga[0], { domain: \u0026#39;auto\u0026#39;, \u0026#39;max-age\u0026#39;: 63072000, path: \u0026#39;/\u0026#39;, secure: true, sameSite: \u0026#39;lax\u0026#39; }); } setPixelResponse(); // Make sure no CORS errors pop up with the response  const origin = getRequestHeader(\u0026#39;Origin\u0026#39;); if (origin) { setResponseHeader(\u0026#39;Access-Control-Allow-Origin\u0026#39;, origin); setResponseHeader(\u0026#39;Access-Control-Allow-Credentials\u0026#39;, \u0026#39;true\u0026#39;); } returnResponse(); } }); }); }  Once the APIs have loaded, the Client pulls in the source\u0026rsquo;s IP address and User-Agent string with this:\nconst ua = getRequestHeader(\u0026#39;user-agent\u0026#39;); const ip = getRemoteAddress();  Next, the isRequestMpv1 API is utilized to automatically check if the incoming HTTP request is a Measurement Protocol (v1, i.e. Universal Analytics) request. If it is, then the Client claims the request, and thus prevents other Clients from processing it.\nThe rest of the Client is concerned with iterating through all the events in the request, parsing them automatically into the required event schema (using the extractEventsFromMpv1 API), and running the container with all the events in the batch.\n Typically there will be just one event in each batch.\n if (!event.ip_override \u0026amp;\u0026amp; ip) event.ip_override = ip; if (!event.user_agent \u0026amp;\u0026amp; ua) event.user_agent = ua;  The two lines of code above have a very important function. They take the user\u0026rsquo;s IP address and User-Agent string and pass them to the event data object so that the Universal Analytics tag can then add them into the outgoing request to Google. If you didn\u0026rsquo;t set these, the IP address and User-Agent string would be set to those of the Server container itself, which is not very useful.\nOnce all the events have been processed by the container, the following code block is run:\nconst ga = getCookie(\u0026#39;_ga\u0026#39;); if (ga \u0026amp;\u0026amp; ga.length) { setCookie(\u0026#39;_ga\u0026#39;, ga[0], { domain: \u0026#39;auto\u0026#39;, \u0026#39;max-age\u0026#39;: 63072000, path: \u0026#39;/\u0026#39;, secure: true, sameSite: \u0026#39;lax\u0026#39; }); }  This piece of code checks the incoming HTTP request for a cookie named _ga. If it finds one, it rewrites the cookie with a Set-Cookie header, thus converting it to an HTTP cookie and helping solve expiration issues with Safari\u0026rsquo;s ITP.\nsetPixelResponse() automatically configures the response back to the source of the request to resemble a 1x1 GIF image with cache-busting headers. Finally, returnResponse() is invoked to signal that the Client has completed its work and can respond back to the source of the initial request.\nSet the permissions While in the template editor, visit the Permissions tab.\n  Set the permissions as follows:\n Accesses response: Any Reads cookie value(s): _ga Reads request: Any Sets a cookie: Set as in the image above.  Once you\u0026rsquo;re done with the permissions, you can Save the template and exit the template editor.\nCreate the Client, trigger, and tag Next, go to Clients and click the New button to create a new Client.\nChoose your new Client template from the list of available Client types. Next, set the Priority field to a high value. It\u0026rsquo;s important that this value is higher than any other Universal Analytics Client you might have in the container.\n  Give the Client a descriptive name (you\u0026rsquo;ll need this shortly), and then save this Client.\nThen, go to the Tags UI and click the New button to create a new Tag.\nChoose Google Analytics: Universal Analytics as the tag type. There\u0026rsquo;s no need to configure this tag in any way.\nUnder Triggers, click the trigger area to select a trigger.\n  In the overlay, click the blue + button in the top right corner to create a new trigger.\n  Set the trigger to look like the above. It just needs the single condition. The name you\u0026rsquo;re checking against is the name you just gave the new Client.\n If you don\u0026rsquo;t see Client Name in the list of available variables, it means you haven\u0026rsquo;t enabled it as a Built-in variable yet. Select Choose Built-in Variable from the drop-down, and pick Client Name from the overlay that appears.\n Once done, save the trigger. Then, save the tag.\nYou now have the Client, the trigger, and the tag.\nYou are ready to test the whole setup!\nPreview and test Click the Preview button in the Server container. A new tab should open with the Server container\u0026rsquo;s Preview panel.\nIn your web container, similarly click the Preview button.\nNow you should have both your web container and the Server container in Preview mode.\nNext, browse to your site and do something to fire one of the Universal Analytics tags you designed for server-side collection. Once the tag fires, check the Preview panel of your Server container. You should see something like this:\n  If you see it, and there are no errors in the Errors tab, it should work!\nYou can debug further by selecting the Universal Analytics tag itself and then clicking the Outgoing HTTP Requests box. This opens the details for the request to Google Analytics.\n  Check the Real Time report in your Google Analytics view (you have created a separate property for your Server-side tagging hits, right?), and make sure data is flowing in.\nFinally, check the cookies in your browser. The _ga cookie should have the value of Secure set to true and the value of SameSite set to Lax. If that\u0026rsquo;s what you see, the cookie rewrite worked!\n  Summary There\u0026rsquo;s a lot of new stuff here, so I wouldn\u0026rsquo;t be surprised if this is difficult to set up. However, if you follow this guide diligently, and make sure there are no rogue tags firing without cookieUpdate, the Client should do its job nicely.\nIf you run into issues, let me know in the comments and we can take a look together.\nServer-side tagging is the new paradigm for Google\u0026rsquo;s tagging solutions. I can see a lot of amazing things you can do with it in the future, but right now you can also utilize it for improving the quality of your data incrementally with solutions such as this one.\n"
},
{
	"uri": "https://www.simoahava.com/tags/cookie/",
	"title": "cookie",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/send-google-analytics-requests-custom-endpoint/",
	"title": "#GTMTips: Send Google Analytics Requests To Custom Endpoint",
	"tags": ["google tag manager", "server-side tagging", "gtmtips", "app+web"],
	"description": "Send the requests compiled by the Google Analytics JavaScript library to a custom endpoint rather than the default of google-analytics.com.",
	"content": "When you use Google Analytics on the web, you are most likely implementing one of analytics.js, the global site tag (gtag.js), or Universal Analytics tags via Google Tag Manager.\nThese libraries all end up doing the same thing: compiling a payload-rich HTTP request to an endpoint at https://www.google-analytics.com.\nWhat if you want to have the JavaScript libraries do their job, but instead of sending the data to Google\u0026rsquo;s servers, you send them to a new, custom endpoint?\nPerhaps you\u0026rsquo;re already using Google Tag Manager\u0026rsquo;s new Server-side tagging setup, and you want to redirect your site\u0026rsquo;s Universal Analytics data collection through your new server-side proxy?\nOr maybe you\u0026rsquo;re using Snowplow Analytics, and you want to leverage its capacity to digest Google Analytics payloads as well.\nA recent update to gtag.js and Google Tag Manager has made it much easier to redirect the payload. In this #GTMTips article, we\u0026rsquo;ll take a look at this new field and how it works.\nTip 116: Redirect the Google Analytics payload to a custom endpoint   Within Google Tag Manager, the Universal Analytics template as well as the Google Analytics Settings variable have a new setting under Advanced Configuration.\nThe Set Transport URL setting expands a text field to which you can now type a base URL string.\n  A valid base URL is a URL string that begins with http:// or https:// and does not end with /.\nTypically, you\u0026rsquo;d just have a base hostname here, assuming your collector domain is housed on a subdomain mapped specifically for collecting the data. An example would be this:\nhttps://collector.simoahava.com\nIt\u0026rsquo;s possible to have the tracker embedded in a path as well, so this would be just as fine as a base URL:\nhttps://www.simoahava.com/collector\nHowever you build it, the payload will be sent to the base URL + /collect. A request might thus look like this:\nhttps://collector.simoahava.com/collect?v=1\u0026amp;t=pageview\u0026amp;tid=UA-12345-1...\nOccasionally, if you have advertising features enabled, the endpoint can be base URL + /r/collect. In some cases the endpoint can also be base URL + /j/collect, so you need to configure your server or service to account for these when configuring the collector APIs.\n NOTE! In a Server container, the extractEventsFromMpv1 API automatically intercepts all possible path variations of /collect, so you don\u0026rsquo;t have to manually configure anything if building your own custom server-side tagging Client.\n gtag.js The gtag.js library also supports this feature.\nIn gtag.js, the field is named transport_url, and it\u0026rsquo;s set in the tracker configuration:\ngtag(\u0026#39;config\u0026#39;, \u0026#39;\u0026lt;MEASUREMENT_ID\u0026gt;\u0026#39;, { transport_url: \u0026#39;https://collector.simoahava.com\u0026#39; });  All hits that utilize this tracker will now send their payloads to https://collector.simoahava.com instead of the endpoint represented by the MEASUREMENT_ID.\nanalytics.js If you have an inline analytics.js implementation, there is a field named transportUrl that you could use for this purpose:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {name: \u0026#39;ss\u0026#39;, transportUrl: \u0026#39;https://collector.simoahava.com\u0026#39;});  However, this is a bit of a relic and doesn\u0026rsquo;t have official support going forward. The transportUrl field doesn\u0026rsquo;t, for example, process the custom paths (/r/, /j/) correctly, which might lead to issues when working with ad integrations in your server-side endpoint.\nThere\u0026rsquo;s no official word if analytics.js will ever be patched to have feature parity with transportUrl, so your best option right now is to use either gtag.js or Google Tag Manager.\nSummary With the introduction of Server-side tagging in Google Tag Manager, having a way to redirect the Google Analytics call away from the GA servers to your own custom endpoint is a necessity. That\u0026rsquo;s why the Transport URL feature was released, and it should help a great deal in building your own event stream to your server-side container.\nHowever, Universal Analytics has been around for so long that the payload format (Measurement Protocol) it uses has been baked into countless analytics pipelines. It\u0026rsquo;s such a familiar schema that if I were to create a custom pipeline for analytics data collection, I would definitely include a processor for the Measurement Protocol format.\nThis setting should prove valuable to anyone working with a server-side tagging setup, where hits are proxied through a Google Tag Manager Server container rather than sent directly to Google\u0026rsquo;s servers. Or, perhaps you\u0026rsquo;ve built something that integrates the GA payload directly into BigQuery, or maybe you\u0026rsquo;re using Snowplow Analytics; regardless, this feature will make life a bit easier for you, in case you want to delegate the JavaScript tracker functionality to Google\u0026rsquo;s own libraries.\n"
},
{
	"uri": "https://www.simoahava.com/tags/google-cloud/",
	"title": "google cloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/server-side-tagging-google-tag-manager/",
	"title": "Server-side Tagging In Google Tag Manager",
	"tags": ["google tag manager", "server-side tagging", "google cloud"],
	"description": "An introduction to Server-side tagging in Google Tag Manager. The article contains examples and walkthroughs for getting started with the new Server container.",
	"content": "Ever since Server-side tagging was publicly announced at SUPERWEEK 2020, Google and the trusted tester community have been hard at work, building something that just might change the landscape of digital analytics for good.\nGoogle Tag Manager has now released Server-side tagging into public beta. In this lengthy article, we\u0026rsquo;ll take a look at what Server-side tagging is, how it should (and should not) be used, and what its implications are on the broader digital analytics community.\n  In short, Server-side tagging means running a Google Tag Manager container in a server-side environment (at the time of writing, the only available environment is the Google Cloud Platform, though I\u0026rsquo;m certain more options will become available in good time).\nMany of the benefits and concerns are tackled in their respective chapters. Even so, I want to emphasize that Server-side tagging has the potential to overturn the current dynamic of data collection and governance for an organization. You own and have full control over the server-side environment. You have access to tools and methods to thoroughly vet and validate the traffic between network sources and your advertising and analytics endpoints.\nYou can run a fully functional digital analytics and marketing setup without loading any third-party code in the user\u0026rsquo;s browser or device. With appropriate monitoring in place, you can say goodbye to PII and credential leaks, cross-site tracking traps, and bloated third-party JavaScript encumbering the client.\nServer-side tagging utilizes many of the concepts familiar to Google Tag Manager users:\n  There are tags which fire on triggers and pull in data from variables.\n  New container versions can be previewed and published.\n  Users can create their own custom templates.\n  However, there are new, fundamentally different features that introduce something of a paradigm shift to the type of dynamic tagging that Google Tag Manager promotes.\n  The container itself is a new Server type; different from the web, app, and AMP containers that precede it.\n  Instead of trigger events, processes are initialized by incoming HTTP requests.\n  These requests are digested by a new type of GTM entity: a Client.\n  The Client parses the requests, generates an event data object, and feeds this into a virtual container, where tags can use this event object to map and send data to their endpoints.\n    This article will not be an exhaustive guide. I will walk you through the main concepts of Server-side tagging and there should be little you\u0026rsquo;ll be left wanting, but to complement this article, I do recommend you consult Google\u0026rsquo;s own, official documentation.\nHow to follow this guide While I hope everyone would devour every last word of this article, I\u0026rsquo;m aware that not all sections are relevant to all readers.\nIf you\u0026rsquo;re looking for an overview of Server-side tagging, perhaps for getting buy-in within your organization, I recommend reading these chapters:\n What is Server-side tagging Key benefits - Reduced client load Key benefits - Content Security Policy Key benefits - Full control and ownership of the data collected by the container Key concerns - All chapters Technical outline - Cost Technical outline - Custom domain Summary  If you\u0026rsquo;re a developer or working in IT, I\u0026rsquo;d recommend focusing on these chapters:\n What is Server-side tagging Key benefits - Reduced client load Key benefits - Keep keys and secrets safe Key benefits - More control over what endpoints collect Key benefits - Content Security Policy Key concerns - All chapters Technical outline - Server container Technical outline - Custom domain Technical outline - Clients and tags Technical outline - Custom templates Technical outline - Resources Summary  Everything else is still important, but I\u0026rsquo;ll forgive you if you gloss over them initially, only to return to them hungrily once you\u0026rsquo;re hooked into all the awesomeness that Server-side tagging brings in its wake.\nI recommend you watch the following two videos regardless.\nThe first one is a general introduction to Server-side tagging, focusing on deployment and getting started with your first Client and tag.\nThe second is a deep-dive into building your own Client template. It\u0026rsquo;s a bit more specialized and can thus be skipped if you\u0026rsquo;re not interested in customizing the container.\nVideo: Introduction to Server-side tagging   If the video doesn\u0026rsquo;t work, you can watch it here.\nVideo: Create a Client template in a Server container  NOTE! The video below has one important omission. When creating the Client template, make sure to update the \u0026ldquo;Sends HTTP Requests\u0026rdquo; permission to include \u0026ldquo;Allow Google Domains\u0026rdquo;. Otherwise the proxying of analytics.js doesn\u0026rsquo;t work.\n   If the video doesn\u0026rsquo;t work, you can watch it here.\nWhat is Server-side tagging?   With Server-side tagging, Google Tag Manager has introduced a new Server container type, which resides in a Google Cloud environment.\nIn a nutshell, the purpose of this setup is to create an endpoint in a server environment that you own. It will act as a sort of a proxy between the hits sent from browsers and devices and the actual endpoints to which the hits are collected. See the next chapter for more details on what this type of proxy can do.\nThe container itself operates as an HTTP API endpoint, to which any browser, device, or other sources that support the HTTP protocol can send requests.\nIdeally, this endpoint would be mapped with a custom subdomain in the same domain hierarchy as the website sending the requests. That way the requests are considered to happen in first-party context, which has a significant impact on how cookies can be read and written, for example.\nWithin the Server container, workers known as Clients are configured to listen for these incoming HTTP requests, which they then parse into a unified event format. The Clients then run a virtual container with the event data object, where tags, triggers, and variables react to the event push similar to how they would with \u0026ldquo;regular\u0026rdquo; Google Tag Manager.\nTags take the information in these event data objects and compile them into HTTP requests to their respective endpoints. Finally, the Client sends an HTTP response back to the source of the initial request.\n Example of a Universal Analytics client responding with a success status and setting the _ga cookie in the response.  All of the above happens within the confines of the Server-side tagging environment. The only way the browser or app sending the data can be made aware of what\u0026rsquo;s going on is if the Client adds information into the HTTP response, which is fully configurable.\nKey benefits Here are some of the key benefits of using Server-side tagging.\nReduced client load By running the logic of building and dispatching hits to the vendor endpoint in your server-side environment, you have a golden opportunity to reduce the amount of (especially third-party) JavaScript run in the user\u0026rsquo;s browser.\nBecause you can configure the Server container to map any incoming HTTP request into the format required by the vendor, you can theoretically reduce your entire third-party pixel and JavaScript load to a single event stream directed into your Server container.\n Imagine if you could reduce the amount of JavaScript loaded and executed in the browser...  This stream can then be intercepted by a Client which proceeds to map the stream into the event model expected by the vendor tags, also running in the Server container.\nThis is the ultimate benefit of a Server-side tagging setup. Even if you don\u0026rsquo;t want to reduce everything to a single stream, you can build your own custom template in the web container, which builds the HTTP request to the Server container without having to load any third-party JavaScript at all (apart from the GTM library itself).\nKeep keys and secrets safe By transporting data processing logic away from the device, where it would be visible for anyone with debugging skills, you will also be able to run secured and credential-based transactions without having to worry about exposing sensitive information to the device.\nFor example, a plague on Google Analytics has been Measurement Protocol spam, where malicious parties crawl potential tracking IDs and then proceed to spam them with automated HTTP requests that masquerade as \u0026ldquo;regular\u0026rdquo; hits from the site. Alternatively, these hackers send spam hits to random tracking IDs, knowing that if they send enough hits, some of them will end up in real Universal Analytics accounts.\nThis type of spam is notoriously difficult to identify and prevent because it\u0026rsquo;s built to resemble actual hits that are sent from the website itself.\nNow that you have the server endpoint handy, you can add a new Custom Dimension within the Server container, which is then sent to Google Analytics. In Google Analytics, you can then create a filter for this Custom Dimension, allowing only traffic that matches it.\nevent[\u0026#39;x-ga-mp1-cd11\u0026#39;] = \u0026#39;my_secret_key\u0026#39;;    By adding this \u0026ldquo;secret key\u0026rdquo; in the Server container, there\u0026rsquo;s no way that a random Measurement Protocol spammer can know it\u0026rsquo;s there. Similarly, it won\u0026rsquo;t help if the spammer crawls your site, looking at the requests sent to Google Analytics, because there are no such requests! There are only requests to your own server-side endpoint, and it would be odd if Measurement Protocol spammers would utilize those to fuel their spam algorithms.\nNaturally, this isn\u0026rsquo;t limited to just what you can do with Universal Analytics. Any third-party servers that identify your access with an API key or credential token can now be proxied through your Server container so that these keys are not exposed in the device!\nMore control over what endpoints collect Because your proxy now resides between the user\u0026rsquo;s device and the endpoint, you are in full control over what is shipped to the vendor.\nUnless the Client specifically overrides things like the IP address and User-Agent in the outgoing HTTP request from the Server container (this is what the built-in Universal Analytics client does by default), the IP and User-Agent string will be that of your Server container rather than the user. So this is a great way to concretely anonymize this aspect of the HTTP protocol that\u0026rsquo;s proven to be problematic in terms of end-user privacy.\n IP address and User-Agent string overridden in a Google Analytics request.  Server-side tagging introduces extra control over privacy simply by existing.\nWithout manual overrides, the requests sent to the vendors are mapped to the App Engine virtual machine instead of the user\u0026rsquo;s browser or device.\nThere are no data leaks with third-party cookies, there are no surprises with injected URL parameters, and the third-party service doesn\u0026rsquo;t have any connection with the user\u0026rsquo;s browser by default. They\u0026rsquo;ll be communicating just with the cloud machine.\nMore control over HTTP traffic To expand the features mentioned above, you will also have full control over what HTTP traffic is passed through the Server container.\nTypically, the browser loads the vendor\u0026rsquo;s JavaScript from their content distribution network (CDN).\nThis act already exposes the user\u0026rsquo;s browser or device to the third party and can lead to things like personally identifiable information (PII) leaks in case the URL of the page has sensitive information.\n It\u0026#39;s better to leak this to your data store rather than a vendor\u0026#39;s.  Because you now have a proxy between the device and the endpoint, the only place where this information is leaked is into your cloud environment.\nSure, it\u0026rsquo;s still not optimal - PII leaks should be eradicated.\nBut you have full control and ownership of all the data collected by the Server container, and you also have all the tools at your disposal to clean up and validate the payloads.\nYou can also cover your legal back by removing fingerprintable surfaces from the outgoing requests from the Server container. Similarly, you can use the APIs available to hash potentially sensitive data. You can also, of course, look for consent strings in the user\u0026rsquo;s cookies (assuming the Server container is in first-party context with the source of the traffic) and act accordingly.\nFinally, in the Client, you can modify the HTTP response back from the Server container to the browser or device. This is a pretty cool thing when considering Apple\u0026rsquo;s Intelligent Tracking Prevention, for example. You can convert cookies written with JavaScript, and thus subject to an expiration limit of 7 days, into HTTP cookies written with a Set-Cookie header, thus extending their lifetime to whatever you choose:\nconst getCookie = require(\u0026#39;getCookieValues\u0026#39;); const setCookie = require(\u0026#39;setCookie\u0026#39;); // Get cookie from the HTTP request let ga = getCookie(\u0026#39;_ga\u0026#39;); // If no cookie exists, generate a new Client ID ga = ga \u0026amp;\u0026amp; ga.length ? ga[0] : generateClientId(); // Write the _ga cookie in the HTTP response setCookie(\u0026#39;_ga\u0026#39;, ga, { domain: \u0026#39;auto\u0026#39;, \u0026#39;max-age\u0026#39;: 63072000, path: \u0026#39;/\u0026#39;, secure: true, sameSite: \u0026#39;lax\u0026#39; });  Ideally, you\u0026rsquo;ll want to set cookies with the HttpOnly flag. This prevents the web page from accessing the cookie with JavaScript (document.cookie). By allowing cookie access only for the webserver receiving the request, you\u0026rsquo;re introducing a decent redundancy measure for mitigating cross-site scripting attacks and preventing cookie values from leaking into cross-site tracking schemes.\nThe reason we\u0026rsquo;re not using HttpOnly in the example above is because cross-domain linking is something Universal Analytics still does client-side with JavaScript.\n Note! You might want to read this article on FPID to see what Google is working on in terms of improving the security of the Google Analytics cookie.\n In any case, using the Set-Cookie header like this removes the need for complicated APIs to do the cookie rewriting for you, as you can just piggy-back the cookie rewrite on the data collection itself.\nContent Security Policy Another benefit of reducing the number of HTTP endpoints with which the browser communicates concerns your site\u0026rsquo;s Content Security Policy (CSP). A CSP is what your site would use to restrict the HTTP traffic to and from the user\u0026rsquo;s browser.\nFor example, if you add a JavaScript library that loads its content from Facebook to a site with a CSP, you\u0026rsquo;ll need to petition the developers to relax that CSP so that Facebook\u0026rsquo;s domains would be allowed to send and receive data from the user\u0026rsquo;s browser.\nNaturally, the more you relax the CSP, the less efficient it becomes.\n CSPs tend to get really bloated and thus inefficient at doing what they\u0026#39;re meant to do.  By reducing the number of HTTP endpoints the browser needs to communicate with (because you\u0026rsquo;ve replaced them with your own Server-side tagging endpoint), you\u0026rsquo;re making the CSP more robust as a result.\nClean up and validate payloads Even if you\u0026rsquo;ve managed to clear the HTTP traffic itself of all potentially harmful information, you might still be left with URL parameters that the vendor requires you to populate. Sometimes, often even, these parameters contain PII, and you\u0026rsquo;ll want to figure out a way to get rid of it.\nI\u0026rsquo;ve written a lot about PII purging, and my customTask solution should be useful if you\u0026rsquo;re sending data from the browser directly to Google Analytics.\n  But now with a Server container, you can build a Client which parses all the request headers and the body looking for PII (so not just those related to Universal Analytics) and then proceeds to clean it up or obfuscate it.\nYou can also use the Server container to validate and fix requests.\nFor example, if you send a non-numeric value as the Event Value of a Universal Analytics request, that event will be collected by Google Analytics but discarded at processing. There\u0026rsquo;s no warning in the browser - these hits just disappear.\nYou could fix this in a Client by looking specifically for a faulty Event Value and converting it into a number:\n Try to convert Event Value into a number, and default to 0 if conversion doesn\u0026#39;t work.  You can see how this could dramatically improve your data quality once you start building Clients specifically designed for cleaning up your data streams.\nFull control and ownership of the data collected by the container This has already been mentioned earlier in this article, but a significant part of building a server-side environment is mapping a subdomain to the endpoint. When the HTTP endpoint is able to respond to requests using a subdomain that\u0026rsquo;s part of the same domain hierarchy as the website sending the requests, the website and the HTTP endpoint exist in same-site or first-party context. This has a significant impact on how browser tracking protections treat the traffic.\n The custom domain should be mapped using A/AAAA DNS records rather than a CNAME alias. The latter is a less useful solution for cookie permanence due to browser tracking protections.\n Other than the question of domain context, a very important aspect of ownership is what\u0026rsquo;s promised by the platform you subscribe to (Google Cloud Platform at the time of release).\nYou have full control and ownership of the data in your Google Cloud project. Yes - you need to trust Google on this promise. Here\u0026rsquo;s what they guarantee in the relevant documentation:\n  Google Cloud only processes data that you instruct it to process.\n  You own your data. No data is processed by Google Cloud for advertising purposes.\n  You\u0026rsquo;ll always be aware of where your data is regionally located.\n  Your data is secured by independently certified and audited security standards.\n    This is pretty significant. Since you own and control the data collected by the Server container, its usage and data processing falls under the privacy policies, T\u0026amp;Cs, and contracts your organization has with its customers and end-users.\nIf a data leak were to happen, for example, the first place it would \u0026ldquo;leak\u0026rdquo; to would be a data store that you own, and you can mitigate the fallout by making sure these leaks do not extend to the third parties to which you send the data from the Server container.\nNaturally, as soon as your Server container does fire tags that send the data to third parties, you introduce additional data processors and controllers to the mix, but having the \u0026ldquo;buffer\u0026rdquo; of a data store and processor that you own in between should help a great deal in mitigating security/privacy risks and liabilities.\nLimitless possibilities By shifting the processing of data to your Server-side endpoint you introduce a fairly inexpensive, scalable, and multi-functional proxy for processing data before it is wrapped up and sent to the vendors.\nIn addition to the benefits listed above, there are so many things you could do with a server-side setup like this:\n  Only collect the bare essentials from the browser: content ID for content, session ID for session, and user ID for user. In the Server container, use other APIs and services at your disposal to enrich the data using these IDs as the keys.\n  Run expensive code in the scalable environment of the server rather than as a burden on the user\u0026rsquo;s device. Things like IP lookups and cryptographic hashing could just as well be done in the Server container.\n  Maybe at some point we\u0026rsquo;ll see native integrations to other Google Cloud products.\n  Imagine being able to write directly to Google BigQuery from the Server container without having to worry about complicated authentication.\n  Imagine triggering Cloud Functions by using Pub/Sub rather than HTTP requests.\n  Imagine utilizing Cloud logging to build a real-time monitoring system for the integrity of your data pipeline.\n    Once the platform matures and once the library of available APIs and custom templates is extended, the reach of Server-side tagging is only limited by the imagination of its users.\nKey concerns Moving tracking away from the device to behind the veil of the server doesn\u0026rsquo;t come without its concerns.\nThe paradigm shift that we can envision with GTM\u0026rsquo;s Server-side tagging isn\u0026rsquo;t just one of improving data collection; it\u0026rsquo;s also one of obfuscating it.\nCircumvent content blocking One of the first knee-jerk reactions many probably have to Server-side tagging has to do with content blocking and browser tracking protections in general.\nA typical approach for privacy-friendly browsers is to restrict or downright block communications between the browser and known trackers. The list of known trackers is usually based on a blocklist such as Disconnect.Me, but it could also be algorithmic and on-device, such as with Intelligent Tracking Prevention.\nIndeed, an endpoint like google-analytics.com could well be targeted by the heuristics used in content blockers, but my-server-side.domain.com probably isn\u0026rsquo;t.\n Direct hit to GA is blocked, but hit proxied via the Server container isn\u0026#39;t.  Can you use Server-side tagging to circumvent content blockers? Absolutely. Should you? Definitely not; at least if that\u0026rsquo;s your primary reason.\nHowever, this does raise an interesting paradox.\nIt\u0026rsquo;s not your fault that content blockers do not target your domains.\nYou are not obliged to exhaustively test if all the actual endpoints are blocked by the browser. That would be a huge waste of resources and counter-productive to what Server-side tagging first and foremost does: reduce client load.\nOnce server-side proxies become the norm (with a popular tool like Google Tag Manager likely to spearhead the transition), content blockers will adapt their heuristics to not just look at domains but also the information that is being sent.\nThe right course of action is to be transparent at what data is being collected on your site, placing behind consent that which is required by law, and giving opt-out mechanisms for the rest.\nAnd if it just so happens that your endpoint gets blocked by content blockers or its URL string is stripped of all useful information, don\u0026rsquo;t try to \u0026ldquo;fix\u0026rdquo; this.\nAlways err on the side of maximum privacy.\nAlways assume that the user knows exactly what they are doing when they choose to block your data collection. Don\u0026rsquo;t defy their wishes.\nOpaque data collection When there\u0026rsquo;s a data leak or a security breach in a company, the party uncovering this is often not related to the company at all.\nThe web is full of companies and individuals who exhaustively audit the HTTP traffic in and out of websites, and their work has been instrumental in uncovering things like Magecart attacks and domain takeovers.\nCookie leaks, cross-site scripting injections, CSP workarounds, and all manner of nasty JavaScript hacks can typically be audited directly in the browser, because the vendor scripts are running right there under the watchful eyes of the auditors.\nWhen you move to Server-side tagging, you are reducing the amount of third-party JavaScript running on the site, which is good. It\u0026rsquo;s a great step to mitigating the issues listed above.\nHowever, you are also removing all traces of what is actually done with the data bundled in the requests. Auditors will have a hard time deciphering just what the event stream to your endpoint actually does, and whether you are compromising the user\u0026rsquo;s right to privacy and security behind the veil of the server, where client-side tools can\u0026rsquo;t reach it.\nThis means that you must document carefully what type of data is being collected and processed on your site. You are already obliged to do so under legal frameworks like GDPR and CCPA/CPRAA, which require you to be upfront and transparent about data collection, storage, and processing.\nAlways err on the side of maximum privacy.\nYou should take preemptive and proactive measures to do transparency and compliance right in order to avoid litigation and potential brand damage when you get caught in the act.\nConsent management is up to the admin This isn\u0026rsquo;t that far removed from what the situation is currently with client-side scripts, but it\u0026rsquo;s still something you need to consider.\nConsent management is a hot topic, and rightfully so. Many sites implement client-side consent management tools, which require opt-in input from the user with regard to what data they allow to be collected from them.\nTypically the consent string is stored in a cookie or localStorage entry, and many vendors can actually proactively react to consent frameworks such as IAB\u0026rsquo;s Transparency \u0026amp; Consent Framework 2.0.\nWhen you move to Server-side tagging, you might have just a single stream of events from the browser to the server. This single stream can be split into dozens and dozens of advertising, marketing, and analytics requests in the Server container.\nThe templates running in the Server container won\u0026rsquo;t be able to leverage client-side consent APIs such as those suggested by TCF. Instead, you need to build the mechanism of interpreting and parsing user consent manually in the container itself.\nPossibly, and hopefully, Google will introduce tools that make this process easier. However, until then you need to make sure that when consent is collected in the browser or device, it is respected in the server as well.\nCost The cost of running a Server-side tagging is large or small, depending on what you\u0026rsquo;re comparing it to.\nIt\u0026rsquo;s large compared to just running the scripts in the browser, thus accumulating zero extra cost.\nIt\u0026rsquo;s small compared to all the benefits you\u0026rsquo;ll get in return for setting up the container, at least I like to think so.\n Forecasted cost with a small throughput  Running a three-instance App Engine setup for my site of modest traffic puts me back about 120€ per month. For me, this is acceptable considering I get more control over the data collection on my site.\nPoor availability of server-side endpoints For the Server container to work as a replacement for your browser- or app-based tracking, the vendors you want to send data to need to be able to collect the HTTP requests sent by the Server container.\nThis isn\u0026rsquo;t necessarily a big issue - vendors always have an HTTP endpoint to which their JavaScript library sends data, and many support simple image pixels for collecting the GET requests.\nHowever, many vendors also stuff the browser with super complicated and heavy JavaScript. If you want to work towards reducing the amount of third-party crap loaded in the browser, the vendor should provide a means to build the payload manually, without having to load their bloated JavaScript libraries.\n  For example, Facebook has a large and very complex set of client-side libraries they want you to download when working with their pixel. The purpose of the library is to let you communicate with Facebook\u0026rsquo;s servers using the fbq() command queue.\nLuckily, Facebook also offers the Conversions API, which specifies a format for the HTTP request with the conversion information. By deciphering the Conversions API documentation, anyone can build their own event stream from the device to the Server container to Facebook without having to load any Facebook JavaScript at all.\nThen there are services like HotJar that are so tightly coupled with client-side interactions that it is unlikely you can ever run HotJar without having to download their JavaScript. It will be interesting to see how vendors like HotJar adapt to a tagging environment that is run completely in the server.\nTechnical outline When you follow the instructions to provision a Google Tag Manager server-side environment, the scripts automatically create a single Standard App Engine instance.\n NOTE! In early beta, there was also the option to load the Server container in a Kubernetes cluster of Compute Engine instances. This is a more advanced setup designed for users who already have a pipeline running in Google Cloud, and they want more control over it than what App Engine\u0026rsquo;s managed environments can offer.\n App Engine is a managed virtual machine platform running in the Google Cloud. By using the Standard environment and a single instance, you can test-drive your Google Tag Manager setup most likely without even expending the free quota you have available.\n  However, as soon as you\u0026rsquo;re ready to deploy the new Server container into a full production environment, you should guarantee best performance and uptime by transferring to a Flexible App Engine environment and increasing the number of instances to a minimum of three. By doing this, you increase the throughput and performance of your server-side endpoint, and you guarantee that it is able to shoulder the incoming load. Follow these instructions for more details.\n  In addition to provisioning extra instances, you should also map a custom domain to the endpoint, preferably one that is a subdomain of your main site.\nCost It\u0026rsquo;s difficult to say what the exact cost for your setup will be, but rest assured that there will be costs associated with production usage.\nAs an example, I\u0026rsquo;m running App Engine in a Flexible environment, using three instances (the minimum recommended setup for production use). The cost associated with this setup is around 4 euros per day.\n  I\u0026rsquo;m only collecting Universal Analytics Page Views to this Server container. As you can see, the graph is pretty much steady regardless of the amount of hits coming in (my site averages just 0.2 requests per second).\nMy site has a visible dip in pageviews over weekends, with around 8,000 pageviews sent over a typical weekday and just one fourth of that over a Saturday or a Sunday. However, these dips don\u0026rsquo;t reflect in the cost of running my current Server container, which means I could probably scale the setup down a little, but on the other hand I fully intend to add additional measurements, so I\u0026rsquo;d have to scale back up anyway.\nWhen you compare the cost forecast above with a Server-side tagging setup that collects around 60 requests per second, we\u0026rsquo;re talking at around 250€ per month instead.\n Forecasted cost with a larger throughput  I hope at some point Google releases case studies and statistics, or even a tool, which allow you to estimate the cost and scale up or down accordingly.\nServer container The Server container itself is visually reminiscent of any Google Tag Manager container.\n  The main difference is the new Client asset type you can see in the left-hand menu. I\u0026rsquo;ll explain more about clients in the associated chapter.\n When using the term incoming HTTP request, I\u0026rsquo;m referring to the HTTP request that is sent from a device or browser to the Server container. When using the term outgoing HTTP request, I\u0026rsquo;m referring to the HTTP request built and dispatched by tags firing in the container.\n Tags Tag-wise there\u0026rsquo;s not much there, yet.\n  There\u0026rsquo;s the native Universal Analytics and App + Web templates, both configured to digest data pushed by their respective Clients. The HTTP Request tag lets you create an outgoing HTTP request to any destination.\nThen there are all the custom tag templates people can imagine creating. Almost any service that accepts HTTP requests can be configured into a custom tag template in the Server container.\nTriggers There\u0026rsquo;s a noticeable lack of available triggers. In fact, there\u0026rsquo;s just a single Custom trigger type.\n  The fact is that a Server container would not be associated with arbitrary triggers such as \u0026ldquo;Page View\u0026rdquo;, \u0026ldquo;Click\u0026rdquo;, or \u0026ldquo;Video\u0026rdquo;. Instead, any tags triggering in a Server container would only trigger if a Client instructed them to do so.\nA Server container is also unrelated to client-side labels such as a \u0026ldquo;page load\u0026rdquo; or a \u0026ldquo;container load\u0026rdquo;. It\u0026rsquo;s running all the time - it\u0026rsquo;s not reset when the page is refreshed. Thus there are no triggers related to the lifecycle of a Server container, though that doesn\u0026rsquo;t mean there won\u0026rsquo;t be at some point.\nVariables The available Built-in variables are:\n   Variable name Description Example     Query String Returns the query string of the incoming HTTP request. v=1\u0026amp;t=pageview\u0026amp;tid=UA-12345-1...   Request Method Returns the method of the incoming HTTP request. GET   Request Path Returns the path of the incoming HTTP request. /collect   Client Name Returns the name of the Client currently processing the request. Facebook Client   Container ID Returns the ID of your Server container. GTM-XXXXXX   Container Version Returns the current version of your Server container. QUICK_PREVIEW   Debug Mode Whether the container is in Preview mode or not. true   Random Number Returns a random positive integer. 12345   Event Name Returns the value of the event_name field in the event data object that was passed to the container. page_view    These can be used to parse information about the incoming request and to retrieve metadata about the event that fired and the container itself.\nAvailable User-defined variables are:\n   Variable name Description Example     Cookie Value Set to the value of the first cookie that matches the name. GA1.2.12345.12345   Query Parameter Set to the value of the first query parameter in the incoming HTTP request that matches the name. UA-12345-1   Query String Returns the query string of the incoming HTTP request. Note! Use the Built-in variable instead. v=1\u0026amp;tid=UA-12345-1\u0026amp;t=pageview...   Request Header Returns the value(s) of the header name from the incoming HTTP request. https://referrer-page.com/   Request Method Returns the method of the incoming HTTP request. Note! Use the Built-in variable instead. POST   Request Path Returns the path of the incoming HTTP request. Note! Use the Built-in variable instead. /j/collect   Client Name Returns the name of the Client currently processing the request. Note! Use the Built-in variable instead. Universal Analytics   Constant Returns whatever string you type into the variable. UA-12345-1   Event Data Returns the value of the key in the event data object. 123.123.123.123   Event Name Returns the value of the event_name field in the event data object that was passed to the container. Note! Use the Built-in variable instead. page_view   Random Number Returns a random positive integer. Note! Use the Built-in variable instead. 123123   Container ID Returns the ID of your Server container. Note! Use the Built-in variable instead. GTM-ABCDE   Container Version Number Returns the current version of your Server container. Note! Use the Built-in variable instead. 13   Debug Mode Whether the container is in Preview mode or not. Note! Use the Built-in variable instead. false    You can, and should, utilize custom templates to build your own variable types.\nCustom domain You are strongly encouraged to map a custom domain to your Server container endpoint.\nThe main reason is that this way you can incorporate the server-side data collection endpoint that you own in your first-party domain namespace. For example, I\u0026rsquo;m using sgtm.simoahava.com as the host of the Server container.\nThis becomes significant when you consider things like Intelligent Tracking Prevention. You might want to make use of cookies in the incoming requests, but if the Server container is hosted on a domain that is different from where the requests are sent (such as your site), these cookies will be considered third-party cookies and thus dropped by many browsers.\n NOTE! Due to upcoming changes in ITP, you should map the domain as a newly verified subdomain. This way you\u0026rsquo;ll be instructed to use A/AAAA DNS records rather than the vulnerable CNAME alias.\n   Similarly, having the endpoint in your first-party domain namespace means you can do things like set first-party cookies with Set-Cookie headers and thus avoid Safari expiring them within 7 days of being set.\n Note that technically you can have more than one domain pointing to your App Engine deployment. This is helpful in case you have a single server-side container responding to requests from multiple domains. The only \u0026ldquo;catch\u0026rdquo; is that some features of the Server container, such as for which domain the Preview mode is shown, are restricted to just one domain. It won\u0026rsquo;t hamper the actual data collection, but it might make it difficult to use some of these features.\n Requests and responses Server-side tagging revolves around incoming HTTP requests to the server container being mapped by a Client, passed to a tag, and then dispatched as an outgoing HTTP request to the tag vendor. Once all the tags have fired for a given Client, the Client sends an HTTP response back to the origin of the request, such as a browser, an app, or some other connected service.\nThis flow is absolutely fundamental to understanding how Server-side tagging works. A \u0026ldquo;perfect\u0026rdquo; end-to-end pipeline would be one where the requests are carefully sculpted to make use of as little client-side code as possible; The Clients are designed to handle both vendor-specific and vendor-agnostic requests; The tags are built to trigger off specific clients\u0026rsquo; event data payloads, finally responding to the Client whether the outgoing HTTP request was a success or not.\nThe easiest way to map a Client to an incoming request is to observe the Request Path. The built-in Universal Analytics Client, for example, is primed to listen to requests that have the /collect path or any of its permutations (such as /j/collect or /r/collect). You could create a Facebook Client that listens for a custom path of /fbook, and a HotJar Client that listens for requests with the path of /hotjar.\n Client looks for a request to /fbq and if one is found, claims the request.  Alternatively, you could approach a single event stream model, where all requests are sent to the same Client. In this case, they would have just a single path, such as /events, and you would configure the Client to parse the request body and turn it into event data objects that are passed to the container.\nWhatever you choose, you need to remember that whatever origin sent the request is actually waiting for a response back from the Server container.\nBy default, the response is a fairly nondescript text/html response, but you can jazz things up using the following APIs.\n setCookie lets you write cookies in the Set-Cookie header of the HTTP response. setPixelResponse automatically configures the response to mimic a 1x1 GIF pixel with headers that prevent caching. setResponseHeader can be used to add any custom HTTP headers to the response. Consider headers like Access-Control-Allow-Origin and Access-Control-Allow-Credentials, which are useful for preflight requests, for example.   Request with setPixelResponse() and setCookie() APIs.  Hopefully at some point we\u0026rsquo;ll see more options for manipulating the request as well. We could envision Clients whose only purpose is to purge the incoming request from PII, before passing the request on to the next client that actually builds the event object for tags to dispatch, now cleared of all PII.\nClients and tags We\u0026rsquo;ve already talked a lot about Clients and tags, but it\u0026rsquo;s good to reiterate as the concept might be a bit alien - especially if you\u0026rsquo;re used to how GTM for the web works.\nThe purpose of a Client is to listen for incoming HTTP requests, parse them into a unified event model, and then run a virtual container with the event model, so that tags can use the details in these event objects to compile the requests to their endpoints.\nBecause Clients do all the legwork, you can start streamlining the event stream itself, moving away from vendor-specific request parsing (which might require vendor-specific JavaScript to run in the user\u0026rsquo;s browser, for example), and leaning towards a more agnostic approach, where a single event stream can be distributed into multiple unrelated endpoints.\nClients operate on a Priority order. The higher the priority of a Client, the sooner it gets to check if the request is its to claim. To claim a request means calling the claimRequest API. This, in turn, means that the current Client tells all the other Clients that this request is MINE! and doesn\u0026rsquo;t allow any other Client to parse the request anymore.\n The Client with Priority 100 always has first dibs on the request.  Once the Client has parsed the request and built an event object out of it, the event is passed to the runContainer API. With this API, the event is passed to the tags to evaluate and potentially trigger with.\nTags can be set to trigger on numerous different things, but most likely you will end up using a combination of Event Name and/or Client Name.\nThe Event Name is something that gtag.js, Firebase Analytics, and more recently App + Web iterated and introduced to the world of web analytics.\nBasically, there\u0026rsquo;s an inventory of suggested or quasi-standardized Event Names, such as page_view, search, and login. Then there is always the opportunity to use a custom Event Name such as new_level_achieved.\nWhen the Client builds the event model, it has to provide an Event Name. This is what ends up showing up in the Preview screen when the request is claimed and mapped by a Client:\n  So if you wanted to fire a bunch of tags whenever a page_view is collected, regardless of Client, you\u0026rsquo;d simply use a trigger that checks the Event Name. You\u0026rsquo;d then just need to assume that the Client has correctly intercepted the incoming HTTP request, and has managed to parse and map it into an event object that can be understood by your tag (you can use Preview mode to analyze what the event data object contained).\n  Alternatively, perhaps you want your tag to fire only when the Facebook Client generated the event object. This is useful in case the tag requires a lot of custom information not available as standard parameters in the event object.\nBy referencing the Client Name, you can ensure that the tag only fires if the correct Client has claimed the incoming HTTP request, assuming the Client that executed runContainer also claimed the request (there might be some edge cases where this is not the case).\n  It might be difficult to wrap your head around Clients and tags, but once the inventory of templates for both multiplies in the community gallery, it will become easier to just use the community templates rather than having to worry about how to build your own.\nEvent model When the Client parses an incoming HTTP request it has claimed, it needs to map values in the request body (typically in a query string) and produce an event data object, which looks something like this:\n{ event_name: \u0026#34;page_view\u0026#34;, event_parameter: \u0026#34;some value\u0026#34;, event_parameter_object: { nested_event_parameter: \u0026#34;some other value\u0026#34; } }  This object is what gets passed to the container with the runContainer API. Tags will then be able to use these values in the trigger, e.g. firing a tag only when event_name is page_view, and they\u0026rsquo;ll be able to map values in the event object to the outgoing HTTP request they dispatch to the vendor endpoint.\nTo keep things streamlined, Google suggests a set of standard event names and parameters that you should try to follow to make sure that the event data object passed to the container can be used with as little friction as possible. If the tags require parameters that are not available in the list of standard parameters, they should be prefixed with x-vendor-. Thus, for example, Facebook\u0026rsquo;s Subscription ID becomes x-fb-subscription_id.\nNote also the preference of snake_case vs. camelCase. It\u0026rsquo;s a syntactical format you should get accustomed to using when working with Server-side tagging.\nYou can always use the Server container\u0026rsquo;s Preview mode to audit what\u0026rsquo;s passed in the event data object by any given client. For example, when collecting a Universal Analytics Measurement Protocol hit, this is what you might see:\n  In the example above, many standard parameters are populated, such as client_id, ip_override, page_location, and user_agent.\nAdditionally, Measurement Protocol uses a number of custom parameters that are (more or less) unique to Google Analytics, such as x-ga-mp1-vp (viewport size), x-ga-measurement_id (web property ID with Universal Analytics), and x-ga-mp1-plt (page load time).\nThis event object is then digested by the Universal Analytics tag, which will be able to take these items and compile the outgoing Measurement Protocol request to Google Analytics. If the event object is correctly compiled, the tag can even utilize the shorthand API sendEventToGoogleAnalytics.\nCustom templates Server-side tagging relies heavily on custom templates. In addition to tag and variable templates, which are available in web containers as well, power users now have the opportunity to create Client templates as well.\nThe available APIs for these are listed in the documentation. Many APIs, such as runContainer have a footnote saying:\n It is recommended that this API be used from a client template.\n Roughly, Clients should typically utilize APIs that claim, validate, and parse the incoming HTTP requests, run the container with the event data object, listen for messages from the tags fired in the container, and finally modify and return a response back to the source of the incoming request.\nHere are some APIs that you\u0026rsquo;d typically run exclusively from a Client:\n   Client API Description     claimRequest Claim the request for the Client.   extractEventsFromMpv1 Parse an incoming Measurement Protocol v1 request, and extract event data objects from it.   extractEventsFromMpv2 Parse an incoming Measurement Protocol v2 request, and extract event data objects from it.   getCookieValues Get the values of all cookies with the given name in the incoming HTTP request.   getRemoteAddress Best-effort attempt to get the IP address of the incoming request.   getRequest* All the getRequest... APIs are designed to parse some aspect of the incoming HTTP request.   isRequestMpv1 Check if the incoming HTTP request is in Measurement Protocol v1 format.   isRequestMpv2 Check if the incoming HTTP request is in Measurement Protocol v2 format.   returnResponse Return the HTTP response with all the set headers back to the source of the incoming request.   runContainer Run the container with the event data object.   setCookie Populate a Set-Cookie header in the response.   setPixelResponse Automatically set response headers to mimic a 1x1 GIF pixel.   setResponse* All the setResponse... headers modify some aspect of the HTTP response finally flushed by the returnResponse API.     Note that you certainly can use some of these APIs in tags. For example, you could set aspects of the response directly in the tag itself. However, it might make sense to have tags communicate their status back to the Client with the sendMessage API, and use the Client to manage all aspects of the request-response flow.\n Tags should typically utilize APIs that parse the event data object, build an HTTP request to the tag endpoint, and message back to the container whatever metadata the outgoing request produced (such as a failed status code or success message).\nHere are some APIs that you\u0026rsquo;d typically run exclusively from a Tag:\n   Tag API Description     getAllEventData Get the event data object passed to the container.   getClientName Get the name of the current Client.   getEventData Get the value of a single key in the event data object.   sendEventToGoogleAnalytics Automatically build and dispatch an outgoing Measurement Protocol request from an event data object formatted correctly.   sendHttpGet Send an HTTP GET request.   sendHttpRequest Send a fully customizable HTTP request.     Note that you could run a Server container without a single tag. All the APIs designed to be used in tags could be run through a Client. But this is orthogonal to how Server-side tagging has been designed to work.\n Here\u0026rsquo;s an example of a Client running some of the recommended APIs:\nconst addMessageListener = require(\u0026#39;addMessageListener\u0026#39;); const claimRequest = require(\u0026#39;claimRequest\u0026#39;); const extractEventsFromMpv1 = require(\u0026#39;extractEventsFromMpv1\u0026#39;); const isRequestMpv1 = require(\u0026#39;isRequestMpv1\u0026#39;); const returnResponse = require(\u0026#39;returnResponse\u0026#39;); const runContainer = require(\u0026#39;runContainer\u0026#39;); const setResponseBody = require(\u0026#39;setResponseBody\u0026#39;); const setResponseHeader = require(\u0026#39;setResponseHeader\u0026#39;); const setResponseStatus = require(\u0026#39;setResponseStatus\u0026#39;); const setPixelResponse = require(\u0026#39;setPixelResponse\u0026#39;); // If Measurement Protocol request, claim and parse if (isRequestMpv1()) { claimRequest(); const events = extractEventsFromMpv1(); // Listen for message from tag signalling completion,  // set response headers accordingly.  addMessageListener(\u0026#39;ga_complete\u0026#39;, (messageType, message) =\u0026gt; { if (message.status === \u0026#39;error\u0026#39;) { setResponseStatus(500); setResponseBody(message.body); } else if (message.status === \u0026#39;redirect\u0026#39;) { setResponseStatus(302); setResponseHeader(\u0026#39;location\u0026#39;, message.location); } else { setPixelResponse(); } }); // Run the container with the parsed event object  let eventsCompleted = 0; events.forEach(event =\u0026gt; { runContainer(event, () =\u0026gt; { // If all the events in the incoming HTTP request have been completed  if (events.length === ++eventsCompleted) { returnResponse(); } }); }); }  And here\u0026rsquo;s what the corresponding tag might do:\nconst getAllEventData = require(\u0026#39;getAllEventData\u0026#39;); const sendEventToGoogleAnalytics = require(\u0026#39;sendEventToGoogleAnalytics\u0026#39;); const sendMessage = require(\u0026#39;sendMessage\u0026#39;); // Access the event data object const event = getAllEventData(); // Send the event to Google Analytics and parse the response sendEventToGoogleAnalytics(event, response =\u0026gt; { if (!response.success) { sendMessage(\u0026#39;ga_complete\u0026#39;, { status: \u0026#39;error\u0026#39;, body: \u0026#39;Request to Google Analytics failed\u0026#39; }); return data.gtmOnFailure(); } if (response.location) { sendMessage(\u0026#39;ga_complete\u0026#39;, { status: \u0026#39;redirect\u0026#39;, location: response.location }); } else { sendMessage(\u0026#39;ga_complete\u0026#39;, { status: \u0026#39;success\u0026#39; }); } data.gtmOnSuccess(); });  As you can see, the Client parses the incoming request and sends it as an event data object to the container. The container then triggers the tag(s), which map the event data object to a Measurement Protocol request.\nGoogle Analytics is a bit special in this case, because both the incoming request parser (extractEventsFromMpv1) and the outgoing request dispatcher (sendEventToGoogleAnalytics) have their own, dedicated APIs built. If you use a custom vendor endpoint, you need to actually manually write the code that turns an incoming request query string into the event data object, and which takes the event data object and maps it to an outgoing HTTP request.\nThe addMessageListener and sendMessage APIs are very useful, as they allow tags and Clients to communicate with each other. This is very helpful, in case you want the Client to encode information about tag execution in the response back to the request source.\n Note! The sample code above is a bit unwieldy, because it assumes runContainer to only trigger one tag. If there\u0026rsquo;s more than one tag firing, the addMessageListener callback would react to each tag messaging back, which means only the message from the last tag that fired would be considered for the response the Client sends back to the source of the incoming request.\n Preview and debug The Server container, just like a web container, has its own Preview and Debug mode. When you click the Preview button in the user interface, a new tab opens with the Preview interface.\n Just like with a web container, the Preview tab only shows hits that originate from your browser - and it has to be the same browser instance that started Preview mode.\n   In the left-hand side navigation, you see all the incoming HTTP requests and whether or not a Client has claimed and created an event data object from the request. In the screenshot, no event data object was created for the favicon.ico request, but both collect requests were claimed by a Client and turned into event data objects.\nIf you select a request, all the tabs in Preview mode will behave as if you\u0026rsquo;d selected Summary. In other words, you\u0026rsquo;ll be able to see how many times a tag would have fired for the request, but you wouldn\u0026rsquo;t be able to look into Variables or Event Data.\nFor this reason, whenever you debug, you should choose the event data object (e.g. page_view in the example) if available.\nRequest tab The Request tab contains information about the incoming HTTP request and the response sent back by the Server container. It will also show you if a Client claimed the request.\n  The very first line contains a summary of the HTTP request itself. After that is the value of the event_name field, parsed by the Client from the incoming HTTP request.\nThe Client box tells you which Client claimed the request.\nThe Incoming HTTP Request box opens up a new overlay when clicked, with details about the request.\n  The Request overview shows you the Request method (e.g. GET or POST), and the URL to which the request was sent.\nRequest Headers list all the HTTP headers present in the request, and the Request Body card shows what was sent as the body of the request (typically just in POST requests).\n  The Response overview has details about what the Server container responded with back to whatever source sent the incoming HTTP request in the first place.\nStatus Code indicates whether or not the request was a success (200).\nResponse Headers typically include things like cache headers for preventing the browser from caching the request, and Set-Cookie headers which write a cookie on the domain running the Server container.\nIf the response has a body, it\u0026rsquo;s displayed in the Response Body card.\nTags tab The Tags tab is pretty self-explanatory. You\u0026rsquo;ll see all the tags that fired (or did not fire) with this event data object. Similar to a web container, you can click open a tag to see what values it sent, and you can scroll down to its triggers to see why it didn\u0026rsquo;t fire.\n  One cool addition is the Outgoing HTTP Requests box. When you click it, a new overlay opens with details about the HTTP requests sent by the tag.\n  You can use this information to debug whether or not the hit to the vendor was dispatched correctly.\nVariables tab The Variables tab, similarly, tells you what the value of each configured variable in the container was when the container was executed with the event data object.\n  This is a useful list of information, as you can use it to debug why a tag might not have sent the correct values to the endpoint.\nEvent Data tab This is a very important tab to familiarize yourself with because it shows you all the values parsed from the incoming HTTP request into an event data object. You can use this with the Request tab to see which parameters might be missing or which were parsed incorrectly.\n  Errors tab If any tag throws an error, this tab would have more information about it. You can read more about the errors tab.\nResources Hopefully, this article will serve as a solid resource for you, especially when you\u0026rsquo;re getting your feet wet with Server-side tagging.\nIn addition to this, I\u0026rsquo;d like to direct you to the official documentation:\n Overview and deployment instructions Provision additional servers (minimum of three is recommended for production) Setup a custom domain API documentation for templates Suggested event_name values and parameters in the event data object  Summary I fully expect Google Tag Manager\u0026rsquo;s Server-side tagging to change the landscape of digital analytics. They\u0026rsquo;re not the first vendor to introduce a handy way of building server-side proxies, but they\u0026rsquo;re Google, and this service doesn\u0026rsquo;t come with a license cost.\nThere are obviously many concerns about Server-side tagging. Moving tracking behind the veil of the server will most certainly bristle some hairs. For this reason, I recommend you strive for absolute transparency when disclosing details about what tracking is going on either directly or indirectly in your digital assets and properties.\nI\u0026rsquo;m certain we\u0026rsquo;ll see a proliferation of new custom templates introduced specifically for the Server container, and I also expect most vendors in the adtech and martech space to facilitate server-to-server communication for their pixels and data collection endpoints.\nPlease let me and other readers know in the comments what you think of Server-side tagging. Do let me know if there are things in this article that require clarification.\n"
},
{
	"uri": "https://www.simoahava.com/tags/apps-script/",
	"title": "apps script",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/lookup-table-generator-google-tag-manager/",
	"title": "Build A Lookup Table Generator For Google Tag Manager",
	"tags": ["google tag manager", "google sheets", "apps script"],
	"description": "Step-by-step guide for building a simple Lookup Table generator for Google Tag Manager, utilizing Google Sheets and Apps Script.",
	"content": "In this step-by-step guide, I\u0026rsquo;ll show you how to build a Lookup Table generator in Google Sheets, utilizing Apps Script and the Google Tag Manager API.\n  The purpose of the Lookup Table generator is to automate the often tedious task of adding many, many rows to a Lookup Table within the Google Tag Manager UI. There are other solutions for this, but none (as far as I know) that uses the Google Tag Manager API.\nAlso, using Google Sheets is a no-brainer, because of the similarities between the Lookup Table variable and a spreadsheet. Both are organized into columns and rows. Google Sheets has a wonderful Apps Script integration, so interacting with the GTM API has been made very simple.\n This tool was inspired by the work I\u0026rsquo;ve been doing with Swappie, a company from Finland that refurbishes second-hand smartphones and has a marketplace that makes buying and selling these phones easy and safe. One of their use cases for web analytics is to collect product margin information into their Ecommerce reports, and this requires a daily updated data sheet where the product SKUs are mapped to their current product margins.\n Fetch the sheet template The First thing you\u0026rsquo;ll want to do is fetch the sheet template. Visit this URL:\nhttps://docs.google.com/spreadsheets/d/1LDJ6NOtMMx_5vwRBdPAx9V0gTLnce-7aglQzSm-Ja7M/\nThen, click the File menu and select Make a copy.\n  This makes a copy of the sheet in your drive, and this new sheet is what you\u0026rsquo;ll be working on.\nConfigure the sheet Next, configure the sheet. For the Lookup Table generator work optimally, you\u0026rsquo;ll need the following things:\n You should create a new Workspace just for the Lookup Table output. That way you don\u0026rsquo;t have to worry about messing with the Default Workspace (which should rarely, if ever, be used). You need to create the Lookup Table variable in the Workspace.  Once you have these two in place, you need to collect the Account ID, Container ID, Workspace ID, and Lookup Table variable ID. The easiest way to do that is to browse to Variables in the GTM UI, and then right-click the Lookup Table variable, and copy its link address.\n  If you explore that URL, it will look something like this:\n.../accounts/23019854/containers/8060344/workspaces/1000128/variables/730\nThe Account ID is the first number in that URL, i.e. 23019854.\nThe Container ID is the second number in that URL, i.e. 8060344.\nThe Workspace ID is the third number in that URL, i.e. 1000128.\nThe Variable ID is the fourth number in that URL, i.e. 730.\nAdd these to sheet in their appropriate places. Note! You can also type default as the Workspace ID, and it will automatically fetch your Default Workspace, assuming that\u0026rsquo;s the workspace you want to work in. Again, I recommend against this practice. It\u0026rsquo;s not how workspaces should be utilized.\n  Add the Apps Script code Next, in the Google Sheet, launch the Script Editor. Click Tools and then choose Script Editor.\n  If this is your first time using the Script Editor, you might need to jump through a few steps, but eventually you should see this:\n  The next thing to do is to rename the project. Click the Untitled project text in the top left corner, and rename the project to e.g. Lookup Table Generator. Once you click OK, the project will be saved, and you can access its details through script.google.com.\nNow, delete all the code in the editor, and replace it with the code copy-pasted from this gist.\n  Remember to click the Raw button to get the code in plain text format, ready to be copy-pasted into the script editor.\nCode walkthrough I\u0026rsquo;m not going to step-by-step you through the code, but I\u0026rsquo;ll briefly introduce what each method does.\n   Method Description     getIds Parses the Google Sheet for the IDs (Account ID, Container ID, Workspace ID, Variable ID) the user has added to their appropriate places.   getDefaultWorkspaceId If the user typed default as the workspace ID, or if the workspace ID they gave does not exist, the sheet falls back to the \u0026ldquo;Default Workspace\u0026rdquo;. This method fetches its workspace ID.   getLookupTable This method uses the Google Tag Manager API to fetch the Lookup Table variable the Variable ID points to.   sendData The data in the Google Sheet is collected, mapped to its correct API resource format, and sent to Google Tag Manager. The contents in the sheet are used to update (i.e. replace) the Lookup Table variable contents in the GTM container.   populateSheet This method uses the IDs (from getIds) to fetch the current contents of the Lookup Table variable into the Google Sheet.    Finalize Apps Script configuration In addition to the code, you also need to enable API access in the Script Editor. Click Resources and select Advanced Google services...\n  In the overlay that opens, scroll down to Tagmanager, make sure it has v2 selected as the version, and then enable it by clicking the toggle at the end.\n  Click OK when done.\nTest with the data fetch To quickly test if the code works, select the populateSheet function from the appropriate menu in the Script Editor.\n  Then, press the Play button just to the left of the \u0026ldquo;Bug\u0026rdquo; icon in the Script Editor toolbar.\nThe script should now prompt to request authorization. Click Review Permissions.\n  Follow the sign-in prompts until you see the \u0026ldquo;This app isn\u0026rsquo;t verified\u0026rdquo; prompt. This screen means that Google hasn\u0026rsquo;t verified this app yet. It\u0026rsquo;s using APIs that can be dangerous in the wrong hands, which is why Google warns about this app.\nSince we are building this for internal use only, you don\u0026rsquo;t have to worry about this prompt. You can click the Advanced link and then the Go to your project (unsafe) link.\n  Finally, click the Allow button at the bottom of the last screen in the flow.\n  At this point, the script will run. Once it\u0026rsquo;s complete, you can step back into your Google Sheet to see it populated with the Lookup Table variable contents.\n  Test with the data update Let\u0026rsquo;s try updating the variable with data from the sheet.\nDo whatever modifications you want with the sheet content first.\n Note! Do not touch the Input and Output headers. Start modifying the sheet from the row after these.\n   Next, go back to Script Editor, and this time choose sendData from the list of functions to run. Click the Play button again.\n  If all works well, you should now be able to see the modified Lookup Table variable in the workspace in the Google Tag Manager UI.\n  Map the buttons in the sheet Finally, to make the sheet a bit easier to use, let\u0026rsquo;s map the two buttons in the sheet to their corresponding functions.\nRight-click the Fetch data button, click the little action menu in the top-right corner and choose Assign script.\n  Type populateSheet in the prompt that opens and click OK.\n  Next, do the same for the Update data button, but instead of populateSheet type sendData into the prompt before clicking OK.\nNow, whenever someone clicks the Fetch data button, the script will populate the sheet with the variable details (assuming the user who clicks the button has authorized access to Google Tag Manager). Whenever someone clicks the Update data button, the script will update the Lookup Table variable in Google Tag Manager with the contents of the sheet.\n If you want to edit the button again, you need to right-click it, as left-clicking it will simply run the script.\n Summary I hope this proof-of-concept was easy to follow. With these steps, you can create a Lookup Table Generator for internal use.\nThere are limitations to how much the script can be run while unverified. You\u0026rsquo;ll want to take a look at this documentation to understand what steps need to be taken if you want to get rid of the security warnings.\nBasically, if the script is only intended for occasional use by the developer (you), or if the script is meant to be used only by users within your G Suite organization, you don\u0026rsquo;t have to verify the app.\nFor any other type of extended use, the app needs to be verified, and this is an intricate, often tedious process.\nGoogle Sheets + Google Apps Script is one of the most powerful, easily accessible automation flows out there. It requires minimal understanding of the Google Cloud Platform, as almost everything is abstracted in the Apps Script sandbox.\n"
},
{
	"uri": "https://www.simoahava.com/tags/google-sheets/",
	"title": "google sheets",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/about-simo-ahava/",
	"title": "About Simo Ahava",
	"tags": [],
	"description": "",
	"content": "   Hi, I\u0026rsquo;m Simo Ahava. I\u0026rsquo;m partner and co-founder at 8-bit-sheep.\nI have also been a Google Developer Expert for Google Analytics and Google Tag Manager since 2014.\nI hail from Espoo, neighboring Helsinki, the capital of Finland. I have a background in academics (English language and linguistics), in IT, in digital marketing, and in web development. I\u0026rsquo;ve been programming actively since 1997, and I built my first website the same year. Marketing, IT disciplines, and web analytics all fell into my sphere of interest shortly after.\nMy blog has a singular purpose: To tell complicated stories in a simple, understandable, and actionable way.\nMost often, I talk about web analytics, but I also have soft spots for digital marketing in general, for SEO, for web development, and for working with the Google Cloud Platform.\nI believe in the power of data and technology. I\u0026rsquo;m also a strong advocate for altruistic knowledge sharing. You won\u0026rsquo;t find a mailing list on this site (there\u0026rsquo;s an RSS feed for you), nor will you find me trying to peddle my wares or trying to push a commercial agenda. This blog is for transferring knowledge between me, my guest writers, and the community participating in the article comments (thank you!).\nBut enough about me. What about you? You\u0026rsquo;ve come to the right place if you\u0026rsquo;re:\n  looking for advice with Google Analytics or Google Tag Manager (just drop me a line in post comments and I\u0026rsquo;ll get right back to you)\n  interested in learning about the latest trends in digital marketing\n  curious about data and what it can do for you\n  searching for a speaker in your conference or seminar\n  Contact me You can contact me through all the means listed in the header / sidebar. I\u0026rsquo;m active all over the place: in Google Tag Manager\u0026rsquo;s subreddit, in Measure Slack, the Facebook Google Tag Manager community, the official product forums, and Google+ for as long as it lasts. I also try to use Twitter as much as possible.\nIn my spare time I play the ukulele, and I enjoy beer \u0026amp; board games. I also have a beautiful and wonderful wife, who makes everything worth it, and a son for whom I would drop everything else in a heartbeat.\n"
},
{
	"uri": "https://www.simoahava.com/tags/cname/",
	"title": "cname",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/cookies/",
	"title": "cookies",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/exploit/",
	"title": "exploit",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/web-development/",
	"title": "Web development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/web-development/whats-in-a-cname/",
	"title": "What&#39;s In A CNAME",
	"tags": ["cname", "exploit", "cookies"],
	"description": "Giving third-party services access to your domain namespace via CNAME redirects and other DNS records can potentially lead to severe data leaks and breaches. In this article, I take a closer look at this phenomenon.",
	"content": "With the rise of ad and content blockers (think Ghostery and uBlock Origin), as well as browser tracking protections (see www.cookiestatus.com), marketing technology vendors have their work cut out for them. And when I refer to \u0026ldquo;their work\u0026rdquo;, I mean they have to proactively identify and exploit any loopholes they can find to keep on collecting their precious data.\n  In this article, I\u0026rsquo;ll take a look at one such exploit vector, the Canonical Name (CNAME) DNS record, in particular.\nThe phenomenon of mapping subdomains to third-party services isn\u0026rsquo;t confined to just advertising and marketing technology industries, though. In this article, I\u0026rsquo;ll also introduce you to the extensive body of research collected by Zach Edwards.\nI wonder if it\u0026#39;s bad that an Intranet subdomain for the Victoria, Australia government has been compromised by the Pick a Flick gang with all kinds of custom phishing \u0026amp; weird stuff. I\u0026#39;m sure this DDOS is unrelated but it\u0026#39;s still interesting to see another massive gov hit by them. https://t.co/zg9jprJKqe pic.twitter.com/EqjNLZ9yNM\n\u0026mdash; ℨ𝔞𝔠𝔥 𝔈𝔡𝔴𝔞𝔯𝔡𝔰 (@thezedwards) June 19, 2020  This research shows how countless subdomains of high value and importance (think government, education) have been compromised and primed for stealing credentials written in first-party cookies.\nIt\u0026rsquo;s important to raise awareness of the risks involved with mapping subdomains to third party servers. Even though much of Zach\u0026rsquo;s research concerns scenarios where subdomains have been taken over by attackers, the end result is the same: the third party has access to data beyond what the site owner or business might have considered.\nBut before we dig into the risks themselves, it\u0026rsquo;s important to understand how we got to the point of surrendering our own domain namespace to third-party vendors and services.\nThe perils of the third-party The fact is that storage access in third-party context, colloquially referred to as \u0026ldquo;third-party cookies\u0026rdquo;, has become unstable and unreliable as the foundation for any data collection scheme, regardless of browser vendor:\n Safari blocks all third-party cookies. Sites and services can request access to third-party storage from the browser user with the Storage Access API. Brave blocks all third-party cookies. Firefox blocks third-party cookies on domains that are in their blocklist. Edge blocks third-party cookies on domains that are in their blocklist, with some mitigations. Chrome is planning to phase out support for third-party cookies by 2022. They are also rolling out a change to how browser cookies are processed in the browser, by defaulting all cookies to SameSite=Lax unless the cookie is manually updated with the appropriate flag when set.   SameSite=Lax is an attribute that specifies the cookie can only be accessed in first-party context. While not a tracking protection per se, it\u0026rsquo;s still a fairly significant move in helping auditors identify which cookies have been flagged for potential cross-site access.\n   What\u0026rsquo;s the solution for vendors that want to continue with their cross-site tracking shenanigans? Well, if third-party context is too unreliable to build any business logic for, it\u0026rsquo;s time to take a look at first-party context.\nThe bliss of the first-party When a vendor decides to move from third-party to first-party context, it typically means one or more of the following.\nThey start decorating incoming links to the site from the vendor\u0026rsquo;s platform where the user has been identified e.g. via a login.\nThis is how Facebook works, for example, as every single link you click in Facebook will be appended with the fbclid parameter. After that, any Facebook (or partner) script running on the site can take that parameter from the URL and send it back to the vendor. Thus Facebook will know that the person for whom the hash was created visited your site.\n  They start utilizing browser fingerprints to identify the user from one HTTP request to the next. Fingerprinting is dangerous because it is completely stateless - there\u0026rsquo;s no need to persist any information in the browser.\nThe user is disarmed of options to actively prevent this. This has led fingerprinting to become one of the rare areas where there is practically universal consensus among browser vendors that it is a Bad Thing and must be prevented.\n  They request the site owner to run their scripts locally or to use local routers. This way they can circumvent ad blockers targeting the vendor domains.\n  They request the site owners to reserve a subdomain in their domain namespace, which is configured to point to the vendor\u0026rsquo;s servers.\nAll of these approaches have been designed to avoid the heuristics in browsers and browser extensions that target third-party trackers. The reliability of these workarounds depends on how easy the site owner is to hoodwink into actively participating in setting them up.\nLink decoration, for example, doesn\u0026rsquo;t require the site owner to do anything but add the vendor JavaScript to the site. However, it\u0026rsquo;s not the most reliable exploit since blockers are most likely already preventing access to the most popular CDNs owned by the tracking vendors.\nThe last option in the list ends up being the most robust one because it requires the site owner to actively and knowingly collude with the third party to improve the quality of the data collection. This is the exploit we\u0026rsquo;ll be discussing in the rest of the article, as it has the biggest ramifications for end-user privacy and data security.\nSubdomain mappings This article shows you how some vendors in the advertising and marketing technology space are approaching the restrictions to third-party context. They are directly contacting their customers and asking for their help in keeping the service alive.\nThe gist is typically that the site needs to reserve a subdomain (e.g. tracking.domain.com), which is then set to point to a vendor domain name (e.g. identity.vendor.com) using a Canonical Name (CNAME) DNS record.\n Alternatively, the site can set up A/AAAA records that point directly to the vendor server IP address, but this is rare. With a CNAME, the vendor has the liberty of shuffling around the IP ranges of their servers without breaking the link to the mapped domain names.\n Once the DNS record resolves, all requests to tracking.domain.com are received by the vendor server behind identity.vendor.com.\nSo, why go through all this trouble? Well, there are a couple of reasons:\n  The site doesn\u0026rsquo;t have to relax its Content Security Policies to allow third-party domains to load and run their scripts on the site. This reduces the friction in deploying the third-party service on the site.\n  It\u0026rsquo;s so easy to setup a CNAME record. There\u0026rsquo;s no need to configure complicated reverse proxies or to create multiple A/AAAA records. It\u0026rsquo;s (typically) just a single DNS change that anyone with adequate access can do.\n  While identity.vendor.com is most likely in most of the blocklists for browsers and browser extensions, tracking.domain.com most likely isn\u0026rsquo;t. This is a pretty solid way to circumvent ad blockers.\n  Since the service is now running in first-party context with the rest of the site, the service can leverage first-party cookies, even those set with the HttpOnly flag.\n  First-party cookies and data leaks When the browser sends an HTTP request to a target address, that request will include all the cookies written on the target domain and any domains \u0026ldquo;higher\u0026rdquo; in the hierarchy, all the way to the top-most privately controlled domain name.\nFor example, when owned.domain sends an HTTP request to sub.sub.owned.domain, that request will include all the cookies written on sub.sub.owned.domain, sub.owned.domain, and owned.domain.\nIf the source and target of the request are same-site (i.e. they share the top-most privately controlled domain name), the request happens in first-party context, which is largely unrestricted by browsers and plugins today.\nWhen you surrender, either willingly by mapping the domain yourself or inadvertently via a takeover, a subdomain to a third party, you open your business for potentially horrendous data and credential leaks.\n Authentication cookie with which the browser is logged in with my Google ID  Since the web server at the end of an HTTP request is privy to a large subset of first-party cookies set on any given site, things like state and authentication tokens are automatically logged by the web server owned by the vendor. This is because many sites still set them on owned.domain, making them available to all subdomains of the domain name.\nGaining control of a subdomain also opens up possibilities for a number of different, very dangerous exploits, ranging from cross-site scripting attacks to intentional brand defamation.\nSometimes these cookies contain personally identifiable information such as email addresses and user names, allowing vendors to enhance the user\u0026rsquo;s cross-site tracking profile without them being able to do anything about it.\n PII and authentication information in first-party cookies  Typically, there are privacy policies and lots of red tape in place, where the vendors offer complicated promises that they will not misuse the information logged by their web servers. But these practices vary wildly.\nFurthermore, because everything happens behind the curtains of server-to-server communications, it\u0026rsquo;s practically impossible for independent auditors to know what\u0026rsquo;s going on.\nAnd this is the main warning of this article:\nWhen you map your subdomain to a third-party service, you are sending much more data to the third party than you might have bargained for.\nCompare this with sending the request directly to the vendor-specific domain. By default, these requests would only include (third-party) cookies written on the vendor domain. Any other information would need to be encoded with JavaScript, which can be parsed and audited by anyone visiting the site.\nBut with a subdomain mapping, all first-party cookies available on that domain will be sent, including those flagged as HttpOnly. These HttpOnly cookies can\u0026rsquo;t be accessed with client-side script at all, which might lull site owners to a false sense of security.\nZach Edwards\u0026rsquo; research on the PickAFlick.com crew Zach Edwards is one of those people on Twitter you simply must follow. He\u0026rsquo;s been conducting exhaustive research into many areas intersecting internet security and digital marketing and analytics. One of his pet peeves at the moment is something he\u0026rsquo;s dubbed the PickaFlick.com attacks.\nIn these scenarios, an attacker takes control of subdomains (usually DNS records left dangling), and then starts stuffing search engine results with bogus links, hoping the visitor will click one of them.\n From https://bit.ly/epic-games-hack  If the visitor does click such a link, that HTTP request will include all the first-party cookies available on that particular subdomain. See the previous chapter for examples of what type of information can be encoded in these cookies.\nThe screenshot above is from thehousepartyapp.com, which Zach quite elaborately reported to Epic Games as having been compromised by subdomain takeovers. This was initially in response to Epic Games\u0026rsquo; ludicrous $1,000,000 bounty for proof of a smear campaign related to a possible hacking incident.\nI recommend you read Zach\u0026rsquo;s article, as it shows how widespread and long-standing the PickaFlick.com attacks (and similar) are. It\u0026rsquo;s also shocking how oblivious so many site and service owners are to trouble brewing in their own backyards.\nGo through your site\u0026rsquo;s search results with a search query like site:mydomain.com \u0026quot;free ebook\u0026quot; and see if bogus results turn up. If they do, it\u0026rsquo;s time for a DNS record audit. Make sure to eliminate any that you don\u0026rsquo;t recognize or control the endpoint for anymore.\n  This diversion to Zach\u0026rsquo;s research isn\u0026rsquo;t necessarily an indictment of CNAME exploits themselves, but they do exemplify how perilous and fragile this aspect of the HTTP protocol is.\nPrevention measures To avoid subdomain mappings from harming your business, there are some mitigations you can act upon.\n  Follow the instructions in the previous chapter to see if you have compromised subdomains. If so, make sure you remove these subdomains from your DNS registry. It would be a good idea to report this to the service you use for your DNS, so that they can take appropriate action with their other customers as well.\n  Make sure all state and authentication tokens, and really anything that can be used to impersonate a user, are written on a subdomain that you have control over. Make sure they are not written directly on the top-most privately controlled domain name. Make sure they are httpOnly and secure. Make sure they are SameSite=Strict, unless you need them in a third-party context.\n  Make sure your Content Security Policy doesn\u0026rsquo;t just wildcard all your subdomains (*.owned.domain). Allow only those domains that you have vetted and have governance over.\n  Periodically audit cookies accessed in both first- and third-party contexts across your sites.\n  It really boils down to the trust you place in the vendors to whom you are mapping subdomains, the governance you have over the data flows, assets, and storage access across your sites, and processes you have in place for periodically auditing these perilous gateways to potentially brand-decimating leaks.\nClosing thoughts Why go through the trouble of ranting about CNAME redirects, when they\u0026rsquo;ve been so eloquently covered by this incredible article by Romain Cointepas? Why discuss subdomain takeover attacks when Zach Edwards has already done all the legwork?\nWell, this is an insidious attack vector because it can be orchestrated with the best of intentions. It needs more exposure. More and more vendors are requesting subdomain mappings, and it\u0026rsquo;s important that site owners know what they\u0026rsquo;re subscribing to.\nMoving vendor services to first-party context isn\u0026rsquo;t necessarily a universally bad thing. As a site owner, you are actively participating in vetting the services integrated on your site, and by mapping DNS records to third parties, you are proactively taking responsibility for the data streams and flows to and from your site. At least, in theory.\nThe problem isn\u0026rsquo;t necessarily related to the contracts and terms of services you sign with the vendors, which promise to restrict the types of data accessed in these requests. No, the problem is with this aspect of the HTTP protocol in general.\nAllowing cookies to pass unchecked through CNAME redirects to possible tracking domains is where the problem lies. Browsers don\u0026rsquo;t (yet) have the capability to detect hazardous CNAME chains, but it is a feature set I wouldn\u0026rsquo;t be surprised to see in their roadmaps.\nIn the European Union, the ePrivacy Directive guides businesses to be aware and audit all keys and values stored in all browser storage at all times. Having an up-to-date audit of first-party cookies, for example, reduces the risk of leaking this information to third parties.\nIf you need to manage state, authentication, and/or personally identifiable information in cookies, make sure they\u0026rsquo;re not set in the root of the domain namespace you control. That way you mitigate the risk of this information leaking to server endpoints you have no control over.\nJust be vigilant and map those data flows. That\u0026rsquo;s a good way to reduce a huge amount of risk embedded in how your site, app, or service persists and processes user data.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/fix-preview-google-chrome-incognito-mode/",
	"title": "#GTMTips: Fix Container Preview In Google Chrome&#39;s Incognito Mode",
	"tags": ["google tag manager", "gtmtips", "chrome"],
	"description": "How to fix Google Tag Manager&#39;s Preview mode in Google Chrome&#39;s Incognito windows.",
	"content": "Since updating to Google Chrome 83, you might have noticed that Google Tag Manager\u0026rsquo;s Preview mode no longer works when browsing Chrome in Incognito mode.\nThis is because starting with Chrome 83, third-party cookies are blocked by default in Incognito windows.\nGoogle Tag Manager uses third-party cookies to serve browsers in Preview mode with the container draft rather than the live container.\nThere\u0026rsquo;s a simple workaround to make sure Preview mode continues working for any site you want to browse in Preview mode.\nTip 115: Fix Container Preview In Chrome\u0026rsquo;s Incognito Mode   First thing you must do is step into Preview mode.\nOpen a regular browser tab (NOT Incognito), and browse to Google Tag Manager with a login that has access to the container you wish to preview. Hit the blue Preview link in the top right corner.\n  Assuming the container draft validates and your current browser instance supports third-party cookies, you should see the orange bar with details about Preview mode. Hit Share Preview.\n  In the overlay, make sure the \u0026ldquo;Turn on debugging when previewing\u0026rdquo; option is checked (assuming you want to use the debug panel). You can add a URL to the site to make it easier to navigate to it. Copy the URL in the text area into your clipboard.\n  Now, open a tab in Incognito mode, and browse to the URL you just copied. By browsing to this URL, your browser sets a cookie on googletagmanager.com, which is then used in third-party context once you visit the site itself.\n  Without closing Incognito mode, either open a new tab or browse the current tab to the site where the container is running.\nJust by visiting the site, the Preview mode panel should not show up, as Chrome is blocking third-party cookie access by default.\nClick the little eye icon in the address bar.\n  In the overlay, click Site not working?.\n  In the next screen, you have two options. You can click Allow cookies, which will enable all third-party cookies for the current site in the current Incognito mode session. If you choose to go down this route, after clicking the button you\u0026rsquo;ll need to do a hard refresh of the page, and GTM\u0026rsquo;s Preview mode should work again.\nIf you want to be a bit more granular than that, you can click the Show cookies and other site data\u0026hellip; option in the overlay. This opens up a list of domains for which third-party cookie access has been allowed/blocked.\nSelect the Blocked tab, and look for www.googletagmanager.com. Select that hostname and then click the blue Allow button.\n  Click Done to close the overlay, and do a hard refresh on the site. Google Tag Manager\u0026rsquo;s Preview mode panel should show up.\nSummary With blocking access to third-party storage, browsers are working hard to prevent malicious cross-site tracking from exploiting the users\u0026rsquo; movement history around the web.\nThis is a noble goal. If you\u0026rsquo;re interested in the phenomenon of cross-site tracking protections, I recommend you take a look at www.cookiestatus.com.\nThe downside is that perfectly normal, benign use cases get thrown out with the bathwater, and you need to do some extra clicking in the browser user interface to re-enable things like GTM\u0026rsquo;s Preview mode.\nIn this article, you learned how to do this in Google Chrome\u0026rsquo;s Incognito mode. If you are a Mac user, you can check how to enable Preview mode in Safari as well.\n"
},
{
	"uri": "https://www.simoahava.com/tags/chrome/",
	"title": "chrome",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/userpilot/",
	"title": "Userpilot - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Userpilot custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description A tag created with this template loads and utilizes the Userpilot JavaScript API.\nThe tag can be used to identify users (with an ID and associated properties), reload Userpilot and re-evaluate page state, track custom events, and trigger specific Userpilot content for the user, regardless of targeting conditions.\nInstructions Once you\u0026rsquo;ve created the tag, you can choose from Identify, Reload, Track, and Trigger.\n  Identify can be used to track the user (based on a User ID), and you can add other user properties to the call. You can also check the \u0026ldquo;Anonymous\u0026rdquo; check box to use a random session ID instead.\n  Reload will re-evaluate Userpilot experiences using the current page state. This is useful if it\u0026rsquo;s a single-page app, for example, as you can tell Userpilot to re-evaluate targeting conditions after the user loads new content.\n  Track can be used to track a custom event with optional parameters you can add to the call.\n  Trigger can be used to override any targeting conditions and force an experience with the given ID to be shown to the user.\n  Release notes    Date Changeset     28 June 2020 Switch deployment URL from deploy.userpilot.io to js.userpilot.io.   2 October 2019 Updated logo and description.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/no-safari-does-not-block-google-analytics/",
	"title": "No - Safari 14 Does Not Block Google Analytics",
	"tags": ["safari", "privacy"],
	"description": "This article aims to subdue the rumors that Safari 14 will block Google Analytics (and other similar libraries) by default. It will not.",
	"content": "Let me start by proclaiming with clarity and sincerity:\n No, Safari 14 (or any other version of Safari) will not block Google Analytics from loading and running on a website.\n In the midst of Apple\u0026rsquo;s yearly Worldwide Developers Conference, the company showcased some of the privacy improvements to the upcoming version of the Safari web browser (version 14).\n  In fact, the biggest revelation was the new Privacy Report, which is designed to elucidate how much the browser is working towards mitigating the damage caused by cross-site trackers.\nFor better or for worse, one of the previews showed that google-analytics.com is listed among the trackers that are being prevented on websites.\nQueue panic and the spread of misinformation like wildfire through the dry brush of first-party analytics.\nApple Insider was quick to report on this, going so far as to say that \u0026ldquo;\u0026hellip;the browser now completely blocks Google Analytics\u0026hellip;\u0026rdquo;.\n  Search Engine Journal picked the story up too, saying that \u0026ldquo;\u0026hellip;Apple specifically shows Google Analytics being blocked by Safari\u0026rdquo;.\n   NOTE! Search Engine Journal has added a footnote that they might have got the story wrong.\n Myth debunked Safari does not block resource loads. That\u0026rsquo;s not how Intelligent Tracking Prevention(ITP) works. It\u0026rsquo;s more elegant than that.\nITP har inte börjat blockera resurser från att ladda. Men ITP gör också mycket mer än begränsar/blockerar kakor som du säkert vet.\n\u0026mdash; John Wilander (@johnwilander) June 24, 2020  That\u0026rsquo;s John Wilander, the WebKit engineer in charge of ITP saying that \u0026ldquo;ITP has not started to block resource loads, but ITP does so much more than just block cookies\u0026rdquo;.\nEarly on, Maciek Stanasiuk tested whether hits are actually blocked, and found the opposite:\nSo @SimoAhava and the folks, an #WWDC20 #ITP update. In the initial release of macOS Big Sur it looks like the new features in #Safari are only UI-focused and nothing new than ITP 2.3 is being implemented. Ergo:\n- Safari now all the 3rd-party domain trackers on the website 1/3 pic.twitter.com/2RLOOmffZl\n\u0026mdash; Maciek Stanasiuk 📈 (@stanasiukcom) June 24, 2020  Similarly, Tom Anthony contributed to the research:\nI\u0026#39;ve tried Safari 14 (on macOS Big Sur), and tested the behaviour of GA vs Safari 13.1, and didn\u0026#39;t immediately see any noticeable difference, other than v14 reports domains on which ITP blocked cookies. cc @SimoAhava @benedictevans\n\u0026mdash; Tom Anthony (@TomAnthonySEO) June 23, 2020  When Safari says it blocks or prevents a tracker, what it means is that the ITP algorithm has flagged some domain as having cross-site tracking capabilities, and Safari has, among other things, stripped it of its capabilities to carry cookies in cross-site requests, also known as third-party cookies.\nTHIS is what Safari means when it\u0026rsquo;s prevented a known tracker in google-analytics.com. That domain has been flagged as a cross-site tracking domain, and Safari assigns certain protective measures to any communications to and from that domain (you can read more about them here).\nJust to prove the point, here\u0026rsquo;s my site with google-analytics.com AND googletagmanager.com flagged as tracking domains, while still merrily loading the JavaScript libraries and sending the HTTP requests to their designated endpoints:\n  How does the Privacy Report work Intelligent Tracking Prevention flags domains as being potential tracking domains if it detects requests being sent to them by the browser from a sufficient number of unique domains.\nIf your Safari browser sends a request to google-analytics.com from domain1.com, domain2.com, and domain3.com, Safari will flag google-analytics.com as having cross-site tracking capabilities.\nITP does this to hundreds upon hundreds of domains for any regular Safari user. It\u0026rsquo;s been doing this since its introduction in 2017. This is how Safari slowly closes the noose around cross-site trackers.\n  The Privacy Report has been designed to shed light on this process. However, instead of listing ALL the domains flagged by ITP, the domains are cross-referenced against DuckDuckGo\u0026rsquo;s tracker lists. If a match is found, the domain is surfaced in the Privacy Report to showcase how ITP is blocking known trackers from reading your data.\nDoes it matter that google-analytics.com is prevented as a tracker? Not really. The fact that google-analytics.com has its ability to leverage third-party storage neutered means nothing to how the tool is actually used.\nGoogle Analytics is a first-party analytics platform.\nIt\u0026rsquo;s downloaded from Google\u0026rsquo;s servers as a JavaScript library, any identifiers are stored in first-party cookies, and any HTTP requests to the GA endpoint use these identifiers and these identifiers alone to specify the source of the tracker. No third-party storage access is being used with the requests to google-analytics.com.\nThat doesn\u0026rsquo;t mean there might not be cookies set on google-analytics.com. I would imagine there are some that are used for debugging and monitoring purposes, for example. These cookies would not work on Safari or any other browser that targets google-analytics.com as a tracking domain.\nFinal thoughts I\u0026rsquo;m disappointed in many things right now.\nI\u0026rsquo;m disappointed in how this bit of misinformation spread so fast, and how reputable journals took a grainy screenshot and a couple of influencer tweets and jumped to conclusions that were quoted over and over again in social media.\nI\u0026rsquo;m disappointed that the Privacy Report has such clumsy wording. To use terms like block, prevent, and tracker can lead to confusion, as the aftermath of WWDC showed, unless they are clearly defined in the report itself. I know it\u0026rsquo;s not the final version of the Privacy Report yet, so hopefully the copy will be clarified.\nI\u0026rsquo;m disappointed it took the whole day to install Big Sur (macOS beta) on an external drive just to test something I already knew was true. And yes, I\u0026rsquo;m disappointed I didn\u0026rsquo;t have enough disk space available to install Big Sur on a proper hard disk partition.\nI\u0026rsquo;m not disappointed in the Privacy Report or Intelligent Tracking Prevention. Both are doing an amazing job at protecting Safari users from something that can have a devastating impact if mishandled.\nITP will keep on evolving and morphing to adjust to the cross-site tracking crowd.\nBut for now, Google Analytics users don\u0026rsquo;t need to worry about Safari. Google Analytics does not require cross-site tracking capabilities, and Safari does not block its use. Naturally, it does limit how effective it is, but that\u0026rsquo;s another story.\n Huge thanks to folks like Maciec Stanasiuk, Tom Anthony, and Charles Farina for working against the rumor mill.\n "
},
{
	"uri": "https://www.simoahava.com/tags/bigquery/",
	"title": "bigquery",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/google-cloud/cookie-audit-with-google-bigquery/",
	"title": "Cookie Audit With Google BigQuery",
	"tags": ["google cloud", "bigquery", "privacy", "cookies"],
	"description": "Use this method to automatically crawl all site pages, and write metadata including cookie information into BigQuery for further analysis.",
	"content": "On New Year\u0026rsquo;s Eve 2018, I published an article which instructed how to scrape pages of a site and write the results into Google BigQuery. I considered it to be a cool way to build your own web scraper, as it utilized the power and scale of the Google Cloud platform combined with the flexibility of a headless crawler built on top of Puppeteer.\nIn today\u0026rsquo;s article, I\u0026rsquo;m revisiting this solution in order to share with you its latest version, which includes a feature that you might find extremely useful when auditing the cookies that are dropped on your site.\n  The purpose of this exercise is to list all the cookies, both 1st party and 3rd party, the crawler encounters when automatically following links within your site. This way you have an idea of what cookie storage is actually being utilized on your site.\nYou can use this information to proactively audit and annotate the cookie use on your site, which is helpful in case you are striving for compliance with EU\u0026rsquo;s cookie regulation, for example.\nHow to set it up This is the easy part: you follow the exact steps as outlined in the original article. The only change you might want to do is set the config flag skipExternal to true in the config.json file, which means that the crawler will no longer crawl external pages that are linked to from your site. This was originally done to get the HTTP status codes of external links, but as you\u0026rsquo;re focusing on a cookie audit, external pages would just add to the confusion.\n  Other than that small change, just follow the steps in the original article. Just to recap, you should have the following:\n A Google Cloud Project with the necessary APIs enabled. The config.json file stored in a Google Cloud storage bucket. The gce-install.sh script modified with the URL to the config file in the storage bucket. Ability to run the command-line script that creates the virtual machine instance.  Once the virtual machine fires up, assuming you\u0026rsquo;ve followed the instructions meticulously, you\u0026rsquo;ll end up with a BigQuery table that collects a stream of scraped pages, together with the newly added cookie metadata as well.\n  The crawler scans cookies in both 1st party and 3rd party requests. Cookie information is parsed for name, value, size, domain, path, expiration, HttpOnly, secure, and SameSite.\nSample queries Once you have the data in the table, here are some BigQuery SQL queries you can run to make the most of the new information.\nThe first query is simple: it gets you the crawled URL together with all the cookies dropped on the site.\nSELECT final_url, cookies FROM `project.dataset.table`   This second query returns just the cookies, grouping similar cookies together. It\u0026rsquo;s a handy way to get a list of all the distinct cookies dropped during the crawl. Each cookie with a domain namespace different from your own is a 3rd party cookie (unless you neglected to set the skipExternal flag to true in the configuration step).\nSELECT c.name, c.domain, c.httpOnly, c.secure, c.session, c.sameSite FROM `project.dataset.table`, UNNEST(cookies) AS c GROUP BY 1, 2, 3, 4, 5, 6 ORDER BY 1 ASC You can find a screenshot of the result at the very beginning of this article.\nCaveats There are some caveats to this solution.\nDynamic sites that reveal navigation links only upon a click, or that load content with lazy-load, will need to be manually configured into a custom crawler utilizing Puppeteer\u0026rsquo;s page APIs. It\u0026rsquo;s not trivial to set up, as you\u0026rsquo;ll basically need to add only the links scraped from a dynamic navigation click (or lazy-load event) into the headless-chrome-crawler queue to avoid duplication.\nAnother problem is that there might be cookies that are set only upon the user interacting with the site. Prime example is a login event, or a conversion event that doesn\u0026rsquo;t rely on a retargeting cookie (which would have fired with the page load). In these cases, a comprehensive cookie audit would need the crawler to be configured with these custom navigation paths, so that all cookies would be audited accordingly.\nIt\u0026rsquo;s also possible that some vendors can detect crawlers and prevent their SDKs from dropping any cookies.\nIn any case, the solution described here should be a starting point for a more comprehensive storage audit. It doesn\u0026rsquo;t tackle other forms of stateful storage (e.g. localStorage or IndexedDB), but it does give you an idea of what cookie storage the scripts and tags running on your site utilize.\nSummary Hopefully this article inspires you to take a look at the web-scraper-gcp project again, especially with the updated cookie crawling capabilities. I believe every single organization in the world should be exercising this type of oversight and governance in the name of accountability. You owe it to your site visitors. And, if your practices fall under legal regulation from e.g. GDPR or California\u0026rsquo;s CPRA, you should be very interested in knowing what browser storage is utilized on your site.\nThe headless-chrome-crawler project hasn\u0026rsquo;t been updated for a couple of years, but it still works. I\u0026rsquo;m hoping to fork it one day and fix the dependency issues, which you might have noticed if running the solution locally.\nLet me know in the comments if you have questions or feedback about the solution - I\u0026rsquo;m happy to help if I can!\n"
},
{
	"uri": "https://www.simoahava.com/categories/google-cloud/",
	"title": "Google Cloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/user-distributor/",
	"title": "User Distributor - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The User Distributor custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Gallery entry   GitHub repo      Description You can use this tag to randomly assign users to buckets, and then to store this information in a first-party cookie. This is useful for quick A/B tests and for creating samples or cohorts of your visitors.\nInstructions The tag has two modes: single and multi.\nIn single mode, you assign a percentage, and a random number will test if the user falls within this percentage. If they do, a cookie is assigned to them with the value true. If they do not fall into the sample, the cookie will be set with the value false. This can be used to, for example, sample only a percentage of your visitors.\nIn multi mode, you can create multiple groups and randomly assign the user to one of them. The total of all propabilities should be no more than 100. If it\u0026rsquo;s 100, then every user will fall into one of the groups and get the cookie assigned accordingly. If the total is less than 100, then users who do not fall into any of the groups will not have the cookie set, and they will be re-evaluated the next time the tag fires.\nYou can read more about the template from the associated blog post.\nRelease notes    Date Changeset     7 June 2020 Fix bug with permissions.   30 November 2019 Add unit tests.   2 October 2019 Update brand name.   20 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/facebook-customer-chat/",
	"title": "Facebook Customer Chat - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Facebook Customer Chat custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description This template implements the Facebook Customer Chat SDK on the website.\n NOTE! You will still need to add the \u0026lt;div\u0026gt; element for the plugin by yourself (either in the page HTML or via GTM), as the custom template does not let you add DOM elements to the page.\n Once the SDK is initialized, it loads the chat button on the page. Interactions with the chat are automatically pushed into dataLayer. You can enable or disable all available event handlers through the corresponding settings in the tag.\nRelease notes    Date Changeset     4 June 2020 Added option of changing SDK locale.   9 April 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/tags/google-optimize/",
	"title": "google optimize",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/optimize-anti-flicker-snippet-delay-test/",
	"title": "Google Optimize Anti-flicker Snippet Delay Test",
	"tags": ["google tag manager", "google optimize", "app+web", "bigquery"],
	"description": "How to test the average duration of the Google Optimize anti-flicker snippet hiding the page while waiting for experiment data to load.",
	"content": "Recently I published an article on how to set up an impact test for the \u0026ldquo;flicker effect\u0026rdquo; omnipresent in client-side A/B-testing tools. Be sure to check out that article first to get some context to what we\u0026rsquo;re going to be talking about here.\n  In this short follow-up, I\u0026rsquo;ll show you how to measure the average time of the anti-flicker snippet delaying page visibility, if you choose to deploy the snippet. The methodology is very similar.\nIf you recall, the anti-flicker snippet hides the entire page while waiting for the Optimize container to load. So we\u0026rsquo;ll be measuring how long it took for the page to become unhidden. Visibility is restored if the Optimize container loads successfully, or if the load ends in a timeout (4 seconds by default).\nThe test below is run by splitting 50% of traffic to the asynchronous Optimize snippet and 50% of the traffic to the Google Tag Manager Optimize tag.\nWe\u0026rsquo;re using Google Analytics: App + Web with its wonderful BigQuery export for the analysis. We\u0026rsquo;ll be using Google Tag Manager to collect and send the data forward.\nModify the page template You need to edit the page template. The anti-flicker snippet must be added directly to the page template, and we also need to write the logic that determines whether the user should see the Optimize snippet or whether Optimize should be loaded via Google Tag Manager.\nAt the very top of the \u0026lt;head\u0026gt; element on your experiment pages, add the following HTML block:\n\u0026lt;!-- The style declaration for the anti-flicker snippet --\u0026gt; \u0026lt;style\u0026gt;.async-hide { opacity: 0 !important} \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; (function() { // Modify the optimizeId to match your Optimize container ID, gtmId  // to match your GTM container ID, and dataLayerName to match the name  // of the dataLayer array on your site.  var optimizeId = \u0026#39;GTM-NGM64B\u0026#39;, gtmId = \u0026#39;GTM-PZ7GMV9\u0026#39;, dataLayerName = \u0026#39;dataLayer\u0026#39;, hideObj = {}, hideGTMId = Math.random() \u0026lt; 0.5 ? optimizeId : gtmId; hideObj[hideGTMId] = true; // Helper to handle the dataLayer.push()  var dPush = function(status) { window[dataLayerName].push({ event: \u0026#39;optimize_anti_flicker_test\u0026#39;, milestones: { antiFlickerStart: window[dataLayerName].hide.start, antiFlickerEnd: new Date().getTime(), testStatus: status } }); }; // MODIFIED anti-flicker snippet  (function(a,s,y,n,c,h,i,d,e) { s.className + = \u0026#39; \u0026#39; + y; h.start = 1 * new Date; h.end = i = function(){ clearTimeout(t); s.className = s.className.replace(RegExp(\u0026#39; ?\u0026#39; + y), \u0026#39;\u0026#39;) }; (a[n] = a[n] || []).hide = h; var t = setTimeout(function() { dPush(\u0026#39;timeout\u0026#39;); i(); h.end = null; }, c); h.timeout = c; })(window, document.documentElement, \u0026#39;async-hide\u0026#39;, dataLayerName, 4000, hideObj); // Determine where to load Optimize from (inline vs. GTM)  if (hideGTMId === optimizeId) { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://www.googleoptimize.com/optimize.js?id=\u0026#39; + optimizeId; el.addEventListener(\u0026#39;error\u0026#39;, function() { dPush(\u0026#39;optimizeSnippetError\u0026#39;); window[dataLayerName].hide.end \u0026amp;\u0026amp; window[dataLayerName].hide.end(); }); document.head.appendChild(el); } else { window[dataLayerName].push({ gtmOptimize: true }); } // Configure the Optimize callback  function gtag() {dataLayer.push(arguments)}; gtag(\u0026#39;event\u0026#39;, \u0026#39;optimize.callback\u0026#39;, { callback: function() { dPush(hideGTMId === optimizeId ? \u0026#39;optimizeSnippet\u0026#39; : \u0026#39;gtmTag\u0026#39;); } }); })(); \u0026lt;/script\u0026gt; You should only add this snippet on pages that are actually running the experiment, to make sure you don\u0026rsquo;t accidentally collect measurements from pages that aren\u0026rsquo;t actually running Optimize.\nIn the first block of variables, make sure you update optimizeId, gtmId, and dataLayerName to reflect your Optimize ID, Google Tag Manager container ID, and name of the dataLayer array, respectively.\nvar hideGTMId = Math.random() \u0026lt; 0.5 ? optimizeId : gtmId; chooses randomly (50% chance) whether to load Optimize using the asynchronous inline snippet or through a Google Tag Manager tag.\nThe anti-flicker snippet is modified in this version. The main change is that if the timeout happens (by default 4000 milliseconds after the page was hidden), a dataLayer.push() is called with this information. Because of this, another modification to the snippet is to stop the timeout counter in case the page is unhidden (to avoid the timeout being erroneously reported to dataLayer):\nh.end = i = function() { clearTimeout(t); ... } var t = setTimeout(function() { dPush(\u0026#39;timeout\u0026#39;); ... }, c);  The following block checks if Optimize should be loaded via the snippet or via GTM.\n// Determine where to load Optimize from (inline vs. GTM) if (hideGTMId === optimizeId) { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://www.googleoptimize.com/optimize.js?id=\u0026#39; + optimizeId; el.addEventListener(\u0026#39;error\u0026#39;, function() { dPush(\u0026#39;optimizeSnippetError\u0026#39;); window[dataLayerName].hide.end \u0026amp;\u0026amp; window[dataLayerName].hide.end(); }); document.head.appendChild(el); } else { window[dataLayerName].push({ gtmOptimize: true }); }  If the inline snippet wins the draw, the Optimize element is added to the page together with an error listener that unhides the page in case there\u0026rsquo;s an error loading Optimize (e.g. user is blocking the script load).\nIf Optimize is loaded via Google Tag Manager, then the key gtmOptimize is pushed to dataLayer with the value true. This is then later used as a trigger condition for the Optimize tag.\nAs soon as the Optimize or Google Tag Manager container loads, or the anti-flicker snippet runs into its timeout, or there\u0026rsquo;s an error in loading Optimize, a dataLayer.push() happens with the following content:\n{ event: \u0026#39;optimize_anti_flicker_test\u0026#39;, milestones: { antiFlickerStart: window[dataLayerName].hide.start, antiFlickerEnd: new Date().getTime(), testStatus: status } }  Here, status is one of gtmTag (if Optimize loaded via Google Tag Manager), optimizeSnippet (if loaded via Optimize), timeout (if the timeout is reached), or optimizeSnippetError (if the Optimize snippet ran into an error).\n One thing to note is that this setup does not test if Google Tag Manager is blocked. This is something you might want to test for as well, if you want to get an even more comprehensive idea of what\u0026rsquo;s going on with your experiment implementations.\n Google Tag Manager setup In Google Tag Manager, we\u0026rsquo;ll need to create an App + Web tag (because we want to do the analysis in BigQuery again). We\u0026rsquo;ll also need a Custom Event trigger and some Data Layer variables.\nThe trigger This is what the Custom Event trigger looks like.\n  This trigger will fire whenever the dataLayer.push() with the snippet test data is executed on the page. There\u0026rsquo;s also a condition to only fire this tag on the homepage, which you can and should modify/remove if you\u0026rsquo;re running experiments elsewhere as well!.\nThe variables You\u0026rsquo;re going to need four dataLayer variables.\n   Variable name Value of the \u0026ldquo;Data Layer Variable Name\u0026rdquo; field Purpose     DLV - milestones.testStatus milestones.testStatus One of gtmTag, optimizeSnippet, timeout, or optimizeSnippetError.   DLV - milestones.antiFlickerStart milestones.antiFlickerStart Timestamp of when page hiding began.   DLV - milestones.antiFlickerEnd milestones.antiFlickerEnd Timestamp of when page hiding ended.   DLV - gtmOptimize gtmOptimize Is true if Optimize should be loaded in a GTM tag.    This is what a variable would look like, per the specification in the table above:\n  The tags First, you\u0026rsquo;ll need to create the App + Web Event tag. Make sure you have a base tag as well!\n  The tag is set to trigger with the Custom Event trigger we created above, and it sends the values of the three milestones variables to App + Web as custom parameters. Feel free to change the keys and event names as you wish.\nNext, we need to fire the Google Optimize tag conditionally, depending on if the gtmOptimize key is in dataLayer with the value true.\nSince Optimize needs to run in a tag sequence, this is actually pretty convoluted to do. In addition to the Google Optimize tag itself, you need a new Universal Analytics Page View tag which only fires when gtmOptimize is true, and you need to block your regular Page View tag in this circumstance as well.\n  Don\u0026rsquo;t worry about the Tag Sequencing setting, you set it up in the new Page View tag that you\u0026rsquo;ll also need to create:\n  Take note of the new trigger: \u0026ldquo;All Pages - Anti-flicker\u0026rdquo; is a Page View trigger with a single condition: {{DLV - gtmOptimize}} equals true.\nFinally, make sure you block your regular Page View tag to avoid double-counting on pages where the Optimize-specific Page View already fired:\n  The blocking trigger is a Custom Event trigger that blocks any event if gtmOptimize is true:\n  Test the setup Once you\u0026rsquo;ve set everything up, try loading the page with the page template modification in Preview mode. Make sure you see a request to App + Web with your custom parameters in place.\n  If it doesn\u0026rsquo;t work, check the browser console for errors. Also, make sure that the Optimize container actually loads, and that you have an experiment running on the page where you are testing!\nDig deep with BigQuery Once the data starts flowing into BigQuery, you should find your events with a query like this:\nSELECT * FROM `project.dataset.events_202006*` WHERE event_name = \u0026#39;optimize_anti_flicker_snippet_test\u0026#39; We\u0026rsquo;re simply loading all the hits with the optimize_anti_flicker_snippet_test data to get an overview of what those hits look like.\n  To get a count of different test types, you can run a query like this:\nSELECT (SELECT value.string_value FROM UNNEST(event_params) WHERE key = \u0026#39;test_status\u0026#39;) as test_status, COUNT(*) as count FROM `project.dataset.events_202006*` WHERE event_name = \u0026#39;optimize_anti_flicker_snippet_test\u0026#39; GROUP BY 1 ORDER BY 2 DESC This query pulls in the value of test_status from the events, and does an aggregate count of each status. This is what the end result would look like:\n  Finally, to get some averages in place, you can modify the query to look like this:\nWITH milestones AS ( SELECT (SELECT value.string_value FROM UNNEST(event_params) WHERE key = \u0026#39;test_status\u0026#39;) as test_status, (SELECT value.int_value FROM UNNEST(event_params) WHERE key = \u0026#39;anti_flicker_start\u0026#39;) as anti_flicker_start, (SELECT value.int_value FROM UNNEST(event_params) WHERE key = \u0026#39;anti_flicker_end\u0026#39;) as anti_flicker_end FROM `project.dataset.events_202006*` WHERE event_name = \u0026#39;optimize_anti_flicker_snippet_test\u0026#39; ) SELECT test_status, COUNT(*) as count, ROUND(AVG(anti_flicker_end - anti_flicker_start), 2) as average_delay_in_ms, FROM milestones WHERE anti_flicker_end - anti_flicker_start \u0026lt; 5000 GROUP BY 1 ORDER BY 3 DESC The WITH...AS block creates a source table with just the test status and the flicker start and end times in place. We can then query this common table expression (CTE) to get our counts and averages properly.\nAs you can see, I have a WHERE clause in place where I make sure the delta is no more than 5000 milliseconds. This is because sometimes the experiment resulted in abnormally high deltas, probably due to the Optimize container being extremely slow to load, and thus producing the end time much later than the timeout.\nWith the WHERE clause, we ignore such abnormal deltas. We can do that because we are only interested in how long the page was hidden, and if the page is hidden more than 4 seconds (and change), it\u0026rsquo;s automatically revealed by the anti-flicker snippet itself.\nAnyway, this query produces the following result for my dataset:\n  The average time for the page to be hidden if the snippet runs into its timeout is 3.8 seconds. That\u0026rsquo;s kinda weird as the timeout shouldn\u0026rsquo;t happen before 4000 milliseconds have passed. I\u0026rsquo;d dig deeper into it, but I can also just assume that all timeout occurrences were hidden the full 4 seconds nevertheless.\nWhen Optimize is loaded using a Google Tag Manager tag, the page is hidden for an average of 964 milliseconds. When loaded with the asynchronous inline snippet, it\u0026rsquo;s hidden for 581 milliseconds by average.\nSummary It\u0026rsquo;s a fairly convoluted setup, but the purpose isn\u0026rsquo;t to do this for every single one of your experiments. It\u0026rsquo;s to satisfy your curiosity about whether or not the anti-flicker snippet is degrading user experience or not, and this gives you just one variable to work with (average delay of page unhiding).\nIt\u0026rsquo;s not without its flaws - the timer starts when the anti-flicker snippet is executed at the top of \u0026lt;head\u0026gt;, when realistically it should only start when the page would normally produce the first visible element (First Contentful Paint).\nThe analysis is also detached - just knowing the delay isn\u0026rsquo;t that interesting. What would make it more significant is to see if the length of the anti-flicker effect had an impact on how the user interacts with the site. It\u0026rsquo;s possible a lengthy delay in painting the page could result in the user bouncing back to the previous page, as they might assume the page does not work.\nJust to belabor the point: this article showed you a methodology you could potentially employ to measure the effect of flicker mitigation efforts in client-side A/B-testing. The flicker is a problem, and knowing just how much of a problem is the first step in solving it.\nLet me know in the comments if you have suggestions for improving the experiment!\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/snowplow-analytics/",
	"title": "Snowplow Analytics - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Snowplow Analytics custom tag template is an official tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo      Description This template implements the Snowplow Analytics JavaScript tracker. The template supports the full spread of features of the tracker, with a few exceptions due to the limitations of custom templates\u0026rsquo; sandboxed JavaScript.\nInstall the template To install the template, browse to Templates in the Google Tag Manager user interface.\nUnder Tag Templates, click Search Gallery, and type snowplow into the gallery overlay search bar.\n  Click the Snowplow Analytics template name, and then click Add to Workspace in the next screen. Review the permissions and click Add to finalize the import.\nAfter importing the template, you can follow the normal process of creating a new tag in Google Tag Manager, and the Snowplow Analytics template will be listed among the Custom tag types you can choose from.\nCaveats To begin with, some of the caveats of using the Custom Template.\n Any methods that require the parsing of HTML elements (e.g. link tracking filter functions) will not work and are thus disabled. Automatic error tracking does not work due to lack of support for the ErrorEvent API. There is no implementation for the standard ecommerce events. Users are encouraged to implement the enhanced ecommerce setup instead.  Instructions Here are basic instructions for how to instrument the JavaScript tracker. You are encouraged to consult the technical documentation for more information on each individual feature.\nIn general, when the tag fires, it first checks if the Snowplow JavaScript library has been loaded from the self-hosted URL provided in the template settings (more on this below). Then, the tag checks whether a tracker with the given Tracker Name has already been initialized. If not, it proceeds to initialize the new tracker.\nFinally, the tag bundles a command from the settings in the tag, and sends it to the given Collector Endpoint.\nTracker Name The first field requires you to add the tracker name. The reason you might have more than one tracker name generated on the site is if you have different configuration objects or tracking endpoints to which you want to send commands.\nWhen the tag runs, it first checks if a tag with this name has already been initialized. If it has, it then proceeds to send the command to this tracker name. If a tracker with this name has not been initialized, a new tracker is initialized with the tracker configuration derived from the tag (or from the linked Snowplow Analytics Settings variable).\nThis means that a tracker configuration is applied only once to the tracker. Thus if you have more than one tag running on the site, each with the same tracker name but different tracker configurations, only the configuration of the tag that fires first will be applied to the tracker.\nCollector Endpoint Hostname This needs to be set to the hostname (e.g. www.domain.com) on which you\u0026rsquo;ve configured the Snowplow Analytics collector.\nAdvanced Configuration Jumping to the end of the template, the Advanced Configuration group has two important settings:\n Global Method Name: If you want to change the global namespace from snowplow to something else (due to a conflict, for example), change the value in this field. Self-hosted Library URL: You need to host the Snowplow JavaScript library yourself. Set this field to the URL where the browser will download the JavaScript library from.  Update permissions If you change the Global Method Name from snowplow to anything else, you need to make a change to the template. Open the template for editing (from the \u0026ldquo;Templates\u0026rdquo; section of the container), and browse to the Permissions tab. Under Accesses Global Variables, edit the four permissions that start with the name snowplow.\n  Change the text snowplow to whatever your preferred method name is (it must match the name set in the tag once you create it). Remember to leave everything else as it is: permissions and suffixes. This is what the changed list would look like if the method name were changed to some_other_snowplow:\n   UPDATE 18 May 2020: The template has been updated to have AWS Cloudfront and Google Cloud Storage destinations supported by default (https://*.cloudfront.net/* and https://storage.cloud.google.com/*). If you load the library from either location, you do not need to update permissions for injectScript!\n Next, once you have the Snowplow library self-hosted, you need to update the Injects Scripts permission to reflect the new location. Delete the content of the Allowed URL Match Patterns field, and type the full URL to the library there. Again, it must match what you input into the tag itself when creating it.\n  Unfortunately, modifying permissions breaks the gallery link and you will no longer be notified about updates to the template. Being able to modify permissions without having the link break is a feature request I hope will get solved very soon.\n  Tag Type Under Tag Type, you can choose what type of command is compiled and sent to the endpoint. The tag types are split here into three groups: commands that utilize a common parameters object, commands that have special conditions, and custom commands.\nTags with a parameter object Tags that can derive their parameters from a Google Tag Manager variable are:\n Ad Tracking Cart Tracking Error Tracking Consent Self-describing Event Site Search Social Interaction Structured Event Timing  You can set the Retrieve Parameters From Variable setting to a Google Tag Manager variable. This parameter must return an object. In the object, the key-value pairs should reflect the named parameters in the event documentation. For example, to have the variable populate an Error event, you could use a Custom JavaScript variable like this:\nfunction() { return { message: \u0026#39;Some Error Happened\u0026#39;, filename: \u0026#39;somefile.js\u0026#39;, lineno: 5, colno: 236, error: null } }  Alternatively, you can set the drop-down to the value No, and add the parameters manually instead:\n  Some tag types will add additional selections to this section. Follow the official tracker documentation for more information about what each option does.\nSpecial tags Some tag types have special conditions and have been separated into their own configurations.\nEnhanced Ecommerce When you select Enhanced Ecommerce, you are left with two options: Use Data Layer or Choose Variable. The way it works is very similar to Enhanced Ecommerce in Google Analytics.\nIf you choose the first, the template will look into the dataLayer structure for the most recently pushed Enhanced Ecommerce object, and map this object to the request to Snowplow Analytics.\nIf you selected Choose Variable, you need to provide a GTM variable that returns an object in the correct, expected format.\nForm Tracking Form tracking has just two options, because filters and transformations won\u0026rsquo;t work with the custom template (due to lack of support for processing HTML elements). The options are to set form and/or field blacklists and whitelists.\nBlacklists and whitelists for forms are a list of comma-separated HTML class names. If blacklisted, then any form element with a listed class will not trigger the form event. If whitelisted, then only form elements with a listed classname will be tracked.\nFor fields, blacklists and whitelists work similarly, except they use the name attribute rather than the class.\nLink Click Tracking The Track Link Click event is similar to regular parameter-based events, as it lets you add parameters and track a link click as a manually encoded hit.\nThe Enable Automatic Link Click Tracking adds listeners to the page, which will track clicks on links permitting they adhere to the blacklisted/whitelisted class names you can optionally provide.\nThe Fix Middle-click Tracking adds a fix for some browsers where middle-clicks were not tracked properly.\nIf you check Track HTML Content Of Clicked Link, then the full text content of the link element will be sent to Snowplow as well.\nPage View You can provide a Custom Page Title if you wish, and you can add a custom context function to the request.\nIf you enable Page Activity Tracking, the tag will setup a heartbeat tracker, and send page pings to Snowplow at intervals that you can specify.\nThe Callback Function is something you can set to a JavaScript function. If you set the callback, then instead of sending the page ping to Snowplow, the function gets invoked instead.\nCustom Commands Here is the list of custom commands you can execute, with details about the arguments (if any).\n   Command Description Arguments     flushBuffer Sends all the queued events in the buffer. N/A   setVisitorCookieTimeout Change the timeout (default 2 years) of the visitor cookie / localStorage entry. Timeout in seconds. Set 0 to use a session cookie, and -1 to disable persisting this information.   setCountPreRendered To enable tracking for pre-rendered pages, set to true. Set to true to enable.   setUserId Set the user ID to a custom value. Variable or hard-coded value for the user ID.   setUserIdFromCookie Set the user ID with a 1st party cookie value. Cookie name.   setUserIdFromLocation Set the user ID with a URL parameter value. URL parameter name.   setUserIdFromReferrer Set the user ID with a URL parameter value in the document.referrer. URL parameter name.   setCustomUrl Set the current page URL to a custom value. URL string.   setReferrerUrl Set the referring page URL to a custom value. URL string.   preservePageViewId Restore the (buggy) way the webPage context used to work on single-page apps. N/A   updatePageActivity Trigger a page ping manually. N/A   refreshLinkClickTracking Parse the page for new links to track with automatic link click tracking. N/A   setOptOutCookie Give the name of a cookie. If this cookie exists, the user will not be tracked. Cookie name.   enableGdprContext Set a GDPR context object for all events that fire after this command is run. GTM variable that returns a Context object.   addGlobalContexts Set to an array of global contexts to add to events. GTM variable that returns the global contexts array.   removeGlobalContexts Set to an array of global contexts to remove. GTM variable that returns the global contexts array.   clearGlobalContexts Remove all global contexts. N/A    Additional Tracking Parameters You can choose a Google Tag Manager variable from the drop-down titled Add Custom Contexts. This lets you add custom contexts to the current hit. The variable must return an array of valid context objects.\nYou can also choose to override the device timestamp with the Set Custom Timestamp field. The format must be UNIX time in milliseconds.\nTracker Configuration Parameters In this group, you can establish the tracker configuration which is applied when the tracker is first created.\n NOTE! If you are adding configurations to a tracker that has already been created, the configuration fields will not do anything.\n You have three ways of applying the configuration:\n Using only a Snowplow Analytics Settings variable. Using only manually added parameters. Using a combination of the two.  If you choose to use a Snowplow Analytics Settings variable, you need to pick the variable from the drop-down menu. If you choose a variable that is not a \u0026ldquo;Snowplow Analytics Settings\u0026rdquo; variable, the tag will fail, so be careful.\nYou can also check the Enable Overriding Settings For This Tag option. This expands a parameter field where you can add key-value pairs of individual tracker configuration fields. See the documentation to know what fields are available.\nIf you choose to use both a Snowplow Analytics Settings variable and the manually entered parameters, then any conflict between the two will be resolved in favor of the manually entered parameters. For example, here we use a Snowplow Analytics Settings variable, but we\u0026rsquo;ve decided to override some fields in the configuration.\n  Advanced Configuration See above.\nRelease notes    Date Changeset     26 May 2020 Fixed a number of issues with the Enhanced Ecommerce integration.   20 May 2020 Updated injectScript permissions to default to GCS and AWS Cloudfront.   14 May 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/tags/ab-testing/",
	"title": "ab testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/experimentation/",
	"title": "experimentation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/simple-way-measure-a-b-test-flicker-impact/",
	"title": "Simple Way To Measure A/B Test Flicker Impact",
	"tags": ["google tag manager", "ab testing", "experimentation", "google-optimize"],
	"description": "This article shares a method with which you can measure how much flicker time there is between the base element being added to the page and the A/B test changing its appearance.",
	"content": "\u0026ldquo;Flickering\u0026rdquo; or \u0026ldquo;Flash Of Original Content\u0026rdquo; (FOOC) is a phenomenon where there\u0026rsquo;s a (typically) slight but observable delay in the browser updating the site or element layout if the user is included in a variant group for experimentation. This manifests in the original, unmodified element being rendered in the visible portion of the page before the experiment library updates it with the variant.\nThere are ways to mitigate the flicker:\n Add the A/B testing library directly into the page template and don\u0026rsquo;t load it via some other, asynchronously loaded dependency (e.g. Google Tag Manager). Load the A/B testing library synchronously, and have it hide the element that is being tested until the library is loaded. Utilize some kind of anti-flicker tech. Run the experiments server-side, and render content with the variant in place.    Typically, the only non-intrusive and consistent way to avoid the flicker is to look into server-side rendering for your experiments. For example, tools like Conductrics offer a robust set of APIs to do all the decision-making logic in your server. Then there are tools like Google Optimize that require you to do the variant selection and assignment manually, but the tool can then handle the data collection and reporting.\nHowever, the reason you\u0026rsquo;ve read thus far is probably because you\u0026rsquo;re worried about client-side testing.\nIntroducing the problem With JavaScript-based experimentation libraries, you\u0026rsquo;re subject to the rules and limitations of the page render in the browser. The flicker happens because the page with the modified element is being rendered from the page HTML source, but the experimentation library needs to wait for an opening to allow the browser to process the experiment data.\nThis is most often a problem when you\u0026rsquo;re running scripts asynchronously. Async load means that once the browser starts to download the library, it doesn\u0026rsquo;t wait for the download to complete. Instead, it proceeds with the page render. Once the download is complete, and as soon as the browser has an available slot in its single thread of execution, it will start parsing and executing the JavaScript within the library.\nBy moving from asynchronous to synchronous loading, you solve part of this issue. However, it\u0026rsquo;s not like synchronous loading actually fixes anything automatically. Since the library is loaded at the top of \u0026lt;head\u0026gt;, a synchronously loaded library doesn\u0026rsquo;t have access to the elements it\u0026rsquo;s designed to modify (since those elements are created in the \u0026lt;body\u0026gt;, which hasn\u0026rsquo;t yet been generated).\nInstead, libraries like Google Optimize, when loaded synchronously, actually hide the element that\u0026rsquo;s being tested. They inject a style declaration that sets the visibility of all elements matching the CSS selector of the experimentation targets to hidden. Only once the element has been actually added to the page, can Optimize then modify it and unhide it. This is fairly elegant but it might introduce a slight flicker of another kind, where the element seems to \u0026ldquo;pop\u0026rdquo; into place out of sequence with the rest of the render.\n  A similar solution is anti-flicker JavaScript. The purpose here is to actually hide the entire page until the experimentation library has loaded. This is, and has been, my biggest objection about how A/B-testing tools are implemented. I simply can\u0026rsquo;t fathom the logic behind potentially sacrificing the usability of the entire page just to get better data quality for your experimentation.\n Considering how crucial page performance and perceived page performance is these days, I steer clear of anti-flicker snippets that hide the entire page. It doesn\u0026rsquo;t matter if there are mitigations in place for ad blockers and download errors. If the endpoint is unresponsive or lags, Google Optimize\u0026rsquo;s default anti-flicker snippet has the page wait for a maximum of 4 seconds (this is adjustable) before revealing the content. Naturally, if the container loads before that, the page is revealed faster. But still, OUCH!\n Measuring the impact of flicker So, let\u0026rsquo;s assume the situation is as follows:\nYou\u0026rsquo;ve got an experiment running that treats a home page element, which is visible above the fold if the page is loaded without a scroll threshold in place.\nYou\u0026rsquo;ve deployed Google Optimize using the new snippet. You\u0026rsquo;ve deployed the asynchronous snippet, and you are not using the anti-flicker JavaScript, so there\u0026rsquo;s a visible and measurable flicker in place.\n Flicker of the original (grey background) before the variant (red background)  In order to measure the severity of this flicker, we need to collect a number of timings:\n Time when the original element was added to the page, Time when the original element became visible in the viewport, Time when the experimentation library was loaded, Time when the experiment change was applied to the page.  The flicker is the time delta between (2) and (4). If the element isn\u0026rsquo;t visible in the viewport, or if the experiment is applied before the base element becomes visible, the flicker is not a problem. (3) is interesting metadata about how the experimentation library itself works, and how fast it manages to apply the change after loading.\nIntroduction to the JavaScript we\u0026rsquo;ll need The solution will rely on two pieces of JavaScript code running directly in the page template. You can\u0026rsquo;t execute this code reliably through a dependency like Google Tag Manager, because Google Tag Manager in many cases loads after all steps (1)-(4) have already happened, meaning you won\u0026rsquo;t get accurate measurements.\nThe first bit of JavaScript is run at the very top of \u0026lt;head\u0026gt;, even before the Optimize snippet. This script uses the optimize.callback API to collect the timestamp of the experimentation library load. This is timing number (3) in the list above.\nThe second JavaScript snippet is added to the top of \u0026lt;body\u0026gt;, because the observers need access to document.body. Here\u0026rsquo;s what it does:\n A MutationObserver waits on the page and reacts to two changes: when the element is first added to the page, and when the element is updated with the variant. These are timings (1) and (4), respectively, in the list above. An IntersectionObserver is added to the page as soon as the original element is rendered. The purpose of the IntersectionObserver is to fire a callback as soon as the original element is visible in the viewport. This is timing (2) in the list above.  Once the timings have been collected, they are pushed into dataLayer to be used in Google Tag Manager.\nOther preparations To best measure the application of the experiment element, I have added the data attribute data-test=\u0026quot;true\u0026quot; to the variant. This makes it easier for me to locate the element using CSS selectors.\nThe attribute is added via the Optimize editor, and is thus only present on the element after it\u0026rsquo;s modified by Google Optimize.\n  Finally, I\u0026rsquo;m collecting all this data using Google Tag Manager, and I\u0026rsquo;m sending it to App + Web because I want to collect it in BigQuery for more granular analysis.\n You could just as well calculate the delta directly in the client and send it to, for example, Universal Analytics as an event. This is entirely up to you. I opted for the BigQuery approach - I justify this later in the article.\n Installing the Optimize library and callback To install the Optimize library, I\u0026rsquo;m adding the \u0026lt;script\u0026gt; element with the async attribute to the top of \u0026lt;head\u0026gt;.\n  By adding it to the top of \u0026lt;head\u0026gt;, I\u0026rsquo;m not eliminating the flicker (because it\u0026rsquo;s still loaded asynchronously), but I am making sure that the download of the library begins as soon as the page render starts. This helps mitigate the flicker a great deal.\nThen, to add the Optimize callback, I\u0026rsquo;m running the following script before the Optimize snippet at the very beginning of \u0026lt;head\u0026gt;.\n\u0026lt;html\u0026gt; \u0026lt;head lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;generator\u0026#34; content=\u0026#34;Hugo 0.61.0\u0026#34; /\u0026gt; \u0026lt;script\u0026gt; function gtag() { window.dataLayer = window.dataLayer || []; window.dataLayer.push(arguments); } gtag(\u0026#39;event\u0026#39;, \u0026#39;optimize.callback\u0026#39;, { callback: function(e) { window.__flickerTestMilestones = window.__flickerTestMilestones || {}; window.__flickerTestMilestones.experimentLoaded = new Date().getTime(); } }); \u0026lt;/script\u0026gt; ... \u0026lt;/head\u0026gt; ... \u0026lt;/html\u0026gt; Here, we first create the gtag queue (because that\u0026rsquo;s what Optimize uses for its API control). Then, we push a callback in the shape of a gtag event. I\u0026rsquo;m passing an anonymous function to the callback argument. This function references a global object which we\u0026rsquo;ll use to collect the milestones. The only milestone we populate in this callback is experimentLoaded, and we attach the current timestamp to it.\nThe callback is invoked as soon as the experiment library has loaded and Optimize has established to which variant the user belongs (and thus which version of the element to show to them).\nInstalling the observer scripts Here\u0026rsquo;s the tricky bit. You need to install two observers (a MutationObserver and an IntersectionObserver). The first checks if an element has been added to the page, and the second checks if an element is in the viewport. I\u0026rsquo;ll show you the code first and then explain what it does.\n\u0026lt;body\u0026gt; \u0026lt;script\u0026gt; (function() { var ftm = window.__flickerTestMilestones = window.__flickerTestMilestones || {}; var testState = \u0026#39;success\u0026#39;; var dpush = function() { if (testState !== \u0026#39;noObservers\u0026#39;) { // If milestones are incomplete and it\u0026#39;s not because of lack of support, do nothing  if (!ftm.experimentLoaded || !ftm.baseElementAddedToPage || !ftm.testElementAddedToPage) return; // If all other milestones are in place, but baseElementVisible is not,  // send the other timings and make note that base element was not visible.  if (!ftm.baseElementVisible) { testState = \u0026#39;baseNotVisible\u0026#39;; } } // Push everything to dataLayer  window.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;optimize_flicker_test\u0026#39;, testMilestones: { baseElementAddedToPage: ftm.baseElementAddedToPage, baseElementVisible: ftm.baseElementVisible, experimentLoaded: ftm.experimentLoaded, testElementAddedToPage: ftm.testElementAddedToPage, testState: testState } }); // Reset the test  window.__flickerTestMilestones = {}; }; // Only run if observers are supported by the browser  if (window.MutationObserver \u0026amp;\u0026amp; window.IntersectionObserver) { var observer = new MutationObserver(function(mutations) { mutations.forEach(function(mutation) { var node = !!mutation.addedNodes.length \u0026amp;\u0026amp; mutation.addedNodes[0]; if (node \u0026amp;\u0026amp; node.matches \u0026amp;\u0026amp; node.matches(\u0026#39;span.talks\u0026#39;)) { if (node.matches(\u0026#39;[data-test]\u0026#39;)) { ftm.testElementAddedToPage = new Date().getTime(); dpush(); } else { ftm.baseElementAddedToPage = new Date().getTime(); dpush(); var intersectionObserver = new IntersectionObserver(function(entries) { if (entries.some(function(e) { return e.intersectionRatio \u0026gt; 0 })) { ftm.baseElementVisible = new Date().getTime(); dpush(); } }); intersectionObserver.observe(node); } } }); }); observer.observe(document.body, { childList: true, subtree: true }); } else { // Make note that there was no support for observers  window.__flickerTestMilestones = {}; testState = \u0026#39;noObservers\u0026#39;; dpush(); } })(); \u0026lt;/script\u0026gt; ... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This script runs at the very top of \u0026lt;body\u0026gt; so that the observers can be primed as fast as possible.\nFirst thing to check is whether the browser supports both MutationObserver and IntersectionObserver. We don\u0026rsquo;t have to support all browsers for this - we just need a representative sample. If there is no support, then the dataLayer.push() includes the testState key with the value noObservers, and we can use that in our analyses.\n I did not opt for a fallback to just polling the page until the element is found. It would have made the code more complex than it already is, and it could have potentially introduced performance issues that I\u0026rsquo;d rather avoid when experimenting with data.\n The script then inserts the MutationObserver. This observer pattern can be used to detect things like DOM elements being added to the page or attributes changing for individual elements.\nI\u0026rsquo;m only interested in child nodes being added to the page, because the original element (by the browser engine parsing the HTML source) and the variant (by the Optimize library) are added as new elements to the page. The observer is primed like this:\nvar observer = new MutationObserver(callback); observer.observe(document.body, { childList: true, subtree: true });  We\u0026rsquo;re attaching the observer to document.body, and we\u0026rsquo;re reacting to any changes in the child nodes, regardless of how deep they are in the subtree. If a change is detected, the callback function is executed.\nIn this case, I\u0026rsquo;m making sure that the observer callback only reacts to when the element I\u0026rsquo;m currently testing is added to the page:\nif (node \u0026amp;\u0026amp; node.matches \u0026amp;\u0026amp; node.matches(\u0026#39;span.talks\u0026#39;)) {  Next, the code checks if the node that was added is the experiment element:\nif (node.matches(\u0026#39;[data-test]\u0026#39;)) {  If you remember, above I mentioned that I add the data-test=\u0026quot;true\u0026quot; attribute to the test element to make debugging easier.\nIf the added element was the experiment element, I update the milestone for testElementAddedToPage with the current timestamp. This is the moment when Optimize added the modified element to the page, and will serve as the endpoint of our delta measurement.\nIf the element was not the experiment element, it has to be the base element, so I update the milestone baseElementAddedToPage together with the timestamp.\nSince I want to know the moment the base element became visible in the viewport, in the callback where I process the non-experiment element, I attach an IntersectionObserver to it as well.\nvar intersectionObserver = new IntersectionObserver(function(entries) { if (entries.some(function(e) { return e.intersectionRatio \u0026gt; 0; })) { ... } }); intersectionObserver.observe(node);  The IntersectionObserver activates whenever the element that is being observed enters the viewport of the browser. I can then check if the element is visible even the tiniest bit (intersectionRatio \u0026gt; 0), and then update the milestone for baseElementVisible with the timestamp.\nAfter every milestone, I check if at least baseElementAddedToPage, experimentLoaded, and testElementAddedToPage milestones have been updated. If they have, the milestones and the test state are pushed into dataLayer.\n  There are two reasons I\u0026rsquo;m not waiting around for baseElementVisible:\n Sometimes the experiment loads the updated element so fast that the base element is already removed from the page when the IntersectionObserver is supposed to go off. Sometimes the user has scrolled beyond the fold, and the baseElementVisible simply does not fire (because the base element is not, well, visible).  Both of these mean that the flicker is basically a non-issue, so it\u0026rsquo;s OK for me to just collect a null in these cases. I update testState with \u0026quot;baseNotVisible\u0026quot; to make it easier to parse these in the analysis.\nSetting up the Google Tag Manager assets In GTM, the trigger I need looks like this:\n  You\u0026rsquo;ll need five Data Layer variables. Each is setup like this:\n  The variable names you\u0026rsquo;ll need are:\n testMilestones.baseElementAddedToPage testMilestones.baseElementVisible testMilestones.experimentLoaded testMilestones.testElementAddedToPage testMilestones.testState  And this is what the App + Web event tag looks like:\n  As you can see, I\u0026rsquo;m also sending a \u0026ldquo;page ID\u0026rdquo;. I can use this to group all the timings for any given user/session/page combination a bit more easily. It\u0026rsquo;s not strictly necessary, but might make some analyses a bit easier.\nfunction() { window.__flickerTestPageId = window.__flickerTestPageId || {{Random GUID}}; return window.__flickerTestPageId; }   The \u0026ldquo;Random GUID\u0026rdquo; variable is another Custom JavaScript variable that returns a random, and fairly unique, identifier.\n By setting a global variable, we make sure the same ID is used for all the events that are being measured.\nWhen I now load the home page of my site, this is what I see being sent to App + Web:\n  BigQuery output In BigQuery, our event parameters are added into the event_parameters record. It\u0026rsquo;s not the classiest way to pass key-value pairs, especially since values are distributed as columns with one column per (potential) type. However, it is the only supported way of exporting custom parameters for now.\nFull table output Here\u0026rsquo;s what those optimize_flicker_test hits look like in our event table:\nSELECT * FROM `project.dataset.events_yyyymmdd` WHERE event_name = \u0026#34;optimize_flicker_test\u0026#34;   Count of test states We can drill down and count the respective number of each test state:\nSELECT ep.value.string_value AS test_state, COUNT(*) as count FROM `project.dataset.events_yyyymmdd`, UNNEST(event_params) as ep WHERE event_name = \u0026#34;optimize_flicker_test\u0026#34; AND ep.key = \u0026#34;testState\u0026#34; GROUP BY 1 ORDER BY 2 DESC   Table with deltas We can also build a query that returns all the milestones with the calculated deltas:\nWITH milestones AS (SELECT (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;baseElementAddedToPage\u0026#34;) as baseElementAddedToPage, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;baseElementVisible\u0026#34;) as baseElementVisible, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;experimentLoaded\u0026#34;) as experimentLoaded, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;testElementAddedToPage\u0026#34;) as testElementAddedToPage FROM `project.dataset.events_yyyymmdd` t WHERE t.event_name = \u0026#34;optimize_flicker_test\u0026#34;) SELECT baseElementAddedToPage, testElementAddedToPage-baseElementAddedToPage AS injection_delta, baseElementVisible, testElementAddedToPage-baseElementVisible AS flicker_delta, experimentLoaded, experimentLoaded-baseElementAddedToPage AS experiment_delta, testElementAddedToPage FROM milestones ORDER BY flicker_delta DESC   This is just one way to approach the data.\nHere, I\u0026rsquo;m calculating injection_delta as the time it takes for the test element to be added to the page after the base element has been added. This could be used as a stand-in to measure the potential flicker.\nThe flicker_delta is the elapsed time from the base element becoming visible to it being replaced with the variant.\nFinally, experiment_delta is the elapsed time from the base element being added to the page to the experiment loading. This value can be negative, meaning the experiment library loaded before the base element was added to the page. This is possible especially if the experiment library loads really fast after being, for example, cached by the browser.\nSimple calculations Once you have the milestones, you can do simple calculations:\nWITH milestones AS (SELECT (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;baseElementAddedToPage\u0026#34;) as baseElementAddedToPage, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;baseElementVisible\u0026#34;) as baseElementVisible, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;experimentLoaded\u0026#34;) as experimentLoaded, (SELECT value.int_value FROM t.event_params WHERE key = \u0026#34;testElementAddedToPage\u0026#34;) as testElementAddedToPage FROM `project.dataset.event_yyyymmdd` t WHERE t.event_name = \u0026#34;optimize_flicker_test\u0026#34;) SELECT AVG(injection_delta) as average_injection_delta, MIN(injection_delta) as minimum_injection_delta, MAX(injection_delta) as maximum_injection_delta, AVG(flicker_delta) as average_flicker_delta, MIN(flicker_delta) as minimum_flicker_delta, MAX(flicker_delta) as maximum_flicker_delta FROM ( SELECT baseElementAddedToPage, testElementAddedToPage-baseElementAddedToPage as injection_delta, baseElementVisible, testElementAddedToPage-baseElementVisible as flicker_delta, experimentLoaded, experimentLoaded-baseElementAddedToPage as experiment_delta, testElementAddedToPage FROM milestones )   Nothing ground-breaking analysis-wise, but this shows what you could do with the data.\nThings to note OK, so I misled you a bit with the title of this article. The purpose wasn\u0026rsquo;t to show you how to measure the impact of the flicker, but rather how to collect the data that allows you to measure the impact of the flicker.\nWith the raw data about element visibility and element injection, you can measure what the delta is and how it impacts conversions, for example. However, this analysis is left for you to calibrate against your unique data set and your unique experiments.\nYou probably also wondered when reading the setup, \u0026ldquo;Why isn\u0026rsquo;t this guy just calculating the delta in the client and sending that to App+Web? Or, better yet, why isn\u0026rsquo;t he just sending it to Google Analytics?\u0026rdquo;. Totally valid questions.\nI decided to build this experiment around raw, uncalculated data. Many analytics tools have happily promoted the idea that calculated metrics don\u0026rsquo;t need to be reverse engineerable. This has led to confusion around things like the sessionization schema of Google Analytics, and the weird, quantum fluctuations that seem to govern any data set when you zoom in close enough.\nWith App+Web, everybody has access to the raw data dump in BigQuery, so I want to start pushing forward the approach where the data you collect is as raw as possible, so that you have full freedom for parsing the data as you wish in BQ.\nYou are, of course, free to modify the solution however you wish.\nSummary There are some caveats to this experiment. Not all browsers support Mutation- and IntersectionObservers.\nThere might also be cases where the baseElementVisible simply refuses to fire in time, even if there was a flicker. The single-threadedness of JavaScript has the tendency to mess with callbacks. That\u0026rsquo;s why I opted to throw the baseElementVisible baby out with the bathwater - if this metric hasn\u0026rsquo;t been collected by the time all the other metrics have, I\u0026rsquo;d rather not send it at all rather than send confusing values where the base element became visible after the test element was added.\nWhen setting up this test, just remember these key points:\n  The JavaScript snippets must be added in the page template. If you\u0026rsquo;re firing Optimize through Google Tag Manager, you could add the Optimize callback in a Custom HTML tag, but there\u0026rsquo;s still the risk of a race condition ruining everything.\n  You need to update the CSS selectors in the observers to match the element you are measuring. Adding the data-test=\u0026quot;true\u0026quot; (or something similar) to the element in the Optimize editor makes it easier to adjust the observer pattern.\n  Let me know in the comments if you have additional insights to setting up the test or the subsequent analysis.\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/snowplow-analytics-settings/",
	"title": "Snowplow Analytics Settings - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The Snowplow Analytics Settings custom variable template is an official variable template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo    Description The Snowplow Analytics Settings custom variable template is meant to be used with the Snowplow Analytics tag template.\nWith this template, you can compile a tracker configuration object, which you can then load into your Snowplow Analytics tags to avoid the need to manually enter the same tracker settings across all your tags.\nInstall the template To install the template, browse to Templates in\tthe Google Tag Manager user interface.\nUnder Variable Templates, click Search Gallery, and type snowplow into the gallery overlay search bar.\n  Click the Snowplow Analytics Settings template name, and then click Add to Workspace in the next screen. Review the permissions and click Add to finalize the import.\nAfter importing\tthe template, you can follow the normal\tprocess\tof creating a new variable in Google Tag Manager, and the Snowplow Analytics Settings template will be listed among the Custom variable types you can choose from.\nInstructions The fields in the template provide a UI for setting the tracker configuration parameters. You are thus encouraged to follow this link to understand what each individual field does.\nThe main caveat is that automatic cross-domain tracking using the crossDomainLinker setting does not currently work. This is because the sandboxed JavaScript of Google Tag Manager\u0026rsquo;s custom templates does not allow for processing of HTML elements.\nThere is a feature request submitted to Snowplow where linker could be configured with CSS selector strings instead, but until this materializes, cross-domain linking will need to be done manually with a Custom HTML tag:\n\u0026lt;script\u0026gt; if (window.snowplow) { window.snowplow(\u0026#39;crossDomainLinker\u0026#39;, function(linkElement) { return !/mydomain\\.com|javascript:|mailto:|tel:/.test(linkElement.href); }); } \u0026lt;/script\u0026gt; The other missing feature is the onload callback. For this, you can use a Custom HTML tag again.\n\u0026lt;script\u0026gt; if (window.snowplow) { window.snowplow(function() { console.log(\u0026#39;Snowplow has loaded.\u0026#39;); }); } \u0026lt;/script\u0026gt; Release notes    Date Changeset     14 May 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/tags/snowplow/",
	"title": "snowplow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/snowplow-analytics-templates-google-tag-manager/",
	"title": "Snowplow Analytics Templates For Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "tag templates", "variable templates", "snowplow"],
	"description": "This blog post describes the recently released Snowplow Analytics custom templates for Google Tag Manager.",
	"content": "When custom templates were released for Google Tag Manager, I updated my workflow for working with GTM. Instead of instinctively rushing to the Custom HTML tag and the Custom JavaScript variable, I started considering whether the custom script that needed to be deployed could be transformed into a custom template first.\nWhile publishing numerous templates into the community gallery, I always spent some time over the past 12 months tinkering on an extremely complicated template translation: the Snowplow Analytics JavaScript tracker.\n  I\u0026rsquo;ve written about Snowplow before, and I have a soft spot in my heart for their wonderful DIY approach to the analytics pipeline.\nIn this article, I\u0026rsquo;ll introduce the two templates I\u0026rsquo;ve created, and I\u0026rsquo;ll share some of the design philosophy with you. It\u0026rsquo;s been a strange ride, and I\u0026rsquo;ve found myself pushing the capabilities of custom templates to their limits.\n This article is not a manual for the templates. You should turn to the respective technical documentation linked in the table below.\n    Template Type Documentation Gallery GitHub     Snowplow Analytics Tag Link Link Link   Snowplow Analytics Settings Variable Link Link Link    Thank you The preliminary, pre-release work on the templates was done by me, but the templates have been released for open-source development under Snowplow\u0026rsquo;s own GitHub organization. I believe this to be vital for keeping them up-to-date and linked with the development of the JavaScript tracker.\nI want to thank in advance anyone who contributes to the development of these templates, even if it\u0026rsquo;s just in online discussions, blog comments, or via GitHub Issues.\nHuge thanks to Paul Boocock from Snowplow. He walked through the template with me to spot any inconsistencies, and he also helped establish the setup for open-source development of the templates.\nThe tag template The Snowplow Analytics tag template is a fairly faithful implementation of the full Snowplow Analytics JavaScript tracker library.\n  The library itself is reminiscent of the Universal Analytics library and the Matomo (former Piwik) JavaScript SDK. These similarities make it easier to translate it to a template, as we can use the native Universal Analytics tag template for inspiration.\nGeneral overview In general, when the tag is fired, it goes through the following motions:\n Loads the tracker library from a self-hosted URL. Creates the global namespace for the method name (similar to how Universal Analytics generates the ga global object). Initializes a new (or reuses an existing) tracker object. Compiles the settings of the tag into a command that is then passed to the global method. The library takes this command from the queue, builds it into a payload, and sends it to the collector endpoint.  All in all, the process is fairly similar to how Universal Analytics works. The biggest difference on a superficial is the DIY nature of the pipeline. You need to self-host the JavaScript library, and you need to build the collector yourself.\nInitializing the tag Converting a library into a template is a complicated task. A lot of time needs to be devoted to understanding how the library works, and then looking at the restrictions of custom templates.\nLoading the library and initializing the global namespace was easy - the injectScript API handled the first, and a copyFromWindow / setInWindow combo the second.\nIf the global namespace isn\u0026rsquo;t established, the script creates it, using a similar queue method that Universal Analytics uses.\nThen, the first hiccup is faced: initializing a tracker.\nThe Snowplow global method doesn\u0026rsquo;t have a function to check for the existence of a tracker. There are ways to drill into the object and seek this information, but because there\u0026rsquo;s no official API for querying tracker status, I didn\u0026rsquo;t want to start parsing the object without guarantee it will work in the future.\nSo, to persist information about trackers that have been created, I used a new global array, window._snowplow_trackers. This array stores all the trackers that have been initialized, and when the tag fires, the tracker name is checked against this list to verify whether a tracker already exists.\nMajor caveat with the self-hosted library There is a major problem with using injectScript.\nBecause I, as the template author, have no idea about the URLs the template users might load the self-hosted script from, it falls on the template admin to make sure the template permissions are updated when the self-hosted library URL is changed.\nThere are instructions for this here.\n UPDATE 18 May 2020: The template has been updated to support AWS Cloudfront and Google Cloud Storage destinations by default (https://*.cloudfront.net/* and https://storage.cloud.google.com/*). If you load the library from either destination, you do not need to update the permissions for injectScript!\n Unfortunately, changing permissions breaks the gallery link, meaning you won\u0026rsquo;t be notified about updates to the template.\n   I have submitted a feature request to Google about this. Changes to permissions should be uncoupled from the gallery link.\n The tracker configuration The tracker is created with an optional argument map of configuration settings.\nAt first, I considered adding all the tracker options as fields into the tag. However, this led to serious bloat, and I wanted to keep the tag as lean as possible (very difficult task, by the way).\nSo then I thought of just adding a table of parameters where the user can type the key-value pairs they want.\n  While this is very functional, it has some drawbacks. It\u0026rsquo;s very unintuitive, and the user needs to browse the Snowplow tracker documentation to build the argument map correctly.\nThe Snowplow Analytics Settings variable Finally, I decided to follow what the Universal Analytics tag does with the Google Analytics Settings variable. The settings variable is another template, where the user can utilize a nice user interface for setting individual settings.\n  Once the variable has been created, the user can add it to the Snowplow tag via the drop-down menu.\nI decided to leave the parameter table as well. The user can check the Enable Overriding Settings For This Tag to add/modify individual parameter fields with the table. Anything added to the table overrides the corresponding parameter in the settings variable.\nThough there\u0026rsquo;s no clear way to use the table to delete a setting from the variable, setting a parameter name to undefined should work.\nMissing pieces There\u0026rsquo;s one thing missing from the tracker configuration process: cross-domain linking. As this relies on parsing HTML elements in the callback, it couldn\u0026rsquo;t be implemented in the custom template due to restrictions of the sandbox API.\nYou can still introduce cross-domain linking with a Custom HTML tag. I also submitted a feature request to Snowplow where cross-domain linking could be done by passing CSS selector strings or href patterns via the callback, rather than link elements themselves.\nBuilding the command(s) Once the library is loaded and the tracker is initialized (if necessary), it\u0026rsquo;s time to take the rest of the settings in the tag and bundle them into a command.\nBuilding the command is split into three different types of user interfaces:\n Commands with a generic parameter object. Commands with special conditions. Custom commands.  Parameter object The Snowplow JavaScript tracker uses positional arguments to handle command logic. This means that the arguments need to be provided to the global method in a specific order, so that the values correspond with their correct, functional counterparts.\nFor example, ad tracking and impression requires the command to be compiled like this:\nsnowplow_name_here(\u0026#39;trackAdImpression\u0026#39;, \u0026#39;67965967893\u0026#39;, // impressionId  \u0026#39;cpm\u0026#39;, // costModel - \u0026#39;cpa\u0026#39;, \u0026#39;cpc\u0026#39;, or \u0026#39;cpm\u0026#39;  5.5, // cost  \u0026#39;http://www.example.com\u0026#39;, // targetUrl  \u0026#39;23\u0026#39;, // bannerId  \u0026#39;7\u0026#39;, // zoneId  \u0026#39;201\u0026#39;, // advertiserId  \u0026#39;12\u0026#39; // campaignId );  Having the architecture setup like this makes life difficult for this particular template author. Because of this, any named arguments need to be mapped into positional arguments. With parameter-based objects, it means that the user can create a parameter map either using a Google Tag Manager variable or the parameter table, and the object would look like this:\n{ impressionId: \u0026#39;67965967893\u0026#39;, costModel: \u0026#39;cpm\u0026#39;, cost: 5.5 }  But because Snowplow uses positional arguments, I can\u0026rsquo;t simply attach this object to the call. That would have been beautifully elegant and simple, and would have shortened the template codebase by about a half!\nInstead, the code needs to map these named parameters to their correct positional counterparts:\n// Pseudo-code snowplow_name_here(\u0026#39;trackAdImpression\u0026#39;, data.argMap.impressionId, data.argMap.costModel, data.argMap.cost );  I could have taken the easy way out and required the user to pass the positions themselves by returning an array instead of an object, but that would have made for a very clunky experience.\nWith named arguments, I can also do validation:\nif ([\u0026#39;cpa\u0026#39;, \u0026#39;cpc\u0026#39;, \u0026#39;cpm\u0026#39;].indexOf(data.argMap.costModel) === -1) return fail(\u0026#39;Invalid \u0026#34;costModel\u0026#34; argument provided in trackAdImpression call!\u0026#39;);  Unfortunately, as mentioned above, mapping the named parameters to their positional counterparts leads to a lot of extra code. I have submitted a feature request to Snowplow to support object arguments as well, as that would elegantly solve the whole problem (and converge with how Universal Analytics works, for example).\nSpecial commands For commands with extra functionality, such as Page View and Link Tracking, I needed to create their own field groups. I could have gone forward with just an argument map again, but with e.g. page ping tracking and automatic link tracking the argument maps wouldn\u0026rsquo;t suffice, as they needed to be executed as separate, additional commands.\nBuilding these special groups wasn\u0026rsquo;t problematic per se, but there were cases where I was pushing against the limitations of the custom template sandbox. For example, automatic link tracking lets you add element-based filters and contexts to the call. Because the sandbox doesn\u0026rsquo;t let you interact with HTML elements, using these features is impossible.\nThat\u0026rsquo;s why automatic link tracking and form tracking have been stripped to just the whitelist and blacklist features.\nCustom commands Finally, there are many commands in the Snowplow library that aren\u0026rsquo;t directly related to an event. These are all grouped under Custom Commands.\nMany of these are just plain commands without any arguments. In such instances, the Command argument field is not shown.\n  In other cases, there\u0026rsquo;s a field where the user can add the argument(s).\n  There\u0026rsquo;s only very little validation for these arguments. Typically it\u0026rsquo;s making sure the parameter is in the correct format, and with e.g. enableGdprContext it turns a comma-separated string into an array.\nHowever, the expectation is that when the template user adds custom commands, they either know exactly what they\u0026rsquo;re doing, or they\u0026rsquo;re reading documentation that explains how to set these things up.\nIn general, I\u0026rsquo;d recommend using Google Tag Manager variables instead of hard-coding the arguments. It makes it easier to switch on the fly if necessary.\nFiring the commands Finally, once the tracker is created and the commands have been built, they are all executed in order by invoking the global method.\nFor example, creating a new tracker, setting up page ping tracking, and sending a pageview would end up with the following items added to the global queue for execution (in order):\n  Summary Building the template has been a very educational journey, and it also helped spawn a number of feature requests for Google Tag Manager and for Snowplow\u0026rsquo;s JavaScript tracker, each of which I\u0026rsquo;m sure will make both solutions even stronger.\nEarly on I had an idea of creating a super smooth user interface, where every single parameter, event, and option was confined to its own field with full validation, value hints, help texts, and so forth.\nThis was a bad idea.\nFirst of all, there\u0026rsquo;s a limitation of something like 100 fields in any given template. Before you shout in indignation, it makes perfect sense. Having a behemoth of a template forced upon the UI every single time a new tag or variable is created is not favorable to Google Tag Manager\u0026rsquo;s performance.\nAlso, if you have a template that requires 100 fields, it\u0026rsquo;s possible you should look into modularizing it.\nThe Snowplow template is big. It\u0026rsquo;s close to the 100 field limit. I know this because I ran into the limit a couple of times and had to refactor. If it turns out to be a performance hog, I\u0026rsquo;ll look into splitting it up into smaller parts (e.g. one template for most used events, another for the rest).\nSeparating the tracker configuration into its own variable template was a step towards modularity, and depending on the feedback we\u0026rsquo;ll (Snowplow and I) see if there needs to be more of it.\nI hope this article has been useful. I\u0026rsquo;m still a huge fan of custom templates and firmly believe they are the future of Google\u0026rsquo;s tag management. I do hope that new APIs are consistently added to the sandbox, as some of the limitations are pretty jarring especially when methods you\u0026rsquo;d expect most JavaScript libraries to leverage are blocked (I\u0026rsquo;m looking at you, JSON.parse).\nIf you have feedback about the templates, be sure to raise them as issues in the respective repositories: Snowplow Analytics tag template and Snowplow Analytics Settings variable template.\n"
},
{
	"uri": "https://www.simoahava.com/tags/tag-templates/",
	"title": "tag templates",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/variable-templates/",
	"title": "variable templates",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/event-listeners/",
	"title": "event listeners",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-interactions-in-shadow-dom-google-tag-manager/",
	"title": "Track Interactions In The Shadow DOM Using Google Tag Manager",
	"tags": ["google tag manager", "javascript", "event listeners"],
	"description": "Tracking events that take place in a shadow DOM is tricky, bceause you need to inspect the path of the event rather than just process the event target. This article guides you in creating a custom listener for tracking events within shadow DOMs (where possible).",
	"content": "The shadow DOM is a way to add HTML structures to the web page so that they remain isolated from the rest of the document object model (DOM). It\u0026rsquo;s a popular concept with web components, as it allows for encapsulation of web structures so that they aren\u0026rsquo;t affected by style declarations of the parent tree, for example.\n  However, being such a \u0026ldquo;hidden\u0026rdquo; structure, anything that happens in the shadow DOM is also hidden from Google Tag Manager\u0026rsquo;s listeners. Or, the click does get registered, but the event target ({{Click Element}}) is the node on the parent structure that hosts the shadow DOM rather than the element that was clicked within the shadow DOM.\n  In this example, GTM would populate the click variables so that the click appeared to land on the \u0026lt;div class=\u0026quot;postShorten-wrap\u0026quot;\u0026gt; when it actually landed on the element within #shadow-root. Similarly, because the shadow DOM\u0026rsquo;s contents are hidden from the parent structure, the matches CSS selector predicate can\u0026rsquo;t be used to see what\u0026rsquo;s inside the clicked element.\nWe can work around this! We can\u0026rsquo;t use GTM\u0026rsquo;s built-in triggers as they don\u0026rsquo;t let us access the event object itself. But we can use a custom event listener.\n For more details on how event handling within the shadow DOM works, take a look at this excellent article on the topic.\n How event handling works with the shadow DOM Event listeners within the shadow DOM work just like event listeners on regular DOM structures. An event is registered, and it populates a path through the layers of the site during the capture and bubble phases.\nSome events bubble up towards the top of the DOM tree, some stay on the node where the event was registered.\nThe main difference with the shadow DOM is that events that start making their way up only cross the shadow DOM boundary if they have the property composed set to true.\nMost events have composed set to true. Typically, the exceptions are events that are not based on UI interactions. Like these:\n load unload abort error  For events that have composed set to true, we can attach a custom event listener on the document node, for example, and the events that take place within the shadow DOM will propagate to our listener (assuming they also bubble, or the listener has been set to detect the capture phase instead).\nHowever, we are still faced with the problem introduced in the beginning of this article. All events that take place in the shadow DOM are auto-delegated to the parent of the #shadow-root. This isn\u0026rsquo;t very helpful. The shadow DOM could be a huge, sprawling thing, so we need precision.\nLuckily, we can use the Event.composedPath() method to get an array that represents the path the event took as it bubbled up. The very first member in the array is the item that was actually clicked (unless the shadow DOM was closed, but we\u0026rsquo;ll get back to that in a minute).\nWe can use this information to build our listener.\nThe listener In Google Tag Manager, create a Custom HTML tag, and type or copy-paste the following code.\n\u0026lt;script\u0026gt; (function() { // Set to the event you want to track  var eventName = \u0026#39;click\u0026#39;, // Set to false if you don\u0026#39;t want to use capture phase  useCapture = true, // Set to false if you want to track all events and not just those in shadow DOM  trackOnlyShadowDom = true; var callback = function(event) { if (\u0026#39;composed\u0026#39; in event \u0026amp;\u0026amp; typeof event.composedPath === \u0026#39;function\u0026#39;) { // Get the path of elements the event climbed through, e.g.  // [span, div, div, section, body]  var path = event.composedPath(); // Fetch reference to the element that was actually clicked  var targetElement = path[0]; // Check if the element is WITHIN the shadow DOM (ignoring the root)  var shadowFound = path.length ? path.filter(function(i) { return !targetElement.shadowRoot \u0026amp;\u0026amp; !!i.shadowRoot; }).length \u0026gt; 0 : false; // If only shadow DOM events should be tracked and the element is not within one, return  if (trackOnlyShadowDom \u0026amp;\u0026amp; !shadowFound) return; // Push to dataLayer  window.dataLayer.push({ event: \u0026#39;custom_event_\u0026#39; + event.type, custom_event: { element: targetElement, elementId: targetElement.id || \u0026#39;\u0026#39;, elementClasses: targetElement.className || \u0026#39;\u0026#39;, elementUrl: targetElement.href || targetElement.action || \u0026#39;\u0026#39;, elementTarget: targetElement.target || \u0026#39;\u0026#39;, originalEvent: event, inShadowDom: shadowFound } }); } }; document.addEventListener(eventName, callback, useCapture); })(); \u0026lt;/script\u0026gt; You can attach a Page View trigger to this tag. After that, every single click on pages where the listener is active will be pushed into dataLayer with an object content that looks like this:\n  In this case, the click fell on a \u0026lt;div\u0026gt; with very few attributes, but which was contained in the shadow Dom (as isShadowDom is true).\nYou can then create a Custom Event trigger for custom_event_click:\n  And you can create Data Layer variables for the individual items in the pushed object like this:\n  By switching eventName to, say, submit, you can listen for form submissions instead.\nIf you want to avoid having the script push a message with every single event instance, you can add checks within the callback that verify the event target was a specific type of element. For example, to only push to dataLayer if the click landed on a link, you could do something like this:\nvar callback = function(event) { ... var targetElement = path[0]; if (targetElement.matches(\u0026#39;a, a *\u0026#39;)) { // Run the dataLayer.push() here } ... ...  Note! Though it was just an example, you should be aware that .matches() won\u0026rsquo;t work in IE, and you\u0026rsquo;ll need to use .msMatchesSelector().\n What about non-composed events? What if you want to track events that don\u0026rsquo;t have the composed flag set to true? If you remember, those events will not propagate past the shadow DOM boundaries. Similarly, if you use the script above, they will also have the inShadowDom flag set to false, as they are practically oblivious to the fact that they are in a shadow DOM (Matrix-style).\nSo, you\u0026rsquo;ll have to do event handling without the power of delegation. In other words, you\u0026rsquo;ll need to add the listeners directly to the elements.\nFor example, if you wanted to track a load event for a \u0026lt;script\u0026gt; within the shadow DOM, the script would look like this:\n\u0026lt;script\u0026gt; (function() { // Set to the event you want to track  var eventName = \u0026#39;load\u0026#39;; // useCapture is irrelevant as we\u0026#39;ll be tracking the element itself  // useCapture = true,  // trackOnlyShadowDom is irrelevant as we\u0026#39;ll be only tracking an element in the shadow DOM  // trackOnlyShadowDom = true;  var callback = function(event) { if (\u0026#39;composed\u0026#39; in event \u0026amp;\u0026amp; typeof event.composedPath === \u0026#39;function\u0026#39;) { // Irrelevant in this solution, as we are tracking the element itself  // var path = event.composedPath();  // Irrelevant.  // var targetElement = path[0];  var targetElement = event.target; // Irrelevant.  // var shadowFound = path.length ? path.filter(function(i) {  // return !targetElement.shadowRoot \u0026amp;\u0026amp; !!i.shadowRoot;  // }).length \u0026gt; 0 : false;  // Irrelevant  // if (trackOnlyShadowDom \u0026amp;\u0026amp; !shadowFound) return;  // Push to dataLayer  window.dataLayer.push({ event: \u0026#39;custom_event_\u0026#39; + event.type, custom_event: { element: targetElement, elementId: targetElement.id || \u0026#39;\u0026#39;, elementClasses: targetElement.className || \u0026#39;\u0026#39;, elementUrl: targetElement.href || targetElement.action || \u0026#39;\u0026#39;, elementTarget: targetElement.target || \u0026#39;\u0026#39;, originalEvent: event, inShadowDom: true } }); } }; // This is where the script sets the listener on the actual element in the shadow root  // The script checks if the container exists in the standard DOM, then it checks if the container  // is the shadow root, and finally it checks if the shadow DOM has the script element.  var shadowContainer = document.querySelector(\u0026#39;.shadowDomContainer\u0026#39;); if (!!shadowContainer \u0026amp;\u0026amp; !!shadowContainer.shadowRoot) { var scriptTarget = shadowContainer.shadowRoot.querySelector(\u0026#39;script#someId\u0026#39;); if (!!scriptTarget) scriptTarget.addEventListener(eventName, callback); } })(); \u0026lt;/script\u0026gt; Here the addEventListener call at the end is quite a bit more complex than the generic one we used before. You first need to find the node to which the shadow DOM is embedded (the shadow root). Then, accessing its shadowRoot property, you are allowed to query for elements within the shadow DOM (assuming the shadow DOM is open).\nAfter the usual checks for whether the element exists, you can then add your listener directly to the element, and it will invoke the callback as soon as it registers the event.\nWhat about a closed shadow DOM? If the shadow DOM is created in closed mode, you are basically unable to do anything with the shadowRoot. If you try to access it with DOM methods, the shadowRoot property will simply return null, and similarly the composedPath() returns an array of elements that stops with the shadow root node.\nThus, on a superficial level, there\u0026rsquo;s really nothing you can do to track the precise interactions within the shadow DOM.\nHowever, there is a workaround.\nWhen the shadow DOM is created, if the developers store a reference to it in a global variable, you can interact with it, and you can add listeners to the elements in the document fragment if you wish.\n// Create a shadow DOM var container = document.querySelector(\u0026#39;.someContainer\u0026#39;); window._shadowRoot = container.attachShadow({mode: \u0026#39;closed\u0026#39;});  In the sample above, the global variable _shadowRoot maintains a reference to the shadow DOM, and thus you can use methods like window._shadowRoot.addEventListener(...) to manipulate and interact with elements within the shadow DOM.\nIt does require some cooperation with the developers, and you\u0026rsquo;d also need to justify why the mode is set to closed in the first place if the developers add an opening by way of a global variable.\nSummary Hopefully this article has been instructive. I have a gut feeling that the shadow DOM is somewhat of a mystery to many web analytics developers simply because it\u0026rsquo;s not the most common way to add elements to the web page.\nHowever, it offers powerful tools for encapsulation and separation of concerns, and especially in lieu of clumsy iframe embeds it might make a lot of sense from a web development point of view.\nThe pointers in this article should give you the tools for identifying yet another potential issue with Google Tag Manager\u0026rsquo;s built-in listeners, and it should help you tackle it with the power of some custom scripting.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/use-all-events-trigger-more-control/",
	"title": "#GTMTips: Use An All Events Trigger For More Control",
	"tags": ["google tag manager", "gtmtips", "triggers"],
	"description": "Create an All Events trigger for more control over when your tag fires in Google Tag Manager. This is especially useful if you don&#39;t have a trigger event you can use consistently.",
	"content": "One of the most versatile triggers in Google Tag Manager is the Custom Event trigger. As its name indicates, you can use it to fire your tags when an event is pushed into dataLayer.\nThis process is at the heart of GTM\u0026rsquo;s dataLayer system. And it\u0026rsquo;s not just custom events. Every single trigger type in Google Tag Manager uses the event key in a dataLayer.push(), which is why you\u0026rsquo;ll see events like gtm.click (for the Click / All Elements trigger) and gtm.timer (for the Timer trigger) ending up in dataLayer, too.\nIn this article, we\u0026rsquo;ll explore a cool use case for the trigger I call an All Events trigger. An All Events trigger is a Custom Event trigger that\u0026rsquo;s set to fire when any event is pushed into dataLayer.\nTip 114: Use an All Events trigger to constantly check for something   Here\u0026rsquo;s the trick.\nLet\u0026rsquo;s say you want to fire your tag when a page element appears. You\u0026rsquo;ve created a DOM Element variable, and you now want to fire a tag as soon as this element is present on the page.\nHowever, you have no clue when the element actually appears. It\u0026rsquo;s not part of the page HTML but rather an asynchronously loading third-party library, for which you have no control over the render sequence. Furthermore, there\u0026rsquo;s no guarantee the element is ever visible in the browser viewport, so an element visibility doesn\u0026rsquo;t help, either.\nSo, what you need to do is create an All Events trigger that looks like this:\n  This trigger will activate with every single trigger event (that\u0026rsquo;s what the regular expression .* does), but it will only fire if the DOM Element variable returns a valid value (i.e. it exists).\nWe\u0026rsquo;re still missing one thing. Can you think of what it is?\nThat\u0026rsquo;s right, you got it! (Or you just read what\u0026rsquo;s in the hero image).\nIf you don\u0026rsquo;t make any modifications to the tag, it will fire for every single trigger event that\u0026rsquo;s pushed after the DOM Element becomes available. We don\u0026rsquo;t (likely) want that! So what you need to do in addition to adding the trigger to your tag, is to change the firing settings of the tag to Once per page:\n  That\u0026rsquo;s it! Now your tag will fire once as soon as the DOM Element appears on the page.\nSummary Simple trick but a very powerful trigger. Obviously it would be best if you\u0026rsquo;d cooperate with your developers to push an actual, predictable custom event into dataLayer as soon as the element becomes available. But if we lived in a perfect world I would have nothing to write about.\nAlso, remember the caveats with \u0026ldquo;Once per page\u0026rdquo; and single-page applications - \u0026ldquo;Once per page\u0026rdquo; is literally \u0026ldquo;Once per page load\u0026rdquo;, so with a SPA it would still fire just once, even if the user navigated to other pages of the site. In these scenarios you\u0026rsquo;ll really need to get the developers to cooperate and build a proper event-driven mechanism into dataLayer.\n"
},
{
	"uri": "https://www.simoahava.com/tags/triggers/",
	"title": "triggers",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/ip-geolocation-api/",
	"title": "IP Geolocation API - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The IP Geolocation API custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo      Description This template loads a JavaScript library from IP Geolocation API. This JavaScript library is automatically compiled to geolocate the user based on their IP address (or a custom IP address if you so choose). This information is then pushed into dataLayer for you to use.\nThe API can also be used to derive things like ISP data.\nThe API has a generous free tier, and the template includes the option of caching the information in browser storage to avoid making too many calls to the API.\nSee the API documentation and my accompanying blog post for more information on how the template can be used.\nRelease notes    Date Changeset     25 April 2020 Fix the latest update. Utilize API without jQuery. Use sessionStorage to cache results.   6 April 2020 Update to latest version of the API.   10 February 2020 Add IP hashing as an option.   3 January 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/use-gtag-js-parameters-google-tag-manager/",
	"title": "#GTMTips: Use gtag.js Parameters In Google Tag Manager",
	"tags": ["google tag manager", "gtmtips", "gtag.js"],
	"description": "You can use gtag.js events to trigger tags in Google Tag Manager, and you can use Data Layer variables to access the parameters sent to gtag.js as well.",
	"content": "With the proliferation of gtag.js implementations, we can see that there\u0026rsquo;s a small-ish paradigm shift in how to implement Google\u0026rsquo;s stack of marketing tools.\nAs adding gtag.js snippets to the site code becomes more and more common (to cater to things like early Optimize loading), you might be at a point where you have lots of interesting information stored in the gtag.js queue but no way to access it in your Google Tag Manager tags and variables.\nFret not! There is a way. Read on for elucidation.\nTip 113: Use gtag.js events and parameters in Google Tag Manager   Whenever gtag.js is used to collect an event, this information can be used to create Custom Event triggers and Data Layer variables in Google Tag Manager.\nThe way it works is very simple. Take this example:\ngtag(\u0026#39;event\u0026#39;, \u0026#39;my_custom_event\u0026#39;, { event_category: \u0026#39;some category\u0026#39;, event_label: \u0026#39;some label\u0026#39;, value: 15.00 });  Here my_custom_event is the \u0026ldquo;event action\u0026rdquo; in Google Analytics parlance. Coincidentally, it also becomes the event pushed into dataLayer. You can verify this in Preview mode, as it will show up in the list of events:\n  This means that you can create a Custom Event trigger with this \u0026ldquo;event action\u0026rdquo; to fire your tags when the gtag.js call happens.\n  As you can see, the parameters are also shown in Preview mode. The catch is that the parameters are embedded in a top-level key named eventModel. Thus, if you wanted to create Data Layer variables for them, you\u0026rsquo;d need to map them to:\n  event_category becomes eventModel.event_category\n  event_label becomes eventModel.event_label\n  value becomes eventModel.value\n    This method works with all events sent to gtag.js - Ecommerce as well, for example.\nIt won\u0026rsquo;t work with the config call, so the initial call to gtag.js, where you initialize the integration to Universal Analytics, for example, cannot be picked up by Google Tag Manager.\nSummary Short and sweet this time. This should prove useful for you if you want to access your site\u0026rsquo;s gtag.js integration with Google Tag Manager.\nIt\u0026rsquo;s not that rare to have both gtag.js and Google Tag Manager running on the site these days. There\u0026rsquo;s a convergence of sorts going on as well, with gtag.js\u0026rsquo; tech stack merging with Google Tag Manager under the hood as well. In fact, if you look at the gtag.js library and compare it with the gtm.js library, you can see a lot of similarities. And then take a look at the Google Optimize library, and the App+Web library. See the trend?\nFor many, Google Tag Manager remains the main way of consolidating tracking on the site, so being able to access the gtag.js queue without resorting to complicated hacks is useful indeed, I think.\n"
},
{
	"uri": "https://www.simoahava.com/tags/gtag.js/",
	"title": "gtag.js",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/persist-campaign-data/",
	"title": "Persist Campaign Data - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Persist Campaign Data custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Gallery entry   GitHub repo      Description This tag stores information in the page referrer and URL parameters in a browser cookie. This cookie can be used, for example, on conversion pages or after the user has given consent to attribute the session to the correct campaign.\nThe template can also be used to overcome the rogue referral problem.\nInstructions When you check Store campaign data in a browser cookie, you can then specify the URL parameters that will trigger a cookie write. If the page URL has any of those parameters when the tag fires, the page URL will be written into the browser cookie.\nIf the page referrer hostname doesn\u0026rsquo;t contain the current page hostname (i.e. the user came from another domain), the referrer will be written in a cookie as well.\nIf you check the Push original location in dataLayer option, the tag will push the current URL into dataLayer, and you can then utilize this to fix the rogue referral problem.\nYou can read more about the template from the associated blog post.\nRelease notes    Date Changeset     20 April 2020 Fix style in the dataLayer option.   30 November 2019 dclid was removed from the default parameters.   30 November 2019 Update name.   30 November 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/qualaroo/",
	"title": "Qualaroo - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Qualaroo custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description A tag created with template loads the Qualaroo JavaScript SDK.\nYou can also use it to initialize the Google Analytics integration, and you can use all of the available Qualaroo JavaScript APIs as well.\nInstructions To enable the Google Analytics integration, check the relevant checkbox and enter the Google Analytics tracking ID. This loads a default tracker with this UA ID, which Qualaroo can then use for its integration.\nThe other options relate to the APIs you can use via the template.\n  The Identity API lets you identify a user with an ID.\n  You can disable automatic displaying of surveys, show and hide surveys, and select a specific nudge. See this article for more information.\n  You can set additional targeting properties.\n  You can attach event handlers to the site with custom callbacks. See this article for more information.\n  Release notes    Date Changeset     20 April 2020 Fix event handler configuration.   7 April 2020 Added _kiq queue interactions and unit tests.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/",
	"title": "Custom Templates for Google Tag Manager",
	"tags": [],
	"description": "",
	"content": "   One of my passion projects has been to create custom templates for Google Tag Manager\u0026rsquo;s community template gallery.\nOn this page, I\u0026rsquo;ll list all the custom templates I\u0026rsquo;ve created for the gallery, as well as provide a link to each template\u0026rsquo;s individual documentation page.\nDo you need help with your template? If you want to commission me for help building your custom template, please contact me at simo (at) simoahava.com.\nRemember to also check out my articles on custom templates.\nClient templates (0)  Tag templates (20) CORE WEB VITALS   Latest release: January 21, 2021  | Comments   This tag adds measurements for Core Web Vitals (LCP, FID, CLS), and pushes them into dataLayer once each is collected.  Continue reading      GTAG GET API   Latest release: December 21, 2020  | Comments   This tag writes into dataLayer selected default and custom fields set with tags that utilize the gtag.js library; Google Analytics 4, for instance.  Continue reading      Transaction ID Logger   Latest release: December 11, 2020  | Comments   This tag stores the Transaction ID provided by the user in a browser cookie and/or localStorage.  Continue reading      Facebook Pixel   Latest release: November 11, 2020  | Comments   This is an unofficial template for the Facebook Pixel. You can use it to load the SDK, initiate the pixel(s), and to send custom and standard events to Facebook with any custom properties and user attributes you wish.  Continue reading      AudienceProject UserReport   Latest release: November 5, 2020  | Comments   This is an unofficial template for AudienceProject\u0026rsquo;s UserReport tag.  Continue reading      Consent Mode (Google tags)   Latest release: October 17, 2020  | Comments   This is a template for deploying Google tags\u0026rsquo; Consent Mode functionality. It lets you deploy Consent Mode with both the \u0026ldquo;default\u0026rdquo; and \u0026ldquo;update\u0026rdquo; commands.  Continue reading      Adform Tracking Point   Latest release: October 1, 2020  | Comments   This is an unofficial template for the AdForm Tracking Point. You can use it to load the SDK, initiate the tracking point, and to send page- and order-level data to AdForm.  Continue reading      Userpilot   Latest release: June 28, 2020  | Comments   The is an unofficial template for Userpilot. The tag can be used to identify users (with an ID and associated properties), reload Userpilot and re-evaluate page state, track custom events, and trigger specific Userpilot content for the user, regardless of targeting conditions.  Continue reading      User Distributor   Latest release: June 7, 2020  | Comments   You can use this tag to randomly assign users to buckets, and then to store this information in a first-party cookie. This is useful for quick A/B tests and for creating samples or cohorts of your visitors.  Continue reading      Facebook Customer Chat   Latest release: June 4, 2020  | Comments   This is an unofficial template for the Facebook Customer Chat SDK. You can use it to load the chat SDK and to attach event handlers to the API events.  Continue reading      Snowplow Analytics   Latest release: May 26, 2020  | Comments   This is the official template for the Snowplow Analytics SDK. It supports the full spread of features for the JavaScript tracker, with a few exceptions due to the sandbox of custom templates.  Continue reading      IP Geolocation API   Latest release: April 25, 2020  | Comments   This is an unofficial template for the IP Geolocation API. With this API, you can query for geolocation data using the user\u0026rsquo;s (or a custom) IP address. This information is then written into dataLayer.  Continue reading      Persist Campaign Data   Latest release: April 20, 2020  | Comments   This tag stores the page referrer and URL parameters in a browser cookie. This cookie can be used, for example, on conversion pages or after the user has given consent to attribute the session to the correct campaign. The template also stores the original location of the page to overcome the rogue referral problem on single-page applications.  Continue reading      Qualaroo   Latest release: April 20, 2020  | Comments   This is an unofficial template for Qualaroo. You can use this template to initialize the Google Analytics integration, and you can use all of the available Qualaroo JavaScript APIs as well.  Continue reading      YouTube iframe API loader   Latest release: April 3, 2020  | Comments   The is an unofficial template for the YouTube Iframe API. A tag created with this template loads the YouTube Iframe API on the page. This is necessary if you want to track interactions in lazy-loaded or dynamically added videos with Google Tag Manager\u0026rsquo;s native YouTube video trigger.  Continue reading      CDNJS - Hosted Libraries   Latest release: January 6, 2020  | Comments   Use this template to load hosted libraries from the CDNJS content distribution network.  Continue reading      Google Tag Manager Monitor   Latest release: November 15, 2019  | Comments   Use this template to set up a monitoring system for Google Tag Manager. With the monitor, you can collect data about tags that fired (or did not fire) with any given event. This information can be used to proactively fix issues with your tags.  Continue reading      Adservice   Latest release: October 2, 2019  | Comments   This is an unofficial template for the AdService SDK.  Continue reading      AppNexus   Latest release: October 2, 2019  | Comments   This is an unofficial template for the AppNexus pixel.  Continue reading      Conductrics   Latest release: October 2, 2019  | Comments   This is an unofficial template for the Conductrics library. You can use it to load the SDK/API on your site.  Continue reading       Variable templates (6) Transaction ID Reader   Latest release: December 11, 2020  | Comments   Designed to work with the Transaction ID Logger tag template, this variable template reads Transaction IDs stored in a browser cookie or localStorage.  Continue reading      EEC Products -\u0026gt; GA4 Items   Latest release: December 9, 2020  | Comments   This variable template takes an Enhanced Ecommerce products/impressions/promotions array as an input, and outputs it in the updated \u0026ldquo;items\u0026rdquo; format for Google Analytics 4 Ecommerce.  Continue reading      Snowplow Analytics Settings   Latest release: May 14, 2020  | Comments   This template is meant to be used with the Snowplow Analytics tag template. It generates a tracker configuration object for the tag, thus removing the need to configure the tracker manually for every single Snowplow Analytics tag in the container.  Continue reading      URL 2.0   Latest release: December 12, 2019  | Comments   This template can be used to replace the built-in URL variable type. It has additional options, such as building the string from different URL components, and for parsing the URL fragment for key-value parameters as well.  Continue reading      String from array of objects   Latest release: November 30, 2019  | Comments   This template takes an array of objects, and produces a string where the values of one of the given keys in these objects is concatenated into a single string.  Continue reading      Enhanced Ecommerce Object Builder   Latest release: September 16, 2019  | Comments   Use this template to build an Enhanced Ecommerce object, compatible with Universal Analytics. This way you do not necessarily have to use the dataLayer at all for populating Ecommerce data into Universal Analytics.  Continue reading       "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/google-analytics-tag-failed/",
	"title": "#GTMTips: Why Does The Google Analytics Tag Show &#39;Failed&#39;",
	"tags": ["google tag manager", "gtmtips", "debug"],
	"description": "Sometimes the Google Analytics tag shows status &#34;Failed&#34; in Preview mode. This article explains why that happens and how you can fix it.",
	"content": "I\u0026rsquo;ve covered the more pervasive issue with tags not firing in Google Tag Manager in my article on the \u0026ldquo;Still Running\u0026rdquo; status. However, there\u0026rsquo;s an additional problem you might face with Google Analytics: tags that show status Failed, and which refuse to send any data to Google Analytics.\nThere are a couple of possible reasons for this, and we\u0026rsquo;ll explore them in this article.\n Note that any tag type in Google Tag Manager can signal Failed. With the advent of Custom Templates, more and more templates are doing it right, meaning they elicit an explicit Failed status when the tag runs into issues. This isn\u0026rsquo;t the case with many native templates - they just gobble up the errors and refuse to signal that something went wrong.\n Tip 112: The Failed status in Universal Analytics tags   There are two reasons (that I could think of) that manifest this behavior.\nReason 1: The analytics.js library is blocked If your browser is blocking the analytics.js library, meaning the client is unable to load it from https://www.google-analytics.com/analytics.js, the tag will signal the Failed status.\nThis can be due to browser extensions such as Ghostery or uBlock Origin, or it could be a firewall setting in your network. It could even be the browser itself (Firefox in a Private Window, for example).\n  Easiest way to check if this is the reason is to open the Network tab of your browser\u0026rsquo;s developer tools, and search for analytics.js. If it\u0026rsquo;s blocked, it should either not show up at all, or it should appear with a clear indicator. In some browsers, you\u0026rsquo;ll see messages about resource blocking in the browser console.\n  The image above is from Firefox.\n Note! If the analytics.js library manages to load, but the extension or browser suppresses the calls to the www.google-analytics.com/collect endpoint, the tag will not signal the Failed status.\n Reason 2: The namespace is occupied When the Google Analytics library loads, it tries to create a global method named ga (by default). This method name can be changed in the analytics.js snippet as well as with Google Tag Manager (see below).\nHowever, if this global variable is already taken, Google Analytics fails to initialize and the tag status is set to Failed.\nEasy way to check this is to run the following command in the JavaScript console:\nconsole.log(window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]);  The response should resemble something like this:\n  However, it\u0026rsquo;s minified and could change, so it\u0026rsquo;s not really a solid way to check. Instead, you can also try this:\nconsole.log(window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]].loaded);  If you get undefined, it means Google Analytics has not loaded, and something else is occupying the namespace.\nIt\u0026rsquo;s very difficult to debug this if some other JavaScript is really overwriting GA\u0026rsquo;s namespace. You can try looking for it in the sources of your browser\u0026rsquo;s developer tools, or you could try blocking scripts one by one until you find the one that causes the conflict.\nSometimes it\u0026rsquo;s not JavaScript that overwrites the object, though. Browsers have a funky way of assigning HTML elements to the global window variable as well, as long as they have an id attribute.\nIn other words, if the page has an HTML element whose ID is the same as the Google Analytics object name (ga by default), the tag will also signal Failed status! Easy way to check is to run the following code in the JavaScript console:\nconsole.log(document.querySelector(\u0026#39;#\u0026#39; + window.GoogleAnalyticsObject));  If this returns an HTML element, it means you have this precise problem.\n  Fix it! So, what\u0026rsquo;s the fix?\nWell, you could run around the site, looking for the JavaScript that\u0026rsquo;s causing issues, or you can simply rename the global method using Google Tag Manager. By renaming the method, you\u0026rsquo;re protecting the variable from further conflicts.\n You only need to do this once, so it doesn\u0026rsquo;t matter if it\u0026rsquo;s in the Google Analytics Settings variable or in the settings of an individual tag. You do want to make sure the change is in the first tag that fires on the page, so making the change in all your tags is a good precaution.\n You can find the setting by expanding the Advanced Configuration of your Universal Analytics tags (or the Google Analytics Settings variable).\nType a new value in the Global Function Name field, and make sure this variable isn\u0026rsquo;t reserved (by e.g. running console.log(window['yourNewVariableName']) in the JavaScript console). I usually choose something like _ga or __ga.\n  Once you make that change, your tags should start working nicely.\nSummary It\u0026rsquo;s an edge case, but when it hits you, it might be very difficult to debug.\nScripts taking over global variables is one of the prime reasons you should always avoid polluting the global namespace when writing JavaScript.\nHowever, when there is a conflict, you\u0026rsquo;re usually left with two options:\n  Find the conflicting script or HTML element, and fix it.\n  Accept defeat and edit the Universal Analytics tag settings that fire on the page.\n  Both have their drawbacks. Sometimes you can\u0026rsquo;t change the conflicting script, because the script has no capability to support changes to the global namespace. And sometimes changing the method name in your tags can lead to problems when trying to figure out why first-party and third-party scripts that rely on the default method name (ga) suddenly stopped working.\nImportant thing is to know how to debug and triage the problem. This article has hopefully given some guidance for how to do this.\n"
},
{
	"uri": "https://www.simoahava.com/tags/debug/",
	"title": "debug",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/youtube-iframe-api-loader/",
	"title": "YouTube iframe API loader - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The YouTube iframe API loader custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description A tag created with this template loads the https://www.youtube.com/iframe_api library on the page. This is necessary if you want to track interactions in lazy-loaded or dynamically added videos with Google Tag Manager\u0026rsquo;s native YouTube video trigger.\nInstructions Simply add a Page View trigger to the tag (you can delimit this further to only fire on pages that will have videos inserted on them), and it will insert the library on these pages.\nYou only need to do this on pages that have dynamically inserted videos. If the page loads with YouTube videos, you do not need this template. You can simply use the default YouTube trigger, as it will automatically load the API if it finds a YouTube video on the page when it first loads.\nRelease notes    Date Changeset     3 April 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/simple-way-exclude-internal-visits-google-analytics/",
	"title": "#GTMTips: Simple Way To Exclude Internal Visits From Google Analytics",
	"tags": ["google tag manager", "universal analytics"],
	"description": "When IP exclusion is not an option, excluding internal traffic to your site in Google Analytics by way of a query parameter in the URL is a simple and elegant option.",
	"content": "With so many people working from home or remotely in these turbulent times, it\u0026rsquo;s time to revisit one of my oldest articles, and discuss the options you have for excluding or segmenting internal traffic in Google Analytics.\nThe traditional method of IP address exclusion is not necessarily the best option anymore, unless all your employees use a specific VPN to connect to the site.\nIn this article, we\u0026rsquo;ll go through some of the tools you have at your disposal.\nTip 111: Exclude internal visits from Google Analytics   In my opinion, the simplest and most elegant way to exclude an arbitrary mass of users is to use the URL query parameter method I\u0026rsquo;ll outline below.\nThe URL query parameter method The way it works is that all employees, remoters, and other users whose traffic should be counted as internal should visit the site using a specific URL query parameter. With Google Tag Manager, you can take this query parameter and send it as a User-scoped Custom Dimension to Google Analytics.\nYou can then create a view filter that excludes this particular Custom Dimension from your main reporting views, or you can use segments to identify the traffic in the reports.\nTo get started, you\u0026rsquo;ll need to create a new URL query variable in Google Tag Manager.\nThe URL query variable Go to Variables, scroll down to User-Defined Variables, and click the NEW button to create a new variable.\nChoose URL as the type.\nSet Component Type to Query, and Query Key to internal.\n  Name the variable something like URL - Query - internal and save the variable.\nThe Custom Dimension In Google Analytics, go to Admin, and click Custom Definitions in the middle column (Property settings). Choose Custom Dimensions.\nClick + NEW CUSTOM DIMENSION to create a new Custom Dimension.\nCreate a User-scoped dimension, and name it Internal Traffic.\n  Once it\u0026rsquo;s created, make note of its Index number.\nModify your Google Analytics tag In Google Tag Manager, find the Page View tag, and open it for editing.\nIf you haven\u0026rsquo;t done so already, check Enable overriding settings in this tag.\nExpand More Settings, then Custom Dimensions, and click +Add Custom Dimension to add a new Custom Dimension.\nSet the Index of the row to the Custom Dimension index number you got from Google Analytics, and set the Dimension Value as the variable you just created.\n  Save the changes.\nTEST! Now you can visit the site. First, visit the page without the query parameter. You should see your Google Analytics tag fire, and the Custom Dimension where you added the URL variable to should show undefined. That\u0026rsquo;s good - we don\u0026rsquo;t want to send a value to GA unless it\u0026rsquo;s true.\n  Then, add ?internal=true (or \u0026amp;internal=true if the URL already has a query string) to the end of the URL, before any possible hash fragment (#hash), and hit enter to reload the page.\nThis time you should see your Google Analytics tag pick the string true and send it as the value of the Custom Dimension to Google Analytics.\n  That\u0026rsquo;s it! Now you\u0026rsquo;re populating the User-scoped Custom Dimension with the value true for all users who visit the site with the query parameter internal=true.\nCreate a view filter You could simply use segments to separate the traffic into internal and the rest, but perhaps you want to use View filters instead to avoid polluting your view with internal traffic.\nThis is what the view filter would look like:\n  When this filter is activated, all sessions from users who have the value true sent even once to GA will be filtered out of the Google Analytics view.\nHow to make the users\u0026rsquo; data reappear If you want to roll this back for any given user, all they have to do is visit the site with the internal query parameter, and use literally any other value except true.\nSo they could visit with internal=false or internal=ratamahatta - anything goes.\nAs soon as they send a non-true value to Google Analytics as the value of the internal parameter, they will be counted among non-internal traffic again.\nOther options If you think using query parameters is cumbersome, you could write a simple URL rewrite rule, which redirects visits to e.g. office.domain.com to www.domain.com/?internal=true. That way users would need to just remember the domain name instead of having to add the URL parameter.\nAlternatively, you could suggest to users to install a browser extension that automatically blocks their hits from given websites. This is simple to do as well, but does require a specific browser to be used.\nIf the users use a VPN when on company hours, you can configure the VPN IP for exclusion in Google Analytics, or you can modify the VPN to add an HTTP header (x-internal-traffic or something), which your web server can then translate to a dataLayer.push().\nThere are many ways to skin the cat, but the key is that there\u0026rsquo;s almost certainly a manual step that all internal users need to perform. The query parameter is a set-and-forget method, and that\u0026rsquo;s why I prefer it personally.\nSummary This article was a reintroduction of a six-year-old solution of mine, and never before has it been more timely. With the COVID-19 pandemic raging, more and more people are working remotely, leading to a surprisingly potent problem of how to block this traffic from your main reporting views in Google Analytics.\nThe URL query parameter solution is simple, because optimally your users only need to do this once per browser. Naturally, with e.g. Safari\u0026rsquo;s ITP and its short cap on cookie expiration, it\u0026rsquo;s not actually quite that simple. But with tracking prevention, nothing is.\nOther solutions were listed at the end of the article, but I\u0026rsquo;d love to hear your feedback - do you have ideas / solutions that are missing from this article? Please share them in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/tags/samesite/",
	"title": "samesite",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/cookieflags-field-google-analytics/",
	"title": "The New cookieFlags Setting In Google Analytics",
	"tags": ["google analytics", "google tag manager", "app+web", "samesite"],
	"description": "The new cookieFlags field for Google Analytics allows you to set fields like SameSite and Secure on the Google Analytics cookies.",
	"content": "With the enforcement of SameSite settings in the latest versions of Google Chrome, it\u0026rsquo;s become a mad scramble to get cookies working across first-party and third-party contexts. I\u0026rsquo;ve covered this phenomenon before in my SameSite article, as well as in my guide for setting up cookieless tracking for iframes.\nRecently, Google Analytics updated its libraries (App+Web, gtag.js, and analytics.js) with a new setting: cookieFlags (analytics.js) or cookie_flags (App+Web and gtag.js).\n NOTE 25 March 2020: The feature isn\u0026rsquo;t yet officially released, and there might be additional features added to the functionality before it is officially documented. I will update this article with any updates, if necessary.\n In this short article, I\u0026rsquo;ll show you how to use it to customize the cookies Google Analytics uses.\n  Cookie directives When you create a cookie, you give it a name and a value. Google Analytics, for example, creates a cookie named _ga with a pseudo-random Client ID generated for the current browser instance.\nIf you create a cookie with nothing but the name and value, it have the following features by default:\n It will be a session cookie, meaning it has no expiration. When the browser closes, all session cookies are cleared (though not always). It will be written on the current domain the browser is on. It will be written on the current path the browser is on.  On top of these, in the latest version of the Google Chrome browser, the cookie will also be treated as having the SameSite=Lax flag. This means the cookie will not work when accessed in a third-party context.\nWhen setting a cookie, you can configure these fields to your liking. The directives available for configuring are:\n   Directive Description Example value Google Analytics field     Expires Maximum lifetime of the cookie, specified with a date string. Expires=Tue, 24 Mar 2020 13:37:28 GMT cookieExpires and cookie_expires   Max-Age Maximum lifetime of the cookie, specified in seconds. Max-Age=7200 -   Domain The domain on which the cookie is written. Domain=simoahava.com cookieDomain and cookie_domain   Path The path on which the cookie is written. Path=/ -   Secure The cookie is only sent to the server if the request is made over HTTPS. Secure -   HttpOnly Prevents the cookie from being accessed with JavaScript. HttpOnly -   SameSite Specifies the context in which the cookie can be accessed. SameSite=Strict -    As you can see, there are some fields that you can already set with pre-existing Google Analytics settings, but you\u0026rsquo;re still missing the option of configuring all aspects of the cookie.\nThe new cookieFlags field The new cookieFlags field allows you to set any cookie directive when the Google Analytics cookie is created.\n Naturally, this excludes HttpOnly as that is only available for cookies set in the HTTP response.\n The value of this setting is a semi-colon separated list of lowercase cookie directives and their respective values. For example, this is a possible value of cookieFlags:\nmax-age=7200;domain=simoahava.com;path=/;secure;samesite=none\nIf set as the value of cookieFlags, it would create the Google Analytics cookie with an expiration of two hours (7200 seconds), set it on the simoahava.com domain and on the root path, and make sure it\u0026rsquo;s only sent with HTTPS requests and make it available as a third-party cookie.\nImportantly, cookieFlags takes precedence if some of the flags have already been set with e.g. cookieExpires and cookieDomain. If there\u0026rsquo;s a conflict, cookieFlags wins.\nSet the field in analytics.js To set the field with Universal Analytics where you\u0026rsquo;re using inline script to collect data (i.e. not Google Tag Manager), you\u0026rsquo;d set it like this:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-XXXXX-Y\u0026#39;, { cookieFlags: \u0026#39;max-age=7200;secure;samesite=none\u0026#39; });  The correct place for the cookieFlags parameter is in the tracker creation method, embedded in an object you pass as a parameter to the ga() method.\nSet the field in gtag.js You can set the field with an inline gtag.js implementation as well. The name of the field is cookie_flags rather than cookieFlags.\ngtag(\u0026#39;config\u0026#39;, \u0026#39;G-N2A3FMNDT5\u0026#39;, { cookie_flags: \u0026#39;max-age=7200;secure;samesite=none\u0026#39; });  Set the field in Google Tag Manager In Google Tag Manager, the main places where you\u0026rsquo;d modify this field are the Universal Analytics tags and the App+Web Configuration tags.\nUniversal Analytics tag This includes the Google Analytics Settings variable.\nThe name of the field is cookieFlags and the value is the string of directives you want to set.\n  App+Web Configuration tag The name of the field is cookie_flags.\n  Summary This is an important update to Google Analytics\u0026rsquo; data collection libraries. There are so many scenarios where the first-party cookies used by Google Analytics need to be accessed in third-party context.\nThese scenarios include, for example, embedded booking flows, embedded forms, and login portals.\nWithout setting the samesite=none;secure flags in Google Analytics\u0026rsquo; settings, the cookies created by GA would not be available in third-party context, thus messing with your ability to track the same user on the parent site and in the embedded resource.\nThe other fields are useful as well - Max-Age is so much easier to use than Expires, and setting the cookie Secure by default is just a good practice.\nLet me know in the comments if you have questions about this new setting!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/keep-ios-android-firebase-containers-sync/",
	"title": "#GTMTips: Keep iOS And Android Firebase Containers In Sync",
	"tags": ["google tag manager", "gtmtips", "firebase", "mobile"],
	"description": "You can export and import containers between iOS and Android Firebase GTM. However, before you can do that, you need to handle some incompatibilities between the two formats.",
	"content": "With iOS and Android containers available for Google Tag Manager, it\u0026rsquo;s tempting to add GTM as an integration into an existing Firebase setup for your apps. It\u0026rsquo;s also a fine way to get acquainted with Firebase in the first place, as it has a plethora of features to make application development easier.\nFurthermore, with the advent of App + Web, there\u0026rsquo;s even more incentive to integrate your app with Firebase.\nAdding Google Tag Manager is basically one extra dependency, and even if the feature set at the moment isn\u0026rsquo;t too spectacular, I\u0026rsquo;d be surprised if things like custom templates wouldn\u0026rsquo;t make their way to mobile at some point.\nIn this article, I\u0026rsquo;ll show a simple trick to keep your Google Tag Manager containers for iOS and Android in sync. They share the same core functionality, so exporting and importing containers across the two platforms is quite frictionless. However, there are some compatibility issues you need to tackle before transferring the data across the platform boundary.\nTip 110: Keep iOS and Android GTM containers in sync   First of all, this will not work with legacy GTM containers. The containers must be Firebase (i.e. the latest version of Google Tag Manager for apps).\nTo sync the containers, you need to export the workspace JSON from the source, and import it into the target. You can read this instruction to learn about importing and exporting in general.\nModify the usageContext key If you want the import/export to work across platforms (Android to iOS, or vice versa), there\u0026rsquo;s one parameter value that you need to change in the JSON. There\u0026rsquo;s a key named containerVersion.container.usageContext that you need to edit. Here\u0026rsquo;s what it looks like on Android:\n{ \u0026#34;exportFormatVersion\u0026#34;: 2, \u0026#34;exportTime\u0026#34;: \u0026#34;2020-03-13 07:15:59\u0026#34;, \u0026#34;containerVersion\u0026#34;: { ... \u0026#34;container\u0026#34;: { ... \u0026#34;usageContext\u0026#34;: [ \u0026#34;ANDROID_SDK_5\u0026#34; ], ... }, ... } } With iOS it\u0026rsquo;s almost the same, except the value is [\u0026quot;IOS_SDK_5\u0026quot;].\nSo, if you want your Android container to work in iOS, you need to change the value from [\u0026quot;ANDROID_SDK_5\u0026quot;] to [\u0026quot;IOS_SDK_5\u0026quot;].\nIf you want your iOS container to work in Android, you need to change the value from [\u0026quot;IOS_SDK_5\u0026quot;] to [\u0026quot;ANDROID_SDK_5\u0026quot;].\nOnce you\u0026rsquo;ve made those changes, the container should be importable between the two platforms.\nHowever, there are certain triggers and variables that only work in one platform but not the other. You need to delete these from the container JSON if you want the import/export to work.\nIncompatibilities Once you\u0026rsquo;ve made the change in the previous chapter, you might see the following error when trying to import the container:\n  This is because there are incompatibilities between some triggers and variables between the two platforms. I\u0026rsquo;ve listed them in this table:\n   Type Name Compatibility     Trigger App Exception Android only   Trigger App Update iOS only   Trigger First Open iOS only   Trigger In-App Purchase iOS only   Trigger Notification Dismiss Android only   Trigger Notification Receive Android only   Variable Device ID Android only   Built-in Variable App Version Name Android only   Built-in Variable In-app Purchase * iOS only   Built-in Variable Notification * iOS only    If the item is \u0026ldquo;X only\u0026rdquo;, it will only work in that type of container, and must be deleted from the export if you want to import the JSON into the other type of container.\nTo delete an item, you need to delete the entire object block containing the parameters. The block starts with { and ends with }. For example, if you wanted to delete the item from the screenshot above, you\u0026rsquo;d find the following block and delete it.\n{ \u0026#34;accountId\u0026#34;: \u0026#34;23019854\u0026#34;, \u0026#34;containerId\u0026#34;: \u0026#34;2750082\u0026#34;, \u0026#34;triggerId\u0026#34;: \u0026#34;42\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;First Open\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;FIREBASE_FIRST_OPEN\u0026#34;, \u0026#34;fingerprint\u0026#34;: \u0026#34;1584083607944\u0026#34; } Built-in Variables are an exception - they won\u0026rsquo;t break the import/export, but they simply won\u0026rsquo;t appear in the target container in the list of Built-in Variables. Any tags, triggers, or variables that reference the missing Built-in Variables will retain those references, meaning the container will not validate if you try to create a version or publish it. You need to manually edit each of these missing variable links to make the container work.\nNote that all the In-App Purchase and all the Notification Built-in Variables are iOS only.\nOnce you\u0026rsquo;ve handled all the incompatibilities, the import/export should work without a hitch.\nSummary Hopefully iOS and Android containers will converge even more in the future, so that you don\u0026rsquo;t have to manually make any edits to get the sync to work in Google Tag Manager.\nOr, even better, hopefully GTM will handle the cross-platform import/export gracefully, automatically changing what can be changed, and flagging differences that require manual input clearly in the interface.\nHaving said that, it\u0026rsquo;s still so much easier to import/export across platforms with Firebase containers compared to what we had with the legacy Google Tag Manager containers.\nThe edits you have to make to get the conversion to work are fairly minor. The bigger headache is trying to figure out how to replace the functionality of items that are not supported. For example, if you have a trigger for In-App Purchase running in iOS, there just isn\u0026rsquo;t a similar built-in trigger in Android. You\u0026rsquo;d need to implement a custom solution to handle this tracking.\nKeeping the containers in sync is more than just editing the JSON. But knowing what changes need to be made is a good first step in improving the workflow - that\u0026rsquo;s why I considered this article to be useful in the first place.\nLet me know in the comments if you have trouble getting the containers to sync across platforms!\n"
},
{
	"uri": "https://www.simoahava.com/tags/firebase/",
	"title": "firebase",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/mobile/",
	"title": "mobile",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/measurement-protocol/",
	"title": "measurement protocol",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/queue-time/",
	"title": "queue time",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/send-hits-past-google-analytics/",
	"title": "Send Hits To The Past With Google Analytics",
	"tags": ["google tag manager", "google analytics", "measurement protocol", "queue time"],
	"description": "Google Analytics has the Queue Time parameter that lets you send hits to the past. However, there are limitations to how this parameter can be used, and these will be explored in the article.",
	"content": "One of the hard-and-fast rules in Google Analytics is that once hits have been collected and processed into your data properties, those hits are untouchable. This means that if you mistakenly collect duplicate or incorrect transactions, PII traffic, or referral spam, for example, it\u0026rsquo;s extremely difficult, if not downright impossible, to purge or change this data in Google Analytics.\nAnother staple of Google Analytics\u0026rsquo; strict schema is that displacing hits in time is also very difficult. Hits are digested as they come in, and they are placed in time in the order of processing. If Google Analytics collects hit B after hit A, then it will be reported as having occurred in that order, too.\nHowever, there is a feature in the Google Analytics collection protocol that does allow you to realign a hit in the past timeline. So even if hit B is sent after hit A, you can still have it enter data tables with a timestamp that precedes hit A.\nThis feature is called Queue Time, and because of some fairly confusing documentation, it has been either easily ignored or misunderstood.\n  In this article, I\u0026rsquo;ll explain how Queue Time works, and how you can use it to realign hits.\nWhat is Queue Time Queue Time is a parameter in the Measurement Protocol (qt). It takes a zero or a positive integer number as its value.\nWith the Queue Time parameter, you can instruct Google Analytics to process the hit so that it would appear as having occurred as many milliseconds ago from the collection moment as specified by the parameter.\nFor example, if you send a hit to Google Analytics at precisely 12:35:05 with the following qt value:\n\u0026amp;qt=5000\nThe hit will have a timestamp of 12:35:00, as you instructed Google Analytics to assign the hit with a timestamp 5 seconds (5000 milliseconds) earlier than it was actually collected.\nThis is a very useful parameter, and is actively used in things like service workers to correctly place hits collected when the network was down, and with mobile app analytics to make sure that batched and deferred hits are collected correctly.\nYou could also use it with other offline data collection systems like point-of-sales machines, in order to place a transaction in Google Analytics at the actual time of the transaction, even if you send the data in batches every evening after business hours.\nLimitation: only same-day hits And now we get to the biggest source of confusion.\n  The documentation states that Values greater than four hours may lead to hits not being processed, but it doesn\u0026rsquo;t really say what this means.\nTo clarify this, here\u0026rsquo;s the main limitation of Queue Time:\nQueue Time only lets you send hits to the current day (with one major caveat, coming right up).\nThus, for any day that starts at 00:00:00 (midnight) and ends at 23:59:59, the hit that uses Queue Time must also occur within that time window.\nHowever. There are two catches.\nCatch 1: You can send hits until 4 a.m. of the following day The time window is artifically extended to 03:59:59 of the following morning.\nThus, if you want to send a hit so that it happened on Friday, March 6th 2020, the hit must be sent either during March 6th, or no later than 03:59:59 in the morning of March 7th.\nThis means that the maximum value for Queue Time is 100799000 milliseconds. That\u0026rsquo;s 27 hours, 59 minutes, and 59 seconds. That hit must be sent at 03:59:59, and it will be processed by Google Analytics as having occurred at 00:00:00 the previous day.\nCatch 2: The concept of \u0026ldquo;time\u0026rdquo; is bound to the timezone settings of the Google Analytics view And here\u0026rsquo;s where it gets tricky.\n  The only way for your 100799000 millisecond Queue Time will work is if you send it at 03:59:59 of the following day in the timezone of the Google Analytics view that will report the hit.\nAs you can probably understand, this will lead to issues if you have a property with multiple views in different timezones.\nFour hours is the maximum length of time that is guaranteed to work Perhaps now the weird four-hour processing limitation from the Measurement Protocol documentation is a bit clearer.\nIf you use Queue Time, then a maximum value of 14400000, that\u0026rsquo;s 4 hours, is guaranteed to work. This is because regardless of timezone in the Google Analytics property, a hit with a Queue Time of 14400000 is always guaranteed to either fall within the previous day (if sent no later than 03:59:59 the following morning), or the current day (if sent at 04:00:00 in the morning or later).\nLet\u0026rsquo;s imagine you send a hit with a Queue Time of 14400000, and the timezone you send it in is in UTC. The date is March 7th, and the local time is 03:59:59 in the morning.\nHere\u0026rsquo;s how the hit is processed from the point-of-view of a Google Analytics view that receives the hit.\n   Google Analytics timezone Date and time of hit sent Date and time of hit processed Valid     UTC March 7, 03:59:59 March 6, 23:59:59 Yes   UTC-06:00 March 6, 21:59:59 March 6, 17:59:59 Yes   UTC+01:00 March 7, 04:59:59 March 7, 00:59:59 Yes   UTC+10:00 March 7, 13:59:59 March 7, 09:59:59 Yes    See how regardless of timezone, the hit is valid. That\u0026rsquo;s because the hit is sent and processed either during the same day, or the hit is sent before 4 a.m. the following day, per the Google Analytics timezone.\nLet\u0026rsquo;s try a value larger than that. We\u0026rsquo;ll attempt to send the hit 7 hours into the past - that\u0026rsquo;s 25200000 milliseconds. We\u0026rsquo;re still in UTC, the date is still March 7th, and it\u0026rsquo;s still 03:59:59 in the morning.\n   Google Analytics timezone Date and time of hit sent Date and time of hit processed Valid     UTC March 7, 03:59:59 March 6, 20:59:59 Yes   UTC-06:00 March 6, 21:59:59 March 6, 17:59:59 Yes   UTC+01:00 March 7, 04:59:59 March 6, 21:59:59 NO   UTC+10:00 March 7, 13:59:59 March 7, 06:59:59 Yes    All other hits are valid except the one collected in the Google Analytics view in the UTC+01:00 timezone. This is because the hit is queued to have happened during March 6, but it\u0026rsquo;s sent after 4 a.m. on March 7th.\nHow to implement To add the Queue Time parameter to a hit, you simply need to set it in the hit itself. If you\u0026rsquo;re using Google Tag Manager, it\u0026rsquo;s as simple as adding a new Fields to set that looks something like this:\n  The field name is \u0026amp;qt, and the value is the delta in milliseconds of when the hit should be processed vs. when it is sent.\nWith analytics.js, you\u0026rsquo;d set it like this:\nga(\u0026#39;send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;Some Category\u0026#39;, \u0026#39;Some Action\u0026#39;, {\u0026#39;\u0026amp;qt\u0026#39;: 5000});  As far as I know, there is no way to set arbitrary Measurement Protocol parameters in a gtag.js call.\nSummary The Queue Time parameter lets you send hits to the past with Google Analytics. As such, it\u0026rsquo;s a fairly powerful utility. There are many reasons why you\u0026rsquo;d want to delay hits from being sent at time of collection, only to still be processed in the correct time within the Google Analytics timeline.\nThe catch is that you can only append hits to the current day. There\u0026rsquo;s no way to send hits more than a day into the past, unfortunately.\nGoogle Analytics extends the time window to stretch until 4 a.m. of the following day, but the problem is that the concepts of \u0026ldquo;current day\u0026rdquo; and \u0026ldquo;following day\u0026rdquo; are established by the timezone of the settings of each individual Google Analytics view.\nThus when you use the Queue Time parameter in Measurement Protocol hits, you need to either correlate the client time (time of the browser or machine sending the hits) with the time of the Google Analytics view to which you send the hits. If you can\u0026rsquo;t do this correlation, then you should make sure to only send hits a maximum of four hours into the past, because those are the only types of queued hits that are guaranteed to work.\n"
},
{
	"uri": "https://www.simoahava.com/tags/api/",
	"title": "api",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/cloud-functions/",
	"title": "cloud functions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/create-slack-notification-system-google-tag-manager-changes/",
	"title": "Create Slack Notification System For Google Tag Manager Changes",
	"tags": ["google tag manager", "google cloud", "cloud functions", "slack", "api"],
	"description": "Use Google Cloud Functions to notify Slack whenever a change is detected in a Google Tag Manager container. The example in this article covers notifying about recently published container versions.",
	"content": "Until recently, I had a feature on GTM Tools that polled the user\u0026rsquo;s Google Tag Manager container(s) for a recently published version. If one was found, a notification was sent to a Slack app, which forwarded it to a workspace and channel of the user\u0026rsquo;s choice.\nThis was fine, except for the fact that polling the GTM and Slack APIs for dozens upon dozens of containers is a total resource hog, and the only way I can maintain GTM Tools is it doesn\u0026rsquo;t have API leaks like that.\nI thus deprecated the feature. Feeling bad about the decision (and because I got a bunch of feedback that users were actually using the feature), I wanted to share a step-by-step guide on how anyone can setup a simple Slack notification system for changes in a Google Tag Manager container.\n  It\u0026rsquo;s a Node.js application running on Google Cloud Functions. Google Cloud Functions are serverless, self-contained functions on the Google Cloud Platform (GCP) that can be triggered with different stimuli, such as an HTTP request or a Pub/Sub message. For our purposes, it\u0026rsquo;s the perfect tech because what we\u0026rsquo;re trying to do is a very simple operation that needs to run periodically with zero- or low-cost.\nHow it works Once it\u0026rsquo;s up and running, here\u0026rsquo;s how the solution works:\n Every N minutes (you can choose the interval), a cron job running on Cloud Scheduler will send a Pub/Sub message. This Pub/Sub message triggers the Cloud Function, which hosts a simple, serverless Node.js application. The application downloads a state object from Cloud Storage, which contains the most recent information (since the previous triggering) about each Google Tag Manager container you\u0026rsquo;ve specified in the application configuration*. Then, the application polls the Google Tag Manager API to fetch details about the published version of each container. If the application detects a change in the version ID of the published version (compared to that stored in the state object), it sends a message using a Slack Incoming Webhook. This webhook posts the message to the appropriate channel in the Slack workspace you\u0026rsquo;ve chosen for the integration.    Once everything is done, the application stores the new, updated state back into Google Cloud Storage and finishes execution.\nWhat you\u0026rsquo;ll need To make the whole thing work, you\u0026rsquo;ll need a Google Cloud Platform account. In addition to that, you\u0026rsquo;ll need:\n Access to a Slack workspace, with permissions to add a bot to a channel. A credit card (this solution will most likely be zero-cost, but you need to add your billing information regardless). A Google Cloud billing account. Permissions to add a new READ user to the Google Tag Manager container(s) you want to observe.  Step 1: Create a new Google Cloud Platform project The first thing you\u0026rsquo;ll need to do is create a new project in GCP.\nYou can do this by following this link.\nGive your project a name, and make sure you select a valid Billing Account for it.\n  The console will hang for a while as your new project is being created. Once the project is created, you can visit it via the project selector.\nStep 2: Configure the GCP project Enable the APIs Next thing you\u0026rsquo;ll need to do is enable the APIs that the application will require. You\u0026rsquo;ll also need to create a bucket in Google Cloud Storage, but we\u0026rsquo;ll get to that after the APIs have been enabled.\nWhile making sure your new project is selected in the console, type Cloud Functions API into the search bar and click the result.\n  In the screen that opens, click Enable.\n  Do the same for Tag Manager API.\nCreate a new Cloud Storage bucket Once you\u0026rsquo;ve enabled these two APIs, it\u0026rsquo;s time to create a new bucket in Google Cloud Storage. This is where we\u0026rsquo;ll store the state of the GTM container. Follow this link to reach the storage browser.\nClick the big blue Create Bucket link in the middle of the screen.\nGive the bucket a global, unique name. I simply named it the same thing as my project: simo-demo-gtm-slack.\nAs Location type, choose Region, and select one that\u0026rsquo;s available for Cloud Functions as well. For example, I chose europe-west1 as the region for the Google Cloud Storage bucket, and I\u0026rsquo;ll use the same location eventually for the Cloud Function.\n  Keep the storage class as the default (Standard).\nKeep access control as the default (Fine-grained).\nDon\u0026rsquo;t touch the Advanced settings.\nClick the blue Create button when ready.\n  Make sure you remember your bucket name (simo-demo-gtm-slack in my example) - you\u0026rsquo;ll need it when configuring the Cloud Function.\nStep 3: Create a new Slack app Create the app You need to create a new Slack app. This application will be in charge of forwarding the messages sent by the Cloud Function to your Slack workspace and channel.\nTo create a new app, visit https://api.slack.com/ and click the big green Start Building button in the middle of the screen.\nGive the app a name (this will appear as the name of the bot in Slack). Choose the workspace you want to add this bot to.\n  When done, click Create App.\nCreate an incoming webhook In the dashboard that opens, choose the Incoming Webhooks functionality.\n  In the configuration screen that opens, click the toggle to activate webhooks.\n  Then, scroll down the configuration screen and click the button to Add New Webhook To Workspace.\n  Now you need to choose which channel to post the messages on, so select one from the list. Once done, click Allow.\n  You should see your new bot enter as a new user into the channel.\n  In the list of webhooks for your app, there should now be a new URL. Keep this tab open for a while, as you\u0026rsquo;ll soon need the URL.\n  Step 4: Configure the Cloud Function Almost done!\nNow, download this archive:\nhttps://github.com/sahava/gtm-slack-integration/raw/master/function.zip\nUnzip the archive to a folder on your computer.\nModify the config.json file Rename the config.sample.json to config.json and open the file for editing with a text editor (or whatever you use to edit JSON files normally).\n  This is what the default config.json looks like:\n{ \u0026#34;gcs\u0026#34;: { \u0026#34;bucketName\u0026#34;: \u0026#34;gcs-bucket-name\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;filename.json\u0026#34; }, \u0026#34;slackOutput\u0026#34;: [{ \u0026#34;slackWebhookUrl\u0026#34;: \u0026#34;https://hooks.slack.com/services/YYYY/XXXX/ZZZZ\u0026#34;, \u0026#34;gtmContainers\u0026#34;: [\u0026#34;12345_23456\u0026#34;, \u0026#34;12345_34567\u0026#34;] }], \u0026#34;verboseLogging\u0026#34;: true } Change the bucketName value to what you set when creating the bucket.\nSet the fileName to what you want to store the GTM state as. For all intents and purposes, this makes very little difference. You can just keep the name as filename.json if you wish.\nSet slackWebhookUrl to the URL of the incoming webhook you created in the previous chapter.\nNow, populate the gtmContainers array with all the combinations of Google Tag Manager account ID and container ID that you want to analyze for changes. The syntax is accountID_containerID, so in the default config.json, there are two containers:\n Account ID 12345, Container ID 23456 Account ID 12345, Container ID 34567  You can find the account ID and container ID in the URL when visiting the container. The account ID is the first numerical string you see, and the container ID is the second one. For example, this is one of the containers I want to monitor:\n  And this is another one:\n  So the array would look like this:\n\u0026quot;gtmContainers\u0026quot;: [\u0026quot;23019854_8060344\u0026quot;, \u0026quot;4702258931_12269166\u0026quot;]\n Remember that you need enough access rights on these containers to add a new user to them!\n You can keep the verboseLogging setting as true. My final config.json now looks like this:\n{ \u0026#34;gcs\u0026#34;: { \u0026#34;bucketName\u0026#34;: \u0026#34;simo-demo-gtm-slack\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;filename.json\u0026#34; }, \u0026#34;slackOutput\u0026#34;: [{ \u0026#34;slackWebhookUrl\u0026#34;: \u0026#34;https://hooks.slack.com/services/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\u0026#34;, \u0026#34;gtmContainers\u0026#34;: [\u0026#34;23019854_8060344\u0026#34;, \u0026#34;4702258931_12269166\u0026#34;] }], \u0026#34;verboseLogging\u0026#34;: true } Save the file when done.\nThen, create a new ZIP archive that has the following files from the folder:\n index.js config.json package.json   NOTE! The files must be in the root of the ZIP archive, so do not compress a folder with the files. There should be no folders in the ZIP.\n Keep this archive handy, you\u0026rsquo;ll need it very shortly.\nCreate a new Cloud Function Go to the Cloud Functions dashboard, and click the Create Function link.\nGive the function a name. Again, my lack of imagination steers me to name it the same as my project and my GCS bucket: simo-demo-gtm-slack.\nKeep the Memory allocated as its default (256MB), and then choose Cloud Pub/Sub from the list of triggers. In the Topic drop-down, click Create new topic.\n  Set the topic name to gtm-slack-start and click CREATE.\n  In the Source Code selection, choose ZIP upload. Set the Runtime to Node.js 10..\nFor ZIP file, click Browse and locate the archive you just created with your modified config.json included in it.\n  Next, click Browse next to Stage bucket, and choose the bucket you created in the beginning of this tutorial.\n  The Google Cloud Platform will use this bucket as a temporary location to stage the Cloud Functions files.\nFor Function to execute, type: getGtmInfo.\nFinally, expand the Environment variables, networking, timeouts and more option, and set your Cloud Functions to run on the same region where you created the Cloud Storage bucket in.\n This isn\u0026rsquo;t a necessary step - you can set the Cloud Functions to run on any region you wish. It does make the application run a bit smoother, and can keep any possible costs down.\n   Finally, click the blue Create button at the bottom of the configuration screen to create your Cloud Function.\nIf all goes well, the icon next to the function name should turn into a green checkmark.\n  Step 5: Add a service account as a user to your GTM containers When you created the Cloud Function, GCP automatically created a service account for you.\nService accounts can be used for server-to-server access between different Google properties. In this case, we want the Cloud Functions to have access to the Google Tag Manager containers you specified in the config.json, so we need to add a service account from your project as a user in Google Tag Manager.\nTo find the service account email address, browse to https://console.cloud.google.com/iam-admin/serviceaccounts. You should see an App Engine default service account here, with email:\nYOUR_PROJECT_ID@appspot.gserviceaccount.com\nIf you don\u0026rsquo;t see a service account here, you might need to wait 10 minutes or so after creating the function, and you might need to refresh the page.\n  Now, copy the email address and add it as a new READ user to all the containers you listed in config.json.\n  Once you\u0026rsquo;ve done these steps, you Cloud Function has permission to communicate with these Google Tag Manager containers.\nStep 6: Test it You can now test the setup.\nFirst, visit https://console.cloud.google.com/functions/list and click your function name to open it.\nIn the dashboard that opens, choose the TESTING tab. Here, click TEST THE FUNCTION.\n  If all goes well, you should see a bunch of log entries like this:\n  Importantly, the state file has now been created. Thus, any time you run the function, if the version of the published containers is different from the one stored in the state file, Slack will be notified.\nLet\u0026rsquo;s try this!\nGo ahead and publish one of your containers. Remember to name the version to describe what was changed.\n  Now, test the Cloud Function again. You should see a new Slack message from your integration.\n  If you see this message, then everything works as it should.\nStep 7: Setup Cloud Scheduler to run the function every X minutes Final step is to automate the polling. You can use Cloud Scheduler for that. It\u0026rsquo;s basically a managed cron service, which you can configure to trigger the Cloud Function every five minutes, for example.\nFirst, visit https://console.cloud.google.com/cloudscheduler and click CREATE JOB to create a new job.\nFrom the location selector, choose a location as close to your Cloud Function as possible. I chose europe-west, as that\u0026rsquo;s the main region where my Cloud Function and Cloud Storage buckets are running.\n  In the next screen, set the following options.\n Name: Whatever you wish, I chose simo-demo-gtm-slack. Description: Leave empty. Frequency: Use crontab.guru if you don\u0026rsquo;t know how to set schedule expressions. I chose */5 * * * * to fire the function every five minutes. Timezone: Irrelevant - choose your own, for example. Target: Choose Pub/Sub. Topic: gtm-slack-start. Payload: true.    Click CREATE when ready.\nNow, publish a different container version again, and wait for the scheduler to go off. As I chose every 5 minutes, it means I have to wait a maximum of five minutes to verify the setup works.\nSummary You\u0026rsquo;ve now got a Cloud Scheduler running your Cloud Function periodically. Each time the Cloud Function runs, it pulls the most recent state object from Cloud Storage, and then it polls the Google Tag Manager API to check if any of the containers in the state object have a newly published version.\nIf a published version is found, the Cloud Function notifies your Slack Workspace with a message that a new version has been found.\nYou can follow the costs in your Google Cloud Platform dashboard. With two containers, my setup costs nothing.\n Note that if you add many more containers to the setup, not only might your costs increase but you also might need to increase the timeout from 60 seconds to something higher from the Google Cloud Functions settings. Be sure to monitor the cloud logs!\n You can modify index.js to check other properties as well. For example, with some programming you could notify Slack about what changes were done. Another useful feature would be to notify if new users have been added to the container, though before you do this you must make sure it\u0026rsquo;s OK to do this legally, as you\u0026rsquo;d have to store the list of users in the state object. You could hash this list to mitigate this, though.\nAnyway, let me know in the comments if you have questions about the setup, or if you have other ideas for this type of a diff-machine!\n"
},
{
	"uri": "https://www.simoahava.com/tags/slack/",
	"title": "slack",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/cookieless-tracking-cross-site-iframes/",
	"title": "Cookieless Tracking For Cross-site Iframes",
	"tags": ["google tag manager", "cross-domain tracking", "iframe", "postmessage", "cookie"],
	"description": "How to track Google Analytics traffic within a cross-site iframe without using cookies. This is useful due to how browser tracking protections prevent access to cookies in third-party (cross-site) context.",
	"content": " Updated 15 April 2020: Fix the message forwarder to properly clone objects before they are passed to postMessage\n Here I am, back with \u0026lt;iframe\u0026gt; and cross-domain tracking. I\u0026rsquo;ve published a couple of articles before on the topic, with my upgraded solution being the most recent one. These articles tackle the general problem of passing the Client ID from the parent to the \u0026lt;iframe\u0026gt;.\nBy doing so, the \u0026lt;iframe\u0026gt; can take the Client ID from the frame URL and create the _ga cookie in the \u0026lt;iframe\u0026gt;, allowing hits from the parent and the \u0026lt;iframe\u0026gt; to use the same Client ID. Great.\nHowever, there\u0026rsquo;s a catch that\u0026rsquo;s become more and more troubling with the increase of browsers\u0026rsquo; tracking protection mechanisms: if the \u0026lt;iframe\u0026gt; loads content cross-site, the browser must allow cookie access in third-party context. In other words, if the browser blocks third-party cookies, the \u0026lt;iframe\u0026gt; will not be able to write the cookie, and Google Analytics tracking will fail.\n  This article offers two solutions to this problem.\nThe first is that it sends the Google Analytics Client ID from the parent to the \u0026lt;iframe\u0026gt; using window.postMessage, and the child frame can poll for this information on every page, meaning cookies are not needed to store the Client ID.\nThe second solution is that the child frame actually sends every single dataLayer message to the parent, so that the child frame doesn\u0026rsquo;t have to track anything by itself. The parent handles the tracking for the child frame instead.\nUsing window.postMessage is more robust than the link decoration of my earlier proposals, because it doesn\u0026rsquo;t force a reload of the frame, nor does it require the child frame to support cookies. It\u0026rsquo;s a bit more elaborate to set up, however.\n Warning! I\u0026rsquo;m not kidding about that last statement. Setting this up requires quite a bit of custom code, and you\u0026rsquo;re working with bilateral communication between two windows. Please read this article carefully so that you understand what\u0026rsquo;s going on before copy-pasting any code to your Google Tag Manager container(s).\n What exactly is the problem? Glad you asked!\nWhen a page loads an \u0026lt;iframe\u0026gt; from a cross-site origin, that frame is loaded in a third-party context, and any access to browser storage from within that \u0026lt;iframe\u0026gt; will require the browser to allow third-party cookies for the \u0026lt;iframe site.\nA key term here is cross-site. This means that the top privately-controlled domain name of the \u0026lt;iframe\u0026gt; is different from that of the parent page.\nIn the examples below, the top privately-controlled domain name is in italics.\n www.simoahava.com www.gtmtools.com www.ebay.co.uk sahava.github.io analytics.google.com  If any one of the sites above loaded any other site from the list in an \u0026lt;iframe\u0026gt;, that content would be loaded cross-site, and any cookie operations within the embedded page would require third-party cookie access.\nThe following examples are all same-site, even if they are cross-origin:\n www.simoahava.com simoahava.com blog.simoahava.com tracking.endpoint.simoahava.com  Any communication between pages from the list above would happen in a same-site (or first-party) context, and cookie access would not be restricted.\nAs I mention in the introduction, this is only going to spell doom for tracking within embedded content due to how browsers implement tracking protections.\n From www.cookiestatus.com  Even though Google recently made a veritable non-announcement by saying they\u0026rsquo;ll phase out third-party cookies by 2022, Google Chrome will actually make things harder for cross-site cookie access much, much sooner.\nChrome v80 (released on February 4, 2020), enforces SameSite cookie restrictions, which means that if a cookie should be accessible in third-party context, it requires the SameSite=None and Secure flags set. By default, these are not set. Unfortunately, the _ga cookie used by Google Analytics does not have these flags set, and currently there is no timeline for when support for these flags are added.\nSo that\u0026rsquo;s the problem! On the majority of browsers, the _ga cookie does not (or will stop to) function in third-party context, which applies to all cross-site \u0026lt;iframe\u0026gt; embeds.\nLuckily, there\u0026rsquo;s a way around this.\nWe can ignore cookies altogether.\nSolution 1: Pass the Client ID from the parent to the child Let\u0026rsquo;s take a look at another celebrated illustration from yours truly:\n  There are many potential race conditions in the mix, so some precautions need to be taken. Here\u0026rsquo;s how the parent page works:\n The parent page starts listening for messages from the \u0026lt;iframe\u0026gt; as soon as Google Tag Manager loads. Once the parent page receives the childReady message, it starts polling for the Google Analytics tracker until a maximum timeout is reached. As soon as the Google Analytics tracker is available, the parent page sends the Client ID back to the \u0026lt;iframe\u0026gt;.  And here\u0026rsquo;s how the \u0026lt;iframe\u0026gt; page reacts:\n The \u0026lt;iframe\u0026gt; page starts sending the childReady message as soon as Google Tag Manager loads. Once the parent page responds with the Client ID (or a timeout is reached), the child page stops sending the message. The child page writes the Client ID into dataLayer.  So now we know how the Client ID will be passed to the \u0026lt;iframe\u0026gt;, but that\u0026rsquo;s not enough, yet.\nEvery single Google Analytics tag in the \u0026lt;iframe\u0026gt; must be configured to work with this setup!\nMore specifically, they all need two fields set, preferably in a Google Analytics Settings variable:\n storage set to none to avoid the tracker failing if it can\u0026rsquo;t write the Client ID cookie. clientId set to the value from the dataLayer.  We\u0026rsquo;ll get to these shortly, don\u0026rsquo;t worry.\nSolution 2: Forward all Data Layer messages from child to parent If you don\u0026rsquo;t want to do any tracking within the \u0026lt;iframe\u0026gt; (I don\u0026rsquo;t blame you), you can actually delegate tracking to the parent by sending all dataLayer messages to the parent for processing.\nThis means the parent page would manage tags for both the parent page\u0026rsquo;s native interactions as well as those that happen within the \u0026lt;iframe\u0026gt;.\n  The process is almost the same as with the first solution. Here\u0026rsquo;s how the parent page works:\n The parent page starts listening for messages from the \u0026lt;iframe\u0026gt; as soon as Google Tag Manager loads. Once the parent page receives the childReady message, it responds with a parentReady message. If the child frame sends a message in dataLayer-compatible format, the parent page pushes this message into its own dataLayer.  On the \u0026lt;iframe\u0026gt;, this is what happens:\n The \u0026lt;iframe\u0026gt; page starts sending the childReady message as soon as Google Tag Manager loads. Once the parent page responds with parentReady, the child frame \u0026ldquo;hijacks\u0026rdquo; the dataLayer.push() method, and sends all the messages passed to it over to the parent page.  The messages from the child frame are namespaced to keep them separate from the parent\u0026rsquo;s \u0026ldquo;native\u0026rdquo; dataLayer messages, and they include some metadata about the \u0026lt;iframe\u0026gt; (mainly the URL and title).\n  The parent page setup This article combines both solutions into a single set of Custom HTML tags, one for the parent page and one for the child \u0026lt;iframe\u0026gt;.\nOn the parent page, i.e. the page sending the Client ID to the \u0026lt;iframe\u0026gt; and waiting for the messages sent from the child, you need to create a Custom HTML tag that fires on a Page View trigger. You can use the All Pages trigger if you wish, but you might as well create a Page View trigger that only fires on pages where you know the \u0026lt;iframe\u0026gt; to exist.\nThe Custom HTML tag The Custom HTML tag itself should contain the following code:\n\u0026lt;script\u0026gt; (function() { // Tracking ID whose _ga cookie to use  var trackingId = \u0026#39;UA-40669554-1\u0026#39;; // Maximum time in milliseconds to wait for GA tracker to load  var maxGATime = 2000; // Set to the origin (\u0026#34;https://www.domain.com\u0026#34;) of the iframe you want to communicate with  var childOrigin = \u0026#39;https://www.gtmtools.com\u0026#39;; // Don\u0026#39;t touch anything that follows  var pollInterval = 200; var postCallback = function(event) { if (event.origin !== childOrigin) return; if (event.data !== \u0026#39;childReady\u0026#39; \u0026amp;\u0026amp; !event.data.event) return; if (event.data === \u0026#39;childReady\u0026#39;) { // Send event that parent is ready  event.source.postMessage(\u0026#39;parentReady\u0026#39;, event.origin); var pollCallback = function() { // Stop polling if maxTime reached  maxGATime -= pollInterval; if (maxGATime \u0026lt;= 0) window.clearInterval(poll); // Only proceed if GA loaded and tracker accessible  var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; if (ga \u0026amp;\u0026amp; ga.getAll) { // Get tracker that matches the Tracking ID you provided  var tracker = ga.getAll().filter(function(t) { return t.get(\u0026#39;trackingId\u0026#39;) === trackingId; }).shift(); // Send message back to frame with Client ID  if (tracker) { event.source.postMessage({ event: \u0026#39;clientId\u0026#39;, clientId: tracker.get(\u0026#39;clientId\u0026#39;) }, event.origin); } // Stop polling if not already done so  window.clearInterval(poll); } }; // Start polling for Google Analytics tracker  var poll = window.setInterval(pollCallback, pollInterval) } // Push dataLayer message from iframe to dataLayer of parent  if (event.data.event) { window.dataLayer.push(event.data); } }; // Start listening for messages from child frame  window.addEventListener(\u0026#39;message\u0026#39;, postCallback); })(); \u0026lt;/script\u0026gt; There\u0026rsquo;s quite a lot happening here, so let\u0026rsquo;s walk through the code! If you don\u0026rsquo;t care about this deep-dive, you can skip right to how you might need to configure the parent page container to support the message forwarding setup.\nConfiguration First, there\u0026rsquo;s the configuration stuff:\n// Tracking ID whose _ga cookie to use var trackingId = \u0026#39;UA-40669554-1\u0026#39;; // Maximum time in milliseconds to wait for GA tracker to load var maxGATime = 2000; // Set to the origin (\u0026#34;https://www.domain.com\u0026#34;) of the iframe you want to communicate with var childOrigin = \u0026#39;https://www.gtmtools.com\u0026#39;;  The trackingId should be set to the Google Analytics tracking ID of the tracker whose _ga cookie you want to use. This is because there might be multiple GA cookies each storing the Client ID for a different Tracking ID. If you\u0026rsquo;re unsure, just type your regular tracking ID as the value of the trackingId variable.\nSet maxGATime to the maximum amount of time that the page waits for Google Analytics to load. You can certainly set this a lot higher than 2000 (2 seconds) if you want, but I would recommend against indefinite polling.\nSet the childOrigin to the origin of the \u0026lt;iframe\u0026gt; you want to send the data to. The origin is everything in the URL up to the first path slash. So, if the URL is https://www.simoahava.com/my-home-page/, the origin would be https://www.simoahava.com.\n It\u0026rsquo;s important to not have the trailing slash, as that\u0026rsquo;s part of the path component and not the origin.\n The listener On the last line of the main code block, we add the message listener:\nwindow.addEventListener(\u0026#39;message\u0026#39;, postCallback);  This means that when an \u0026lt;iframe\u0026gt; sends a postMessage to the parent page, the listener fires and executes the postCallback function.\nvar postCallback = function(event) { if (event.origin !== childOrigin) return; if (event.data !== \u0026#39;childReady\u0026#39; \u0026amp;\u0026amp; !event.data.event) return; if (event.data === \u0026#39;childReady\u0026#39;) { // Send event that parent is ready  event.source.postMessage(\u0026#39;parentReady\u0026#39;, event.origin); var pollCallback = function() { // Stop polling if maxTime reached  maxGATime -= pollInterval; if (maxGATime \u0026lt;= 0) window.clearInterval(poll); // Only proceed if GA loaded and tracker accessible  var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; if (ga \u0026amp;\u0026amp; ga.getAll) { // Get tracker that matches the Tracking ID you provided  var tracker = ga.getAll().filter(function(t) { return t.get(\u0026#39;trackingId\u0026#39;) === trackingId; }).shift(); // Send message back to frame with Client ID  if (tracker) { event.source.postMessage({ event: \u0026#39;clientId\u0026#39;, clientId: tracker.get(\u0026#39;clientId\u0026#39;) }, event.origin); } // Stop polling if not already done so  window.clearInterval(poll); } }; // Start polling for Google Analytics tracker  var poll = window.setInterval(pollCallback, pollInterval) } // Push dataLayer message from iframe to dataLayer of parent  if (event.data.event) { window.dataLayer.push(event.data); } };  First, if the message retrieved was not expected (e.g. coming from a different \u0026lt;iframe\u0026gt; or has the wrong content), the callback stops execution.\nIf the message was from the \u0026lt;iframe\u0026gt;, the first thing that\u0026rsquo;s done is notify the child frame that the parent has received the message and is ready to start bilateral communication.\nNext, a window.setInterval starts polling the parent page every 200 milliseconds up to the default of 2 full seconds. With each poll, the script checks if the Google Analytics tracker has been created. If it has, the script takes the clientId from the tracker, and sends it back to the \u0026lt;iframe\u0026gt; using event.source.postMessage(). After this is done, the polling is manually stopped to avoid having the Client ID being sent multiple times to the \u0026lt;iframe\u0026gt;.\nIn essence, the parent page needs to wait for two things:\n For the child page to signal it is ready to receive messages. For the Google Analytics tracker to be available, so that the Client ID can be grabbed from it.  The last code block checks if the message from the child frame contains an object with the event property, in which case it pushes this entire object into the parent page dataLayer.\nConfiguring the message forwarding system The parent page listens for dataLayer messages forwarded from the embedded \u0026lt;iframe\u0026gt;. You can create tags, triggers, and variables that react to these messages.\nAll triggers will be of type Custom Event trigger. That\u0026rsquo;s because all the events sent from the frame will be appended with the iframe. prefix - even those without an event value (they\u0026rsquo;ll be sent as iframe.Message). So, if you want to fire a tag when a link is clicked in the \u0026lt;iframe\u0026gt;, the trigger could look like this:\n  It requires quite a bit of manual configuration to get the whole thing up and running. But it can make your whole setup run much smoother, as you won\u0026rsquo;t need to embed any extra code within the \u0026lt;iframe\u0026gt;.\nEach message sent from the \u0026lt;iframe\u0026gt; is automatically enhanced with some page-level information:\n{ iframe: { pageData: { url: \u0026#39;https://www.iframe-domain.com/iframe-page?iframequery=iframevalue#iframeHash\u0026#39;, title: \u0026#39;The Iframe Page Title | Iframe Company\u0026#39; } } }  If you want to update your parent page tags to use the \u0026lt;iframe\u0026gt; page-level data, you need to create Data Layer variables for iframe.pageData.url (the URL of the \u0026lt;iframe\u0026gt; page) and iframe.pageData.title (the page title).\nOnce you have all these configured, you\u0026rsquo;re ready to configure the \u0026lt;iframe\u0026gt; page!\nThe embedded (child) page setup This is where things get tricky. In this guide, we\u0026rsquo;re covering a fairly typical use case where the \u0026lt;iframe\u0026gt; content is only ever interacted with as an embed. So there\u0026rsquo;s no scenario where the user would visit the page loaded in the \u0026lt;iframe\u0026gt; in a first-party or top-frame context. I\u0026rsquo;ll briefly discuss that scenario later as well, but for now let\u0026rsquo;s assume that the page in the \u0026lt;iframe\u0026gt; is only accessed as an embedded element.\nYou\u0026rsquo;ll need to do three things in the Google Tag Manager container of the \u0026lt;iframe\u0026gt; page.\n Create a Custom HTML tag that communicates with the parent page. Update the settings for all of your Universal Analytics (and App+Web while you\u0026rsquo;re at it) tags. Update the triggers for your Universal Analytics tags so they don\u0026rsquo;t fire until they\u0026rsquo;ve received the Client ID from the parent.  The Custom HTML tag Create a new Custom HTML tag, and set it to fire on the All Pages trigger. If the \u0026lt;iframe\u0026gt; is a single-page app, you should still only fire the Custom HTML tag on the All Pages trigger, and not with every SPA page change, for example.\nHere\u0026rsquo;s what you should copy-paste into the tag:\n\u0026lt;script\u0026gt; (function() { // If not in iframe, do nothing  try { if (window.top === window.self) return; } catch(e) {} // Set to false to prevent dataLayer messages from being sent to parent  var sendDataLayerMessages = true; // Set the prefix that will be used in the event name, and under which all  // the dataLayer properties will be embedded  var dataLayerMessagePrefix = \u0026#39;iframe\u0026#39;; // Set to parent origin (\u0026#34;https://www.domain.com\u0026#34;)  var parentOrigin = \u0026#39;https://www.simoahava.com\u0026#39;; // Maximum time in milliseconds to poll the parent frame for ready signal  var maxTime = 2000; // Don\u0026#39;t touch anything that follows  var pollInterval = 200; var parentReady = false; var postCallback = function(event) { if (event.origin !== parentOrigin) return; if (event.data.event !== \u0026#39;clientId\u0026#39; \u0026amp;\u0026amp; event.data !== \u0026#39;parentReady\u0026#39;) return; if (event.data.event === \u0026#39;clientId\u0026#39;) { window.dataLayer.push({ event: \u0026#39;clientId\u0026#39;, clientId: event.data.clientId }); } if (event.data === \u0026#39;parentReady\u0026#39; \u0026amp;\u0026amp; !parentReady) { window.clearInterval(poll); if (sendDataLayerMessages) startDataLayerMessageCollection(); parentReady = true; } }; var pollCallback = function() { // If maximum time is reached, stop polling  maxTime -= pollInterval; if (maxTime \u0026lt;= 0) window.clearInterval(poll); // Send message to parent that iframe is ready to retrieve Client ID  window.top.postMessage(\u0026#39;childReady\u0026#39;, parentOrigin); }; var createMessage = function(obj) { if (!Array.isArray(obj) \u0026amp;\u0026amp; typeof obj === \u0026#39;object\u0026#39;) { var flattenObj = JSON.parse(JSON.stringify(obj)); var message = {}; // Add metadata about the page into the message  message[dataLayerMessagePrefix] = { pageData: { url: document.location.href, title: document.title } }; for (var prop in flattenObj) { if (flattenObj.hasOwnProperty(prop) \u0026amp;\u0026amp; prop !== \u0026#39;gtm.uniqueEventId\u0026#39;) { if (prop === \u0026#39;event\u0026#39;) { message.event = dataLayerMessagePrefix + \u0026#39;.\u0026#39; + flattenObj[prop]; } else { message[dataLayerMessagePrefix][prop] = flattenObj[prop]; } } } if (!message.event) message.event = dataLayerMessagePrefix + \u0026#39;.Message\u0026#39;; return message; } return false; }; var startDataLayerMessageCollection = function() { // Send the current dataLayer content to top frame, flatten the object  window.dataLayer.forEach(function(obj) { var message = createMessage(obj); if (message) window.top.postMessage(message, parentOrigin); }); // Create the push listener for future messages  var oldPush = window.dataLayer.push; window.dataLayer.push = function() { var states = [].slice.call(arguments, 0); states.forEach(function(arg) { var message = createMessage(arg); if (message) window.top.postMessage(message, parentOrigin); }); return oldPush.apply(window.dataLayer, states); }; }; // Start polling the parent page with \u0026#34;childReady\u0026#34; message  var poll = window.setInterval(pollCallback, pollInterval); // Start listening for messages from the parent page  window.addEventListener(\u0026#39;message\u0026#39;, postCallback); })(); \u0026lt;/script\u0026gt; The following chapters will walk through this code. You can skip right to configuring the Google Analytics tags if you don\u0026rsquo;t care about the walkthrough.\nConfiguration This is the first code block:\n// If not in iframe, do nothing try { if (window.top === window.self) return; } catch(e) {} // Set to false to prevent dataLayer messages from being sent to parent var sendDataLayerMessages = true; // Set the prefix that will be used in the event name, and under which all // the dataLayer properties will be embedded var dataLayerMessagePrefix = \u0026#39;iframe\u0026#39;; // Set to parent origin (\u0026#34;https://www.domain.com\u0026#34;) var parentOrigin = \u0026#39;https://www.simoahava.com\u0026#39;; // Maximum time in milliseconds to poll the parent frame for ready signal var maxTime = 2000;  First of all, if the page is not loaded in an \u0026lt;iframe\u0026gt;, the code does not and should not execute at all. There\u0026rsquo;s no parent page to communicate with, so executing any of the following code would be unnecessary.\nYou can set sendDataLayerMessages to false if you don\u0026rsquo;t want to forward the dataLayer messages from the child to the parent. This is useful if you\u0026rsquo;re comfortable with tracking everything within the \u0026lt;iframe\u0026gt; itself, and don\u0026rsquo;t want to bother with setting up the corresponding tags in the parent page.\nThe dataLayerMessagePrefix value is what will be used to namespace the dataLayer messages that are forwarded from the child to the parent (if you haven\u0026rsquo;t disabled forwarding per the previous paragraph).\nThe parentOrigin should be set to the origin of the parent page, i.e. the page to which the messages are sent. Remember, origin is everything from the protocol to the first slash of the path component. In other words, the origin of https://www.simoahava.com/some-page is https://www.simoahava.com.\nFinally, the maxTime is how long the \u0026lt;iframe\u0026gt; tries to send the childReady message to the parent before it stops.\nThe poller The child frame polls the parent page with the childReady message to signal it\u0026rsquo;s ready for the bilateral communication to start.\nvar pollCallback = function() { // If maximum time is reached, stop polling  maxTime -= pollInterval; if (maxTime \u0026lt;= 0) window.clearInterval(poll); // Send message to parent that iframe is ready to retrieve Client ID  window.top.postMessage(\u0026#39;childReady\u0026#39;, parentOrigin); }; // Start polling the parent page with \u0026#34;childReady\u0026#34; message var poll = window.setInterval(pollCallback, pollInterval);  It runs every 200 milliseconds until the maximum poll time (2000 milliseconds by default) is reached.\nThe listener At the same time as it starts polling, a postMessage listener is also initiated. This listener waits for two things:\n The parent to signal parentReady, so that the \u0026lt;iframe\u0026gt; can start forwarding its dataLayer messages to the parent. The parent to return a Client ID string, so that the \u0026lt;iframe\u0026gt; can use this in Google Analytics tags.  var postCallback = function(event) { if (event.origin !== parentOrigin) return; if (event.data.event !== \u0026#39;clientId\u0026#39; \u0026amp;\u0026amp; event.data !== \u0026#39;parentReady\u0026#39;) return; if (event.data.event === \u0026#39;clientId\u0026#39;) { window.dataLayer.push({ event: \u0026#39;clientId\u0026#39;, clientId: event.data.clientId }); } if (event.data === \u0026#39;parentReady\u0026#39; \u0026amp;\u0026amp; !parentReady) { window.clearInterval(poll); if (sendDataLayerMessages) startDataLayerMessageCollection(); parentReady = true; } }; // Start listening for messages from the parent page window.addEventListener(\u0026#39;message\u0026#39;, postCallback);  If the parent sends a clientId message, then the page pushes this data into dataLayer, and Google Analytics tags firing in the \u0026lt;iframe\u0026gt; can utilize this information in their settings.\nIf the parent sends a parentReady message, the \u0026lt;iframe\u0026gt; stops sending the childReady message. Then, it fires up the dataLayer message forwarder (unless the user has chosen to prevent this).\nThe message forwarder The forwarder comprises two methods: createMessage(obj) and startDataLayerMessageCollection().\nWithout going into too much detail, the basic setup is this:\n The createMessage() method is a utility that wraps the dataLayer message from the \u0026lt;iframe\u0026gt; page with the prefix configured in the beginning of the Custom HTML tag (\u0026quot;iframe\u0026quot;) by default. This prefix is used with the event name (so gtm.js becomes iframe.gtm.js) as well as with the object itself (so {key: 'value'} becomes {iframe: {key: value}}). The createMessage() method also automatically adds a pageData object which contains the url and title of the page in the \u0026lt;iframe\u0026gt;. The startDataLayerMessageCollection() method first sends everything in the dataLayer array collected thus far to the parent. Then the method rewrites the dataLayer.push function to send everything pushed into dataLayer to the parent page as well.  In other words, everything added to the dataLayer array within the \u0026lt;iframe\u0026gt; is forwarded to the parent page, so that the parent page can handle tracking of interactions and events within the \u0026lt;iframe\u0026gt; as well.\nFor example, here\u0026rsquo;s how the forwarded transpiles and sends a dataLayer message to the parent page.\n// This is what\u0026#39;s pushed into dataLayer: { event: \u0026#39;userLogin\u0026#39;, user: { id: \u0026#39;abcd-1234\u0026#39;, status: \u0026#39;gold\u0026#39; } } // This is what\u0026#39;s sent to the parent page { event: \u0026#39;iframe.userLogin\u0026#39;, iframe: { pageData: { url: \u0026#39;https://www.iframe-domain.com/iframe-page/\u0026#39;, title: \u0026#39;Iframe Page Title\u0026#39; }, user: { id: \u0026#39;abcd-1234\u0026#39;, status: \u0026#39;gold\u0026#39; } } }  Google Analytics tags If you do want to collect data to Google Analytics from within the \u0026lt;iframe\u0026gt; page, you need to configure all your Google Analytics tags with the following settings:\n  As the \u0026lt;iframe\u0026gt; will no longer use cookies to persist the Client ID, you need to set the storage field value to none.\nThen, because the child frame uses the Client ID pushed into dataLayer by the message listener, you\u0026rsquo;ll need to update the clientId field to use a Data Layer variable for clientId. It should look like this:\n  None of your tags should fire if the Client ID is not yet available. You can use the clientId in a Custom Event trigger to fire your tags when Client ID has been pushed to dataLayer.\n  You can also update your existing triggers to not fire until clientId has a valid value.\n  Multi-purpose container One important caveat to note is that if you set up all your tags per the instructions in the previous chapter, then the container running in the \u0026lt;iframe\u0026gt; will not function properly if the page is visited directly as a top-level page, i.e. not as an embed.\nI would generally recommend to avoid mixing too many use cases in a single container. It might be wiser to create a different container for scenarios where the page is visited directly, and have the developers update the site code to load the correct container, depending on whether the page is embedded in an \u0026lt;iframe\u0026gt; or not.\nHowever, if you do want the container to cater to different use cases, then a good idea is to create a separate set of tags for when the page is accessed in the top frame vs. when the page is accessed as an embed.\nYou can use a simple utility variable to check if the page is accessed as an \u0026lt;iframe\u0026gt; or not. This Custom JavaScript variable returns true if the page is NOT in an \u0026lt;iframe\u0026gt; and false otherwise.\nfunction() { try { return window.top === window.self; } catch(e) { return false; } }  You can then check for this variable value in your tags, triggers, and variables, to make sure that tracking is configured correctly depending on how the page is accessed.\nSummary This has been a difficult article to write, and I feel a bit embarrassed to leave so much work to you, my noble reader.\nThe thing is - working with this type of a bilateral messaging setup requires both the parent page and the \u0026lt;iframe\u0026gt; page to be in sync. The Custom HTML tags I\u0026rsquo;ve designed have been built to naturally reject race conditions, but it\u0026rsquo;s possible you\u0026rsquo;ll need to modify one or the other (or both) to make things work on your site.\nOver the course of my career, and especially over the time I\u0026rsquo;ve been blogging about Google Tag Manager, trying to perfect tracking of \u0026lt;iframe\u0026gt; elements has been my personal Mount Everest. Cross-site tracking of embedded content has take over my life to such an extent that I\u0026rsquo;m losing sleep and seriously considering hunting down Ibrahim or Isabella Frame, or whoever the person is who thought the \u0026lt;iframe\u0026gt; is a great addition to the HTML spec.\nRegardless, it\u0026rsquo;s still such a common use case on the SaaS- and micro service -rich web to embed cross-site content. Naturally, there\u0026rsquo;s an incentive to know what happens in these \u0026lt;iframe\u0026gt; blocks that I consider the time I\u0026rsquo;ve spent trying to solve this particular riddle to be almost worthwhile.\nAnyway, I expect and hope that you have comments and/or questions about this setup. Let\u0026rsquo;s continue the discussion in the comments of this article, thank you!\n"
},
{
	"uri": "https://www.simoahava.com/tags/cross-domain-tracking/",
	"title": "cross-domain tracking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/iframe/",
	"title": "iframe",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/postmessage/",
	"title": "postmessage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/collect-isp-custom-dimension-google-analytics/",
	"title": "#GTMTips: Collect ISP As A Custom Dimension In Google Analytics",
	"tags": ["google tag manager", "custom templates", "api", "ip"],
	"description": "Since Google Analytics has deprecated the Service Provider and Network Domain dimension, you will have to use a custom solution to bring this information back. In this article, I&#39;ll show you how to use a gallery template to return the Service PRovider dimension.",
	"content": " Update 6 April 2020: I updated the template in the gallery to the latest version of the IP Geolocation API SDK, which no longer requires jQuery. Also, the SDK now handles API request caching to browser storage automatically, so the \u0026ldquo;Enable Session Storage\u0026rdquo; option was added to the template.\n Google Analytics had been foreshadowing the deprecation of the Network Domain and Service Provider custom dimensions since late 2019. On February 4, the plug was finally pulled, and both these dimensions started flatlining to (not set) in Google Analytics reports. For more information about this, see these articles:\n Google Analytics Deprecating \u0026lsquo;Network Domain\u0026rsquo; And \u0026lsquo;Service Provider\u0026rsquo; Mourning the Death of \u0026ldquo;Service Provider\u0026rdquo; \u0026amp; \u0026ldquo;Network Domain\u0026rdquo; in Google Analytics Dear Google, Why Oh Why Did You Remove Service Provider and Network Domain from Analytics?  This article is not a post-mortem. I won\u0026rsquo;t go into speculating why they made this change, nor will I anguish over the loss of these dimensions. Instead, I want to give you a practical way forward: a method with which you can reinstate some of the lost information as early as \u0026hellip; today!\nWe\u0026rsquo;ll be using Google Tag Manager, my IP Geolocation API template in the template gallery, and you\u0026rsquo;ll also need a (free) API key from ipgeolocation.io.\nTip 109: Get your Service Provider data back    As you can see, this solution only reinstates the ISP information - not the Network Domain. This should typically be enough for bot / spam removal, but in case you do want the qualified domain names of the IP addresses, you\u0026rsquo;ll need to look at other IP APIs out there.\n Get the API key First, you\u0026rsquo;ll need to visit ipgeolocation.io and get an API key. The free key gives you 30K requests a month (1K maximum per day), which is not a lot, I know. You can upgrade to paid plans to increase the quotas, and you can add some client-side throttling (covered later in this article) to reduce the number of API calls being made.\n  Configure the template Once you have the API key, visit the template gallery and fetch the IP Geolocation API custom template.\nThen, in your workspace, create a new tag, and choose the IP Geolocation API template as the foundation.\nAdd your API Key to the respective field. You can leave the other settings at their defaults, but if you want, you can expand Other Settings and check the new Add Hashed IP configuration.\n  This setting hashes the user\u0026rsquo;s IP address (using SHA256), and writes it into the dataLayer object with the key geoData.hashedIp. Since it\u0026rsquo;s hashed and can\u0026rsquo;t be decrypted, you can send this as a custom dimension to Google Analytics. As it can no longer be reversed to identify the machine the user is using, it won\u0026rsquo;t be in violation of Google Analytics\u0026rsquo; TOS. Naturally, it is still a potential user identifier, so you need to make sure sending it is in compliance with laws governing the passing and processing of such identifiers.\nYou can set the tag to fire on the All Pages trigger.\nCreate the custom dimensions Once the tag is in place, you can create the custom dimensions in Google Analytics. Whether you use session scope or user scope is up to you. The first gives you some flexibility for tracking / blocking internal traffic, and the second might be more useful for identifying bots.\n  Create the Event tag, trigger, and variables Finally, you need to create the Event tag, the trigger to fire it with, and any Data Layer variables you want to send.\nThe IP information is written into a dataLayer object that looks roughly like this:\n{ event: \u0026#39;geolocate\u0026#39;, geoData: { ip: \u0026#39;123.123.123.123\u0026#39;, hashedIp: \u0026#39;9iCCGWzjVQPV74KJa3iiS9RKiof6m7y/8aSD9WyT0mw=\u0026#39;, isp: \u0026#39;My ISP Org\u0026#39;, ... } }  To fire your tag, create a new Custom Event trigger:\n  You can then create Data Layer variables for the isp and hashedIp fields (and any other fields you might want to utilize from the API response object).\nHere\u0026rsquo;s an example of what the Data Layer variable for the ISP information looks like:\n  Finally, you need an Event tag to send the information to Google Analytics. It could look like this:\n  Key things are naturally the custom dimensions, but also setting the Non-Interaction Hit field to True.\nMake sure the tag fires on the Custom Event trigger you just created.\nTesting the setup To test the setup, use the GTM/GA Debug extension.\nLoad the Preview container version in your browser, with the extension active, and then check that a hit is sent to Google Analytics with all the information you included in it.\n  Remember to check your Google Analytics data in a few hours (you might need to wait even longer if it\u0026rsquo;s a new custom dimension) to make sure the data is being populated correctly.\nThrottling the API requests Remember that in the free version of the API you only have 1000 requests per day? That\u0026rsquo;s definitely not a lot. So you can throttle the requests to make sure they only fire once per session, or even once per user.\nFor this, you need browser cookies or some other form of browser storage.\nThe session cookie is something you can manipulate with customTask. In fact, I\u0026rsquo;ve written a solution for updating a session cookie with every Google Analytics hit.\nOnce you have the customTask in place, you\u0026rsquo;ll need to modify the trigger on the IP Geolocation API tag to only fire if the cookie does not exist (i.e. there\u0026rsquo;s no active session). Instead of an All Pages trigger, you\u0026rsquo;ll need a new Page View trigger. Also, you\u0026rsquo;ll need a 1st Party Cookie variable as instructed in the guide. The final trigger should look something like this:\n  Using this trigger on the IP Geolocation API tag, you\u0026rsquo;ll restrict the API request to only happen once every 30 minutes of GA inactivity. That should keep the number of requests down nicely.\nYou can also throttle per user, by creating a cookie after the API request has completed (e.g. in a new custom template or a Custom HTML tag), and then using the existence of this cookie to prevent the API tag from firing again.\nI would personally recommend to throttle only per session, as ISPs can change as the user moves around, and scoping the ISP to the user might block actual, valid traffic as well.\nSummary In this article, we\u0026rsquo;ve learned how to repurpose a gallery template to return some of the data we\u0026rsquo;ve lost as Google deprecated the Service Provider and Network Domain dimensions.\nIn addition to that, we go the additional bonus of collecting the hashed IP address of the user. This can be used to further filter out bots, as you can identify hashed IP addresses belonging to bots and add them to a blacklist either in your Google Tag Manager triggers, or in Google Analytics\u0026rsquo; filters.\nThe downside of the IP Geolocation API is that it doesn\u0026rsquo;t expose the Network Domain dimension. For this, you\u0026rsquo;ll need some other API and some other solution. I\u0026rsquo;m sure other authors will be quick to introduce solutions similar to mine, which you can then check out and implement if they seem better for your use cases.\nThere\u0026rsquo;s an upside to using an API solution of your own rather than relying on Google\u0026rsquo;s reverse lookups - you have more transparency, more options, and more methods to comply with laws and regulations that govern the parsing of user identifiers, personal data, and IP address lookups.\nI hope you found this useful! Please let me know in the comments if you have suggestions for improving the setup, or if you didn\u0026rsquo;t understand something - I\u0026rsquo;m here to help.\n"
},
{
	"uri": "https://www.simoahava.com/tags/ip/",
	"title": "ip",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/custom-html/",
	"title": "custom html",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/custom-html-tag-guide-google-tag-manager/",
	"title": "The Custom HTML Tag Guide For Google Tag Manager",
	"tags": ["google tag manager", "custom html"],
	"description": "In this article, we talk about Custom HTML tags, and the opportunities they offer for dynamic element insertion with Google Tag Manager.",
	"content": "One of the most prominent features of Google Tag Manager since the dawn of time (actually, late 2012) is the Custom HTML tag. This little piece of magic lets Google Tag Manager inject an HTML element to the page. Since time immemorial (still late 2012), it\u0026rsquo;s allowed us to turn Google Tag Manager from a sandboxed prisoner of the native tag templates to a no-holds-barred client-side content management solution.\n  In this article, we\u0026rsquo;ll discuss how the Custom HTML tag works, and what you might be tempted to use it for.\nThe Custom HTML tag As the name implies, the Custom HTML tag lets you add HTML elements to the page.\nAnything you type into the Custom HTML tag will be interpreted as HTML.\nAdding content to the tag Let\u0026rsquo;s create a Custom HTML tag that looks like this:\n  \u0026lt;script\u0026gt; console.log(\u0026#39;Hello!\u0026#39;); \u0026lt;/script\u0026gt; \u0026lt;div\u0026gt; \u0026lt;span\u0026gt;Hello!\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; This tag adds three elements to the page: a \u0026lt;script\u0026gt; block compiled into and executed as JavaScript, a div block, and a \u0026lt;span\u0026gt; embedded in the \u0026lt;div\u0026gt;.\nWhen you publish the container and take a look at the minified source of the container JavaScript (as we all do), here\u0026rsquo;s what it looks like:\n  The reason your beautifully formatted tag looks like this is because the HTML is encoded - a wise step to take before any strings are evaluated into HTML elements.\n You might wonder what the enableIframeMode and enableEditJsMacroBehavior flags are - well, they\u0026rsquo;re legacy features that are no longer visible in the UI. If you know your stuff, you can actually make them show up in the Custom HTML tag interface, but they won\u0026rsquo;t do anything.\n So, you\u0026rsquo;ve now created a Custom HTML tag, and you\u0026rsquo;ve seen how the tag is added to the container library. But how does it end up on the page? And where?\nThe injection When it\u0026rsquo;s time for Google Tag Manager to \u0026ldquo;fire\u0026rdquo; the Custom HTML tag, it goes through the following motions:\n A dummy \u0026lt;div\u0026gt; is created to which the encoded string representing your Custom HTML tag is added using the .innerHTML property. This forces the browser to parse the encoded string as HTML, resulting in the tags you added to the Custom HTML tag to actually turn into HTML elements. One by one, these elements are removed from the dummy \u0026lt;div\u0026gt; and passed for injection. Upon injection, each element is added as the last child of the document.body element.  There are some nuances to this process, such as how Google Tag Manager handles the onHtmlSuccess and onHtmlFailure callbacks in tag sequencing, and how \u0026lt;script\u0026gt; elements are stripped of all custom attributes before injection. But overall, this is the process.\nNow, what this essentially means is that anything you type into a Custom HTML tag is added to the end of body, whatever that is at the time of injection. Typically, this translates to: \u0026ldquo;The bottom of the page\u0026rdquo;, but this is not a given with today\u0026rsquo;s fluid layouts.\n  One extremely important thing to keep in mind is that when you add a new element to the page you force a reflow of the content. Basically, the browser has to calculate again dimensions, positioning, layouts, and attributes of elements preceding, surrounding, and nested within the injected element.\nThis is not a painless operation. Each element you add compounds to the problem, and on single-page apps which might not reset the DOM between transitions, you could end up with hundreds of injected elements on the page, each hurting the performance in exponential increments.\nWe\u0026rsquo;ll get to this in the summary, but just to foreshadow the conclusion:\nAvoid using Custom HTML tags unless it\u0026rsquo;s absolutely necessary.\nYes, I do recognize the irony of such a disclaimer on this particular article.\nScenarios for Custom HTML tags Why use Custom HTML tags? Why use a tag management solution for element injection?\nGreat question! And one I don\u0026rsquo;t have a hard-and-fast answer for. I\u0026rsquo;d say an abundance of Custom HTML tags in a container is symptomatic of one (or more) of the following things:\n You have use cases far too complicated for GTM\u0026rsquo;s native tags or custom templates to handle. You have too little knowledge about GTM (or JavaScript) to know that some of your custom HTML tags could be replaced with native tags or custom templates. You have an inflexible organization, where getting the web developers to add your customizations directly to the page templates (or other site code) is not an option. You\u0026rsquo;ve found a cool thing online, and you want to quickly proof-of-concept it before passing it to the web developers for proper implementation. You are drunk with power over what you can do on your site with Google Tag Manager, and no bald dude from Finland is going to stop you from doing it.  It\u0026rsquo;s your container - you are of course free to use it however you wish. But if scenarios (2) and (3) ring a bell, I strongly urge you to seek change to the status quo. Being ignorant about the intricacies of GTM and JavaScript can be counter-productive to the beneficial effects that these technologies enable. Similarly, working against the constraints set by your organization will cause more friction in the long run, and can lead to inflamed communication structures, a crappy site, and unstable data collection.\nHaving said all that, let\u0026rsquo;s explore some of the scenarios where you might be tempted to use a Custom HTML tag!\nAdding an element to a particular position on the page The downside of the Custom HTML tag is that it injects the element to the end of \u0026lt;body\u0026gt;. But what use is that? If the element is a visual component (something that should be shown on the screen), then most likely the end of \u0026lt;body\u0026gt; is not where you want it to end up in.\nTo rectify this, you need to use JavaScript and its DOM manipulation methods.\nThe trick is to find some element that\u0026rsquo;s already on the page, and then position the new element relative to that.\nFor example, let\u0026rsquo;s say I want to add a little subtitle to this current page, so that it ends up looking like this:\n  Now, if I created a Custom HTML tag with just this:\n\u0026lt;h3\u0026gt;It\u0026#39;s really cool - I promise!\u0026lt;/h3\u0026gt; The element would be added to the end of \u0026lt;body\u0026gt; and wouldn\u0026rsquo;t look very good.\nSo, instead, I need to create a new element with JavaScript, and then position this new element relative to the title of the page.\n\u0026lt;script\u0026gt; (function() { // Create a new H3 element  var h3 = document.createElement(\u0026#39;h3\u0026#39;); // Add the text content  h3.innerText = \u0026#34;It\u0026#39;s really cool - I promise!\u0026#34;; // Get the reference to the current heading  var title = document.querySelector(\u0026#39;h1\u0026#39;); // Inject the new element right after the H1  if (title) { title.parentElement.insertBefore(h3, title.nextSibling); } })(); \u0026lt;/script\u0026gt; The end result? You can see it in the screenshot above.\nThere\u0026rsquo;s a subtle irony here - you are using Custom HTML tag to create an element (the \u0026lt;script\u0026gt;) that creates an element (the \u0026lt;h3\u0026gt;). Yes, it would be cool if you could specify in a Custom HTML tag to where the element is inserted. Actually, it would be even cooler if there were a custom template that allows you to create elements with positioning options. That way you wouldn\u0026rsquo;t need the invasive end-of-body script injection at all!\nBut I digress.\nAdding a script \u0026ldquo;as high up\u0026rdquo; in \u0026lt;head\u0026gt; as possible This is somewhat related to the previous scenario, but it deserves its own treatment because of how often the scenario emerges.\nSometimes, a vendor instructs you to \u0026ldquo;Place our script as HIGH UP in \u0026lt;head\u0026gt; as possible\u0026rdquo;.\nThis is instructed because the vendor wants their script to fire as soon as possible on the page. The higher an element in \u0026lt;head\u0026gt;, the sooner the browser parses it as part of the page render.\n  However, this benefit is largely lost when using Google Tag Manager. When the Google Tag Manager library has loaded, it\u0026rsquo;s typical that the parsing of \u0026lt;head\u0026gt; has already finished, and the browser is well into rendering the \u0026lt;body\u0026gt;. Because of this, trying to inject a script as high up in \u0026lt;head\u0026gt; as possible doesn\u0026rsquo;t make any sense, and is actually detrimental to the intended end result.\nWhy? Because when you create a Custom HTML tag that then creates an element and injects it into \u0026lt;head\u0026gt;, the browser first needs to inject the Custom HTML tag (performance hit), and then it needs to create the new element (another performance hit), and finally it needs to inject the new element into \u0026lt;head\u0026gt; (performance hit).\n  Instead - all you need to do is add the \u0026lt;script\u0026gt; directly to the Custom HTML tag. This way it will get inserted to the end of \u0026lt;body\u0026gt;, and the browser will execute it as soon as possible.\nLoad vendor JavaScript In fact, let\u0026rsquo;s continue the thought experiment from the previous scenario. Let\u0026rsquo;s say you do have a vendor whose JavaScript you want to load on the site, and you\u0026rsquo;ve established that all you need to do is add the \u0026lt;script\u0026gt; element to the page with a Custom HTML tag.\nMy friend. Do not use a Custom HTML tag at all.\nInstead, create a custom tag template that uses the injectScript API to load the library.\nCustom templates are optimized to inject and load the JavaScript, and they introduce a permissions and policies model for safe(r) injection.\n  Going forward, this will be the best way to proceed with \u0026lt;script\u0026gt; injection. It won\u0026rsquo;t help you with some of the other scenarios, as the JavaScript sandbox of templates is extremely restricting. So if you want to add custom event listeners, for example, you\u0026rsquo;ll still need ye olde Custom HTML tag.\nModify user experience One of the things you might be tempted to do with Google Tag Manager\u0026rsquo;s Custom HTML tags is modify the user experience. This could be adding something like a cookie banner, or maybe editing the styles on the page, or perhaps adding an \u0026lt;iframe\u0026gt; that loads some fancy widget to your ecommerce site.\nI\u0026rsquo;d like to warn you of the perils of doing these things with Google Tag Manager.\n  First of all, Google Tag Manager can be blocked by browsers (e.g. Brave) and by ad/content blockers. This is a rising trend (with browsers\u0026rsquo; tracking protections reducing the need for separate ad blockers).\nThe second reason is that you are separating functionality and/or experience from the site itself. You\u0026rsquo;re most likely relying on the positioning and selection of specific HTML elements to anchor your custom code. However, Google Tag Manager is decoupled from your site builds, and it\u0026rsquo;s extremely perilous to assume that the site will stay unchanged.\nThus, if even a single selector in your querySelector() argument changes on the site, your custom-built code can stop working or, even worse, break something else on the site.\nOn top of these are the reasons related to performance that I\u0026rsquo;ve mentioned earlier in this article. Each dynamic element injection will exponentially degrade the page performance, leading to annoying things like your custom elements flickering in and out, to degraded data quality (when an iframe you dynamically modify has time to load before you make the changes), all the way to page-stuttering, user-experience-decimating slowness, especially on single-page apps.\nSo, please, do consider not using Google Tag Manager as a content management system.\nSummary This was a brief foray into the magical world of Custom HTML tags.\nIf I could talk to the 2012 me, I would tell him to start considering the downsides of Custom HTML tags sooner, and stop getting drunk on Google Tag Manager\u0026rsquo;s infinite capability for script injection and rather think about the whole organization, the whole site, and the whole context of the environment where GTM runs, before making hazardous decisions just because.\nThat being said, Custom HTML tags do have their use cases today. In fact, creating a click listener with a Custom HTML tag (document.addEventListener()) can actually be better than firing some custom code with Google Tag Manager\u0026rsquo;s click triggers.\n  This is because a click trigger would fire the tag again and again (injecting it again and again) whenever the click trigger is a success. Whereas if you create your own little click listener in a Custom HTML tag and handle the repetitive task in the callback, you\u0026rsquo;ll avoid the injection mayhem.\nI also strongly vouch for the usefulness of Custom HTML tags for building proof-of-concepts. You can quickly try out different experiences and designs, and you can publish the changes for a sample of your visitors. If you\u0026rsquo;re happy with the results, you can then propose these changes to be added to the site code proper.\nNevertheless, perhaps one day, I hope, custom templates will make Custom HTML tags obsolete.\nAs parting words, when using Custom HTML tags especially to inject code you\u0026rsquo;ve found online, a famous Russian proverb is not out of place:\nTrust, but verify.\nIf you don\u0026rsquo;t understand what the code does, consult your friendly local web developer and ask them.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/cdnjs-hosted-libraries-template-google-tag-manager/",
	"title": "CDNJS Hosted Libraries Template For Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "tag templates"],
	"description": "Use this Google Tag Manager custom tag template to load hosted libraries from the CDNJS repository.",
	"content": "One of the prime things to use Google Tag Manager for is script injection. Loading a third-party JavaScript library is trivial to do with a Custom HTML tag, and works like a charm.\nHowever, using Custom HTML tags for, well, anything, is no longer the preferred way to add custom code to the site. Custom HTML tags are pretty expensive DOM injections, and they can be incredibly dangerous tools (for UX, security, and privacy) in the wrong and/or inexperienced hands.\nLuckily, custom templates were introduced a while ago to help get rid of custom JavaScript snippets from floating around in the container. Furthermore, the injectScript API is just what the doctor ordered for loading third-party SDKs via Google Tag Manager.\n Please note that loading any third-party JavaScript on a website is risky, as injection attacks could invite malicious code being executed on your site. If you\u0026rsquo;re concerned about this, you should take a look at Content Security Policies and Subresource Integrity checks to make sure only properly vetted third-party code is executed on the site.\n For this purpose, I\u0026rsquo;ve created the CDNJS - Hosted Libraries custom tag template. It allows you to load libraries from the CDNJS CloudFlare repository.\n  You can download the template from the template gallery.\nUsing a listed library The template allows you to either choose from a list of some of the most popular third-party libraries, or you can also just type the path to the library if it\u0026rsquo;s not in the list.\nThe listed libraries are:\n D3.js - a visualization library. Dojo - a framework for designing and building applications. Ext Core - a framework for designing and building applications. Hammer.js - a library for multi-touch gestures. jQuery - a library for DOM operations. jQuery Mobile - a touch-optimized framework for mobile and tablet devices. jQuery UI - a library for building user interfaces using jQuery. Knockout - a framework for building responsive user interfaces. MooTools - a light-weight object-oriented framework. Prototype - a framework for designing and building applications. Three.js - a library for creating and displaying 3D graphics. Web Font Loader - a library for loading web fonts.  When you choose one of these, you need to specify the version of the library. To know which version number to input, you need to visit CDNJS to see what\u0026rsquo;s the latest version (or if there\u0026rsquo;s an older version you need).\n  The benefit of using a listed library is that there\u0026rsquo;s some internal logic in the template which checks whether a global method added by the library already exists, and in that case the library load is blocked. This is useful because it reduces the potential overhead of reloading libraries that have already been downloaded.\nManually adding a library path If the library you need is not in the list, or if you want to avoid the global method check described in the previous paragraph, you can also add the library path by selecting Enter path manually in the template options.\nFor example, if you want to add clickspark.js to the site because you\u0026rsquo;re just dying for that little sparkle effect on every mouse click, you\u0026rsquo;d first search for clickspark.js in CDNJS.\nOnce you find the library page, you\u0026rsquo;ll see the list of available versions and the URL to the SDK. Always try to choose a minified version of the library. They usually end with .min.js. This reduces the size of the library downloaded by the user\u0026rsquo;s browser.\nFrom this URL, copy everything from /ajax/libs/ onwards. For clickspark.js, you\u0026rsquo;d copy /ajax/libs/clickspark.js/1.16.0/clickspark.min.js.\n  Then, in the template, paste this path into the respective field.\n  The template has whitelisted all URLs from https://www.cdnjs.com/ajax/libs, so there should be no issues as long as you paste the path correctly.\nSummary This is a simple template that serves a specific purpose. It lets you have some control over what third party libraries are loaded on the site via Google Tag Manager.\nA gentle reminder:\nTry not to load through Google Tag Manager anything that might serve a more common purpose than just tracking or advertising.\nFor example, don\u0026rsquo;t load jQuery through GTM if you need jQuery for site functionality. Why? Because users can block GTM, resulting in an awkward user experience for those that don\u0026rsquo;t want Google Tag Manager to load on the page.\nI also want to point out the warning in the introduction chapter of this article. Third-party libraries are vulnerable to malicious attacks, and by loading them through Google Tag Manager you are opening up a potential attack vector.\nOther than that, enjoy the template! Let me know if I\u0026rsquo;ve missed some absolutely obvious third-party library that should definitely be in the list!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/fetch-ip-geolocation-data-using-google-tag-manager/",
	"title": "Fetch IP Geolocation Data Using Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "tag templates", "ip"],
	"description": "Use the IP Geolocation API and a Google Tag Manager custom tag template to fetch IP address and geolocation data for the user.",
	"content": " Update 6 April 2020: I updated the template in the gallery to the latest version of the IP Geolocation API SDK, which no longer requires jQuery. Also, the SDK now handles API request caching to browser storage automatically, so the \u0026ldquo;Enable Session Storage\u0026rdquo; option was added to the template.\n My latest custom tag template tackles a use case I\u0026rsquo;ve referred to a number of times before, especially in my article on sending weather data to Google Analytics. The problem is two-fold:\n How to fetch the user\u0026rsquo;s IP address into dataLayer How to get latitude and longitude (as well as other geographical) data into dataLayer  For this purpose, I\u0026rsquo;ve created a new Google Tag Manager custom tag template that leverages the IP Geolocation API service. The API is inexpensive to use, and it offers HTTPS by default.\n  You can find the template in the template gallery.\nIP Geolocation API You can register for a free account at https://ipgeolocation.io/ to get an API key. If you register for a paid subscription, in addition to getting more quota (the free subscription only allows 1,000 requests per day), you\u0026rsquo;ll be able to use the template without exposing your API key to the public, so it\u0026rsquo;s something you should really consider if geolocation and/or IP resolution is what you want to do.\nSign up at https://ipgeolocation.io/signup.html.\n  Once you\u0026rsquo;ve signed up, you\u0026rsquo;ll find the dashboard with your API key. You\u0026rsquo;ll need this for the tag you create with the custom template.\n  If you\u0026rsquo;ve opted for a paid plan, you can add the origin (e.g. www.simoahava.com) to the list of Request Origins. If you do this, you don\u0026rsquo;t need to add the API key to the template.\nOnce you have the API key, and once you\u0026rsquo;ve downloaded the custom template into your container, you\u0026rsquo;re ready to proceed!\nCreate the tag Create a new tag, and use IP Geolocation API as the template.\n  Then, fill in the tag fields. Make sure to add the API key fetched from your IP Geolocation API dashboard, unless you\u0026rsquo;ve added the origin where the tag will fire to the list of Request Origins (see previous chapter).\nThe other settings you can change are listed in the following chapters.\nYou can add whatever trigger you want to the tag. A trigger such as All Pages is OK, because it will resolve the API call as the earliest convenience. You can also use a more detailed rule to prevent the tag from firing on every single page, or to defer it until Window Loaded or similar.\nThe tag relies on the jQuery library. If your site doesn\u0026rsquo;t already use it, the template will automatically load the minified jQuery library along with the API SDK.\nData Layer Settings If you\u0026rsquo;re using some other name for the dataLayer structure than dataLayer, you must type it into the Data Layer global variable name field, or the solution will not work properly.\nThe Custom event name field is where you can define what the value of the event key will be in the dataLayer object that contains the geolocation and IP address data.\nThe dataLayer object itself will be listed under the key geoData and will look something like this:\n  By the way, I am not in Salo, but in Espoo (some 100 kilometres distance). Geolocation is not a perfect art when using IP lookups.\nIP Address Settings The default is Client IP, which means it uses whatever public IP address is assigned to the machine in use.\nIf you want, you can also provide a Custom IP in case you\u0026rsquo;ve resolved the IP some other (more reliable, perhaps) way. The Custom IP can take a hard-coded value (not sure why you\u0026rsquo;d ever do so, though), or you can use a GTM variable for it. For example, to load the IP from a Data Layer Variable named DLV - user.ip, you could do this:\n  Other Settings You can type a comma-separated list of fields to only include in the response by editing the Fields to include field.\n  The example above would only include the ip (always included), longitude, and latitude fields in the response.\nSimilarly, you can list fields you want to exclude from the response by editing the Fields to exclude field.\nFinally, you can change the language of some of the response values by switching the Response language in the respective field. This might translate location names and other translatable values to the target language.\nWhat to do with the data Once you\u0026rsquo;ve got it up and running - what then?\nWell, a new object will be populated in dataLayer using the custom event name you\u0026rsquo;ve set in the tag settings (geolocate by default). You can now fire tags that require IP or geolocation data using a Custom Event trigger for this event name.\n  The rest of the utilities would be Data Layer variables you create for the individual parts of the response you want to access. For example, a Data Layer variable for the user\u0026rsquo;s IP address would look like this:\n  A Data Layer Variable for the user\u0026rsquo;s latitude would look like this:\n  And so on and so forth.\nYou can use these for a variety of things:\n Send geographical data to Google Analytics in order to benchmark the accuracy of the API or GA\u0026rsquo;s own geolocation lookups Check the IP address against a range of \u0026ldquo;internal\u0026rdquo; IP addresses to flag the user as internal traffic using Google Tag Manager Use the currency data to establish a local currency by default for the user Use the isp and organization data to check ISP and network domain information about the user.  Summary As always, there\u0026rsquo;s caveats galore with IP lookups. So many users use VPNs these days to (understandably) mask their location. This makes any type of geolocation and IP resolution extremely unreliable on a larger scale.\nSimilarly, even without a VPN, the accuracy is iffy at best. In the example used in the article, my location was resolved to be Salo, which is around 100 kilometres away from Espoo where the test data was generated.\nSo I wouldn\u0026rsquo;t put any UX value to this information - don\u0026rsquo;t redirect the user based on the API response, don\u0026rsquo;t tie any business critical logic to either IP or geolocation data, and don\u0026rsquo;t assume the API has an even passable success rate.\nHowever, the use cases listed in the previous chapter are all more or less valid even with inaccurate data. You just need to shift your hypotheses from requiring exact, infallible answers to being satisfied with a broader spectrum of accuracy.\nIn any case, there are use cases for inspecting IP address and geolocation data within the browser. Some of them have been listed in the previous chapter. This custom template was created to help with these use cases.\nThank you for reading, and please share your questions, suggestions, and use cases in the comments of this article!\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/cdnjs-hosted-libraries/",
	"title": "CDNJS - Hosted Libraries - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The CDNJS - Hosted Libraries custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Blog post   Gallery entry   GitHub repo      Description You can use this template to load third-party libraries from the CDNJS content distribution network.\nThere is a preset list of the most popular libraries, and you can also just provide a path to the library manually.\nIf you use the drop-down list to select the library, the template will check if the global method for that library has already been initialized, in which case the template is blocked from redundantly loading the library again.\nRelease notes    Date Changeset     6 January 2020 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/chrome-samesite-warnings-google-tag-manager/",
	"title": "#GTMTips: Chrome Samesite Warnings For Google Tag Manager",
	"tags": ["google tag manager", "gtmtips", "privacy", "cookies"],
	"description": "With the upcoming change to how web browsers process cross-site cookies, you might have seen a SameSite warning for Google Tag Manager&#39;s cookies. This article sheds light on the phenomenon.",
	"content": " Update 17 February 2020: Google Tag Manager\u0026rsquo;s Preview mode cookies have been updated with the necessary flags, so they will not break once SameSite enforcement begins.\n If you\u0026rsquo;ve opened the browser console in Google Chrome (since Chrome 76), you might have seen a bunch of warnings in a yellow background related to something called a SameSite cookie attribute that is either missing or incompletely set for cookies set on external domains. If you use Google Tag Manager, especially in Preview mode, you might have seen a warning about the http(s)://www.googletagmanager.com domain.\nEven though the warning is very prominent, hogging up some prime real estate in the browser console warning, it is, for now, just a warning.\nThis article article briefly explains what the SameSite hoopla is all about, and how it relates to Google Tag Manager.\nTip 108: SameSite cookie attribute and Google Tag Manager   This is the full text of such a warning:\n A cookie associated with a cross-site resource at https://www.googletagmanager.com/ was set without the SameSite attribute. A future release of Chrome will only deliver cookies with cross-site requests if they are set with SameSite=None and Secure. You can review cookies in developer tools under Application\u0026gt;Storage\u0026gt;Cookies and see more details at https://www.chromestatus.com/feature/5088147346030592 and https://www.chromestatus.com/feature/5633521622188032.\n What is the SameSite attribute? The SameSite attribute is configured when the cookie is set, and is used to describe the contexts in which the cookie is available for reading.\nIt has three possible values:\n  Strict - A cookie set with this SameSite value is only available in requests where the request host shares the public suffix of the request origin (domain.com would be the public suffix of www.domain.com, sub.domain.com and ecommerce.blog.domain.com). Thus if a page on domain.com requested a resource from www.domain.com, the SameSite=Strict cookie would be sent with the headers. But if the page on domain.com requested a resource from doubleclick.net, any SameSite=Strict cookies written on doubleclick.net would not be included in the request.\nNotably, This applies to navigation as well - when you navigate from domain.com to anotherdomain.com, any SameSite=Strict cookies written on anotherdomain.com would not be included in the request headers.\n  Lax - If set with this value, the SameSite cookie behaves similar to Strict cookies, but it does allow for top-level navigation to include the cookie in the HTTP headers. Thus, if the user moves from domain.com to anotherdomain.com, the SameSite=Lax cookies written on anotherdomain.com would be included in the request headers.\nThis is also the default value for cookies without an explicitly set SameSite attribute.\n  None - This attribute allows the cookie to be accessed in first-party and third-party contexts without restrictions.\n  Basically, if you have a cookie that needs to be distributed to multiple domains in HTTP headers, you need to set the SameSite attribute to None or it will cease to work when SameSite becomes enforced.\nThe catch is that a SameSite=None cookie must also have the Secure flag, or it will not work.\nIf the cookie does not have the SameSite parameter, the default behavior is Lax. Thus, again, all cookies intended for third-party access would cease to work unless explicitly set to None.\nThis is a great article for SameSite information: SameSite cookies explained.\nWhat\u0026rsquo;s the deadline? The SameSite change needs to be implemented by February 4, 2020. On that date, Chrome 80 will become the new stable build, and users upgrading to it will see that the SameSite attribute is enforced.\nThus, after that date, any cookies sent with cross-site requests must have SameSite=None and Secure flags or the browser will reject them.\nSo what about Google Tag Manager? You might have two concerns about Google Tag Manager.\n Why does Google Tag Manager use third-party cookies?! - Simple: Preview mode. GTM uses a third-party cookie set on www.googletagmanager.com to ensure that if your browser is in Preview mode, then your browser is privy to the container draft rather than the latest, live version. What can you do about GTM\u0026rsquo;s SameSite warnings? - Nothing. If you read the previous chapters carefully, you\u0026rsquo;ll see that the SameSite change needs to be done by the party setting the cookie, i.e. Google. So Google needs to update the authentication cookies written on www.googletagmanager.com to include the SameSite=None and Secure flags for GTM\u0026rsquo;s Preview mode to continue working in Chrome (and any browsers that similarly choose to enforce SameSite).  Luckily, there is confirmation from the Google Tag Manager team that they will make sure the cookies are updated before the SameSite change enters the stable build.\nSimilarly, I\u0026rsquo;m positive all other Google properties that require cookie access in third-party context will be adequately processed as well.\nThe big concern with SameSite might not thus be the vendors (e.g. adtech) that use cookies in third-party contexts to run their tracking schemes, but rather in-house setups where things like authentication, shopping carts, and SSO are handled with a consolidated domain passing persistent information in third-party cookies.\nIt\u0026rsquo;s a potentially momentous change, so if your company\u0026rsquo;s IT department isn\u0026rsquo;t on top of things, they\u0026rsquo;re running out of time.\nTo find out about the scope of this change and how ill-prepared the general public is, I recommend looking no further than Zach Edwards\u0026rsquo; Twitter feed, as it\u0026rsquo;s full of examples where things have gone (or are going to go) awry.\nZach\u0026rsquo;s webinar on the topic is also illuminating (you can find the presentation here).\nI hope this article was illuminating - please let me know in the comments if you have further concerns about SameSite cookies!\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/url-2-0/",
	"title": "URL 2.0 - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The URL 2.0 custom variable template is a variable template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Gallery entry   GitHub repo      Description The URL 2.0 template is designed to partially or completely replace the built-in URL variable in Google Tag Manager.\nYou can use it to build a custom URL string. With this option, you select the parts of the URL to combine into a new string.\nFor example, if the user is on https://www.mydomain.com/my-path/?login=true#customClient, and you select Hostname, Path and Fragment from the variable settings, the end result would be:\nwww.mydomain.com/my-path#customClient\nIn Advanced Settings, you can choose whether to parse the Page URL or the Referrer URL. Unfortunately, you can\u0026rsquo;t use a Google Tag Manager variable to produce the URL string, because the template APIs do not support parsing custom URLs into the type of object required by the template.\nFrom Variable type you can also choose to retrieve a value for a Query key or a Fragment key.\nThe first looks through the query string (?key=value\u0026amp;key2=value2) for the given key, and returns the value of the first match as a string, or all matches as an array (if the query key is incorrectly added multiple times).\nThe second looks through the fragment (#somekey=value) for the given key, and returns its value.\nRelease notes    Date Changeset     12 December 2019 Fix fragment key handling to parse question marks as well.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/tags/consent/",
	"title": "consent",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/persist-campaign-data-landing-page/",
	"title": "Persist Campaign Data On The Landing Page",
	"tags": ["google tag manager", "custom templates", "single-page apps", "consent"],
	"description": "Persist landing page URLs with campaign parameters and external referrer strings in browser cookies (for e.g. post-consent campaign attribution).",
	"content": "Universal Analytics utilizes two components (by default) to attribute a browser session to a specific campaign: query parameters in the URL and the referrer string.\nThe page URL is sent with every hit to Google Analytics using the Document location field, which also translates to the \u0026amp;dl parameter in the Measurement Protocol.\nThe referrer string is sent with a hit to Google Analytics using the Document referrer field, as long as the referrer hostname does not exactly match that of the current page and the referrer string is not empty.\nWhen attributing a hit to a campaign session, Google Analytics first checks the location value for the following parameters:\n gclid - for attributing the session to Google Ads utm_source - for attributing the session to a custom campaign  If these parameters are not found, Google Analytics looks at the referrer string.\n   Yes, there are additional steps to campaign attribution (see the flowchart), but these two are relevant in the context of this article.\n There are two contexts where attributing campaigns based on URLs and referrer strings becomes complicated.\n If the site blocks Google Analytics on the landing page (due to missing consent), but then makes it available on subsequent pages (due to granted consent), the original campaign URL and referrer string are no longer available. If the site is a single-page application, particularly Google Tag Manager-based tags become susceptible to the rogue referral problem.  For this purpose, I have created a new custom template: Persist Campaign Data.\n  This template tackles both of the problems listed above. However, you will need to make additional changes to your Google Analytics settings before the solution is complete.\nYou can download the template from the template gallery.\n1. Store campaign data in a session cookie The first problem is particularly bothersome on sites that utilize consent management systems to block Google Analytics if consent has not been granted.\nIf consent is granted on the landing page, then there should be no issues. The page is reloaded with the campaign parameters in the URL and the referral string intact, and Google Analytics tags can make use of these when attributing the session to a specific campaign.\nHowever, if the user grants consent only after navigating deeper into the site, the tags that fire on these pages will no longer have access to the initial campaign that brought the user to the site, which leads to inflation of direct traffic.\nTo solve this problem, the template tag (assuming Google Tag Manager is not blocked, too) writes the URL of the current page in a cookie if it has campaign-setting parameters. Similarly, the referrer string is also written in a cookie if its hostname does not contain the current page hostname.\n  Then, when the Google Analytics tags finally fire, they can take the original location and referrer from these cookies to attribute the session to its correct source.\nSetup the Persist Campaign Data tag Create a new tag with the Persist Campaign Data template. Check the box next to Store campaign data in a browser cookie and check that the settings are OK.\n  By default, the following URL parameters will cause the tag to set the campaign URL cookie:\n utm_source utm_medium utm_campaign utm_term utm_content utm_id gclid  Naturally, you can add additional parameters to the list if you wish.\nNot all of these parameters have the ability to start a new session (only utm_source and gclid do, in fact), but the assumption is that if you\u0026rsquo;ve set any UTM parameters, you want to include them with your hits.\nNext, you can change the default name of the URL cookie and the referrer cookie, if you wish.\n NOTE! If you do change the name of either cookie, you must edit the template permissions and allow the template to set the new cookie name(s).\n The tag will only write the cookies in specific conditions:\n The URL cookie is only written if the URL query string has any one of the listed trigger parameters. If such a parameter is found, the full URL is written into the campaign URL cookie. The Referrer cookie is only written if the referrer hostname does not contain the current page hostname, and the referrer string is not empty. If both conditions are good, the full referrer string is written into the referrer cookie.  Thus it\u0026rsquo;s safe to set it on the All Pages trigger, as it doesn\u0026rsquo;t matter if it fires on every single page.\nNote! You might want to actually set it on a trigger that fires once user consent has been established - if the user gives consent or has already given consent, the tag does not need to (and should not) fire, because then your Google Analytics tags would fire normally and your wouldn\u0026rsquo;t need to save the URL and referrer data in cookies.\nCreate the cookie variables You\u0026rsquo;ll need two new, user-defined 1st Party Cookie variables.\nThe first, {{Cookie - __gtm_campaign_url}}. The second, {{Cookie - __gtm_referrer}}.\nMake sure you check the URI-decode cookie box.\n    Create the customTask To update your Google Analytics tags, we\u0026rsquo;ll be using a customTask. This is an easy way to make sure the cookies are only used if they exist, and it\u0026rsquo;s a great way to actually delete the cookies as soon as they\u0026rsquo;ve been used, so that they don\u0026rsquo;t persist needlessly.\n NOTE! You don\u0026rsquo;t have to use a customTask. You can set the location, page, and referrer fields manually on the tag, if you wish. You just need to add proper fallback values if the cookies do not exist, otherwise you risk corrupting your data with invalid location and referrer fields.\n If you need the cookies for something else as well, then you can set the customTask to not delete cookies after the values have been \u0026ldquo;used\u0026rdquo;.\nCreate a new Custom JavaScript variable, name it {{customTask - set location and referrer from cookie}}, and add the following code within:\nfunction() { return function(customModel) { // Set to the cookie variables  var storedLocation = {{Cookie - __gtm_campaign_url}}; var storedReferrer = {{Cookie - __gtm_referrer}}; // Set to the cookie names  var storedLocationCookieName = \u0026#39;__gtm_campaign_url\u0026#39;; var storedReferrerCookieName = \u0026#39;__gtm_referrer\u0026#39;; // Set to the root domain of your site (e.g. domain name without any subdomain, \u0026#34;mysite.com\u0026#34;)  var domainName = \u0026#39;simoahava.com\u0026#39;; // Choose whether to delete a cookie after its value is used (true/false)  var deleteCookieToggle = true; var deleteCookie = function(name) { document.cookie = name + \u0026#39;=; path=/; domain=\u0026#39; + domainName + \u0026#39;; expires=Thu, 01 Jan 1970 00:00:00 GMT\u0026#39;; }; if (typeof storedLocation !== \u0026#39;undefined\u0026#39;) { customModel.set(\u0026#39;location\u0026#39;, storedLocation); customModel.set(\u0026#39;page\u0026#39;, document.location.pathname + document.location.search); if (deleteCookieToggle === true) { deleteCookie(storedLocationCookieName); } } if (typeof storedReferrer !== \u0026#39;undefined\u0026#39;) { customModel.set(\u0026#39;referrer\u0026#39;, storedReferrer); if (deleteCookieToggle === true) { deleteCookie(storedReferrerCookieName); } } }; }  There are some things you might need to edit.\nChange the storedLocation and storedReferrer variables to point to the correct Google Tag Manager 1st Party Cookie variables you\u0026rsquo;ve created, if you\u0026rsquo;ve diverted from the default values used in this guide.\nChange the storedLocationCookieName and storedReferrerCookieName to the correct cookie names, if you\u0026rsquo;ve diverted from the default values used in this guide.\nSet the domainName to the correct root domain (domain name without any subdomains included).\nSet the deleteCookieToggle to false if you don\u0026rsquo;t want the task to remove cookies once their values have been \u0026ldquo;used\u0026rdquo;.\nThis customTask sets the location andreferrer fields if the respective cookies exist, and then (optionally) deletes the cookies for those fields that were set. If a cookie doesn\u0026rsquo;t exist, its field is not set.\nIf the location is set, the customTask also sets the page field to the current page path and query string. If it didn\u0026rsquo;t do this, then the tag would take the page path from the location as well, which would inflate the pageviews of the landing page rather than increment them for the page the user is actually on.\nAdd the customTask to your Google Analytics tags Finally, add the customTask variable you just created to your Google Analytics tags. You only need to have it fire on the first tag that fires on the page, but just to be safe it might make sense to add it to your Google Analytics Settings variable, and then attach that to all your tags.\n  The field name is customTask and the value is {{customTask - set location and referrer from cookie}}.\nStore campaign data - summary If the user lands on your site with campaign parameters in the URL and/or a referrer that is external to your website\u0026rsquo;s domain, the following happens:\n The Persist Campaign Data tag creates new cookies where necessary; one for the URL of the page (if it had campaign parameters) and one for the referrer string (if it\u0026rsquo;s an external domain). Once the user gives consent, your Google Analytics tags utilize a customTask which sets the location and referrer fields from the cookies, and subsequently deletes the cookies. Thus the campaign URL and the referrer string are preserved for Google Analytics tags that might not fire until later in the site navigation, when the original URL parameters and referrer string are just a memory.  There are some caveats to note.\n If you already have a customTask set, you need to modify it to include the customTask code from this guide. See this for inspiration. There might be a race condition where the Persist Campaign Data tag completes after the Google Analytics tag has fired, leading to cookies being created but not utilized (and subsequently deleted). An easy way to avoid this problem is to make sure the Persist Campaign Data tag only fires if user has not given consent. That way it will only fire if the Google Analytics have been blocked. The cookies are session cookies so if the user closes the browser and then re-opens it to continue navigation, the stored data is lost. Cross-domain tracking is something that is very likely lost if you delay firing your tags beyond the landing page - this solution will not help with this problem. You could take the Client ID from the linker parameter, but since you\u0026rsquo;re not validating the linker parameter, the data would be vulnerable to link sharing (and any users who follow the shared link would be counted as the same user by Google Analytics).  2. Push original location in dataLayer The rogue referral problem is a scenario where tags firing on a single-page app use the \u0026ldquo;virtual\u0026rdquo; location for campaign attribution (often missing the original campaign parameters) and thus Google Analytics reverts to the referrer instead. This results in sessions where hits sent on the first page are part of a google / cpc session, but as soon as the user navigates to other parts of the SPA, the campaign changes to the referral source, such as google / organic.\n   NOTE! Please understand that these steps must only be taken if the site is a single-page app or has single-page app components. There\u0026rsquo;s no harm in setting this up on a regular site, but it\u0026rsquo;s just extra work and clutter in the container.\n To fix this, you need the following.\nSetup the Persist Campaign Data tag Create a new tag with the Persist Campaign Data template. Check the box next to Push original location in dataLayer and check that the settings are OK.\n  Set this tag to fire on the All Pages trigger.\nThis tag, when fires, does a very simple thing. It writes the current URL into the dataLayer using the key originalLocation.\nIt\u0026rsquo;s important that this tag only fires once on the initial page load (hence the All Pages trigger).\nCreate the Data Layer Variable Next, create a Data Layer variable named {{Original Location}}, and set it to point to the originalLocation key. Importantly, set its Default Value to {{Page URL}}.\n   The Default Value is very important. It\u0026rsquo;s more than likely that a Google Analytics tag fires before originalLocation is pushed into dataLayer, so the variable simply returns the current page URL (which should be the same as originalLocation) in case such a race condition emerges.\n Update your Google Analytics tags Now that you have the template tag firing on the All Pages trigger, and you have the Data Layer variable ready, you need to edit every single one of your Google Analytics tags. Easiest way to do this is with the Google Analytics Settings variable.\n NOTE! It\u0026rsquo;s vital that every single Google Analytics tag has this modification. If there\u0026rsquo;s even a single tag that fires without the fixed location, it\u0026rsquo;s possible that you campaign data will be broken.\n You need to add two fields to the variable:\nField name: location\nValue: {{Original Location}}\nField name: page\nValue: {{Page Path}}\nThe value of page should be whatever you currently use for your virtual pageviews on your single-page app. It could be a Data Layer variable, a history state variable, or it could simply be the current page URL or current page path, as in the example above.\nBoth of these fields are required to be set in your tags.\nHow it works Once you\u0026rsquo;ve set it up, here\u0026rsquo;s what happens.\n When the user lands on the page, the page URL is stored in the originalLocation Data Layer variable. When a Google Analytics tag fires, it takes the location field from this variable. This is because the location field is used for campaign attribution, and the landing page has all these parameters. The page field is set so that a virtual page path could be set for your single-page app page views.  Thus by always sending the URL of the initial page load as the value of the location field, the rogue referral problem is fixed.\nSummary The Persist Campaign Data template addresses two problems with campaign tracking in Google Analytics.\n Delayed / deferred Google Analytics tracking, where the first hits of the session are sent after the user has navigated away from the landing page. Single-page application tracking, where campaign sessions get incorrectly attributed to the referrer string rather than the parameters in the URL.  You can, of course, utilize the features for other things as well. Persisting campaign parameters can have uses beyond Google Analytics tracking, and identifying the URL of the first page load of a SPA could similarly be used to understand navigation patterns better.\nAs always, I look forward to hearing from you in the comments. Do you see problems with either approach? Do you have suggestions for improving the template or the implementation thereof?\n"
},
{
	"uri": "https://www.simoahava.com/tags/single-page-apps/",
	"title": "single-page apps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/string-from-array-of-objects/",
	"title": "String from array of objects - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The String from array of objects custom variable template is a variable template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Gallery entry   GitHub repo    Description Given an array of objects, you can use this template to create a new string with the values of just one of the keys shared in all or some of these objects. For example, say this is what the array looks like:\n[ { id: \u0026#39;12345\u0026#39;, name: \u0026#39;product1\u0026#39; }, { id: \u0026#39;23456\u0026#39;, name: \u0026#39;product2\u0026#39; }, { id: \u0026#39;34567\u0026#39;, name: \u0026#39;product3\u0026#39; } ]  If you configure this as the Input array of the variable, and then id as the Object property, and | as the Delimiter, the variable will return this:\n\u0026quot;12345|23456|34567\u0026quot;\nRelease notes    Date Changeset     30 November 2019 Add unit tests.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/tag-template-test-walkthrough/",
	"title": "#GTMTips: Walkthrough Of Custom Template Tests",
	"tags": ["google tag manager", "gtmtips", "custom templates", "testing"],
	"description": "Walkthrough of how to build custom template unit tests for tag templates and for variable templates in Google Tag Manager.",
	"content": "Google Tag Manager now lets you add unit tests directly to your custom templates. This is useful, since it allows you to control the code stability of your templates, especially if you\u0026rsquo;ve decided to share those templates with the public.\nI recently shared a general guide for how template tests work, but I wanted to expand the topic a little, and share with you two walkthroughs of custom template tests: one for a variable template and one for a tag template.\nThere\u0026rsquo;s a video if you prefer a more visual way to walk through these steps.\n  Tip 107: How to build tag and variable template tests   Let\u0026rsquo;s get started. We\u0026rsquo;ll kick things off with a variable template.\nVariable template: String from array of objects This walkthrough will utilize my String from array of objects template.\n  The template is very simple. It takes a GTM variable as an input, where the variable must return an array of objects (e.g. [{id: '123'},{id: '234'}]).\nThen, the user specifies which property from the objects they want to concatenate the values of into a string (e.g. id), and finally they can specify a delimiter other than ,. The end result would be something like '123,234' if following the example from the previous paragraph.\nHere is the template code in all its simplicity:\nconst callInWindow = require(\u0026#39;callInWindow\u0026#39;); const getType = require(\u0026#39;getType\u0026#39;); const inputArray = data.inputArray; const keyToConcatenate = data.keyToConcatenate; const delimiter = data.delimiter; // If not an array, return undefined if (getType(inputArray) !== \u0026#39;array\u0026#39;) { return; } return inputArray .map(obj =\u0026gt; obj[keyToConcatenate]) .filter(obj =\u0026gt; obj) .join(delimiter);  When we deconstruct the variable code, there are four different things that can happen.\n The input is not an array, in which case the getType() check succeeds and the variable returns undefined. The array does not have any objects within, in which case the .map() and .filter() calls end up returning an empty array, which is then turned into an empty string by .join(). The array has objects but none of them has the specified key, in which case the .map() and .filter() calls end up returning an empty array, which is then turned into an empty string by .join(). The array has at least one object with the given key, in which case the variable returns a string where the key values are concatenated into a string using the delimiter.  In other words, we need four tests.\nTest 1: Return undefined if not array If the variable is not an array, the variable returns undefined. To test this, we create a mock data object, where we pass a non-array as the input.\nThis is what the test looks like:\n// Call runCode to run the template\u0026#39;s code. const mockData = { inputArray: \u0026#39;notAnArray\u0026#39;, keyToConcatenate: \u0026#39;name\u0026#39;, delimiter: \u0026#39;,\u0026#39; }; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isUndefined();  The mockData object has the inputArray set to a string, which is not an array. This object is passed to runCode(), which executes the variable template as if the user had created the template with the non-array as the input.\nFinally, the result of runCode() is asserted by passing the test if the return value of the variable is undefined, by using the isUndefined() assertion.\nTest 2: Return empty string if no objects in array In the next test, we need to create a scenario where the user does provide an array (so the getType() check doesn\u0026rsquo;t catch it), but this array will have no objects within. In that case, the variable would return an empty string.\n// Call runCode to run the template\u0026#39;s code. const mockData = { inputArray: [1, 2, 3], keyToConcatenate: \u0026#39;name\u0026#39;, delimiter: \u0026#39;,\u0026#39; }; const expected = \u0026#39;\u0026#39;; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isEqualTo(expected);  Here we use the expected constant to make it easier to manage and read the code. Again, the mock data object is passed to runCode(), and the variable result is then asserted with the isEqualTo() check, this time passing the test if the variable result is exactly an empty string.\nTest 3: Return empty string if objects in array do not have the key To be fair, this could be bundled up with the previous test, because they test the same lines of code and the end result is the same.\n// Call runCode to run the template\u0026#39;s code. const mockData = { inputArray: [{id: \u0026#39;123\u0026#39;},{category: \u0026#39;shoes\u0026#39;}], keyToConcatenate: \u0026#39;name\u0026#39;, delimiter: \u0026#39;,\u0026#39; }; const expected = \u0026#39;\u0026#39;; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isEqualTo(expected);  As you can see from the mock object, we are looking for the key name in the objects of the array. Neither one of the objects in the inputArray has that key.\nOnce this mock object is passed to runCode(), the variable code is executed, and the variable ends up returning an empty string. This is asserted by checking the result against the expected value of ''.\nTest 4: Return concatenated string with delimiter for valid object keys The last test is the only positive check we need to do.\n// Call runCode to run the template\u0026#39;s code. const mockData = { inputArray: [{name: \u0026#39;firstName\u0026#39;},{name: \u0026#39;secondName\u0026#39;}], keyToConcatenate: \u0026#39;name\u0026#39;, delimiter: \u0026#39;,\u0026#39; }; const expected = \u0026#39;firstName,secondName\u0026#39;; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isEqualTo(expected);  This time, the mock data array has two objects, where both have the key name. Thus the expected constant is set to 'firstName,secondName', because that is what we expect the variable to return.\nThis is asserted with the isEqualTo() check again.\nTag template: User distributor The second walkthrough will utilize my user distributor template. I wrote about how the template works here.\n  The tag template is quite a bit more complex than the variable template above. For one, we have branching decisions, and we also have collective catch-alls that apply to all branches.\nThe user distributor lets you choose whether to isolate a single group or multiple groups of users. If a random number matches the distribution you set for either option, the user can be assigned to a group, which is designated by writing a cookie in the browser.\nIn case the user is already in a group, the code is not executed, and in case the random number does not match a distribution range, the cookie is not written.\nSo, the scenarios we need to test for are these:\n If cookie exists, i.e. the user has already been assigned to a group, do nothing. If using a single distribution, set cookie to true if the user is assigned to the group. If using a single distribution, set cookie to false if the user is not assigned to the group. Make sure the single distribution cookie is written with the correct options. If using a multi distribution, set cookie to the first group when the randomizer is within the range. If using a multi distribution, set cookie to the second group when the randomizer is within the range. If using a multi distribution, do not set cookie if the randomizer is outside all group ranges. Make sure the multi distribution cookie is written with the correct options.  These eight tests should get 100% coverage for the tag template. And for reference, here is the full tag template code:\nconst setCookie = require(\u0026#39;setCookie\u0026#39;); const readCookie = require(\u0026#39;getCookieValues\u0026#39;); const generateRandom = require(\u0026#39;generateRandom\u0026#39;); const makeInteger = require(\u0026#39;makeInteger\u0026#39;); const log = require(\u0026#39;logToConsole\u0026#39;); // Percentage accumulation for multi let acc = 0; // Randomizer const rand = generateRandom(1, 100); // User data const cookieName = data.cookieName; const cookieDomain = data.cookieDomain; const cookieMaxAge = data.cookieExpires * 24 * 60 * 60; const single = data.singlePercent; const multi = data.multiPercent; // Only run if cookie is not set if (readCookie(cookieName).length === 0) { // If single distribution  if (single) { setCookie( cookieName, (rand \u0026lt;= single ? \u0026#39;true\u0026#39; : \u0026#39;false\u0026#39;), { domain: cookieDomain, \u0026#39;max-age\u0026#39;: cookieMaxAge } ); } // If multi distribution  if (multi) { multi.some(obj =\u0026gt; { acc += makeInteger(obj.probability); if (rand \u0026lt;= acc) { setCookie( cookieName, obj.value, { domain: cookieDomain, \u0026#39;max-age\u0026#39;: cookieMaxAge } ); return true; } }); } } data.gtmOnSuccess();  Let\u0026rsquo;s start with a test setup.\nSetup: initialize the mock objects This time around, we\u0026rsquo;ll be running multiple tests to check different aspects of the same mock data setups. Thus, instead of always initializing the mock objects for each test with (almost) identical values, we can instead create the mock objects in the Setup of the test, after which they will be in the scope of all test cases.\n  This is what the setup code looks like:\nconst mockSingleData = { cookieName: \u0026#39;userDistributor\u0026#39;, cookieDomain: \u0026#39;auto\u0026#39;, cookieExpires: 365, singlePercent: 50 }; const mockMultiData = { cookieName: \u0026#39;userDistributor\u0026#39;, cookieDomain: \u0026#39;auto\u0026#39;, cookieExpires: 365, multiPercent: [{ value: \u0026#39;A\u0026#39;, probability: 33 },{ value: \u0026#39;B\u0026#39;, probability: 33 }] };  The single distribution mock sets the cookie userDistributor to 'true' if the randomizer generates a number between 1 and 50. Numbers between 51 and 100 set the cookie value to 'false'.\nThe multi distribution mock sets the cookie userDistributor to 'A' if the randomizer generates a number between 1 and 33, to 'B' for values between 34 and 66, and does not set the cookie for values between 67 and 100.\nIn both cases, the cookie is written with the 'auto' domain, and the expiration is set to 365 days.\nTest 1: Do nothing if cookie exists The first scenario we\u0026rsquo;ll test is what happens if the cookie named userDistributor has already been set. In this case, the template shouldn\u0026rsquo;t really do anything - at least, it shouldn\u0026rsquo;t reset the cookie with a new value.\nmock(\u0026#39;getCookieValues\u0026#39;, [true]); // Call runCode to run the template\u0026#39;s code. runCode(mockSingleData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasNotCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasNotCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  We use the mock() API here, which lets us fake the result of a custom template API. In this case, we\u0026rsquo;re instructing the template to return the value [true] whenever the getCookieValues API is invoked. The value within the array is inconsequential - all it has to do is return an array with a positive length for the template to believe that the cookie has been set.\nNext, the single mock data object is passed to runCode().\nAfter that, we simply check if the code has run by checking which custom template APIs have been called. We can see from the code that generateRandom() and getCookieValues() are called before any checks are done, but makeInteger() and setCookie() are only called if the cookie has not been set yet.\nThus, we can use the wasCalled() and wasNotCalled() APIs to check which branch of the template code was executed.\nFinally, we ensure that gtmOnSuccess() was called, since otherwise the tag would stall.\nTest 2: Set single cookie to true when in distribution The next case we\u0026rsquo;ll test is that the cookie is set to value 'true' when the randomizer generates a value that is within the range of the single distribution group probability.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 50); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (value !== \u0026#39;true\u0026#39;) { fail(\u0026#39;setCookie not called with \u0026#34;true\u0026#34;\u0026#39;); } }); // Call runCode to run the template\u0026#39;s code. runCode(mockSingleData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasNotCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Check out the mock() calls. The first one is similar to the one in the previous test, except this time we have it return an empty array to signify the cookie has not been set.\nThen, we mock the generateRandom() API to have it return the value 50, just to test the upper bound of the distribution range of 1...50 as set in the mock object within the Setup of the test.\nThe third mock() is interesting! We are mocking setCookie() because a) we don\u0026rsquo;t want it to actually set a cookie, and b) we can use this patch to check what setCookie() was called with. In case setCookie() was not called with the value 'true', we fail the test.\nYou can use the mock() together with fail() to check what APIs have been called with - it\u0026rsquo;s a nice workaround while we wait for wasCalledWith() for the assertApi() API.\nThen, we run the code with the mock data object again.\nThe list of APIs we expect to be called is almost the same as in the previous test, with the exception that now we expect setCookie() to be called, since the userDistributor cookie does not exist in this scenario.\nThis test succeeds if the setCookie() API is called with the expected value of 'true' for the cookie.\nTest 3: Set single cookie to false when not in distribution This test is basically a mirror of the previous one, except we set a different return value for the generateRandom() mock to ensure the cookie is set with the value 'false'. We\u0026rsquo;re using the lower bound of the range (51) just to test the extremes of the algorithm.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 51); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (value !== \u0026#39;false\u0026#39;) { fail(\u0026#39;setCookie not called with \u0026#34;false\u0026#34;\u0026#39;); } }); // Call runCode to run the template\u0026#39;s code. runCode(mockSingleData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasNotCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Other than those two changes, the test code is identical to the previous one.\nTest 4: Set single cookie with correct options This test is a simple check to make sure that the cookie settings (name, value, domain, and expiration) correspond with those set in the template configuration. It\u0026rsquo;s a solid test to write, because often the little configuration mistakes that don\u0026rsquo;t really break the code are the ones that slip through the cracks when creating incremental updates.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 99); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (name !== mockSingleData.cookieName) fail(\u0026#39;setCookie called with incorrect cookie name\u0026#39;); if (value !== \u0026#39;false\u0026#39;) fail(\u0026#39;setCookie not called with \u0026#34;false\u0026#34;\u0026#39;); if (options.domain !== mockSingleData.cookieDomain) fail(\u0026#39;setCookie called with incorrect cookie domain\u0026#39;); if (options[\u0026#39;max-age\u0026#39;] !== mockSingleData.cookieExpires * 24 * 60 * 60) fail(\u0026#39;setCookie called with incorrect max-age\u0026#39;); }); // Call runCode to run the template\u0026#39;s code. runCode(mockSingleData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasNotCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  The magic happens in the setCookie() mock. Basically, we fail the test if the setCookie() API is called with values that do not correspond to those we set in the mock data object.\nTest 5: Set multi cookie to A when equal to 33 When we start testing the multi distribution options, we are basically checking if the randomized number falls into the distributions established in the tag settings.\nWe\u0026rsquo;ll first test the simplest scenario: the randomizer picks the first group the user defined. We use this by setting it to the upper bound of the first group.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 33); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (value !== \u0026#39;A\u0026#39;) { fail(\u0026#39;setCookie not called with \u0026#34;A\u0026#34;\u0026#39;); } }); // Call runCode to run the template\u0026#39;s code. runCode(mockMultiData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  The setup is very similar to the previous single distribution tests, with the difference that the cookie value is now established in the mock data object, and we expect the makeInteger() API to be called for the template test to succeed.\nTest 6: Set multi cookie to B when equal to 34 This test is practically identical to the previous one, with the exception that we set the generateRandom() API to return a value that sits within the lower bound of the second group of the mock object, and we modify the setCookie() API to check for the corresponding value ('B') instead.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 34); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (value !== \u0026#39;B\u0026#39;) { fail(\u0026#39;setCookie not called with \u0026#34;B\u0026#34;\u0026#39;); } }); // Call runCode to run the template\u0026#39;s code. runCode(mockMultiData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Test 7: Do not set multi cookie if not within distributions In case the randomizer does not fall into any of the probability ranges established in the multi distribution settings, the cookie should not be set.\nThe easiest way to check this is by making sure the setCookie() API was not called. You can do this with assertApi('setCookie').wasNotCalled(), but you can also mock the setCookie() API to fail if called. I\u0026rsquo;ve opted to do both just for giggles.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 67); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { fail(\u0026#39;setCookie should not have been called\u0026#39;); }); // Call runCode to run the template\u0026#39;s code. runCode(mockMultiData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasNotCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Here, we generate a random number that is above 66 to make sure the number doesn\u0026rsquo;t belong to any group. Then, we mock setCookie() to fail the test if ever called (regardless of what values it was called with), and we also use assertApi() to pass the test only if the API was not called.\nTest 8: Set multi cookie with correct options This is similar to Test 4, since we are just checking that the cookie is set with options that correspond with those configured in the template settings (or in the mock data object as in this case). The test will fail if there are any differences.\nmock(\u0026#39;getCookieValues\u0026#39;, []); mock(\u0026#39;generateRandom\u0026#39;, 20); mock(\u0026#39;setCookie\u0026#39;, (name, value, options) =\u0026gt; { if (name !== mockMultiData.cookieName) fail(\u0026#39;setCookie called with incorrect cookie name\u0026#39;); if (value !== mockMultiData.multiPercent[0].value) fail(\u0026#39;setCookie called with incorrect cookie value\u0026#39;); if (options.domain !== mockMultiData.cookieDomain) fail(\u0026#39;setCookie called with incorrect cookie domain\u0026#39;); if (options[\u0026#39;max-age\u0026#39;] !== mockMultiData.cookieExpires * 24 * 60 * 60) fail(\u0026#39;setCookie called with incorrect max-age\u0026#39;); }); // Call runCode to run the template\u0026#39;s code. runCode(mockMultiData); assertApi(\u0026#39;generateRandom\u0026#39;).wasCalled(); assertApi(\u0026#39;getCookieValues\u0026#39;).wasCalled(); assertApi(\u0026#39;makeInteger\u0026#39;).wasCalled(); assertApi(\u0026#39;setCookie\u0026#39;).wasCalled(); // Verify that the tag finished successfully. assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Summary In this article, I showed you two walkthroughs for how to write tests for your variable and tag templates.\nA good way to get started is to figure out all the branches of your code, and then write tests that check for all the permutations.\nTo reduce the number of tests you need, make sure you utilize the field configurations (particularly the validation rules) efficiently. By blocking invalid input before the tag or variable can even be saved you\u0026rsquo;re reducing the number of validation checks you need to write (and test for) in the code.\nHopefully we\u0026rsquo;ll get some test coverage details into the UI, so that you can plan your tests to cover all possible branches and features of the template code.\nFinally, remember to update the tests when you update the template itself. Writing and updating tests should become part and parcel of your quality assurance process when writing templates. You owe it to the users who end up installing your template into their Google Tag Manager containers!\n"
},
{
	"uri": "https://www.simoahava.com/tags/testing/",
	"title": "testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/custom-templates/google-tag-manager-monitor/",
	"title": "Google Tag Manager Monitor - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Google Tag Manager Monitor custom tag template is a tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Gallery entry   GitHub repo      Description Use this template to set up a monitoring system for Google Tag Manager. With the monitor, you can collect data about tags that fired (or did not fire) with any given event. This information can be used to proactively fix issues with your tags.\nInstructions The tag itself is simple to setup. You add the collector endpoint and then choose whether to send the hits in batches or not. Sending them in batches reduces the number of HTTP requests made to the collector endpoint.\nThe biggest steps are setting up the collector and configuring all your tags to pass metadata to the monitor system. Please refer to the blog post to learn how to set this up.\nRelease notes    Date Changeset     15 November 2019 Fix maxTags to parse as a number instead of string.   2 October 2019 Update logo, brand name.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/writing-tests-for-custom-templates-google-tag-manager/",
	"title": "Writing Tests For Custom Templates In Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "tag templates", "variable templates", "testing"],
	"description": "A guide for writing unit tests for Google Tag Manager&#39;s custom templates, using the Tests interface of the template editor.",
	"content": "Google Tag Manager introduced the capability to add tests to your Custom Templates. Tests, in this context, refer specifically to unit tests that you write in order to make sure your template code works in a predictable way. Unit tests are also used to drive development, ensuring that you have added contingencies for all the different scenarios that the template, when coupled with user input, might introduce.\n  In this guide, I\u0026rsquo;ll introduce how the Tests feature works. Some new APIs are introduced, as well as a grammar of sorts that you might be used to if you\u0026rsquo;ve already worked with tests in a software development context.\nUnit tests in Custom Templates Unit tests comprise code that is executed against the actual functions and variables of your template code. The purpose of the test is to validate the template code, so that it returns an expected output for any given, tested input.\nThe purpose of unit tests is to make your code more resilient to changes. By writing tests, you have a tool with which to verify that the changes you make in one place do not break something that happens in another place.\nWith unit tests, you are testing the most atomic components of your code: the functions and variables. When you write tests, you aim for high code coverage. Coverage means the number of lines of code that are evaluated in some test.\nSimple example of coverage For example, say that you have a simple variable template like this:\nconst getType = require(\u0026#39;getType\u0026#39;); const input = data.input; if (getType(input) === \u0026#39;array\u0026#39;) { return; } return input;  This template takes the user input and returns it. If the input is of type array, then undefined is returned instead.\nWhen writing tests for this variable, there are basically two scenarios you would need to test.\n1. Does the variable return undefined for array input // Call runCode to run the template\u0026#39;s code. const mockData = { input: \u0026#39;notAnArray\u0026#39; }; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isUndefined();  Here you create a mock object that provides the input. This object has key-value pairs where each key corresponds to a property in the data object, reflecting the input the user would have added to the template fields.\nThis object is passed to the runCode function, which is a template API that runs the template code against the data object.\nFinally, the result of runCode is passed to assertThat, which is an assertion method where you check whether the result of the code resolves to an expected value.\n2. Does the variable return the input const mockInput = \u0026#39;something\u0026#39;; const mockData = { input: mockInput }; const expected = mockInput; const variableResult = runCode(mockData); // Verify that the variable returns a result. assertThat(variableResult).isEqualTo(expected);  As you can see, I\u0026rsquo;m using constants and variables where possible to avoid hard-coding any values. This prevents simple typos from creating false positives or negatives in your tests.\nWith these two tests, you actually achieve 100% test coverage for the variable template, because the code tests all the possible variations of the variable.\nWhat about missing input? When writing tests, you\u0026rsquo;re faced with interesting design decisions as well.\nTake the example above. We\u0026rsquo;re not specifically testing a scenario where the user didn\u0026rsquo;t add any input to the field. Why? Because we\u0026rsquo;ve decided to handle that in the user interface, by adding a validation rule that prevents the template code from running if there is no input in the field.\nSo when writing tests, you need to know how the template works, and you need to be able to comprehensively approach the different scenarios that imaginative users can exploit when working with your template code.\nThe anatomy of a test You\u0026rsquo;ve learned above that unit tests have some specific components. They are, in order:\nMock of the data object\nMocks of the template APIs (mock())\nThe test runner (runCode())\n(Optional fail())\nAssertions (assertApi(), assertThat())\n1. Mocking To test a function, you use mock data. This is because with automated tests there is no prompt for user input. The test needs to run independently of the template itself. With unit tests, you are only testing the code itself, not the user experience.\nIn Google Tag Manager\u0026rsquo;s custom templates, mock data comes in two guises: the contents of the data object, and the result of running template APIs.\nMocking the data object The first is simple to do. You build an object, where each key corresponds to a property of the data object that your code processes.\nFor example, if your template has a text input field named name and then a simple table field named optionalParameters, you could build a mock object like this:\nconst mockData = { name: \u0026#39;Simo Ahava\u0026#39;, optionalParameters: [{ parameterName: \u0026#39;country\u0026#39;, parameterValue: \u0026#39;Finland\u0026#39; },{ parameterName: \u0026#39;hair_color\u0026#39;, parameterValue: \u0026#39;n/a\u0026#39; }] };  This mock data object would correspond with field input like this:\n  You pass this mock data object to the runCode method (more on this below) so that the template code is run with input from the object.\nMocking template APIs When you run tests for your template, you might not want the template to actually perform the API calls it is configured to do.\nCalling sendPixel over and over again with different inputs might not make a lot of sense.\nThis is where API mocking plays a vital row. In the Tests interface, you can replace API functionality with a mock function. This way you can run the test as if the template had called the API, and process the returned value as if the API had returned it.\nFor example, let\u0026rsquo;s say we have a tag template that injects a script onto the page, where the domain name is input by the user into a template field. This is what the template code would look like:\nconst sendPixel = require(\u0026#39;sendPixel\u0026#39;); const domainName = data.domainName; sendPixel(domainName, data.gtmOnSuccess, data.gtmOnFailure);  To test this, you could write a test that simply checks if gtmOnSuccess was called for a valid hostname and gtmOnFailure for an invalid one:\nconst mockData = { domainName: \u0026#39;https://invalid.com\u0026#39; }; runCode(mockData); assertApi(\u0026#39;gtmOnFailure\u0026#39;).wasCalled();  However, this test actually tries to send the pixel request to the domain name, meaning it\u0026rsquo;s difficult to test positive and negative results without polluting the endpoint.\nInstead, you can mock the API like this:\nconst mockData = { domainName: \u0026#39;https://invalid.com\u0026#39; }; mock(\u0026#39;sendPixel\u0026#39;, (url, onSuccess, onFailure) =\u0026gt; { onFailure(); }); runCode(mockData); assertApi(\u0026#39;gtmOnFailure\u0026#39;).wasCalled();  With mock, you instruct the test to use the function stub you provide, meaning whenever the sendPixel API is invoked, it always runs the onFailure() method regardless of input (at least, in this current test).\nSo when using APIs that invoke external endpoints, it might be a good idea to mock them.\nMocks are reset with every test, so you don\u0026rsquo;t have to worry about your onFailure() leaking into other tests, too.\n2. Running the code Once you have your mocks ready, you can invoke the runCode() method.\nThis method runs the template code, mocking the APIs you have chosen to replace, and using the data object that you pass to it as an argument.\nIf you do not pass any object to it as an argument, or if you pass an empty object, the template will be run as if the user had not added any input to any field.\nWith tag templates, you just need to execute runCode(), because your assertions will be made against the APIs that have been called. Tag templates do not return anything, so there\u0026rsquo;s nothing to assess from the result of runCode().\nWith variable templates, the template should actually return something, and thus your assertions should also be done against the result of runCode().\nconst mockData = {input: \u0026#39;test\u0026#39;}; // With tag templates runCode(mockData); assertApi(\u0026#39;someApi\u0026#39;).wasCalled(); // With variable templates const result = runCode(mockData); assertThat(result).isEqualTo(\u0026#39;test\u0026#39;);  Once the test has run, you are ready to assert the results.\n3. Failing the test Custom Templates offer an API you can use to immediately fail a test.\nNote that the typical use case for fail() is to evaluate the test itself rather than the code that is being tested.\nSome like to use a fail mechanism to indicate tests that have not yet been written, others like to use it to expose paths that should never be encountered by tests.\nTypically, you wouldn\u0026rsquo;t use fail() in production-ready code, because there would not be incomplete tests and assertion logic would handle all possible permutations of the template code.\nTo use fail() is dead simple - just add the command to a branch of the code, and set the failure message as the argument.\nconst mockData = {input: \u0026#39;test\u0026#39;}; const result = runCode(mockData); if (result === undefined) { fail(\u0026#39;Result should never be undefined.\u0026#39;); } assertThat(result).isEqualTo(\u0026#39;test\u0026#39;);  You can use fail() to create custom assertions. For example, to check if the object returned by the code contains some property, you could do this:\nconst mockData = {input: \u0026#39;test\u0026#39;}; const result = runCode(mockData); if (result.hasOwnProperty(\u0026#39;someProperty\u0026#39;)) { assertThat(true); } else { fail(\u0026#39;Variable did not have \u0026#34;someProperty\u0026#34;.\u0026#39;); }  It\u0026rsquo;s not pretty, but it does its job while we wait for new assertion APIs to be added.\n4. Assertions With assertions, you are making statements about the results of the test. The best assertion libraries are a pleasure to use, because assertions are written with a grammar and syntax reminiscent of actual language.\nFor example, to assert that the variable template code returns a specific value, you add something like:\nassertThat(runCodeResult).isEqualTo(\u0026#39;some value\u0026#39;);  The test is a success if the assertion resolves to true. In other words, if the runCodeResult had exactly the value 'some value', the test would pass.\nWith APIs, you are asserting whether the API was called or not.\nassertApi(\u0026#39;sendPixel\u0026#39;).wasCalled();  This test would be a success if the sendPIxel API was invoked during the execution of the test code.\n NOTE! To test whether an API was called with specific parameters, you should mock that API instead.\n You can add more than one assertion to a test case. All the assertions must pass for the test to be a success. I do recommend trying to keep the number of assertions per test to a minimum, though, as having granular test cases lets you more easily identify the link between the failed test and the broken code that caused it to fail.\nUsing assertThat The assertThat API allows you to make assertions about the result of the code execution. The assertions follow the syntax used by e.g. Google\u0026rsquo;s Truth and the AssertJ library.\nThe assertThat API is most useful when testing variable templates, because the assertions would be run against the result (the returned value) of the variable template itself.\nThe syntax is this:\nassertThat(someValueToTest).assertion(someExpectedValue);\nThe assertThat() method takes the value to test as an argument, and it returns an object with all the different assertion APIs listed below.\nSome of the APIs (e.g. isEqualTo()) take an argument, where the asserted value is tested against this argument.\nMany APIs don\u0026rsquo;t take any arguments at all - they are used for quick, simple assertions such as whether the asserted value was defined or not.\nHere are the assertions supported by the API.\n   Assertion Description     isEqualTo(someValue) Checks if the asserted value is equal to someValue. Equality is not strict - you can check if [1, 2, 3] equals [1, 2, 3] and the test will pass. Similarly, you can check complex objects and the assertion will succeed if they have the exact same keys and values.   isNotEqualTo(someValue) The assertion is a success if the asserted value is not equal to someValue. Same rules about strictness apply here.   isStrictlyEqualTo(someValue) Similar to isEqualTo, except this time the asserted value must have exactly the same value as the target value. Thus objects and arrays would not pass the test, as they would point to different references.   isStrictlyNotEqualTo(someValue) The assertion is a success if the asserted value is not the exact equal of someValue.   isDefined() Assertion is a success if the asserted value is other than undefined.   isUndefined() Assertion is a success if the asserted value is undefined.   isNull() Assertion is a success if the asserted value is null.   isNotNull() Assertion is a success if the asserted value is anything other than null.   isTrue() Assertion is a success if the asserted value is true.   isFalse() Assertion is a success if the asserted value is false.   isTruthy() Assertion is a success if the asserted value is not falsy (see the next assertion for more details).   isFalsy() Assertion is a success if the asserted value is falsy. Falsy values are: undefined, null, false, NaN, 0, and ''.    This is the current list of available assertions. I\u0026rsquo;m sure more will be added with time.\nUsing assertApi The assertApi() function can be used to check if the template code has made calls to specific template APIs.\nThis is a typical flow test - you are checking that certain things happen in order.\nFor example, take the following template code:\nconst log = require(\u0026#39;logToConsole\u0026#39;); const makeTableMap = require(\u0026#39;makeTableMap\u0026#39;); const sendPixel = require(\u0026#39;sendPixel\u0026#39;); const injectScript = require(\u0026#39;injectScript\u0026#39;); // Log input to console log(data.input); // Shape into object const obj = makeTableMap(data.input, \u0026#39;propertyName\u0026#39;, \u0026#39;propertyValue\u0026#39;); if (obj.someValue) { sendPixel(\u0026#39;https://endPoint.com/?value=\u0026#39; + obj.someValue, data.gtmOnSuccess, data.gtmOnFailure); } else { injectScript(\u0026#39;https://endPoint.com/script.js\u0026#39;, data.gtmOnSuccess, data.gtmOnFailure); }  To test the flow of this template, you could use a test like this:\nconst mockData = {someValue: true}; runCode(mockData); assertApi(\u0026#39;logToConsole\u0026#39;).wasCalled(); assertApi(\u0026#39;makeTableMap\u0026#39;).wasCalled(); assertApi(\u0026#39;sendPixel\u0026#39;).wasCalled(); assertApi(\u0026#39;injectScript\u0026#39;).wasNotCalled(); assertApi(\u0026#39;gtmOnSuccess\u0026#39;).wasCalled();  Pay attention to the last line - with tag templates you should always have a test that checks whether the gtmOnSuccess API was called. You can also test for gtmOnFailure in some circumstances (specifically, when the template executes data.gtmOnFailure()).\nassertApi(apiName) returns an object that has two assertions:\n   Assertion Description     wasCalled() Success if the API was called during test execution.   wasNotCalled() Success if the API was NOT called during test execution.    This is very simple, by design. You can only check if the API was called or not.\nIf you want to know whether it was called with some specific value, you need to mock the APIs instead.\nThe test Setup When opening the Tests interface, you\u0026rsquo;ll see the Setup box. You can use this to establish conditions that apply to all tests. It\u0026rsquo;s a great place to define your mock objects for example, because then you don\u0026rsquo;t have to write them over and over again in each test.\n  I recommend making liberal use of Setup - it can make it much easier to add more tests as your code goes through iterations.\nRunning tests To run the tests, simply click the Run tests button at the top of the Tests view, or hit the play button next to any individual test.\n  The console will log any test results, so play close attention to it.\n  Summary So. Why should you test? Well, let\u0026rsquo;s start with this cryptic tweet from Google Tag Manager\u0026rsquo;s tech lead, Brian:\nI can assure you...there will be incentives for having tests :)\n\u0026mdash; Brian Kuhn (@briankuhn) November 7, 2019  I imagine at some point tests will become mandatory for templates added to the gallery.\nAnother point is that tests bring you, the developer, comfort, since you can use them to validate that you don\u0026rsquo;t generate broken code.\nUnit tests are problematic - the tests rely on mock data rather than actual use cases (those are explored with integration and functional tests). Their only purpose is to test the code you write, but the tests are done using \u0026hellip; code you write.\nHowever, think of your template\u0026rsquo;s users. When they download updates to your template from the gallery, they trust that the code works as it used to. Once your test gets more and more complex, it will be more and more difficult to manage the branching code.\nAt that point, unit tests will be worth every second you spend with them.\nDo note that tests alone don\u0026rsquo;t make your code work. You still need a good template. And writing tests just for coverage\u0026rsquo;s sake doesn\u0026rsquo;t make sense - you still need the tests to reflect actual use cases.\nI\u0026rsquo;m looking forward to how the new Tests feature evolves in Google Tag Manager - it\u0026rsquo;s a great leap forward for avoiding surprises and breaking mistakes in your template code!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/custom-parameter-reporting-google-analytics-app-web/",
	"title": "Custom Parameter Reporting In Google Analytics: App + Web",
	"tags": ["app+web", "google analytics", "parameter reporting"],
	"description": "How custom parameter reporting works in Google Analytics: App + Web, and how to enable it event by event, parameter by parameter.",
	"content": "In Google Analytics: App + Web, you collect events. One event differs from another event by the name it uses. An event with the name page_view is different from, say, an event with the name file_download.\nThis is all run-of-the-mill stuff. You know this.\nHowever, the fundamental change that App + Web introduces, when compared to Universal Analytics, is how event parameters are collected and processed.\n  This gets more complicated than it should be. The complication arises from the fact that there are several components to how App + Web handles event parameters, including:\n Data collection time - limitations on the number of parameters that can be collected with the hit. Data processing time - events are processed, and the parameters and their values are shown in the real-time stream views. A maximum of 25 custom parameters included in the hit are made available to reports and the BigQuery Export (assuming you\u0026rsquo;ve enabled it for your App + Web property). Data reporting time - to be able to actually segment and report on these parameters, you need to enable them as custom parameters in your reports. This is the main focus of the article you are reading.  Data collection - parameter limit Google Analytics: App + Web has the same limitation as Google Analytics for Firebase (makes sense, since they share the same technical infrastructure) - you can only send up to 25 custom event parameters per hit. This is in addition to the automatically collected parameters (such as page_location for the page_view event).\nFor example, here I have a custom event with 26 custom parameters:\n  When that tag is fired on the web page, all 26 custom parameters are sent with the hit.\nIn the App + Web reporting interface, only 25 custom parameters are made available. What makes it even more unfortunate is that there doesn\u0026rsquo;t seem to be a discernible pattern for which parameter is dropped. In the example below, parameter param21 is the one that was dropped.\n  The next time I dispatched the event, it was param04 that was dropped. It seems to me that the dropping of events is completely random, which is unfortunate. I would have expected an alphabetic sort and truncation from either head or tail of the list.\nIn any case, you need to be mindful of this. If you want to process the event parameters in the App + Web reporting user interface, you need to make sure to send a maximum of 25 custom event parameters per hit.\nData processing - real time When you send hits with custom parameters, you\u0026rsquo;ll be able to see the parameters in real time reports.\nThe DebugView is a great place to debug the custom parameters. If you\u0026rsquo;re using Google Tag Manager, entering GTM\u0026rsquo;s Preview mode will automatically flag App + Web hits as originating from a debug device.\n  Find your user agent in the Debug Devices list, and then click the event you want to analyze. The parameters will be displayed in one of the DebugView widgets.\nYou can also see the parameters in the Realtime report (StreamView in Firebase lingo).\n  Choose Events as the filter, then click the Top Events widget, find your custom event, and expand its parameters.\nYou can also find the parameters in the All Events report, assuming the events have been collected long enough ago to actually appear in the report. After clicking the event name to enter its dashboard, you can see a Realtime widget where you can choose to filter by parameter name.\n  As noted before, these are all subject to the 25 parameter limit in data collection. All of the reports in the Google Analytics: App + Web reporting user interface will show you the truncated list.\nData processing - BigQuery export Once again, I profess my love for Google BigQuery. Enabling the BigQuery export for your Google Analytics: App + Web data is the smartest thing you can do today, so don\u0026rsquo;t dally!\nThe BigQuery export gives you the raw data from your App + Web data collection.\nIt is subject to the same 25 custom parameter limitation per hit as the reporting interface, but you do not have to worry about enabling custom parameters for reporting in BigQuery - all the parameters and their values per hit are collected to BQ.\n  Data reporting - enabling custom parameters And now we get to the meat of this article.\nGoogle Analytics: App + Web gives you 50 text parameters and 50 number parameters that you can enable as custom parameters in your reports. That\u0026rsquo;s per event and per project.\nThis means that if you want to include your custom parameters in your reports as their own widgets, and if you want to utilize them in the Analysis reports (such as Exploration), you need to first enable these parameters for reporting.\nEach time you enable a parameter for an event, you exhaust the quota available for your project. So if you want to use page_location in your custom reports, you need to enable it for the page_view event as a text parameter, and now you have 49 text parameters remaining to enable in your project.\n  The kicker is that if you enable custom_parameter_01 for an event named my_custom_event, it will exhaust the quota by 1. Then, if you want to use custom_parameter_01 with some other event, e.g. my_other_event, you need to enable it for THAT, too, and it will again exhaust the quota by 1.\nThese quotas are pretty strict, and I hope they are lifted. If they aren\u0026rsquo;t, at least you\u0026rsquo;ll have the BigQuery export to fall back on.\nAnyway, to enable custom parameters, you need to go to Events -\u0026gt; All events, and then click the little action menu at the end of the row with the event name.\nNote! This means that you need to first collect the event before you can enable parameters for it. Hopefully it will be possible at some point to enable parameters for events that are yet to be collected.\n  From that menu, choose Edit parameter reporting.\n You can always click the Parameter reporting tab on top of the All events report to see a list of the events that have custom parameters enabled, and you can click the event name to edit the parameters.\n     Once you see the parameter selector, you can search for the parameters name in the list. The list will show all parameters that have been collected with the given event in the date range selected.\nIf you find the parameter, select it and click ADD.\n  You can also add a parameter that does not appear in the list. Just type its name in the search field and click Add.\n  Once you\u0026rsquo;ve added the parameter, you can set its type, which impacts what you can actually do with the parameter in the reports.\n  Once you\u0026rsquo;re happy, click Save to save the changes.\nRemember, there is a quota - make your decisions carefully!\nArchiving a parameter When you edit parameters, you can also choose to archive a parameter to free up quota.\n Note! If you DO archive a parameter, it is no longer available in the reports. So the change is retroactive - the data will not be recoverable.\n   The BigQuery export is, again, your best friend, because archiving a parameter will not delete it from the BigQuery tables you have already collected.\nSummary I hope this guide has managed to shed light on how event parameters work in App + Web. The focus was specifically on custom parameters, as they seem to be the biggest source of confusion.\nIt\u0026rsquo;s important to rid yourself of the analogy with Custom Dimensions and Custom Metrics in Universal Analytics. Custom reporting in App + Web has fundamental differences to ye olde Google Analytics, particularly in how the same parameter name across different events is treated as a different entity each time, and how archiving a parameter removes it from your collected data, too.\nSimilarly to Universal Analytics, custom parameters are not retroactive, so they will start collecting data in reports only after being enabled per event.\nI\u0026rsquo;m not sure what to think of custom parameters yet. In the reporting interface they are oddly handicapped with the quotas and the hard-and-fast removal process, but in the BigQuery export they are really valuable.\nWe\u0026rsquo;ll see how custom parameters evolve as the beta matures; I think they will play an important part in deciding whether to migrate from Universal Analytics to App + Web.\n"
},
{
	"uri": "https://www.simoahava.com/tags/parameter-reporting/",
	"title": "parameter reporting",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/calculate-readability-scores-for-content/",
	"title": "Calculate Readability Scores For Content",
	"tags": ["google tag manager", "google analytics", "content"],
	"description": "Calculate readability scores (e.g. Flesch-Kincaid, SMOG) for your content, and send the values as Custom Dimensions to Google Analytics.",
	"content": "There are lots of different readability formulas out there, which seek to provide an index on how readable any given excerpt of text is. Typically, these formulas output a grade-level score, which indicates, roughly, the level of education required to read the text excerpt with ease.\nAny \u0026ldquo;quality index\u0026rdquo; that seeks to reduce the complexity of something as multi-faceted as reading should be subject to scrutiny. This is true for Bounce Rate, this is true for Time On Page, and this is true for a readability score.\n  However, I do think it\u0026rsquo;s interesting metadata to collect for your content. Who knows, perhaps you\u0026rsquo;ll find correlations between the difficulty of reading any piece of text and the amount of engagement that the content engenders.\nIn this article, I\u0026rsquo;ll show you how to create a Custom JavaScript variable in Google Tag Manager that creates a utility function. This function can be called with any piece of text (e.g. the text content of your main content DOM element), and it will return an object with a variety of scores based on different readability formulas, as well as a median grade level calculated from these scores.\nYou can use this information to annotate your Page View hits with Custom Dimensions (with a caveat regarding its triggers) when sending the data to Google Analytics.\nYou can also send the data to a Google Analytics: App + Web property ready for that juicy BigQuery data export.\nI\u0026rsquo;ll show you how to do both in this guide.\n Thanks to Munaz Anjum for inspiring me to write this article!\n The Custom JavaScript variable In this chapter, we\u0026rsquo;ll go over the Custom JavaScript variable you\u0026rsquo;ll need to create to make this work.\n\u0026ldquo;Why a Custom JavaScript variable? Why not a custom template?\u0026rdquo; I hear you ask. I know! This would be perfect for a custom template. Alas, templates do not support the full extent of regular expressions yet, and they are absolutely vital for the algorithms required by the formulae in the variable.\nWhat you\u0026rsquo;ll get The Custom JavaScript variable returns a function, which you can then invoke with a string in another Custom JavaScript variable or a Custom HTML tag.\nWhen you call the function with a string, e.g.:\nvar readabilityFunction = {{readabilityFunction}}; var text = \u0026#39;At the heart of this paper lies the elusive concept of \\ variation: how speakers can simultaneously share a common knowledge \\ of a unified language system and develop it in idiosyncratic and unique \\ ways. Studies of variation do not come without their fair share of \\ controversy. Since they necessarily incorporate the attention to detail \\ that a descriptivist approach demands together with a level of abstraction \\ that is likewise required by all studies of linguistic phenomena, the \\ results are often as debatable as they are revelatory. It is thus the \\ researcher’s job to describe the phenomenon as lucidly as possible, \\ without drawing conclusions that extend beyond the scope of the data at hand.\u0026#39; console.log(readabilityFunction(text)); // Outputs // { // automatedReadabilityIndex: 15.48, // colemanLiauIndex: 13.12, // fleschKincaidGrade: 16.41, // fleschReadingEase: 28.17, // linsearWriteFormula: 18.25, // medianGrade: 17, // readingTime: 27.34, // rix: 9.5, // smogIndex: 17.41 // }  This particular excerpt (from my master\u0026rsquo;s thesis) has a median readability grade of 17 which means, loosely translated, that it takes around 17 years of education to read text like this with ease. The reading time is around 27 seconds.\nThe object for the article you are reading looks like this (ouch!):\n{ automatedReadabilityIndex: 15.48, colemanLiauIndex: 3.77, fleschKincaidGrade: 16.96, fleschReadingEase: 39.67, linsearWriteFormula: 13.2, medianGrade: 15, readingTime: 1524.22, rix: 7.11, smogIndex: 15.85 }  I\u0026rsquo;ll share what these different numbers and names mean in a bit.\nVariable contents The code is a JavaScript rewrite of the textstat.py Python library. I did not include any readability grades that rely on an English wordlist (so any formulae that require a definition of what is a \u0026ldquo;common\u0026rdquo; or \u0026ldquo;easy\u0026rdquo; word in English is ignored).\nCreate a new Custom JavaScript variable, give it a name (e.g. readabilityFunction), and copy-paste the following JavaScript in the body.\nfunction() { return function(text) { /* To speed the script up, you can set a sampling rate in words. For example, if you set * sampleLimit to 1000, only the first 1000 words will be parsed from the input text. * Set to 0 to never sample. */ var sampleLimit = 1000; // Manual rewrite of the textstat Python library (https://github.com/shivam5992/textstat/)  /* * Regular expression to identify a sentence. No, it\u0026#39;s not perfect. * Fails e.g. with abbreviations and similar constructs mid-sentence. */ var sentenceRegex = new RegExp(\u0026#39;[.?!]\\\\s[^a-z]\u0026#39;, \u0026#39;g\u0026#39;); /* * Regular expression to identify a syllable. No, it\u0026#39;s not perfect either. * It\u0026#39;s based on English, so other languages with different vowel / consonant distributions * and syllable definitions need a rewrite. * Inspired by https://bit.ly/2VK9dz1 */ var syllableRegex = new RegExp(\u0026#39;[aiouy]+e*|e(?!d$|ly).|[td]ed|le$\u0026#39;, \u0026#39;g\u0026#39;); // Baseline for FRE - English only  var freBase = { base: 206.835, sentenceLength: 1.015, syllablesPerWord: 84.6, syllableThreshold: 3 }; var cache = {}; var punctuation = [\u0026#39;!\u0026#39;,\u0026#39;\u0026#34;\u0026#39;,\u0026#39;#\u0026#39;,\u0026#39;$\u0026#39;,\u0026#39;%\u0026#39;,\u0026#39;\u0026amp;\u0026#39;,\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;(\u0026#39;,\u0026#39;)\u0026#39;,\u0026#39;*\u0026#39;,\u0026#39;+\u0026#39;,\u0026#39;,\u0026#39;,\u0026#39;-\u0026#39;,\u0026#39;.\u0026#39;,\u0026#39;/\u0026#39;,\u0026#39;:\u0026#39;,\u0026#39;;\u0026#39;,\u0026#39;\u0026lt;\u0026#39;,\u0026#39;=\u0026#39;,\u0026#39;\u0026gt;\u0026#39;,\u0026#39;?\u0026#39;,\u0026#39;@\u0026#39;,\u0026#39;[\u0026#39;,\u0026#39;]\u0026#39;,\u0026#39;^\u0026#39;,\u0026#39;_\u0026#39;,\u0026#39;`\u0026#39;,\u0026#39;{\u0026#39;,\u0026#39;|\u0026#39;,\u0026#39;}\u0026#39;,\u0026#39;~\u0026#39;]; var legacyRound = function(number, precision) { var k = Math.pow(10, (precision || 0)); return Math.floor((number * k) + 0.5 * Math.sign(number)) / k; }; var charCount = function(text) { if (cache.charCount) return cache.charCount; if (sampleLimit \u0026gt; 0) text = text.split(\u0026#39; \u0026#39;).slice(0, sampleLimit).join(\u0026#39; \u0026#39;); text = text.replace(/\\s/g, \u0026#39;\u0026#39;); return cache.charCount = text.length; }; var removePunctuation = function(text) { return text.split(\u0026#39;\u0026#39;).filter(function(c) { return punctuation.indexOf(c) === -1; }).join(\u0026#39;\u0026#39;); }; var letterCount = function(text) { if (sampleLimit \u0026gt; 0) text = text.split(\u0026#39; \u0026#39;).slice(0, sampleLimit).join(\u0026#39; \u0026#39;); text = text.replace(/\\s/g, \u0026#39;\u0026#39;); return removePunctuation(text).length; }; var lexiconCount = function(text, useCache, ignoreSample) { if (useCache \u0026amp;\u0026amp; cache.lexiconCount) return cache.lexiconCount; if (ignoreSample !== true \u0026amp;\u0026amp; sampleLimit \u0026gt; 0) text = text.split(\u0026#39; \u0026#39;).slice(0, sampleLimit).join(\u0026#39; \u0026#39;); text = removePunctuation(text); var lexicon = text.split(\u0026#39; \u0026#39;).length; return useCache ? cache.lexiconCount = lexicon : lexicon; }; var getWords = function(text, useCache) { if (useCache \u0026amp;\u0026amp; cache.getWords) return cache.getWords; if (sampleLimit \u0026gt; 0) text = text.split(\u0026#39; \u0026#39;).slice(0, sampleLimit).join(\u0026#39; \u0026#39;); text = text.toLowerCase(); text = removePunctuation(text); var words = text.split(\u0026#39; \u0026#39;); return useCache ? cache.getWords = words : words; } var syllableCount = function(text, useCache) { if (useCache \u0026amp;\u0026amp; cache.syllableCount) return cache.syllableCount; var count = 0; var syllables = getWords(text, useCache).reduce(function(a, c) { return a + (c.match(syllableRegex) || [1]).length; }, 0); return useCache ? cache.syllableCount = syllables : syllables; }; var polySyllableCount = function(text, useCache) { var count = 0; getWords(text, useCache).forEach(function(word) { var syllables = syllableCount(word); if (syllables \u0026gt;= 3) { count += 1; } }); return count; }; var sentenceCount = function(text, useCache) { if (useCache \u0026amp;\u0026amp; cache.sentenceCount) return cache.sentenceCount; if (sampleLimit \u0026gt; 0) text = text.split(\u0026#39; \u0026#39;).slice(0, sampleLimit).join(\u0026#39; \u0026#39;); var ignoreCount = 0; var sentences = text.split(sentenceRegex); sentences.forEach(function(s) { if (lexiconCount(s, true, false) \u0026lt;= 2) { ignoreCount += 1; } }); var count = Math.max(1, sentences.length - ignoreCount); return useCache ? cache.sentenceCount = count : count; }; var avgSentenceLength = function(text) { var avg = lexiconCount(text, true) / sentenceCount(text, true); return legacyRound(avg, 2); }; var avgSyllablesPerWord = function(text) { var avg = syllableCount(text, true) / lexiconCount(text, true); return legacyRound(avg, 2); }; var avgCharactersPerWord = function(text) { var avg = charCount(text) / lexiconCount(text, true); return legacyRound(avg, 2); }; var avgLettersPerWord = function(text) { var avg = letterCount(text, true) / lexiconCount(text, true); return legacyRound(avg, 2); }; var avgSentencesPerWord = function(text) { var avg = sentenceCount(text, true) / lexiconCount(text, true); return legacyRound(avg, 2); }; var fleschReadingEase = function(text) { var sentenceLength = avgSentenceLength(text); var syllablesPerWord = avgSyllablesPerWord(text); return legacyRound( freBase.base - freBase.sentenceLength * sentenceLength - freBase.syllablesPerWord * syllablesPerWord, 2 ); }; var fleschKincaidGrade = function(text) { var sentenceLength = avgSentenceLength(text); var syllablesPerWord = avgSyllablesPerWord(text); return legacyRound( 0.39 * sentenceLength + 11.8 * syllablesPerWord - 15.59, 2 ); }; var smogIndex = function(text) { var sentences = sentenceCount(text, true); if (sentences \u0026gt;= 3) { var polySyllables = polySyllableCount(text, true); var smog = 1.043 * (Math.pow(polySyllables * (30 / sentences), 0.5)) + 3.1291; return legacyRound(smog, 2); } return 0.0; }; var colemanLiauIndex = function(text) { var letters = legacyRound(avgLettersPerWord(text) * 100, 2); var sentences = legacyRound(avgSentencesPerWord(text) * 100, 2); var coleman = 0.0588 * letters - 0.296 * sentences - 15.8; return legacyRound(coleman, 2); }; var automatedReadabilityIndex = function(text) { var chars = charCount(text); var words = lexiconCount(text, true); var sentences = sentenceCount(text, true); var a = chars / words; var b = words / sentences; var readability = ( 4.71 * legacyRound(a, 2) + 0.5 * legacyRound(b, 2) - 21.43 ); return legacyRound(readability, 2); }; var linsearWriteFormula = function(text) { var easyWord = 0; var difficultWord = 0; var roughTextFirst100 = text.split(\u0026#39; \u0026#39;).slice(0,100).join(\u0026#39; \u0026#39;); var plainTextListFirst100 = getWords(text, true).slice(0,100); plainTextListFirst100.forEach(function(word) { if (syllableCount(word) \u0026lt; 3) { easyWord += 1; } else { difficultWord += 1; } }); var number = (easyWord + difficultWord * 3) / sentenceCount(roughTextFirst100); if (number \u0026lt;= 20) { number -= 2; } return legacyRound(number / 2, 2); }; var rix = function(text) { var words = getWords(text, true); var longCount = words.filter(function(word) { return word.length \u0026gt; 6; }).length; var sentencesCount = sentenceCount(text, true); return legacyRound(longCount / sentencesCount, 2); }; var readingTime = function(text) { var wordsPerSecond = 4.17; // To get full reading time, ignore cache and sample  return legacyRound(lexiconCount(text, false, true) / wordsPerSecond, 2); }; // Build textStandard  var grade = []; var obj = {}; (function() { // FRE  var fre = obj.fleschReadingEase = fleschReadingEase(text); if (fre \u0026lt; 100 \u0026amp;\u0026amp; fre \u0026gt;= 90) { grade.push(5); } else if (fre \u0026lt; 90 \u0026amp;\u0026amp; fre \u0026gt;= 80) { grade.push(6); } else if (fre \u0026lt; 80 \u0026amp;\u0026amp; fre \u0026gt;= 70) { grade.push(7); } else if (fre \u0026lt; 70 \u0026amp;\u0026amp; fre \u0026gt;= 60) { grade.push(8); grade.push(9); } else if (fre \u0026lt; 60 \u0026amp;\u0026amp; fre \u0026gt;= 50) { grade.push(10); } else if (fre \u0026lt; 50 \u0026amp;\u0026amp; fre \u0026gt;= 40) { grade.push(11); } else if (fre \u0026lt; 40 \u0026amp;\u0026amp; fre \u0026gt;= 30) { grade.push(12); } else { grade.push(13); } // FK  var fk = obj.fleschKincaidGrade = fleschKincaidGrade(text); grade.push(Math.floor(fk)); grade.push(Math.ceil(fk)); // SMOG  var smog = obj.smogIndex = smogIndex(text); grade.push(Math.floor(smog)); grade.push(Math.ceil(smog)); // CL  var cl = obj.colemanLiauIndex = colemanLiauIndex(text); grade.push(Math.floor(cl)); grade.push(Math.ceil(cl)); // ARI  var ari = obj.automatedReadabilityIndex = automatedReadabilityIndex(text); grade.push(Math.floor(ari)); grade.push(Math.ceil(ari)); // LWF  var lwf = obj.linsearWriteFormula = linsearWriteFormula(text); grade.push(Math.floor(lwf)); grade.push(Math.ceil(lwf)); // RIX  var rixScore = obj.rix = rix(text); if (rixScore \u0026gt;= 7.2) { grade.push(13); } else if (rixScore \u0026lt; 7.2 \u0026amp;\u0026amp; rixScore \u0026gt;= 6.2) { grade.push(12); } else if (rixScore \u0026lt; 6.2 \u0026amp;\u0026amp; rixScore \u0026gt;= 5.3) { grade.push(11); } else if (rixScore \u0026lt; 5.3 \u0026amp;\u0026amp; rixScore \u0026gt;= 4.5) { grade.push(10); } else if (rixScore \u0026lt; 4.5 \u0026amp;\u0026amp; rixScore \u0026gt;= 3.7) { grade.push(9); } else if (rixScore \u0026lt; 3.7 \u0026amp;\u0026amp; rixScore \u0026gt;= 3.0) { grade.push(8); } else if (rixScore \u0026lt; 3.0 \u0026amp;\u0026amp; rixScore \u0026gt;= 2.4) { grade.push(7); } else if (rixScore \u0026lt; 2.4 \u0026amp;\u0026amp; rixScore \u0026gt;= 1.8) { grade.push(6); } else if (rixScore \u0026lt; 1.8 \u0026amp;\u0026amp; rixScore \u0026gt;= 1.3) { grade.push(5); } else if (rixScore \u0026lt; 1.3 \u0026amp;\u0026amp; rixScore \u0026gt;= 0.8) { grade.push(4); } else if (rixScore \u0026lt; 0.8 \u0026amp;\u0026amp; rixScore \u0026gt;= 0.5) { grade.push(3); } else if (rixScore \u0026lt; 0.5 \u0026amp;\u0026amp; rixScore \u0026gt;= 0.2) { grade.push(2); } else { grade.push(1); } // Find median grade  grade = grade.sort(function(a, b) { return a - b; }); var midPoint = Math.floor(grade.length / 2); var medianGrade = legacyRound( grade.length % 2 ? grade[midPoint] : (grade[midPoint-1] + grade[midPoint]) / 2.0 ); obj.medianGrade = medianGrade; })(); obj.readingTime = readingTime(text); return obj; }; }  It\u0026rsquo;s a mouthful, so let\u0026rsquo;s go over each formula together with the code that relates to it.\nSetup The first couple of steps in the variable establish much of what follows.\nvar sampleLimit = 1000;  Running regular expressions can be slow. If you have an article of 5000 words, for example, counting the syllables of every single word can take a looooooong time, and can be hazardous if executed at page load.\nI\u0026rsquo;ve introduced the sampleLimit variable, which basically establishes a maximum sample size to run the readability calculations against. If sampleLimit is set to the default of 1000, the script will sample the first 1000 words of the input. The end result should be in the ball park of a score calculated against the full text, but this of course depends on what type of text is in question.\nIf you want to avoid sampling, set sampleLimit to 0.\nvar sentenceRegex = new RegExp(\u0026#39;[.?!]\\\\s[^a-z]\u0026#39;, \u0026#39;g\u0026#39;); var syllableRegex = new RegExp(\u0026#39;[aiouy]+e*|e(?!d$|ly).|[td]ed|le$\u0026#39;, \u0026#39;g\u0026#39;);  The first regular expression is used to split a chunk of text in sentences. No, it\u0026rsquo;s not perfect.\nBut that\u0026rsquo;s OK - we\u0026rsquo;re looking for abstractions and the more text you have the more the score will converge towards a norm.\nThe second regular expression counts the number of syllables in a text. No, it\u0026rsquo;s not perfect either. It\u0026rsquo;s based on English, and perhaps other Germanic languages, but syllabification can change from language to language so if you want to analyze content in other languages, you need to modify the regex.\nThe rest of the setup steps are listed here.\n   Function / variable signature Description     freBase The baseline numbers for calculating the Flesch Reading-Ease score in the English language.   cache Utility that caches most of the calculations, so that the regexes are run just once for the cached items.   punctuation List of punctuation symbols.   legacyRound() Function to round the input number to the given precision.   charCount() Count the characters in the input.   removePunctuation() Remove all punctuation from the input.   letterCount() Count the letters in the input.   lexiconCount() Count the words in the input.   getWords() Returns the words in the input.   syllableCount() Count the syllables in the input.   polySyllableCount() Count the number of words with three or more syllables in the input.   sentenceCount() Count the number of sentences in the input.   avgSentenceLength() Count the average length of sentences (number of words) in the input.   avgSyllablesPerWord() Count the average number of syllables per word in the input.   avgCharactersPerWord() Count the average number of characters per word in the input.   avgLettersPerWord() Count the average number of letters per word in the input.   avgSentencesPerWord() Count the average number of sentences per word in the input.    These utilities are used to calculate the readability formulae.\n1. Flesch Reading-Ease The Flesch Reading-Ease test is probably one of the better known readability tests. It\u0026rsquo;s used by e.g. Yoast and Grammarly to calculate their readability scores.\nFor the English language, the algorithm is:\n206.835 - 1.015 x (words / sentences) - 84.6 x (syllables / words)\nThis outputs a score between 1 and 100, where the higher the score, the easier to read the text is.\nA score between 70 and 80 would be considered easy to read for a 7th grader in the US.\nIn the Custom JavaScript variable, Flesch Reading-Ease is generated like this:\nvar fleschReadingEase = function(text) { var sentenceLength = avgSentenceLength(text); var syllablesPerWord = avgSyllablesPerWord(text); return legacyRound( freBase.base - freBase.sentenceLength * sentenceLength - freBase.syllablesPerWord * syllablesPerWord, 2 ); };  2. Flesch-Kincaid Grade The Flesch-Kincaid Grade Level score outputs a number that corresponds to the U.S. grade level generally required to understand the text. If the score returns a number higher than 10, it can also be used to calculate the years of education required to understand the text.\nThe algorithm is:\n0.39 x (words / sentences) + 11.8 x (syllables / words) - 15.59\nIn the Custom JavaScript variable, this translates to:\nvar fleschKincaidGrade = function(text) { var sentenceLength = avgSentenceLength(text); var syllablesPerWord = avgSyllablesPerWord(text); return legacyRound( 0.39 * sentenceLength + 11.8 * syllablesPerWord - 15.59, 2 ); };  3. SMOG grade The SMOG grade, short for Simple Measure of Gobbledygook, estimates the years of education required to understand any given text. It is particularly preferred (over Flesch-Kincaid) in the healthcare industry for content design.\nThe algorithm is:\n1.0430 x squareroot(polysyllables x (30 / sentences)) + 3.1291\nIn the variable, this would be:\nvar smogIndex = function(text) { var sentences = sentenceCount(text, true); if (sentences \u0026gt;= 3) { var polySyllables = polySyllableCount(text, true); var smog = 1.043 * Math.pow(polySyllables * (30 / sentences), 0.5) + 3.1291; return legacyRound(smog, 2); } return 0.0; };  Note that the formula ignores content that has three or fewer sentences, as the grade would lose any semblance of accuracy in those cases.\nJust a note on the JavaScript, using Math.pow(x, y) calculates x to the power of y. When you set y to 0.5, you get the square root of x.\n4. Coleman-Liau index The Coleman-Liau index again outputs an approximation of the U.S. grade level required to understand the text. It\u0026rsquo;s better suited for computer programs as it doesn\u0026rsquo;t care about the complexity of syllables but rather relies on character counts.\nThe algorithm is:\n0.0588 x average_letters_per_100_words - 0.296 x average_sentences_per_100_words - 15.8\nIn JavaScript, this would be:\nvar colemanLiauIndex = function(text) { var letters = legacyRound(avgLettersPerWord(text) * 100, 2); var sentences = legacyRound(avgSentencesPerWord(text) * 100, 2); var coleman = 0.0588 * letters - 0.296 * sentences - 15.8; return legacyRound(coleman, 2); };  Now, you can see how abstract this is, again. It calculates the average number of letters in a word from the entire text, and multiplies this by 100 to get the average letters in a 100 word sample. Same thing for sentences. Thus, the number gets more accurate the larger the data sample.\n5. Automated Readability Index The Automated Readability Index returns yet again a U.S. grade level score thought to correspond with how readable the text is.\nThe formula for ARI is:\n4.71 x (characters / words) + 0.5 x (words / sentences) - 21.43\nSimilar to the Coleman-Liau index, ARI relies on characters rather than syllables, so is thus friendlier to machine calculation but can lead to a more diverse spread of scores compared to syllable-based systems.\nIn JavaScript, ARI is calculated like this:\nvar automatedReadabilityIndex = function(text) { var chars = charCount(text); var words = lexiconCount(text, true); var sentences = sentenceCount(text, true); var a = chars / words; var b = words / sentences; var readability = ( 4.71 * legacyRound(a, 2) + 0.5 * legacyRound(b, 2) - 21.43 ); return legacyRound(readability, 2); };  6. Linsear Write The Linsear Write metric uses a sample of 100 words to calculate the U.S. grade level required to understand the text. It also divides the text into \u0026ldquo;easy words\u0026rdquo; (two syllables or less) and \u0026ldquo;difficult words\u0026rdquo; (three syllables or more).\nThe algorithm is:\nr = (count_of_easy_words + 3 x count_of_difficult_words) / sentences\nif r \u0026gt; 20, result = r / 2\nif r \u0026lt;= 20, result = r / 2 - 1\nIn JavaScript, this would be:\nvar linsearWriteFormula = function(text) { var easyWord = 0; var difficultWord = 0; var roughTextFirst100 = text.split(\u0026#39; \u0026#39;).slice(0,100).join(\u0026#39; \u0026#39;); var plainTextListFirst100 = getWords(text, true).slice(0,100); plainTextListFirst100.forEach(function(word) { if (syllableCount(word) \u0026lt; 3) { easyWord += 1; } else { difficultWord += 1; } }); var number = (easyWord + difficultWord * 3) / sentenceCount(roughTextFirst100); if (number \u0026lt;= 20) { number -= 2; } return legacyRound(number / 2, 2); };  Yeah, it\u0026rsquo;s a bit messy. The problem is that the \u0026ldquo;easy words\u0026rdquo; and \u0026ldquo;difficult words\u0026rdquo; need to be calculated from the lexicon without any punctuation, and the sentence count needs to be calculated from a sample with punctuation intact.\nIn any case, the end result is the Linsear Write score for the text input with decent accuracy.\n7. Rix Rix is quite plainly the number of long words divided by the number of sentences in the text. A long word is a word with more than six characters.\nThe resulting score can be converted to U.S. grade-level.\nIn JavaScript, you can calculate Rix like this:\nvar rix = function(text) { var words = getWords(text, true); var longCount = words.filter(function(word) { return word.length \u0026gt; 6; }).length; var sentencesCount = sentenceCount(text, true); return legacyRound(longCount / sentencesCount, 2); };  8. Reading time To calculate reading time of the text, we use the benchmark of 250 words per minute read by the average adult. This translates to 4.17 words per second.\nThe algorithm is simple:\nvar readingTime = function(text) { var wordsPerSecond = 4.17; // To get full reading time, ignore cache and sample  return legacyRound(lexiconCount(text, false, true) / wordsPerSecond, 2); };  This outputs a value in seconds corresponding to the time it would take for the average adult to read the text.\nMedian grade The median grade is calculated by pushing all the grades (both lower and upper bound integers for floating point values), and then taking the median grade.\nThis is not a scientific result in any way. Heck, the whole concept of readability escapes scientific scrutiny (just look at the number of different tests with different values!). If you want to modify it, you could use the average grade as well (take both rounded down and rounded up to get the grade spread). Or you could find what grade is the most common in the whole spread and use that instead.\nRegardless, I\u0026rsquo;ve chosen median as it excludes the outliers nicely and typically the grades would converge enough to have the median reflect the average anyway.\nHow to use the utility variable Once you create and save the Custom JavaScript variable, you can invoke it from other Custom JavaScript variables or Custom HTML tags.\nFor example, let\u0026rsquo;s say you have all the main content of any given page stored in an HTML element that can be accessed with the CSS selector .post-content .main-content-wrap.\nHere\u0026rsquo;s what a handful of different Custom JavaScript variables would look like, each returning a different value from the readabilityFunction output.\nThere are many ways to skin this cat, so feel free to get creative.\nGet the Flesch-Kincaid Score function() { var el = document.querySelector(\u0026#39;.post-content .main-content-wrap\u0026#39;).textContent; return {{readabilityFunction}}(el).fleschKincaidGrade; }  Get the median grade function() { var el = document.querySelector(\u0026#39;.post-content .main-content-wrap\u0026#39;).textContent; return {{readabilityFunction}}(el).medianGrade; }  Get the reading time in minutes and seconds function() { var el = document.querySelector(\u0026#39;.post-content .main-content-wrap\u0026#39;).textContent; var readingTime = {{readabilityFunction}}(el).readingTime; return Math.floor(readingTime / 60) + \u0026#34;m\u0026#34; + Math.round(readingTime % 60) + \u0026#34;s\u0026#34;; }  This would convert a reading time (in seconds) of 129.67 to \u0026quot;2m10s\u0026quot;.\nGoogle Analytics setup Once you have the values you want to access, all you need to do is send them to Google Analytics.\nHere\u0026rsquo;s a sample setup. It sends the median grade to Google Analytics as a Custom Metric, and it collects the reading time as another Custom Metric. To make results easier to scan at a glance, we\u0026rsquo;ll also send the median grade as a Custom Dimension.\nBut before we start, here\u0026rsquo;s a caveat you need to be aware of.\nDOM Ready If you use DOM methods to check the readability of content from the web page, you might not be able to use a Google Analytics tag that fires on the All Pages trigger.\nThis is because a potential race condition can emerge where the All Pages trigger can fire before your DOM content is available for Google Tag Manager.\nIn this case, you are left with two options:\n Send the readability data with an event (or another pageview) where you set the nonInteraction field to true. Delay the Page View hit a bit to fire on a DOM Ready trigger instead.  The main problem with (1) is that you won\u0026rsquo;t be able to parse the data in your main Page View report (which is the logical place for it), and would need to join the data outside Google Analytics instead (or perhaps use a custom report creatively).\nThe main problem with (2) is that you risk missing a fraction of your pageviews because the hit is delayed a bit.\nIf it\u0026rsquo;s a single-page app, you are in luck as you can instruct the developers not to fire the virtual pageview event until the content has been fully rendered on the page.\n1. Custom JavaScript variable for median grade Remember to change the el variable assignment to match whatever content you want to evaluate. In this example, it\u0026rsquo;s an HTML element accessible with the CSS selector .post-content .main-content-wrap.\nfunction() { var el = document.querySelector(\u0026#39;.post-content .main-content-wrap\u0026#39;).textContent; return {{readabilityFunction}}(el).medianGrade; }  2. Custom JavaScript variable for reading time Remember to change the el variable assignment to match whatever content you want to evaluate. In this example, it\u0026rsquo;s an HTML element accessible with the CSS selector .post-content .main-content-wrap.\nfunction() { var el = document.querySelector(\u0026#39;.post-content .main-content-wrap\u0026#39;).textContent; return {{readabilityFunction}}(el).readingTime; }  3. Google Analytics - Admin - Custom Definitions Here\u0026rsquo;s what the Custom Metric for Reading Time looks like in Google Analytics admin:\n  If you want, you can build a Calculated Metric to get the average reading time across pageviews.\n  Here\u0026rsquo;s what the Custom Metric for Median Grade looks like:\n  Here\u0026rsquo;s what the Custom Dimension for Median Grade looks like in Google Analytics admin:\n  4. Google Analytics tag Finally, here\u0026rsquo;s what a sample Page View tag would look like, modified.\n  Just remember the caveat about DOM ready triggering I mentioned above.\n5. How to access the results The Custom Dimension will be available as a secondary dimension or in a custom report.\n  The Custom Metrics (and the Calculated Metric if you opted to create one) will be available in custom reports as well.\nNote that instead of messing around with awkward custom reports, you might want to extract the data out of Google Analytics using e.g. the Google Sheets add-on as this gives you far more flexibility in building your own reports.\nGoogle Analytics: App + Web setup If you want to send the data to the new App + Web then all the power to you! Setting it up is easy - just create a new App + Web event tag for your Page View (you might want to disable the default Page View from the config tag), and add the variables as new event parameters.\nMake sure this tag, too, triggers on DOM Ready.\n  Once you do this, the events should start showing up in the App + Web streams:\n  In order to build reports for these parameters, you need to include them in parameter reporting.\nOnce you\u0026rsquo;ve collected some data, head over to Events -\u0026gt; All Events, and click the little action menu at the end of the row with the page_view event. Select Edit parameter reporting from this menu.\n  In the overlay that appears, find the two readability_* parameters to add them to the list of parameters included in reporting. Set the type of the median grade parameter to Text, and the type of the reading time parameter to Number / Seconds.\n  Once you\u0026rsquo;ve done these, and once you\u0026rsquo;ve accumulated some new data, you\u0026rsquo;ll be able to find the two parameters as their own widgets when you select the page_view event in the event list, and when moving through the Analysis reports.\nNote that if you\u0026rsquo;ve configured a BigQuery export, the parameters will be included whether or not you add them via the event parameter reporting user interface!\n  Summary Let\u0026rsquo;s discuss the caveats first.\nThe script itself is fairly simple, basically chopping down the text into constituent parts and then performing calculations with those parts. However, it can perform poorly if its fed a lot of content to process. Performance of regular expressions degrade in correlation with the amount of content, and with a large body of text the parsing can disrupt your page load. This is mitigated with the use of a cache and a sampling threshold, but you\u0026rsquo;ll need to tinker with these to get the best accuracy and the best performance.\nThe regular expressions are not perfect. Particularly parsing sentences turns out to be quite difficult to manage with clarity, and I have opted for a fairly simple algorithm in the script.\nAll in all, readability scores need to be taken with a grain of salt. They abstract away much of what is gained with context, and they know nothing about the background of the reader. They count things like syllables, long words, and short words, and try to assign labels like \u0026ldquo;easy\u0026rdquo; and \u0026ldquo;difficult\u0026rdquo; based on these calculations.\nThe grade-level abstractions are also calculated based on U.S. averages. Whether you want to draw any conclusions is totally up to you, but I do think how much different the levels would be in a country like Finland, where the education system is somewhat different.\nHaving said all that, it\u0026rsquo;s still a fun idea to play around with especially if you\u0026rsquo;re in the business of creating content. Readability is a great concept to work towards, and the grades for my own content (almost categorically past college-level) have given me some pause. I think it\u0026rsquo;s a great way to design content to be as readable as possible, and tools like this should give you some insight.\nWhether or not Google Analytics is the right place to measure this remains to be seen. But I would imagine there to be benefits to correlating readability and reading time with overall engagement metrics. Something to optimize against, I think!\nLet me know in the comments what you think about this approach!\n"
},
{
	"uri": "https://www.simoahava.com/tags/content/",
	"title": "content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-outbound-links-google-tag-manager/",
	"title": "#GTMTips: Track Outbound Link Clicks In Google Tag Manager",
	"tags": ["google tag manager", "gtmtips", "links"],
	"description": "A very efficient, easy way to track outbound link clicks with Google Tag Manager.",
	"content": "I\u0026rsquo;ve written about outbound link click tracking before. It\u0026rsquo;s a very solid way to track interactions on the site, as clicking a link that leads away from a site is a signal of \u0026hellip; well, something.\nIn Google Tag Manager it\u0026rsquo;s now extremely easy and efficient to track outbound link clicks, thanks to the introduction of a new configuration in the Auto-Event variable.\nThis article will introduce the new method and show you how you can quickly set it up!\nTip 106: Track outbound link clicks with the Auto-Event variable   The method is simple. When you create a new Auto-Event variable in GTM, you can choose Element URL as the Variable Type.\nWithout special modifications, this variable would return the value of the href attribute of the clicked link.\nHowever, you can now specify a new Component Type for this variable, and it\u0026rsquo;s called Is Outbound.\nThis variable returns true if the clicked link URL did not match the current domain, and false if it did. You can use this in your Just Links trigger to have the trigger only fire for outbound link clicks.\nHere are the steps you need to take.\nStep 1: Create the Auto-Event variable Go to Variables in the Google Tag Manager user interface, and scroll down to User-Defined Variables. Click New.\n  From the variable selector, choose Auto-Event variable as the variable template.\nThen, to configure it, select Element URL as the Variable Type.\nSelect Is Outbound as the Component Type.\nIf you want Google Tag Manager to consider other domains as internal domains as well (to not have links to your blog or your webshop be considered as outbound link clicks, for example), you can add them as a comma-separated list in the Affiliated Domains field.\n  Give the variable a name such as Is Outbound Link and save it.\nStep 2: Create the Just Links trigger Then, go to Triggers in the GTM user interface, and click New.\n  Choose Click / Just Links as the trigger type.\nSet the Wait for tags and Check validation options as you wish (see this for more details). If you do check either, you\u0026rsquo;ll need to make sure you set the trigger enabling options correctly (see this for more details).\nThen, choose Some Link Clicks as the firing setting for the trigger.\n   Choose Is Outbound Link as the variable for the condition. Choose equals as the predicate. Set true as the expected value.  Once you save that trigger, any tag you add it to will only fire if the click landed on a link, and if the link leads to another domain than the one the user is currently on.\n Note! The link href must be a valid URL. If it\u0026rsquo;s e.g. a mailto: link or a javascript:void(0), it will not be considered an outbound link.\n What about Google Analytics: App + Web? Well, you don\u0026rsquo;t have to worry about tracking outbound link clicks if you have Google Analytics: App + Web deployed on your site. Its enhanced measurement capabilities automatically track outbound link clicks. You can find them in the reporting UI by checking the outbound parameter value for the events.\n  If you want to do more elaborate analysis on the outbound parameter, you need to add it to parameter reporting, after which you can add it to the Analysis reports.\nIf you have BigQuery export configured, the outbound parameter is included in all click events.\n  Summary I love writing articles like this. It\u0026rsquo;s all about some huge simplification in the Google Tag Manager user interface, and simpler is ALWAYS better.\nThis is a good direction for Google Tag Manager, and I hope they introduce smoother and smoother UI options for the most common use cases.\n"
},
{
	"uri": "https://www.simoahava.com/tags/links/",
	"title": "links",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/mysterious-macro-call-custom-html-tags/",
	"title": "#GTMTips: The Mysterious .macro() Call In Custom HTML Tags",
	"tags": ["google tag manager", "gtmtips", "custom html tags", "preview mode"],
	"description": "",
	"content": "When previewing Custom HTML tags in Google Tag Manager you\u0026rsquo;ve almost certainly run into a situation where the GTM variable shows up as a weird JavaScript method resembling something like this:\ngoogle_tag_manager[\u0026quot;GTM-ABCD123\u0026quot;].macro(15)\nAnd this is when you were expecting it to show the actual, resolved value!\nIt doesn\u0026rsquo;t help that every now and then the preview mode actually shows to correct value in the preview mode.\nWhat\u0026rsquo;s up with that? Well, there\u0026rsquo;s a fairly logical explanation to this. Read on!\nTip 105: The .macro() call in Custom HTML tags   Let\u0026rsquo;s start with the good news.\nYour tag and the variables are most likely working as they should.\nWhen you create a Custom HTML tag in Google Tag Manager, you are actually creating HTML elements that are appended to the end of the \u0026lt;body\u0026gt; element when the tag fires.\nThis means that if the HTML element you are injecting is a \u0026lt;script\u0026gt;\u0026lt;/script\u0026gt; block, i.e. a JavaScript execution context, the code isn\u0026rsquo;t run until 1) the HTML element is appended to the end of \u0026lt;body\u0026gt;, and 2) the browser has rendered the element and is ready to execute the JavaScript within the \u0026lt;script\u0026gt; block.\nNow, when you add a Google Tag Manager variable to a Custom HTML tag, you use the {{variableName}} syntax.\n  Because it would make little sense to resolve the variable before the surrounding JavaScript code is run, what follows is that GTM does not resolve the variables until the Custom HTML tag has been injected to the end of \u0026lt;body\u0026gt;.\nTo be able to defer the variables to wait for injection, this must mean that the variables themselves must be some type of function calls that the surrounding script will execute and resolve once the tag has been injected.\nEnter google_tag_manager['GTM-ABCD123'].macro(n)!\nEvery single Google Tag Manager used in a Custom HTML tag will receive its own, unique value for n, which will then be passed to the .macro() method when the tag code is executed.\n Note! This is not a deterministic value. The n can change from container version to another, so you should NOT build any sort of logic around having some specific variable always assigned some specific value for n.\n So why doesn\u0026rsquo;t it resolve in Preview mode? First of all, this is a good question. Google Tag Manager shows you what the call to macro() looks like and doesn\u0026rsquo;t actually replace the call with the value of the variable at the time of inspecting the tag.\nThe most likely reason why GTM doesn\u0026rsquo;t resolve the macro() call is because it would have to resolve the entire Custom HTML tag content to be able to adjust the value returned by macro() depending on context.\nAnd this makes no sense - GTM can\u0026rsquo;t start resolving Custom HTML tags within the Preview mode window - how would it render things like HTML elements? How would it resolve things like document.getElementById() when the iframe has no access to the actual page itself?\nSo GTM can\u0026rsquo;t resolve the Custom HTML tag while in Preview mode, which is why it shows you the pre-injection status of the Custom HTML tag, together with the .macro() calls and all.\nIt\u0026rsquo;s only after the Custom HTML tag is added to the page that the .macro() calls are executed. This bears repeating since it\u0026rsquo;s an important point.\nBut why do SOME variables resolve in Preview mode? Hahaa! You found the exception.\nTake a look at this Custom HTML tag:\n\u0026lt;script\u0026gt; (function() { console.log({{Container ID}}); console.log(\u0026#34;{{Container ID}}\u0026#34;); })(); \u0026lt;/script\u0026gt; See the difference? The first logs the variable plainly, without regard to what its return type is. The second casts the variable as a string before logging it.\nLet\u0026rsquo;s take a look at what Preview mode shows.\n  Fascinating! Why does casting the variable as a string force GTM to resolve the variable?\nSimple. Imagine if GTM did NOT resolve the .macro() call. The second line would become:\nconsole.log(\u0026quot;google_tag_manager['GTM-ABCD123'].macro(15)\u0026quot;);\nIn other words, the whole function call would be cast into a string, which would end up writing google_tag_manager['GTM-ABCD123'].macro(15) into the browser console rather than GTM-ABCD123!\nSo when you cast the variable into a string, GTM will resolve it pre-injection.\nHow do I know it\u0026rsquo;s working? Because Google Tag Manager shows variables as macro() calls, it\u0026rsquo;s really difficult to quickly check if the Custom HTML tag nevertheless works. However, here are some tips:\n1. Check the value of the variable in the Variables tab This is an easy way to get a good idea if the variable works or not. You know what the variable should be (e.g. \u0026ldquo;Container ID\u0026rdquo; in my case), so all you have to do is select the trigger event that fires the Custom HTML tag, open the Variables tab in Preview mode, and check the value of the variable.\n  The variable value should match the one in the injected Custom HTML tag.\n2. Add some console.log() calls Another slightly awkward way to debug is to add console.log() calls to the Custom HTML tag. Have the variable value log into console just before you use it to verify it actually has the right value.\n\u0026lt;script\u0026gt; (function() { var log = function(msg) { if ({{Debug Mode}}) { console.log(msg); } }; log({{Some Variable}}); window.someFunction({{Some Variable}}); })(); \u0026lt;/script\u0026gt;  In the code sample above, there is a global method window.someFunction to which we want to pass the variable {{Some Variable}}. However, we are uncertain what its actual value is since Preview mode isn\u0026rsquo;t very helpful.\nBecause of this, we log the value of {{Some Variable}} into the browser JavaScript console.\nFurthermore, I\u0026rsquo;ve built a helper function named log() that only logs the value to console if GTM is in Preview mode. This way you avoid your tags from writing arbitrary log messages into the live browser console.\n3. Debug the tag result This is the most difficult way to debug whether or not the variable worked. Unfortunately, it\u0026rsquo;s also the best way to do it.\nWhen you add JavaScript to a Custom HTML tag, it should do something. Maybe it adds some text to an HTML element, maybe it creates a new pixel or Ajax call, or perhaps it pushes something to dataLayer.\nIn any case, the best way to ensure whether or not the .macro() call resolved to its correct value is to check the output or result of the tag itself to see if whatever value the variable was supposed to pass was done correctly.\nFor example, let\u0026rsquo;s say your tag creates a pixel request like this:\n\u0026lt;script\u0026gt; (function() { var imgId = \u0026#39;?id=\u0026#39; + {{Container ID}}; var imgUrl = \u0026#39;https://www.somedomain.com/img\u0026#39; + imgId; var el = document.createElement(\u0026#39;img\u0026#39;); el.src = imgUrl; document.body.appendChild(el); })(); \u0026lt;/script\u0026gt; It\u0026rsquo;s a simple, almost nonsensical example, but it should serve to highlight the debug process nicely.\nThe tag creates a new image element whose URL is https://www.somedomain.com/img plus the string ?id= concatenated with whatever is returned by the {{Container ID}} variable.\nThis is what it looks like in Preview mode:\n  This is what it looks like after injection in the Inspect Elements pane:\n   Note! As you can see, in Inspect Elements the script element still has the .macro() call. This is how the browser works. GTM injected an element with the .macro() call and this is what was added to the page. If you remember, the script isn\u0026rsquo;t executed until AFTER injection.\n Finally, you can inspect the Network Requests to find the image request with the ID in place.\n  Also, because the tag did its own little injection with document.body.appendChild(el), you can find the image element at the end of \u0026lt;body\u0026gt; by inspecting the elements on the page.\nSummary I hope this article has alleviated your concerns. If you see a call to .macro() in Google Tag Manager\u0026rsquo;s Preview mode, don\u0026rsquo;t worry. It\u0026rsquo;s what you\u0026rsquo;re supposed to see.\nGTM will not resolve variables until the Custom HTML tag has been injected at the end of \u0026lt;body\u0026gt; and the browser has begun to render any JavaScript between \u0026lt;script\u0026gt; and \u0026lt;/script\u0026gt; within the Custom HTML tag.\nThe main exception is variables cast into strings, which Google Tag Manager will have to resolve before injection or else it will end up casting the function expression itself into a string which is not helpful.\nHopefully this article is helpful in offering you a number of ways you can debug those pesky .macro() calls just to be sure that things are working as they should.\nLet me know if you\u0026rsquo;ve run into edge cases where it still doesn\u0026rsquo;t work, or if you have other examples in addition to cast strings where GTM resolves the variable in Preview mode.\n"
},
{
	"uri": "https://www.simoahava.com/tags/custom-html-tags/",
	"title": "custom html tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/improve-google-analytics-bot-detection-with-recaptcha/",
	"title": "Improve Google Analytics Bot Detection With reCAPTCHA",
	"tags": ["google tag manager", "google analytics", "php", "node.js", "recaptcha"],
	"description": "Build your own reCAPTCHA v3 verification mechanism and send the results to Google Analytics for improved bot detection.",
	"content": "There are thousands upon thousands of bots, crawlers, spiders, and other creepy-crawlies out there doing nothing but crawling through websites and harvesting the content within for whatever purposes they have been fine-tuned to. While Google Analytics provides a bot filtering feature to filter out \u0026ldquo;spam\u0026rdquo; and \u0026ldquo;bot traffic\u0026rdquo; from views, this is far from comprehensive enough to tackle all instances of bot traffic that might enter the site.\n  You might have noticed bot traffic in your data even if you have bot filtering toggled on. For instance, such traffic could be characterized as follows:\n Traffic without a discernible source/medium Average Session Duration under \u0026lt;00:00:01 Same / outdated user agent Non-converting traffic / high bounce rate Network location a known cloud host (e.g. AWS)  Luckily, there are better ways to identify this traffic than looking at noisy Google Analytics data. Google has introduced reCAPTCHA v3 which lets you verify traffic without any user interaction required. This is done with a simple JavaScript API combined with a server-side verification endpoint.\n  In this article, Sebastian Pospischil and Philipp Schneider from Resolution Media will share the solution with you. They\u0026rsquo;ll show you how to build a simple PHP API on your Apache web server to handle the requests from reCAPTCHA.\n  I\u0026rsquo;ll contribute by building the same thing with Node.js. Remember that you can have both (Apache and Node.js) running in Google Cloud using e.g. the App Engine standard environment. This is helpful if you do not have the resources or capabilities to run something like this on your own web servers.\nHuge thanks to Philipp and Sebastian for sharing this nugget of wisdom with this blog!\nGetting started To start with, you\u0026rsquo;ll need to generate a (free) site key and a (free) secret key for your reCAPTCHA API use. Head on over to https://www.google.com/recaptcha/admin/create, and register a new site to use with reCAPTCHA v3. Make sure you enter all the domains from which these verification requests will be sent.\n  Once you\u0026rsquo;ve added everything and checked all the necessary boxes, click SUBMIT to get your keys.\n  Copy the keys, as you\u0026rsquo;ll need them soon when configuring the Custom HTML tag in Google Tag Manager as well as the server-side endpoint.\nPHP endpoint In this chapter, Philipp and Sebastian will walk you through the steps of creating a PHP endpoint for verifying the reCAPTCHA requests. They\u0026rsquo;ll also show you how to create the Custom HTML tag in Google Tag Manager for loading the reCAPTCHA JavaScript API and ascertaining the \u0026ldquo;bot-ness\u0026rdquo; of the user.\nCustom HTML tag First things first - let\u0026rsquo;s setup Google Tag Manager. In GTM, create a new Custom HTML Tag.\n\u0026lt;style\u0026gt; /* hides the recaptcha badge */ .grecaptcha-badge { visibility: hidden !important; } \u0026lt;/style\u0026gt; \u0026lt;script src=\u0026#34;https://www.google.com/recaptcha/api.js?render=_reCAPTCHA_site_key_\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; grecaptcha.ready(function() { grecaptcha.execute(\u0026#39;_reCAPTCHA_site_key_\u0026#39;, { action: \u0026#39;homepage\u0026#39; }).then(function(token) { var xhr = new XMLHttpRequest(); xhr.onload = function() { if (xhr.response !== \u0026#39;noChange\u0026#39;) { var greResult = JSON.parse(xhr.response); window.dataLayer.push({ event: \u0026#39;recaptcha\u0026#39;, recaptchaAnswer: greResult.success, recaptchaScore: greResult.score }); } }; xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;/gtm/recaptcha.php\u0026#39;, true); //replace this with URL to your PHP fil  xhr.setRequestHeader(\u0026#39;Content-type\u0026#39;, \u0026#39;application/x-www-form-urlencoded\u0026#39;); xhr.send(\u0026#39;token=\u0026#39; + token + \u0026#39;\u0026amp;action=homepage\u0026#39;); }); }); \u0026lt;/script\u0026gt; You can set this tag to fire on whatever trigger you wish, though a trigger that fires when the page loads (e.g. All Pages, DOM Ready, or Window Loaded) is recommended.\nMake sure you edit the following:\n The _reCAPTCHA_site_key string on lines 7 and 10 with the actual site key you copied from the reCAPTCHA admin console in the previous chapter. The URL of your PHP endpoint on line 24.  This block of code runs the reCAPTCHA JavaScript API in the user\u0026rsquo;s browser, and sends the tokenized result to the PHP endpoint you\u0026rsquo;ll configure in the next chapter. When it gets a response, the score and success state are pushed into dataLayer.\n  The very first code block in the tag hides the reCAPTCHA badge from showing up on the site. Remember to read this part of the FAQ to learn how to modify your site to allow the hiding of the badge.\nPHP file for your server Create a file recaptcha.php with the following code, and upload it to all the domains you wish to validate reCAPTCHA requests on. Place it in a subdirectory named /gtm/.\n NOTE! You can use whatever path and filename you want for the PHP file - just remember to update the Custom HTML tag\u0026rsquo;s HTTP request endpoint URL accordingly.\n \u0026lt;?php // reCaptcha info  $url = \u0026#39;https://www.google.com/recaptcha/api/siteverify\u0026#39;; $secret = \u0026#34;_reCAPTCHA_secret_key_\u0026#34;; $remoteip = $_SERVER[\u0026#39;REMOTE_ADDR\u0026#39;]; // Form info \t$action = $_POST[\u0026#39;action\u0026#39;]; $response = $_POST[\u0026#39;token\u0026#39;]; // Botscore \t$botscore = $_COOKIE[\u0026#39;_rbs\u0026#39;]; // Curl Request \t$curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_POST, true); curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, array( \u0026#39;secret\u0026#39; =\u0026gt; $secret, \u0026#39;remoteip\u0026#39; =\u0026gt; $remoteip, \u0026#39;action\u0026#39; =\u0026gt; $action,\t\u0026#39;response\u0026#39; =\u0026gt; $response )); $curlData = curl_exec($curl); curl_close($curl); $curlJson = json_decode($curlData, true); //defining the answer \t$answer = $curlData; //refresh the cookie \tsetcookie(\u0026#39;_rbs\u0026#39;, $curlJson[\u0026#39;score\u0026#39;], time()+1800, \u0026#39;/\u0026#39;,\u0026#39;\u0026#39;, 0); // only fire $answer if botscore cookie is not set or different from acutal score \tif ($botscore != $curlJson[\u0026#39;score\u0026#39;]) { echo $answer; } else { echo \u0026#34;noChange\u0026#34;; } ?\u0026gt;  Again, make sure you replace the _reCAPTCHA_secret_key on line 4 with the secret key you got from the admin console.\nThis PHP code takes the response sent by Google Tag Manager, validates it against the reCAPTCHA verification API, and then sends the result object in the response, which will, in turn, be pushed into dataLayer for your tags to use.\nThe code also writes a cookie in the HTTP response. This cookie is named _rbs and contains the score of the reCAPTCHA action. The results are only returned if the reCAPTCHA score has changed in the last 30 minutes. If the results have not changed, the response will be \u0026quot;noChange\u0026quot;, which blocks the dataLayer.push() from taking place.\nNode.js endpoint In this chapter, Simo will show you how to do the same as above by using a Node.js application instead.\nCustom HTML tag The main difference (apart from the tech stack) to the PHP solution is that we\u0026rsquo;ll log errors as well. They are sent as the value of the recaptchaScore key in dataLayer, so in your report you\u0026rsquo;ll end up with a mix of valid scores and error texts, but this should be easy to filter in your tables.\nThis is what the Custom HTML tag looks like:\n\u0026lt;style\u0026gt; /* hides the recaptcha badge */ .grecaptcha-badge { visibility: hidden !important; } \u0026lt;/style\u0026gt; \u0026lt;script src=\u0026#34;https://www.google.com/recaptcha/api.js?render=_reCAPTCHA_site_key\u0026amp;onload=__recaptchaCallback\u0026#34; async defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.__recaptchaCallback = function() { if (window.grecaptcha) { window.grecaptcha.execute(\u0026#39;_reCAPTCHA_site_key\u0026#39;, {action: \u0026#39;homepage\u0026#39;}).then(function(token) { var url = \u0026#39;https://mydomain.com:3000/recaptcha/\u0026#39;; var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;GET\u0026#39;, url + \u0026#39;?g-recaptcha-response=\u0026#39; + token, true); xhr.onreadystatechange = function(data) { if (this.readyState === XMLHttpRequest.DONE \u0026amp;\u0026amp; this.status === 200) { var responseJson = JSON.parse(xhr.response); window.dataLayer.push({ event: \u0026#39;recaptcha\u0026#39;, recaptchaAnswer: responseJson.status, recaptchaScore: responseJson.recaptcha.score }); } } xhr.send(); }); } }; \u0026lt;/script\u0026gt; Remember to change the _reCAPTCHA_site_key on lines 7 and 11.\nThe url variable on line 12 needs to point to your Node.js application.\nThis setup is different from the PHP request. Here, we use a GET request, and we parse the response, pushing both the success and error into dataLayer using a success callback for the HTTP request.\nAlso, since we load the JavaScript API asynchronously, we create a global callback method __recaptchaCallback, whose name we pass in the script loader as a parameter (\u0026amp;onload).\nThe end result is the same as with the PHP endpoint - a dataLayer.push() event with the reCAPTCHA status and score.\nNode.js application The Node.js application example here is the simplest possible thing you can build. For it to work, you need to have Node.js installed on in the environment where you want to run the endpoint (for example, a web server running on your site or in the cloud).\nCreate an empty directory, and initialize a new npm project with npm init. After following through the setup steps, run:\n$ npm install express express-recaptcha cookie-parser --save This installs the required dependencies to run a simple express.js web server as well as the files required to run the reCAPTCHA middleware and some cookie parsing logic.\nThen, create a file named server.js, open it for editing, and add the following code within:\nconst express = require(\u0026#39;express\u0026#39;); const cookieParser = require(\u0026#39;cookie-parser\u0026#39;); const Recaptcha = require(\u0026#39;express-recaptcha\u0026#39;).RecaptchaV3; const app = express(); app.use(cookieParser()); const siteKey = \u0026#39;_reCAPTCHA_site_key\u0026#39;; const secretKey = \u0026#39;_reCAPTCHA_secret_key\u0026#39;; const recaptcha = new Recaptcha(siteKey, secretKey, {action: \u0026#39;homepage\u0026#39;}); app.get(\u0026#39;/recaptcha/\u0026#39;, recaptcha.middleware.verify, (req, res) =\u0026gt; { const botCookie = req.cookies._rbs; res.header(\u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;*\u0026#39;); // Set this to the actual domain that will be sending the requests  if (!req.recaptcha.error) { const recaptcha = req.recaptcha.data; const score = recaptcha.score; res.cookie(\u0026#39;_rbs\u0026#39;, score, {path: \u0026#39;/\u0026#39;, maxAge: 30 * 60 * 1000}); // Only send response if cookie does not exist or does not match the latest score  if (botCookie !== score) { res.send({status: \u0026#39;1\u0026#39;, recaptcha: req.recaptcha.data}); } } else { const score = req.recaptcha.error; res.cookie(\u0026#39;_rbs\u0026#39;, score, {path: \u0026#39;/\u0026#39;, maxAge: 30 * 60 * 1000}); res.send({status: \u0026#39;0\u0026#39;, recaptcha: {score: score}}); } }); app.listen(3000, () =\u0026gt; console.log(\u0026#39;App listening on port 3000!\u0026#39;));  Edit lines 8 and 9 with the actual site key and secret key, respectively.\nEdit line 15 to allow requests only from your site (so if your site is running on https://mydomain.com, that\u0026rsquo;s what you would add as a valid origin instead of *).\nThis application runs on port 3000 of whatever hostname is configured for your web server. It creates a simple GET request endpoint to /recaptcha/, where a piece of middleware will inspect the incoming request for a reCAPTCHA token.\nIf the token is encountered, it is validated and the recaptcha results are written to dataLayer as well as a cookie named _rbs.\nIf the validation fails, the \u0026ldquo;score\u0026rdquo; will be the error message instead of the actual score.\nRest of the setup Google Analytics Custom Dimensions In Google Analytics, create two new Custom Dimensions:\n reCAPTCHA score (session scope) reCAPTCHA answer (session scope)    These two Custom Dimensions allow us to permanently filter out bot traffic from our Google Analytics production views.\n  Note that the Google Analytics tags are only fired if the score changes (or if the score was previously an error). That way you won\u0026rsquo;t constantly send the same reCAPTCHA score to Google Analytics while the session is active.\nThe Custom Dimensions are sent with an event tag.\nGoogle Tag Manager setup First, let\u0026rsquo;s create two new Data Layer variables.\nData Layer Variables The first variable is named dl_recaptchaAnswer, and should have its Data Layer Variable Name field pointing to recaptchaAnswer.\nThe second variable is named dl_recaptchaScore, and should have its Data Layer Variable Name field pointing to recaptchaScore.\nHere\u0026rsquo;s what the latter would look like:\n  Trigger The trigger is a simple Custom Event trigger that looks like this:\n  Event tag The main function of the event tag is to send the Custom Dimensions. You can arrange the other fields however you like. This is what a sample Universal Analytics event tag might look like:\n    Just make sure you have the Custom Dimension index and value settings correct per what you configured in Google Analytics admin earlier.\n NOTE! One enhancement would be to set the Custom Dimensions only if recaptchaAnswer is 1 (success), so that you don\u0026rsquo;t end up overwriting a valid score for the session with an errant error message.\n Analyze the results When you set the whole thing up, all users on your page will be vetted in the background for their probability to be a bot. The script fires if the user hasn\u0026rsquo;t been validated with a proper score within 30 minutes since the last validation. This ensures that you don\u0026rsquo;t flood Google Analytics unnecessarily with events that just have the same score sent with each one.\n  Here\u0026rsquo;s what the custom dimensions will end up populated with:\n   Key Google Analytics fields Explanation Sample value     recaptchaAnswer Event Action, Custom Dimension Whether or not the validation worked. 1 for success, 0 for failure. 1   recaptchaScore Event Label, Custom Dimension Score between 0.0 and 1.0 (where 1.0 is most likely to be human and 0.0 most likely to be bot). If the validation fails, the error message is sent instead. 0.9, or invalid-input-response    You can drill down to the 0.1 score to identify idiosyncrasies in this low-score traffic.\n  To get a better understanding of the amount of bot traffic, create segments based on reCAPTCHA score. After a few weeks, you’ll have enough data in order to:\n Determine the ratio of bot traffic on your site. Decide whether you should permanently filter out flagged bot traffic from you Google Analytics production views.  Summary Thank you so much to Philipp and Sebastian for sharing their amazing idea here. Since reCAPTCHA v3 calculates the score based on user interactions on the site, it\u0026rsquo;s a perfect candidate for measuring with Google Analytics, since we can surreptitiously collect the \u0026ldquo;bot-ness\u0026rdquo; of the visitor and leverage GA\u0026rsquo;s custom dimensions to collect the data.\nIt\u0026rsquo;s not trivial to implement, as it requires a web service to poll for the reCAPTCHA score. Perhaps someone could do the world a service and create a free, public API for polling this information? Until then, you\u0026rsquo;re stuck with an on-premise solution or, better yet, leveraging the cloud for setting up the API endpoint.\nThis solution should serve well to uncover the complexity of bot traffic on any given website. Bots and crawlers come in different shapes and sizes - there are malicious agents as well as services that aim to do good. Regardless, bot traffic is most likely not your target traffic when building your website, so being able to segment it out in Google Analytics is a major win for data quality indeed.\nLet us know in the comments what you think of this solution, and whether you have ideas for improving it!\n"
},
{
	"uri": "https://www.simoahava.com/tags/node.js/",
	"title": "node.js",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/php/",
	"title": "php",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/recaptcha/",
	"title": "recaptcha",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/bigquery-query-guide-google-analytics-app-web/",
	"title": "#BigQueryTips: Query Guide To Google Analytics: App + Web",
	"tags": ["bigquery", "app+web", "firebase"],
	"description": "This is a Getting Started guide to using BigQuery with the dataset pulled in from a Google Analytics App+Web view.",
	"content": "I\u0026rsquo;ve thoroughly enjoyed writing short (and sometimes a bit longer) bite-sized tips for my #GTMTips topic. With the advent of Google Analytics: App + Web and particularly the opportunity to access raw data through BigQuery, I thought it was a good time to get started on a new tip topic: #BigQueryTips.\nFor Universal Analytics, getting access to the BigQuery export with Google Analytics 360 has been one of the major selling points for the expensive platform. The hybrid approach of getting access to raw data that has nevertheless been annotated with Google Analytics\u0026rsquo; sessionization schema and any integrations (e.g. Google Ads) the user might have enabled is a powerful thing indeed.\nWith Google Analytics: App + Web, the platform is moving away from the data model that has existed since the days of Urchin, and is instead converging with Firebase Analytics, to which we have already had access with native Android and iOS applications.\n  Fortunately, BigQuery export for App + Web properties comes at no additional platform costs - you only pay for storage and queries just as you would if creating a BigQuery project by yourself in Google\u0026rsquo;s cloud platform.\nSo, with data flowing into BigQuery, I thought it time to start writing about how to build queries against this columnar data store. One of the reasons is because I want to challenge myself, but the other reason is that there just isn\u0026rsquo;t that much information available online when it comes to using SQL with Firebase Analytics\u0026rsquo; BigQuery exports.\n  To get things started with this new topic, I\u0026rsquo;ve enlisted the help of my favorite SQL wizard in #measure, Pawel Kapuscinski. He\u0026rsquo;s never short of a solution when tricky BigQuery questions pop up in Measure Slack, so I thought it would be great to get him to contribute with some of his favorite tips for how to approach querying BigQuery data with SQL.\nHopefully, #BigQueryTips will expand to more articles in the future. There certainly is a lot of ground to cover!\nGetting started First of all, you\u0026rsquo;ll naturally need a Google Analytics: App + Web property. Here are some guides for getting started:\n Step by Step: Setting up an App + Web Property (Krista Seiden) Getting Started With Google Analytics: App + Web (Simo) App + Web Properties tag and instrumentation guide (Google)    Next, you need to enable the BigQuery export for the new property, and for that you should follow this guide.\n  Once you have the export up-and-running, it\u0026rsquo;s a good time to take a moment to learn and/or get a refresher on how SQL works. For that, there is no other tutorial online that comes even close to the free, interactive SQL tutorial at Mode Analytics.\n  And once you\u0026rsquo;ve learned the difference between LEFT JOIN and CROSS JOIN, you can take a look at some of the sample data sets for iOS Firebase Analytics and Android Firebase Analytics. Play around with them, trying to figure out just how much querying a typical relational database differs from accessing data stored in columnar structure as BigQuery does.\nAt this point, you should have your BigQuery table collecting daily data dumps from the App + Web tags firing on your site, so let\u0026rsquo;s work with Pawel and introduce some pretty useful BigQuery SQL queries to get started on that data analysis path!\nTip #1: CASE and GROUP BY Our first tip covers two extremely useful SQL statements: CASE and GROUP BY. Use these to aggregate and group your data!\nCASE   CASE statements are similar to the if...else statements used in other languages. The condition is introduced with the WHEN keyword, and the first WHEN condition that matches will have its THEN value returned as the value for that column.\nYou can use the ELSE keyword at the end to specify a default value. If ELSE is not defined and no conditions are met, the column gets the value null.\nSOURCE TABLE    user age     12345 15   23456 54    SQL SELECT user, age, CASE WHEN age \u0026gt;= 90 THEN \u0026#34;90+\u0026#34; WHEN age \u0026gt;= 50 THEN \u0026#34;50-89\u0026#34; WHEN age \u0026gt;= 20 THEN \u0026#34;20-49\u0026#34; ELSE \u0026#34;0-19\u0026#34; END AS age_bucket FROM some_table QUERY RESULT    user age age_bucket     12345 15 0-19   23456 54 50-89    The CASE statement is useful for quick transformations and for aggregating the data based on simple conditions.\nGROUP BY   GROUP BY is required every time you want to summarize your data. For example, when you do calculations with COUNT (to return the number of instances) or SUM (to return the sum of instances), you need to indicate a column to group these calculations by (unless you\u0026rsquo;re only retrieving the calculated column) . GROUP BY is thus most often used with aggregate functions such as COUNT, MAX, ANY_VALUE, SUM, and AVG. It\u0026rsquo;s also used with some string functions such as STRING_AGG when aggregating multiple rows into a single string.\nSOURCE TABLE    user mobile_device_model     12345 iPhone 5   12345 Nokia 3310   23456 iPhone 7    SQL SELECT user, COUNT(mobile_device_model) AS device_count FROM table GROUP BY 1 QUERY RESULT    user device_count     12345 2   23456 1    In the query above, we assume that a single user can have more than one device associated with them in the table that is being queried. As we do a COUNT of all the devices for a given user, we need to group the results by the user column for the query to work.\nUSE CASE: Device category distribution across users Let\u0026rsquo;s keep the theme of users and devices alive for a moment.\nUsers and events are the key metrics for App + Web. This is fundamentally different to the session-centric approach of Google Analytics (even though there are reverberations of \u0026ldquo;sessions\u0026rdquo; in App + Web, too). It\u0026rsquo;s much closer to a true hit stream model than before.\nHowever, event counts alone tell as nothing without us drilling into what kind of event happened.\nIn this first tip, we\u0026rsquo;ll learn to calculate the number of users per device category. As the concept of \u0026ldquo;User\u0026rdquo; is still bound to a unique browser client ID, if the same person visited the website on two different browser instances or devices, they would be counted as two users.\nThis is what the query looks like:\nSELECT CASE WHEN device.category = \u0026#34;desktop\u0026#34; THEN \u0026#34;desktop\u0026#34; WHEN device.category = \u0026#34;tablet\u0026#34; AND app_info.id IS NULL THEN \u0026#34;tablet-web\u0026#34; WHEN device.category = \u0026#34;mobile\u0026#34; AND app_info.id IS NULL THEN \u0026#34;mobile-web\u0026#34; WHEN device.category = \u0026#34;tablet\u0026#34; AND app_info.id IS NOT NULL THEN \u0026#34;tablet-app\u0026#34; WHEN device.category = \u0026#34;mobile\u0026#34; AND app_info.id IS NOT NULL THEN \u0026#34;mobile-app\u0026#34; END AS device, COUNT(DISTINCT user_pseudo_id) AS users FROM `dataset.analytics_accountId.events_2*` GROUP BY 1   The query itself is simple, but it makes use of the two statements covered in this chapter effectively. CASE is used to segment \u0026ldquo;mobile\u0026rdquo; and \u0026ldquo;tablet\u0026rdquo; users further into web and app groups (something you\u0026rsquo;ll find useful once you start collecting data from both websites and mobile apps), and GROUP BY displays the count of unique device IDs per device category.\nTip #2: DISTINCT and HAVING The next two keywords we\u0026rsquo;ll cover are HAVING and DISTINCT. The first is great for filtering results based on aggregated values. The latter is used to deduplicate results to avoid calculating the same result multiple times.\nDISTINCT   The DISTINCT keyword is used to deduplicate results.\nSOURCE TABLE    user device_category session_id     12345 desktop abc123   12345 desktop def234   12345 tablet efg345   23456 mobile fgh456    SQL SELECT user, COUNT(DISTINCT device_category) AS device_category_count FROM table GROUP BY 1 QUERY RESULT    user device_category_count     12345 2   23456 1    For example, if the user had three sessions with device categories desktop, desktop, and tablet, then a query for COUNT(DISTINCT device.category) would return 2, as there are just two instances of distinct device categories.\nHAVING   The HAVING clause can be used at the end of the query, after all calculations have been done, to filter the results. All the rows selected for the query are still processed, even if the HAVING statement strips out some of them from the result table.\nSOURCE TABLE    user device_category session_id     12345 desktop abc123   12345 desktop def234   12345 tablet efg345   23456 mobile fgh456    SQL SELECT user, COUNT(DISTINCT device_category) AS device_category_count FROM table GROUP BY 1 HAVING device_category_count \u0026gt; 1 QUERY RESULT    user device_category_count     12345 2    It\u0026rsquo;s similar to WHERE (see next chapter), but unlike WHERE which is used to filter the actual records that are processed by the query, HAVING is used to filter on aggregated values. In the query above, device_category_count is an aggregated count of all the distinct device categories found in the data set.\nUSE CASE: Distinct device categories per user Since the very name, App + Web, implies cross-device measurement, exploring some ways of grouping and filtering data based on users with more than one device in use seems fruitful.\nThe query is similar to the one in the previous chapter, but this time instead of grouping by device category, we\u0026rsquo;re grouping by user and counting the number of unique device categories each user has visited the site with. We\u0026rsquo;re filtering the data to only include users with more than one device category in the data set.\nThis is an exploratory query. You can then extend it to actually detect different models instead of just using device category. Device category is a fickle dimension to use, as in the example dataset used for this article, many times a device labelled as \u0026ldquo;Apple iPhone\u0026rdquo; was actually counted as a Desktop device.\nSee this article by Craig Sullivan to understand how messed up device attribution in web analytics actually is.\nSELECT user_pseudo_id, COUNT(DISTINCT device.category) AS used_devices_count, STRING_AGG(DISTINCT device.category) AS distinct_devices, STRING_AGG(device.category) AS devices FROM `dataset.analytics_accountId.events_2*` GROUP BY 1 HAVING used_devices_count \u0026gt; 1   As a bonus, you can see how STRING_AGG can be used to concatenate multiple values into a single column. This is useful for identifying patterns that emerge across multiple rows of data!\nTip #3: WHERE WHERE   If you want to filter the records against which you\u0026rsquo;ll run your query, WHERE is your best friend. As it filters the records that are processed, it\u0026rsquo;s also a great way to reduce the (performance and monetary) cost of your queries.\nSOURCE TABLE    user session_id landing_page_path     12345 abc123 /home/   12345 bcd234 /purchase/   23456 cde345 /home/   34567 def456 /contact-us/    SQL SELECT * FROM table WHERE user = \u0026#39;12345\u0026#39; QUERY RESULT    user session_id landing_page_path     12345 abc123 /home/   12345 bcd234 /purchase/    The WHERE clause is used to filter the rows against which the rest of the query is made. It is introduced directly after the FROM statement, and it reads as \u0026ldquo;return all the rows in the FROM table that match the condition of the WHERE clause\u0026rdquo;.\nDo note that WHERE can\u0026rsquo;t be used with aggregate values. So if you\u0026rsquo;ve done any calculations with the rows returned from the table, you need to use HAVING instead.\nDue to this, WHERE is less expensive in terms of query processing than HAVING.\nUSE CASE: Engaged users As mentioned before, App + Web is event-driven. With Firebase Analytics, Google introduced a number of automatically collected, pre-defined events to help users gather useful data from their apps and websites without having to flex a single muscle.\nOne such event is user_engagement. This is fired when the user has engaged with the website or app for a specified period of time.\nSince this is readily available as a custom event, we can create a simple query that uses the WHERE clause to return only those records where the user was engaged.\nThe key to WHERE is that it\u0026rsquo;s executed after the FROM clause (which specifies the table to be queried). If a row doesn\u0026rsquo;t match the condition in WHERE, it won\u0026rsquo;t be used to match the rest of the query against.\nSELECT event_date, COUNT(DISTINCT user_pseudo_id) AS engaged_users FROM `dataset.analytics_accountId.events_20190922` WHERE event_name = \u0026#34;user_engagement\u0026#34; GROUP BY 1   See how we\u0026rsquo;re including data from just one date? We\u0026rsquo;re also only including DISTINCT users, so if a user was engaged more than once during the day we\u0026rsquo;re only counting them once.\nTip #4: SUBQUERIES and ANALYTIC functions A subquery in SQL means a query within a query. They can emerge in multiple different places, such as within SELECT clauses, FROM clauses, and WHERE clauses.\nAnalytic functions let you do calculations that cover other rows than just the one that is being processed. It\u0026rsquo;s similar to aggregations such as SUM and AVG except it doesn\u0026rsquo;t result in rows getting grouped into a single result. That\u0026rsquo;s why analytic functions are perfectly suited for things like running totals/averages or, in analytics contexts, determining session boundaries by looking at first and last timestamps, for example.\nSUBQUERY   The point of a subquery is to run calculations on table data, and return the result of those calculations to the enclosing query. This lets you logically organize your queries, and it makes it possible to do calculations on calculations, which would not be possible in a single query.\nSOURCE TABLE    user event_name     12345 session_start   23456 user_engagement   34567 user_engagement   45678 link_click    SQL SELECT COUNT(DISTINCT user) AS all_users, ( SELECT COUNT(DISTINCT user) FROM table WHERE event_name = \u0026#39;user_engagement\u0026#39; ) AS engaged_users FROM table QUERY RESULT    all_users engaged_users     4 2    In this example, the subquery is within the SELECT statement, meaning the subquery result is bundled into a single column of the main query.\nHere, the engaged_users column retrieves the count of all distinct user IDs from the table, where these users had an event named user_engagement collected at any time.\nThe main query then combines this with a count of all distinct user IDs without any restrictions, and thus you get both counts in the same table.\nYou couldn\u0026rsquo;t have achieved with just the main query, since the WHERE statement applies to all SELECT columns in the table. That\u0026rsquo;s why we needed the subquery - we had to use a WHERE statement that only applied to the engaged_users column.\nANALYTIC FUNCTIONS   Analytic functions can be really difficult to understand, since with SQL you\u0026rsquo;re used to going over the table row-by-row, and comparing the query against each row one at a time.\nWith an analytic function, you stretch this logic a bit. The query still goes over the source table row-by-row, but this time you can reference other rows when doing calculations.\nSOURCE TABLE    user event_name event_timestamp     12345 click 1001   12345 click 1002   23456 session_start 1012   23456 user_engagement 1009   34567 click 1000    SQL SELECT event_name, user, event_timestamp FROM ( SELECT user, event_name, event_timestamp, RANK() OVER (PARTITION BY event_name ORDER BY event_timestamp) AS rank FROM table ) WHERE rank = 1 QUERY RESULT    event_name user event_timestamp     click 34567 1000   user_engagement 23456 1009   session_start 23456 1012    In this query, we take each event and see which user sent the first such event in the table.\nFor each row in the table, the RANK() OVER (PARTITION BY event_name ORDER BY event_timestamp) AS rank is run. The partition is basically a \u0026ldquo;reference\u0026rdquo; table with all the event names matching the current row\u0026rsquo;s event name, and their respective timestamps.\nThese timestamps are then ordered in ascending order (within the partition). The RANK() OVER part of the function returns the current event name\u0026rsquo;s rank in this table.\nTo walk through an example, let\u0026rsquo;s say the query engine encounters the first row of the table. The RANK() OVER (PARTITION BY event_name ORDER BY event_timestamp) creates the reference table that looks like this for the first row:\n   user event_name event_timestamp rank     12345 click 1000 1   12345 click 1001 2   12345 click 1002 3    The query then checks how the current row matches against this partition, and returns 2 as the first row in the table was rank 2 of its partition.\nThis partition is ephemeral - it\u0026rsquo;s only used to calculate the result of the analytic function.\nFor the purposes of this exercise, this analytic function is furthermore done in a subquery, so that the main query can filter the result using a WHERE for just those items that had rank 1 (first timestamp of any given event).\nI recommend checking out the first paragraphs in this document - it explains how the partitioning works.\nUSE CASE: First interaction per event Let\u0026rsquo;s extend the example from above into the App + Web dataset.\nWe\u0026rsquo;ll create a list of client IDs together with the event name, the timestamp, and the timestamp converted to a readable (date) format.\nSELECT user_pseudo_id, event_name, event_timestamp, DATETIME(TIMESTAMP_MICROS(event_timestamp), \u0026#34;Europe/Helsinki\u0026#34;) AS date FROM ( SELECT user_pseudo_id, event_name, event_timestamp, RANK() OVER (PARTITION BY event_name ORDER BY event_timestamp) AS rank FROM `dataset.analytics_accountId.events_20190922` ) WHERE rank = 1 ORDER BY event_timestamp   The logic is exactly the same as in the introduction to analytic functions above. The only difference is how we use the DATETIME and TIMESTAMP_MICROS to turn the UNIX timestamp (stored in BigQuery) into a readable date format.\nDon\u0026rsquo;t worry - analytic functions in particular are a tough concept to understand. Play around with the different analytic functions to get an idea of how they work with the source idea.\nThere are quite a few articles and videos online that explain the concept further, so I recommend checking out the web for more information. We will also return to analytic functions many, many times in future #BigQueryTips posts.\nTip #5: UNNEST and CROSS JOIN The dataset exported by App + Web does not end up in a relational database where we could use the JOIN key to quickly pull in extra information about pages, sessions, and users.\nInstead, BigQuery arranges data in nested and repeated fields - that\u0026rsquo;s how it can have a single row represent all the hits of a session (as in the Google Analytics dataset).\nThe problem with this approach is that it\u0026rsquo;s not too intuitive to access these nested values.\nEnter UNNEST, particularly when coupled with CROSS JOIN.\nUNNEST means that the nested structure is actually expanded so that each item within can be joined with the rest of the columns in the row. This results in the single row becoming multiple rows, where each row corresponds to one value in the nested structure. This column-to-rows is achieved with a CROSS JOIN, where every item in the unnested structure is joined with each column in the rest of the table.\nUNNEST and CROSS JOIN   The two keywords go intrinsically together, so we\u0026rsquo;ll treat them as such.\nSOURCE TABLE    timestamp event params.key params.value     1000 click time\ntype\ndevice 10\nright-click\niPhone    SQL SELECT timestamp, event, event_params.key AS event_params_key FROM table AS t CROSS JOIN UNNEST (t.params) AS event_params QUERY RESULT    timestamp event event_params_key     1000 click time   1000 click type   1000 click device    See what happened? The nested structure within params was unnested so that each item was treated as a separate row in its own column. Then, this unnested structure was cross-joined with the table. A CROSS JOIN combines every row from table with every row from the unnested structure.\nThis is how you end up with a structure that you can then use for calculations in your main query.\nIt\u0026rsquo;s a bit complicated since you always need to do the UNNEST - CROSS JOIN exercise, but once you\u0026rsquo;ve done it a few times it should become second nature.\nIn fact, there\u0026rsquo;s a shorthand for writing the CROSS JOIN that might make things easier to read (or not). You can replace the CROSS JOIN statement with a comma. This is what the example query would look like when thus modified.\nSELECT timestamp, event, event_params.key AS event_params_key FROM table AS t, UNNEST (t.params) AS event_params USE CASE: Pageviews and Unique Pageviews per user Let\u0026rsquo;s look at a query for getting the count of pageviews and unique pageviews per user.\nSELECT event_date, event_params.value.string_value as page_url, COUNT(*) AS pageviews, COUNT(DISTINCT user_pseudo_id) AS unique_pageviews FROM `dataset.analytics_accountId.events_20190922` AS t, UNNEST(t.event_params) AS event_params WHERE event_params.key = \u0026#34;page_location\u0026#34; AND event_name = \u0026#34;page_view\u0026#34; GROUP BY 1, 2 ORDER BY 3 DESC   In the FROM statement we define the table as usual, and we give it an alias t (good practice, as it helps keep the queries lean).\nThe next step is to UNNEST one of those nested fields, and then CROSS JOIN it with the rest of our table. The WHERE clause is used to make sure only rows that have the page_location key and the page_view event type are included in the final table.\nFinally, we GROUP BY the event date and page URL, and then we sort everything by the count of pageviews in descending order.\nConceptually, the UNNEST and CROSS JOIN create a new table that has a crazy number of rows, as every row in the original table is multiplied by the number of rows in the event_params nested structure. The WHERE clause is thus your friend in making sure you only analyze the data that answers the question you have.\nTip #6: WITH\u0026hellip;AS and LEFT JOIN The final keywords we\u0026rsquo;ll go through are WITH...AS and LEFT JOIN. The first lets you create a type of subquery that you can reference in your other queries as a separate table. The second lets you join two datasets together while preserving rows that do not match against the join condition.\nWITH\u0026hellip;AS   WITH...AS is very similar conceptually to a subquery. It allows you to build a table expression that can then be used by your queries. The main difference to a subquery is that the table gets an alias, and you can then use this alias wherever you want to refer to the table\u0026rsquo;s data. A subquery would need to be repeated in all places where you want to reference it, so WITH makes custom queries more accessible.\nSOURCE TABLE    user event_name event_timestamp     12345 click 1000   12345 user_engagement 1005   12345 click 1006   23456 session_start 1002   34567 session_start 1000   34567 click 1002    SQL WITH users_who_clicked AS ( SELECT DISTINCT user FROM table WHERE event_name = \u0026#39;click\u0026#39; ) SELECT * FROM users_who_clicked QUERY RESULT    user     12345   34567    It\u0026rsquo;s not the most mind-blowing of examples, but it should illustrate how to use table aliases. After the WITH...AS clause, you can then reference the table in e.g. FROM statements and in joins.\nSee the next example where we make this more interesting.\nLEFT JOIN   A LEFT JOIN is one of the most useful data joins because it allows you to account for unmatched rows as well.\nA LEFT JOIN takes all rows in the first table (\u0026ldquo;left\u0026rdquo; table), and the rows that match a specific join criterion in the second table (\u0026ldquo;right\u0026rdquo; table). For all the rows that did not have a match, the first table is populated with a null value. Rows in the second table that do not have a match in the first table are discarded.\nSOURCE TABLE    user event_name event_timestamp     12345 click 1000   12345 user_engagement 1005   12345 click 1006   23456 session_start 1002   34567 session_start 1000   34567 click 1002    SQL WITH users_who_clicked AS ( SELECT DISTINCT user FROM table WHERE event_name = \u0026#39;click\u0026#39; ) SELECT t1.user, CASE WHEN t2.user IS NOT NULL THEN \u0026#34;true\u0026#34; ELSE \u0026#34;false\u0026#34; END AS user_did_click FROM table as t1 LEFT JOIN users_who_clicked as t2 ON t1.user = t2.user GROUP BY 1, 2 QUERY RESULT    user user_did_click     12345 true   23456 false   34567 true    With the LEFT JOIN, we take all the user IDs in the main table, and then we take all the rows in the users_who_clicked table (that we created in the WITH...AS tutorial above). For all the rows where the user ID from the main table has a match in the users_who_clicked table, we populate a new column named user_did_click with the value true.\nFor all the user IDs that do not have a match in the users_who_clicked table, the value false is used instead.\nUSE CASE: Simple segmenting Let\u0026rsquo;s put a bunch of things we\u0026rsquo;ve now learned together, and replicate some simple segmenting functionality from Google Analytics in BigQuery.\nWITH engaged_users_table AS ( SELECT DISTINCT event_date, user_pseudo_id FROM `dataset.analytics_accountId.events_20190922` WHERE EVENT_NAME = \u0026#34;user_engagement\u0026#34;), pageviews_table AS ( SELECT DISTINCT event_date, event_params.value.string_value AS page_url, user_pseudo_id FROM `simoahava-com.analytics_206575074.events_20190922` AS t, UNNEST(t.event_params) AS event_params WHERE event_params.key = \u0026#34;page_location\u0026#34; AND event_name = \u0026#34;page_view\u0026#34;) SELECT t1.event_date, page_url, COUNTIF(t2.user_pseudo_id IS NOT NULL) AS engaged_users_visited, COUNTIF(t2.user_pseudo_id IS NULL) AS not_engaged_users_visited FROM pageviews_table AS t1 LEFT JOIN engaged_users_table AS t2 ON t1.user_pseudo_id = t2.user_pseudo_id GROUP BY 1, 2 ORDER BY 3 DESC   Let\u0026rsquo;s chop this down to pieces again.\nIn the query above, we have two table aliases created. The first one named engaged_users_table should be familiar - it\u0026rsquo;s the query we built in tip #3.\nThe second one named pageviews_table should ring a bell as well. We built it in tip #5.\nWe create these as table aliases so that we can use them in the subsequent join.\nNow, let\u0026rsquo;s look at the rest of the query:\nSELECT t1.event_date, page_url, COUNTIF(t2.user_pseudo_id IS NOT NULL) AS engaged_users_visited, COUNTIF(t2.user_pseudo_id IS NULL) AS not_engaged_users_visited FROM pageviews_table AS t1 LEFT JOIN engaged_users_table AS t2 ON t1.user_pseudo_id = t2.user_pseudo_id GROUP BY 1, 2 ORDER BY 3 DESC Focus on the part after the FROM clause. Here, we take the two tables, and we LEFT JOIN them. The pageviews_table is the \u0026ldquo;left\u0026rdquo; table, and engaged_users is the \u0026ldquo;right\u0026rdquo; table.\nLEFT JOIN takes all the rows in the \u0026ldquo;left\u0026rdquo; table and the matching rows from the \u0026ldquo;right\u0026rdquo; table. The matching criterion is established in the ON clause. Thus, a match is made if the user_pseudo_id is the same between the two tables.\nTo illustrate, this is what a simple LEFT JOIN without any calculations or aggregations would look like:\n  Here you can see that on rows 1, 9, and 10 there was no match for the user_pseudo_id in the engaged_users table. This means that the user who dispatched this pageview was not considered \u0026ldquo;engaged\u0026rdquo; in the date range selected for analysis.\nIf you\u0026rsquo;re wondering what COUNTIF does, then check out these two statements detail:\nCOUNTIF( t2.user_pseudo_id IS NOT NULL ) AS engaged_users_visited, COUNTIF( t2.user_pseudo_id IS NULL ) AS not_engaged_users_visited The first one increments the engaged_users_visited counter by one for all the rows where the user visited the page AND had a row in the engaged_users table.\nThe second one increments the not_engaged_users_visited counter by one for all the rows where the user visited the page and DID NOT have a row in the engaged_users table.\nWe can do this because the LEFT JOIN leaves the t2.* columns null in case the user ID from the pageviews_table was not found in the engaged_users_table.\nSUMMARY Phew!\nThat wasn\u0026rsquo;t a quick foray into BigQuery after all.\nHowever, if you already possess a rudimentary understanding of SQL (and if you don\u0026rsquo;t, just take a look at the Mode Analytics tutorial), then the most complex things you need to wrap your head around are the ways in which BigQuery leverages its unique columnar format.\nIt\u0026rsquo;s quite different to a relational database, where joins are the most powerful tool in your kit for running analyses against different data structure.\nIn BigQuery, you need to understand the nested structures and how to UNNEST them. This, for many, is a difficult concept to grasp, but once you consider how UNNEST and CROSS JOIN simply populate the table as if it had been a flat structure all along, it should help you build the queries you want.\nAnalytic functions are arguably the coolest thing since Arnold Schwarzenegger played Mr. Freeze. They let you do aggregations and calculations without having to build a complicated mess of subqueries across all the rows you want to process. We\u0026rsquo;ve only explored a very simple use case here, but in subsequent articles we\u0026rsquo;ll definitely cover these in more detail.\nFinally, huge thanks to Pawel for providing the SQL examples and walkthroughs. Simo contributed some enhancements to the queries, but his contribution is mainly editorial. Thus, direct all criticism and suggestions for improvement to him, and him alone.\nIf you\u0026rsquo;re an analyst working with Google Analytics and thinking about what you should do to improve your skill set, we recommend learning SQL and then going to town on either any one of the copious BigQuery sample datasets out there, or on your very own App + Web export.\nLet us know in the comments if questions arose from this guide!\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/adservice/",
	"title": "Adservice - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Adservice custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description This template loads the Adservice SDK on the site. You can use it to send both RoutedContainer and RoutedLastClick calls. You can also add the optional order_id and pricevariable parameters.\nRelease notes    Date Changeset     2 October 2019 Update logo and icon.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/appnexus/",
	"title": "AppNexus - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The AppNexus custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Gallery entry   GitHub repo      Description This template implements the AppNexus pixel on the website. You can choose between a segment pixel and a conversion pixel.\nRelease notes    Date Changeset     2 October 2019 Update logo and icon.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/custom-templates/conductrics/",
	"title": "Conductrics - Custom Tag Template",
	"tags": [],
	"description": "",
	"content": "   The Conductrics custom tag template is an unofficial tag template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Vendor documentation   Gallery entry   GitHub repo      Description This template loads the main SDK for Conductrics. Provide the template with the fully qualified URL from where Conductrics instructs you to load the library.\nRelease notes    Date Changeset     2 October 2019 Add terms of service.   16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/analytics/introducing-tag-manager-template-gallery/",
	"title": "Community Template Gallery For Google Tag Manager - Introduction And Tips For Use",
	"tags": ["google tag manager", "custom templates"],
	"description": "Tips for using the Tag Manager Template Gallery for Google Tag MAnager.",
	"content": "Since the introduction of custom templates in May 2019, the community (myself included) has been anxiously waiting for some official solution for curating and distributing templates created by the community.\nNow, finally, we have it. It\u0026rsquo;s called the Community Template Gallery! Read Google\u0026rsquo;s announcement in this blog post.\n  I\u0026rsquo;m not going to go over the basics in this article, since Google\u0026rsquo;s own documentation stands fine on its own feet. However, I do want to discuss some of the ways in which you can streamline updating and sharing those lovely templates you\u0026rsquo;ve created.\nDon\u0026rsquo;t forget that there are other sources for templates as well, namely:\n GTM Templates - open-source template library for community templates (see here) for more information. GoogleTagManagerTemplates - GitHub repository for community templates.  What is the Community Template Gallery? The Community Template Gallery is Google Tag Manager\u0026rsquo;s own, integrated library for uploading and distributing templates across users, accounts, and containers.\nAnybody in the community can submit a template to the gallery, and thus get their work in the reach of all Google Tag Manager users around the world.\n NOTE! Not all templates will be published. Your template must follow the style guide, and if there\u0026rsquo;s a template with similar functionality already in the gallery, your template might not get published.\n   Templates can be added to a container by clicking the Search gallery button next to the Tag Templates and Variable Templates listings in the Templates section of the container.\n  This opens an overlay where you can do string searches for the template you\u0026rsquo;d like to add to your container.\n  The template overlay lets you add the template to the workspace by clicking the respective button (1). Here, you can take a final look at the permissions before choosing to add the template to the current workspace.\n  The little arrow link in the corner opens the template page in the gallery itself (2).\nYou can click the author name to access their GitHub profile (3). The green checkmark indicates that this organization has been verified in GitHub.\nThe permissions overview is important, as it will let you evaluate what types of permissions the template might require (4). The red danger and the grey warning icons alert you that the template will load potentially hazardous code, and you should only add the template if you trust the author.\nThere are direct links to the template repository in GitHub, and to the issues section of the repository (5).\n  You can also visit the Community Template Gallery directly at the URL https://tagmanager.google.com/gallery/. It has all the same features as the overlay.\nGitHub As you might have noticed above, to submit a template you need to create a GitHub repository for it.\n  GitHub is a community for managing and sharing code. It\u0026rsquo;s used to manage and coordinate projects running on the Git version-control system.\nGitHub is free to use, although there is a paid tier as well, which gives you benefits such as the ability to create private repositories.\nFor the purposes of the Community Template Gallery, each template requires its own public repository in GitHub. This might cause some overhead for users who submit lots of templates, but it makes it easier for Google Tag Manager to parse the correct template every time, to manage metadata, and to have the Issues feature (basically bug and feature tracking) be devoid of ambiguities, as there is only one template to which the issues relate.\n  To be a valid template for the gallery, the repository requires the following three files in the root:\n   File Description     template.tpl You get this when you export a template from Google Tag Manager. NOTE! It must be named template.tpl. All other names will break the upload.   metadata.yaml Template metadata, where you can provide things like links to the documentation and, importantly, the changelog of commits where you\u0026rsquo;ve made changes to the template and want to update them to the template users.   LICENSE LICENSE text file that uses the Apache 2.0.    These three files must be in the repository root or you will not be able to upload the template to the Community Template Gallery.\nThe repository can have other files as well (such as a README), but these three are mandatory.\nEasiest way to get started is to create a public repository, and then use the file upload option to add files to the repository.\nOnce you\u0026rsquo;re comfortable with GitHub, I recommend installing the git command line tools and working from your terminal.\ntemplate.tpl For the template file to be accepted into the gallery, you must check the box named \u0026ldquo;Agree to Community Template Gallery Terms of Service\u0026rdquo; available in the Info tab of the template editor.\n  You\u0026rsquo;ll also need to manually edit the template.tpl file (I\u0026rsquo;m sure this will be replaced by a UI functionality soon) to add categories to it. Categories will be used to categorize (d\u0026rsquo;oh) templates in the gallery UI.\n{ \u0026#34;displayName\u0026#34;: \u0026#34;My template\u0026#34;, \u0026#34;categories\u0026#34;: [\u0026#34;AFFILIATE_MARKETING\u0026#34;, \u0026#34;ADVERTISING\u0026#34;], // etc... } Read more about the available categories here.\nOther than that, there\u0026rsquo;s not much to say about the template file. You\u0026rsquo;ll need to export the file from GTM and make sure it\u0026rsquo;s renamed to template.tpl before uploading it to your GitHub repository.\n  metadata.yaml The metadata file is provided in the YAML format (short for YAML Ain\u0026rsquo;t Markup Language). The only requirement is that it has the versions key populated with the hashes of all the commits you want to include as version changes in the template gallery entry.\nHere\u0026rsquo;s a sample metadata.yaml file:\nhomepage:\u0026#34;https://www.simoahava.com/\u0026#34;documentation:\u0026#34;https://www.simoahava.com/analytics/create-facebook-pixel-custom-tag-template/\u0026#34;versions:# Latest version- sha:d7bad4f7a6e39c72dbc72afc7773d7fb8fdc358cchangeNotes:Updatetemplatemetadata.# Older versions- sha:ff7bf31a0777e22a370d6371607022fe2a6d1b7cchangeNotes:|2Addsupportfor\u0026#34;consent\u0026#34;command.Addsupportforusingavariabletopopulateobjectproperties.- sha:e1206f9c677fe67348c6678c4669952cb0c0b97bchangeNotes:Initialrelease. Use the homepage key to indicate the website home page URL for the template, and use the documentation key to indicate where documentation can be found.\nThe versions key is where you nest all the commits to the template.tpl file that you want to consider as new versions of the template.\nEach version comprises of two keys. The first key, sha, is mandatory, and corresponds to the commit hash for the commit where you made the final changes to the template.tpl file. The second key, changeNotes, is optional, and is the place where you add details about what was changed.\nThe versions should be listed so that the newest version is the first entry, and the older versions are listed after this from newest to oldest.\nYou can follow my example and use the comment # Latest version to visually separate the older versions (listed after the # Older versions comment).\nSo how do you find this sha hash? Well, whenever you make a commit to GitHub, the commit will receive a hash that will uniquely identify the commit.\nThus, whenever you make a commit that includes changes to template.tpl, and you consider this commit to be important enough to warrant a new version of the template, you need to browse to the commits of your GitHub repository, find the commit in question, and copy its hash.\nEasiest way to find the commits is to take your GitHub repo URL and add /commits/ to the end, e.g. https://www.github.com/gtm-templates-simo-ahava/user-distributor/commits/.\nThen, find the commit whose hash you want, and click the clipboard button to copy the hash to the clipboard.\n  Finally, add a new item under versions in the metadata.yaml with the hash under the sha key and any change notes under the changeNotes property. You can specify multi-line notes with the pipe symbol |2 as in the example in the beginning of this chapter.\n Note! You don\u0026rsquo;t have to add every single commit to template.tpl as a new version. You might do incremental updates first, and then when you\u0026rsquo;re ready with a version update, you copy the hash of the last commit, as it would include all the previous commits as well.\n At its bare minimum, the metadata.yaml file just needs the sha keys, so this would also work:\nversions:- sha:abcd12345- sha:bcde23456- sha:cdef34567 The purpose of versioning the templates like this is that in time, the gallery will introduce features that allow uses who have imported the templates to be notified of changes to the template sources. For this to work, the template author must add versioning information to the metadata file.\nLICENSE You can find the Apache 2.0 license text here: https://www.apache.org/licenses/LICENSE-2.0.txt. Save it as LICENSE in your repository, and make sure you edit the copyright text at the end.\n  GitHub tip #1: Add new template to the gallery When you want to add a new template to the gallery, this is (roughly) the process you would follow.\nSTEP 1: Create a new repository in GitHub. Make sure it\u0026rsquo;s Public and that it has the Apache 2.0 license added. If you\u0026rsquo;re creating templates under an organization, make sure the repository is created in the correct place.\n  STEP 2: Create the template in Google Tag Manager. Make sure you\u0026rsquo;ve checked the box for the Community Gallery Terms of Service in the Info tab. Export the template as a template.tpl file.\nSTEP 3: In the GitHub repo, choose Upload files to upload your template file into GitHub. Commit the change with a clear commit name, e.g. \u0026ldquo;Initial publish of template.tpl\u0026rdquo;. Commit directly to the master branch.\n  STEP 4: Visit the /commits/ page of the repo, and copy the commit hash of the latest commit to the clipboard.\n  STEP 5: Back in the repo root, click Create new file. Name the file metadata.yaml. The contents need to be at least the following:\nversions:- sha:paste_commit_hash_here You might as well add the homepage and documentation keys while you\u0026rsquo;re at it.\n  Make sure you commit the new file directly to the master branch.\n  STEP 6: Head over to the gallery submission form. Add the URL to the repo root in the URL field, and hit Submit. If all validates OK, you should get a response that the template has been submitted. If there was a problem, there error message will tell you where the validation failed.\n  That\u0026rsquo;s it! Your template should appear in the gallery after it goes through some further validation.\n NOTE! Just a reminder that not all templates will be published. Your template must follow the style guide, and if there\u0026rsquo;s a template with similar functionality already in the gallery, your template might not get published.\n Finally, it might make sense to clone the repo to your local machine just so that you can use the git command line tools instead.\nGitHub tip #2: Update the template Once the template has been submitted to the gallery, you update it by committing changes to the template.tpl file.\nOnce you\u0026rsquo;ve got a commit of the template.tpl file merged with master that you want to submit to the Community Template Gallery as a new version of the template, you need to copy its commit hash, and add it as a new entry in the metadata.yaml file. It needs to become the new first entry under the versions key.\n  Once you\u0026rsquo;ve updated the metadata.yaml file, you need to commit that and make sure your changes are merged with the master branch.\nThen you\u0026rsquo;ll just need to wait for the gallery to pull in your latest changes, validate them, and update the template listing in the gallery with the changes.\nThe key thing here is the process:\n Make incremental changes to template.tpl. Once happy and ready to create a new version of the template in the gallery, commit the final version of template.tpl, merge it to master, and copy the commit hash. Edit metadata.yaml and add this commit hash as the newest version of the template. Commit the changes to metadata.yaml and make sure it\u0026rsquo;s merged to master. Wait.  GitHub tip #3: Set up an organization and submodules If you\u0026rsquo;re providing a lot of templates, it might make sense to logically order them in GitHub as an organization. After creating an organization, any public templates you want to upload would be created as repositories within this organization, giving you a single point of origin for managing your templates.\nAnother benefit of creating an organization is that you can verify the organization (by showing GitHub that you own the domains referenced in the organization profile), and as a consequence get the green checkmark to show next to your organization name in the gallery listing.\n  Here\u0026rsquo;s what my template organization looks like:\n  Another thing you might want to do is create a new \u0026ldquo;roll-up\u0026rdquo; repository, to which you add all the template repositories as submodules. This is, again, a way to logically manage your templates. The use case is similar to organizations, so if you\u0026rsquo;re already grouping the templates together with an organization, using submodules might not be at all useful. Nevertheless, here\u0026rsquo;s what my \u0026ldquo;roll-up\u0026rdquo; repository looks like. See how each template repo has its own folder within? Those are submodules.\n  These are just quality-of-life things, and definitely not required to get set up with the template gallery.\nSummary The Community Template Gallery is finally here! We now have a way to distribute the templates we\u0026rsquo;ve worked on - how exciting.\nDon\u0026rsquo;t be turned off by the seemingly complex way of managing your custom templates via GitHub. First of all, if you\u0026rsquo;ve managed to create a template and add all the required metadata, chances are you\u0026rsquo;re already proficient with GitHub and should find it trivial (if somewhat time-consuming) to operate.\nAnd even if GitHub is a new thing for you, take this as an excellent lesson in version control. As your files are directly used by Google Tag Manager, being able to discretely annotate each version is really important.\nSimilarly, being able to curate community-submitted issues on a template-by-template and commit-by-commit basis is useful not just for template gallery but for you as well.\nI\u0026rsquo;m sure our community will rise up to the occasion and write little tools that will make it even easier to update the templates (a command-line tool to copy the hash from the most recent commit and add it as a new entry in the metadata.yaml? Any takers? Ping me - I\u0026rsquo;ve got ideas).\nTime will tell how popular the gallery will be, but I have a hunch it will be the main way for brands to distribute their script snippets in the near future. I see no reason why a vendor should offer a block of custom code to be copy-pasted into a Custom HTML tag anymore, especially after the introduction of the template gallery. Nor do I see any reason for them to share their templates only on their own site, when they can get the additional brand visibility by offering the templates via the gallery.\nIn any case, custom templates continue firmly at the top position of my \u0026ldquo;most important and most inspiring Google Tag Manager feature of the recent years\u0026rdquo; list, and the Community Template Gallery is an excellent, complementary feature to support the template suite of features.\nWhat do you think about the Community Template Gallery? Do you have ideas for improvement? Let your voice be heard in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/still-running-google-tag-manager-tags/",
	"title": "&#39;Still Running&#39; Google Tag Manager Tags – What To Do?",
	"tags": ["google tag manager", "still running", "preview", "debug"],
	"description": "What does it mean and what can you do when Google Tag Manager reports a tag as &#39;Still Running&#39; in Preview mode?",
	"content": "Sometimes, in Google Tag Manager\u0026rsquo;s Debug mode, you\u0026rsquo;ll see tags appear with the status Still Running, and you\u0026rsquo;ll (eventually) notice that these tags are not doing what they are supposed to be doing.\n  When you see this message on a tag, it technically means this:\nThe tag failed to signal Google Tag Manager that it is \u0026ldquo;done\u0026rdquo;.\nThe technical explanation is, naturally, too simple to be useful. In this article, I\u0026rsquo;ll explore what \u0026ldquo;done\u0026rdquo; means, and how especially Google Analytics tags manifest this behavior.\nIf you\u0026rsquo;re only interested in the latter, feel free to jump to  the relevant section of this article.\nWhen is a tag \u0026ldquo;done\u0026rdquo;? By default, a tag is done when its code is executed, and the browser is ready to move on to the next script block in queue. This can happen if the browser reaches the end of the script in turn to be executed, or if the script throws an error that is not caught.\nFor example, take a Custom HTML Tag that looks like this:\n\u0026lt;script\u0026gt; console.log(\u0026#39;Hello!\u0026#39;); \u0026lt;/script\u0026gt; When it\u0026rsquo;s time to fire this tag, Google Tag Manager injects this tag to the end of \u0026lt;body\u0026gt; in the document object model, and then the browser proceeds to execute the code within the \u0026lt;script\u0026gt; block.\nAs this is all run synchronously, by the time the browser has written Hello! to the console, it\u0026rsquo;s ready to move to the next script waiting to be executed, and GTM will thus know that this tag is now \u0026ldquo;done\u0026rdquo;. Preview mode will show that the tag has Succeeded.\nWhat about if we have an asynchronous operation in the middle of the tag?\nLet\u0026rsquo;s simulate async with a simple timeout.\n\u0026lt;script\u0026gt; window.setTimeout(function() { console.log(\u0026#39;Hello!\u0026#39;); }, 5000); \u0026lt;/script\u0026gt; This time, when the tag\u0026rsquo;s JavaScript is executed, the browser waits five seconds before writing to the console. So when does GTM signal tag completion? No, not after 5 seconds, but rather after the browser reaches the end of the setTimeout expression.\nWith asynchronous operations, Google Tag Manager doesn\u0026rsquo;t wait. As with synchronous scripting, as soon as the browser reaches the end of the code block, the tag has Succeeded, regardless of how many asynchronous operations are still waiting for completion.\nYou can instruct Google Tag Manager to wait for an asynchronous operation to complete by utilizing callbacks. There are four different callbacks you\u0026rsquo;d use, depending on use case, to signal Google Tag Manager that a tag has run its course:\n  window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}}) - this is the callback you\u0026rsquo;d use in Tag Sequencing to signal the next tag in the sequence that the current tag has completed successfully. It requires the Container ID and HTML ID Built-in Variables to be activated.\n  window.google_tag_manager[{{Container ID}}].onHtmlFailure({{HTML ID}}) - this is the callback you\u0026rsquo;d use in Tag Sequencing to signal the next tag in the sequence that the current tag failed in its execution.\n  data.gtmOnSuccess() - this is the callback you\u0026rsquo;d use in custom tag templates to signal the tag has completed successfully.\n  data.gtmOnFailure() - this is the callback you\u0026rsquo;d use in custom tag templates to signal the tag has failed in its execution.\n  So, if we wanted the browser to wait for the setTimeout call to complete, we\u0026rsquo;d use it like this:\n\u0026lt;script\u0026gt; window.setTimeout(function() { console.log(\u0026#39;Hello!\u0026#39;); window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}}); }, 5000); \u0026lt;/script\u0026gt; If invoking the callback with a significant delay (e.g. 5 seconds), you might see the tag as Still Running in Preview mode only for as long as the timeout lasts. As soon as the success callback is invoked, the status will change from Still Running to Succeeded (or Failure if that\u0026rsquo;s the callback you ran with).\nSo why do tags stay in \u0026ldquo;Still running\u0026rdquo; state? If you recall from the previous chapter, if the tag has no callbacks, GTM will signal completion once it reaches the end of the code block or if the code block throws a syntax error. Thus there\u0026rsquo;s no actual way for a tag like this to signal it is \u0026ldquo;Still running\u0026rdquo;, as if the browser never reaches the end of the code block and the code never throws an error, it means the code is stuck in an infinite loop or memory leak, and the whole browser would freeze as a result.\nHowever, as soon as you add either the google_tag_manager[...] callbacks in a Custom HTML tag or data callbacks in a custom tag template, GTM will actively wait for these callbacks to execute before signalling the tag is done.\nErgo, if the browser never reaches these callbacks when they are nevertheless present in the code, GTM will never receive the completion signal and will remain in \u0026ldquo;Still running\u0026rdquo; state.\nHere\u0026rsquo;s an example that will result in the tag being stuck in \u0026ldquo;Still running\u0026rdquo; limbo.\n\u0026lt;script\u0026gt; if (false) { window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}}); } \u0026lt;/script\u0026gt; The browser will never reach the expression within the if {...} block because false will never evaluate to true. Thus, GTM has detected a success callback written in the code block, and it anxiously waits for it to be executed, but it never will. Thus in GTM\u0026rsquo;s view, the tag is perpetually running.\nYou can replace the callback with onHtmlFailure() to achieve the same result. It\u0026rsquo;s enough for either callback to be present for Google Tag Manager to actively wait for resolution.\nWith tag templates, it\u0026rsquo;s the same thing. If you run this:\nif (false) { data.gtmOnSuccess(); }  This template would always have its tags be stuck in \u0026ldquo;Still running\u0026rdquo; state, since there\u0026rsquo;s no way to reach that callback.\nThe lesson to be learned here?\nWhenever you use callbacks in Custom HTML tags, or whenever you create custom tag templates, make sure at least SOME callback is always reached.\nGoogle Analytics tags \u0026ldquo;Still running\u0026rdquo; Many of us have experienced the \u0026ldquo;Still running\u0026rdquo; phenomenon with Google Analytics tags.\nBased on what we learned in the previous chapters, we\u0026rsquo;re now at an impasse. Google Analytics tags are not Custom HTML tags that you can fix the callbacks in, nor are they custom tag templates that you can modify to have the callback be invoked. So what are they?\nThey are tag templates, where the success callback is called as soon as the Google Analytics tag\u0026rsquo;s hitCallback method is reached. This hitCallback is automatically added by Google Tag Manager (preserving any hitCallback you add manually to the tag).\n  So, when a Google Analytics tag is \u0026ldquo;Still running\u0026rdquo;, it means its hitCallback was never reached.\nHow does this happen? Here are the three most common reasons.\n1. Using an incorrect variable in the Tracking ID field The Tracking ID field in your Google Analytics tags (in Google Tag Manager) is reserved for the tracking ID. The tracking ID is your property\u0026rsquo;s UA-XXXXXX-Y string.\nThus, the only thing you can have in a Tracking ID field is that string, or a variable which returns that string and nothing else.\nIt\u0026rsquo;s still extremely common to see people add a Google Analytics Settings variable to the Tracking ID field. This results in the Tracking ID field value resolving to an object (of your Google Analytics settings) rather than the required UA-XXXXX-Y string.\n  The result is that a tracker is created with incorrect settings, and the hitCallback is never reached.\nBest way to debug this is to use the Google Analytics Debugger browser extension, and checking the console output. If you see something like this, your setup is broken:\n  How to fix this? Simple. Never use a Google Analytics Settings variable anywhere else except a Google Analytics Settings field. Always check and double-check that the Tracking ID field contains a value or variable that resolves to UA-XXXXXX-Y.\n2. The ga() method is hijacked by some other code running on the site If some other tool overwrites the global ga() method, it means that there is no longer such a thing as a \u0026ldquo;Google Analytics hit\u0026rdquo;, or \u0026ldquo;Google Analytics tracker\u0026rdquo;, and as a consequence the hitCallback is never reached. Google Tag Manager sees the callback in the code and thus waits for it, but because the global method no longer works, the callback is never reached.\n  Easiest way to verify this is to open the JavaScript console of your page, and running the following command:\nconsole.log(window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]].answer);  If the method has not been overwritten, you should see the number 42 written into the console. This is a sign that the analytics.js library still works with the global method.\nIf you see anything else, most likely either undefined or an error, it would mean that there\u0026rsquo;s something wrong with the global method.\nNOTE! Ad blockers should not have a say here. If the Google Analytics endpoint is blocked, the hitCallback would still be reached, and if the entire analytics.js library is blocked, the ga() command would never be invoked in the first place.\nHow to fix this? You can change the global method name to e.g. my_ga in your Google Analytics tags (in GTM) by expanding Advanced Configuration and updating the Global Function Name field. This must be done consistently in all your tags, so you might want to use a Google Analytics Settings variable.\n  If you\u0026rsquo;re using analytics.js, you can rename the method name in the snippet.\n3. Nonexistent Optimize container loaded on the site This is super annoying. It\u0026rsquo;s a bona fide bug in my opinion, and I\u0026rsquo;ve reported it as such.\nIf you create a Google Optimize tag, it actually loads the Optimize container as a Universal Analytics plugin. The reason for this is that the plugin is used to dispatch experiment data to Google Analytics.\nThe problem is that in case the Optimize container does not exist (you\u0026rsquo;ve deleted it or you had a typo in the container ID), the plugin load is never resolved and the processing of the ga() queue is halted.\nThis, in turn, means that any Google Analytics tags firing after the Optimize container tag has fired will fail to execute. Because the plugin is never loaded, the queue is perpetually waiting for the plugin load to resolve.\nYou can see this by using the Google Analytics Debugger extension again. In the console, you\u0026rsquo;ll see something like this:\n  How to fix this? Best way to fix this is to avoid deleting Optimize containers when they are no longer in use. That way you\u0026rsquo;ll avoid stalling the site.\nNext, make sure you\u0026rsquo;ve got the container ID correctly written in the tag.\nFinally, a good practice for site load as well is to only load the Optimize container on pages where you are running experiments. It doesn\u0026rsquo;t make sense to introduce the extra load on pages where the container is not used.\nSummary For all the good that Google Tag Manager\u0026rsquo;s debug mode does, it does leave something to be desired in terms of verbosity and identifying where the problem lies.\nNative templates especially are black boxes. There\u0026rsquo;s no way to know why a Google Analytics tag signals completion or timeout without trial-and-error testing.\nWith the instructions provided in this article, you should be able to debug and fix those pesky \u0026ldquo;Still running\u0026rdquo; problems you might have with your tags.\nLet me know in the comments if you run across a use case not covered by this article. I\u0026rsquo;ll happily update the text with kudos to you for helping to identify an unexplored problem case!\n"
},
{
	"uri": "https://www.simoahava.com/tags/preview/",
	"title": "preview",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/still-running/",
	"title": "still running",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/user-distributor-tag-template/",
	"title": "User Distributor Custom Tag Template For Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "tag templates"],
	"description": "Custom tag template for Google Tag Manager. Sample your visitors, or distribute them to different groups of different (or similar) weights.",
	"content": "I\u0026rsquo;ve enjoyed working with custom templates for Google Tag Manager. A lot. So much so that whenever the need to add some custom code to a container emerges, my first thought is how to turn that into a custom template.\nGoogle has been forthcoming in introducing new APIs steadily, and I think the variety of things you can do with template is improving with every new API release.\nIn this article, I\u0026rsquo;ll show you how to use a simple tag template for distributing your users to groups, based on a random split.\n  You can download and/or install the template here.\nHow it works When you create a tag from the template, you have two options.\n1. Single distribution If you choose to create a single distribution, you indicate an integer percent value between 1 and 100 (inclusive). When the tag fires, a randomizer will pull a number in that range, and if the number falls into the range you gave, a cookie named _gtm_group will be written with the value \u0026quot;true\u0026quot;.\n  You can use this cookie to sample your visitors, for example, to keep your data collection below quotas or limitations imposed by the platforms you use.\nIf the number does not fall into the distribution, the cookie will be set with the value false. This is to make sure the cookie is set just once per browser.\nHow to use If you set this tag to fire with the All Pages trigger, then all your users will be assigned to either the true bucket or the false bucket. Then, you can create a First Party Cookie variable for _gtm_group like this:\n  You can check for this cookie value in your triggers to make sure your tags only fire for those included in the group:\n{{First party cookie}} equals true\nOr, conversely, you can switch the value check to false to fire your tags only for those who were \u0026ldquo;sampled out\u0026rdquo;.\n2. Multi distribution The multi distribution lets you create different groups, where each group has a probability weight of including the user. You add the groups as rows in the table, where the Group value is what the _gtm_group cookie will receive as a value, and Distribution is the percentage weight (out of 100) you want to assign to the group.\n  The distributions\u0026rsquo; sum should not exceed 100. The groups are processed from top to bottom, so if the sum of distributions is over 100, the groups at the end of the table will not get their distribution probabilities met.\nThe sum can be under 100. It just means that there will be a probability distribution that does not set the cookie at all.\nFor example, let\u0026rsquo;s say you have the following items:\n   Group value Distribution     control 10   variation1 20   variation2 20   variation3 20   variation4 20    When the tag fires, it randomizes an integer between 1 and 100 (inclusive). This number is then checked against the groups, starting from the top, and calculating the distribution as increments until 100 is reached.\nFor example, if the random number is 15., it gets included in the group variation1, because that group would include values between 11 and 30.\nIf the random number is 68, it gets included in the group variation3, because that group would include values between 51 and 70.\nIf the random number is 93, it doesn\u0026rsquo;t get included in any group and the cookie is not set.\nHow to use Similar to the single distribution, you can create a First Party Cookie and use that to selectively fire your tags depending on which group the user is in. You can also use this to create distributions for A/B-testing, in case you want to control what type of content to show to users in different groups.\nCookie settings By expanding the Cookie settings group, you can change details about the cookie that is set.\n  The default values are:\nCookie name: _gtm_group\nCookie domain: auto\nCookie maximum age (in days): 365\nHaving auto in the cookie domain fields means GTM will attempt to write the cookie on the highest domain name in the hierarchy it can. So if the user is browsing sub.domain.co.uk, GTM will write the cookie on domain.co.uk.\nNote that if you change the cookie name, **you must edit the template permissions to allow GTM to read and write the new cookie name.\n  Summary I hope you find this template useful. It\u0026rsquo;s not an exact science - the random number distribution isn\u0026rsquo;t necessarily the most robust thing in the world, and as with anything random, you need enough data to avoid outliers in the distribution.\nBut especially for sampling, it should be close enough for comfort.\nLet me know in the comments if you have suggestions for improving the template!\n"
},
{
	"uri": "https://www.simoahava.com/custom-templates/enhanced-ecommerce-object-builder/",
	"title": "Enhanced Ecommerce Object Builder - Custom Variable Template",
	"tags": [],
	"description": "",
	"content": "   The Enhanced Ecommerce Object Builder custom variable template is a variable template for Google Tag Manager\u0026rsquo;s community template gallery.\n   Resource     Blog post   Gallery entry   GitHub repo      Description The Enhanced Ecommerce Object Builder template can be used to build a fully functional Enhanced Ecommerce object.\nWhen you build a variable with this template, you populate the fields with whatever hard-coded values or Google Tag Manager vairables you consider important. Then, you add the variable to the \u0026ldquo;Ecommerce\u0026rdquo; settings of your Google Analytics tag (or, in some cases, your Google Analytics Settings variable).\nThe variable is added by unchecking \u0026ldquo;Use Data Layer\u0026rdquo;, and choosing the variable from the drop-down list that appears.\nYou can find more detailed instructions in the blog post.\nRelease notes    Date Changeset     16 September 2019 Initial release.    "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/click-variables-undefined/",
	"title": "#GTMTips: When Click Variables Are Undefined",
	"tags": ["google tag manager", "gtmtips", "click variables", "click tracking"],
	"description": "If Click Variables show up as undefined in Google Tag Manager&#39;s Preview mode, you can follow the steps in this guide to fix the issue.",
	"content": "A surprisingly common question in forums and communities seems to be why the built-in Click variables show up as undefined in Google Tag Manager\u0026rsquo;s Preview mode, even if you click around the site.\nIn this article, I\u0026rsquo;ll walk you through some of the reasons why this might happen.\nTip 104: What to do when Click variables are undefined   Here\u0026rsquo;s the situation: you want to create a Click trigger for your tags, but in order to do so, you\u0026rsquo;d need to see what values the built-in Click variables (e.g. Click Classes, Click ID) get. Thus, it\u0026rsquo;s a chick-and-the-egg situation - you can\u0026rsquo;t create the trigger until you know what to track, and you can\u0026rsquo;t track until you create the trigger.\nWell, here\u0026rsquo;s the thing.\nYou MUST create a trigger for the Click variables to populate with values.\nThe variables will not start collecting data until you enable one of the following triggers:\n  A Click - All Elements trigger - when you want to track clicks on any element.\n  A Click - Just Links trigger - when you want to track clicks on link elements (\u0026lt;a href=\u0026quot;...\u0026quot;\u0026gt;).\n  A Form Submit trigger - when you want to track submissions of a form element.\n  An Element Visibility trigger - when you to track the visibility or appearance of an HTML element.\n  Naturally, if you want to track clicks, you\u0026rsquo;ll need to create either an All Elements or a Just Links trigger, as both track clicks. The only difference is that the latter is more restrictive, since it only tracks clicks on link elements or nested elements within a link node.\nSo, the first thing to check if Click variables are undefined is whether you\u0026rsquo;ve remembered to create a Click trigger. Easiest thing to do is create a simple All Elements trigger with no restrictions.\nYou don\u0026rsquo;t even have to add it to a tag. Just by creating the trigger will GTM start listening to click events.\n  Once the trigger is enabled, you can enter Preview mode. At this point, any click on your web page should introduce a Click element into the Preview mode window. By selecting that Click element you should see the variables populate:\n  Which brings me to the next tip. If Click variables are undefined, make sure you\u0026rsquo;ve selected the Click event in the Preview mode message list. GTM Preview mode shows you the values of variables that existed at the time when the selected message was processed.\nAlways avoid debugging with Summary selected, as that simply shows you the latest values. This is irrelevant information if you want to know what values existed when a tag fired, for example, as you would need to select the trigger event for the tag to know that.\nThe final thing to check is whether you\u0026rsquo;ve incorrectly overwritten the dataLayer and thus broken GTM\u0026rsquo;s triggers.\n  A tell-tale sign is that there\u0026rsquo;s no \u0026ldquo;Page View\u0026rdquo; event in dataLayer, as this is automatically generated by the container snippet and would be overwritten with the dataLayer. However, if GTM\u0026rsquo;s container manages to load before the overwrite happens, you might still see the Page View event even if it\u0026rsquo;s absent from the dataLayer. This leaves you with a couple of options to verify if this is the case:\n  Open the JavaScript Console, and type window.dataLayer and press enter. The array should be empty or there should be no object with gtm.js as the value of the event key.\n  Open the JavaScript Console, and type window.dataLayer.push({event: 'debug'}) and press enter. If dataLayer is broken, you will not see an event with the name debug appear in the Preview mode message list.\n    If this is the case, you need to convert that overwrite into a safer format, mainly a dataLayer.push(). Follow this article for the steps on how to do so.\nSummary If Click variables are undefined, it\u0026rsquo;s almost always because you neglected to create the necessary trigger(s).\nHowever, there are some edge cases where you or your devs have inadvertently overwritten Google Tag Manager\u0026rsquo;s dataLayer, which results in GTM being unresponsive to dataLayer.push() commands, including those created by the triggers.\nWith the steps in this article, you should get your Click variables to populate with useful data in no time!\n"
},
{
	"uri": "https://www.simoahava.com/tags/click-tracking/",
	"title": "click tracking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/click-variables/",
	"title": "click variables",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/enable-bigquery-export-google-analytics-app-web/",
	"title": "Enable BigQuery Export For Google Analytics: App + Web Properties",
	"tags": ["google analytics", "firebase", "app+web", "bigquery"],
	"description": "How to enable BigQuery export for your Google Analytics App + Web property. This works in the free version of Google Analytics as well.",
	"content": " Update 7 October 2020: BigQuery Export can now be configured via the property settings of Google Analytics: App + Web, so you don\u0026rsquo;t need to follow the steps in this article. Check out Charles Farina\u0026rsquo;s guide for how to do this.\n Here\u0026rsquo;s yet another article inspired by the fairly recent release of Google Analytics: App + Web properties. This new property type surfaces Firebase\u0026rsquo;s analytics capabilities for websites as well, when before they were restricted to mobile apps only (see my guides for iOS and Android).\nEven though the feature set of Google Analytics for Firebase is still somewhat bare in the user interface, here\u0026rsquo;s a perk which might push you over the edge and give the new measurement model a shot.\nBigQuery export is available with no extra cost!\nOK, there\u0026rsquo;s one caveat: you will of course need to pay for BigQuery usage and you\u0026rsquo;ll need to upgrade to the Firebase Blaze plan. But you won\u0026rsquo;t have to dish out some $150K a year to have access to raw data, and the free tiers of Google Cloud are extremely generous, so you might end up not having to pay at all for this wonderful storage!\n   Note! There\u0026rsquo;s no saying this is a permanent state of affairs. Once Google Analytics: App + Web is out of beta, it\u0026rsquo;s possible some sort of tiered pricing is introduced. But let\u0026rsquo;s enjoy this while we can!\n The steps you\u0026rsquo;ll need to take to enable the export are outlined in this article. Remember to check out the BigQuery Export Schema for Firebase so you\u0026rsquo;ll know how the data will be aligned in the BigQuery table.\nStep 1: Check your Google Analytics: App + Web property First step is obvious - you need a Google Analytics: App + Web property to be able to export data out of it to Firebase.\nIf you haven\u0026rsquo;t created one yet, follow these steps to do so.\nTo verify it\u0026rsquo;s up and running and integrated to Firebase, go to the Firebase console, open your project, and go to Project Settings.\n  Click over to Integrations. You should see Google Analytics in the Enabled state, but click Manage to verify.\n  Here you should make sure all the details look correct. At this point, make a note of the Property ID in case you want to decide in which geographic location (e.g. US, EU) you want to create the BigQuery dataset in.\n  Now, head back to the Firebase Project Overview dashboard.\nStep 2: Create a dummy app This is kind of silly, but for the export to work, you need to create an app in the project. The app doesn\u0026rsquo;t have to do anything, nor do you have to verify it or validate it, it just needs to exist.\nThe reason for this is that the BigQuery export was created before the web stream concept was introduced with Google Analytics: App + Web, and in its current state, having just a web stream will not enable the export.\nI\u0026rsquo;m certain this will change at some point in the future.\nIn the dashboard, click the iOS button to create a new iOS app.\n  Give the app some bundle ID - it can be anything you want. I used com.example.dummyapp. Click Register app.\n  You can now ignore all the instructions and just click the Next button until you reach the final step. Here, click the Skip this step link to create the app.\n  Congratulations, you\u0026rsquo;ve created an iOS app (har har).\nStep 3: Upgrade to Blaze plan To be able to export the data to BigQuery, you\u0026rsquo;ll need to upgrade your Firebase account to use the Blaze (pay as you go) plan.\nClick the Upgrade link in the bottom of your Firebase navigation.\n  Next, click Select plan in the Blaze column.\n  Now, you will need to choose a Billing Account configured for your Google Cloud organization / login. If you don\u0026rsquo;t have one yet, you will be prompted to create it. You will need your credit card for this!\n  Click Continue and Purchase to confirm that you will use the selected billing account for any charges incurred by your Firebase and BigQuery usage.\nStep 4: (Optional) Create the BigQuery dataset In case you want to store the BigQuery data elsewhere than the United States, you can actually create the BigQuery dataset beforehand, choosing the data storage location that way.\nIf this is what you want to do, you\u0026rsquo;ll need to visit the BigQuery console for your project, so open the console in your browser.\nFirst, make sure you have the correct project selected from your project selector. The project should match the name of your Firebase project (www-simoahava-com in my case).\n  In the BigQuery project navigator you should find your project as well. The BigQuery project will have the same name as your Firebase project ID (simoahava-com for me).\nClick the project to open its datasets (there should be none).\n  Now, click CREATE DATASET in the right-hand side of the dataset explorer.\nIn the overlay that opens, you need to type in the Dataset ID. The Dataset ID must be named in the following format:\nanalytics_\u0026lt;GA property ID\u0026gt;\nThis is where the Property ID from step 1 comes into play, as you\u0026rsquo;ll need to name the dataset accordingly.\n If you do not name the dataset correctly, the data will not flow into it, and another dataset (one named correctly) will be automatically created for you in the United States.\n Now you can also set the location to your liking.\n  Click Create dataset at the bottom of the overlay when done.\nThis is what you should see in the dataset explorer view:\n  Step 5: Set up the export Finally, you can set up the export itself.\nIn the Firebase console, head on over to Project Settings and click to Integrations again. This time, in the BigQuery box, click Link.\n  Make sure you read the fine print in the first step, and click Next when adequately enlightened.\nIn the next step, you\u0026rsquo;ll have an overview of what is included in the export. If you want to include \u0026ldquo;advertising identifiers\u0026rdquo; in the export, be sure to check the box. It sounded nefarious to me so I chose not to do so.\nClick Link to BigQuery when ready.\n  The next view shows you that the link is now done. Make sure the toggle for your dummy app is ON in the Analytics card. This enables the data export from your web stream to BigQuery.\n  And that\u0026rsquo;s it! The day after creating this link, you should see a new table in the dataset prefixed with events_, which will contain all the data for each day of the export, and you can start running those sweet queries on your raw data in no time!\nSummary It\u0026rsquo;s great that we have this \u0026ldquo;free\u0026rdquo; BigQuery export available for our Firebase analytics projects. I really, really hope it won\u0026rsquo;t be taken away from us. Ever.\nIn its current state, the user interface and reporting capabilities of Google Analytics: App + Web leave quite a lot to be desired, but the BigQuery export pretty much makes up for the limitations of the UI. Being able to work with row-level data is huge, as it, for example, gives us access to event parameters beyond the 50 each project is allocated by default.\nHave fun playing around with BigQuery, and let me know if you had trouble with setting up the export!\n"
},
{
	"uri": "https://www.simoahava.com/tags/ios/",
	"title": "ios",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/ios-quickstart-google-analytics-firebase-tag-manager/",
	"title": "iOS Quickstart With Google Analytics For Firebase And Google Tag Manager",
	"tags": ["google tag manager", "ios", "swift", "firebase", "app+web"],
	"description": "Quickstart guide to iOS development, with focus on integrating Google Analytics for Firebase and Google Tag Manager into a simple application.",
	"content": "A while ago, I published an article on how to build an Android application, and bundle it with Firebase. The purpose of that article, and the one you are reading now, is to slowly introduce the world of mobile app development and Firebase, given the latter is getting more and more traction as Google\u0026rsquo;s go-to analytics platform.\nAfter finishing work on the Android guide, I immediately started working on its counterpart for iOS, and that\u0026rsquo;s the one you\u0026rsquo;re reading now.\n  The process is essentially the same. We\u0026rsquo;ll develop the application using Mac\u0026rsquo;s own integrated development environment (Xcode), and the steps to create and integrate Firebase and Google Tag Manager deviate only very little from Android.\nI\u0026rsquo;ve also created a video you can watch if you prefer.\n  What you\u0026rsquo;ll build We\u0026rsquo;ll build an iOS application using the Swift programming language. The application will be built with Xcode, so you\u0026rsquo;ll need to use a Mac computer for this as there\u0026rsquo;s no native support for Xcode on other platforms.\nNext, we\u0026rsquo;ll create a Firebase project, and connect it to a Google Analytics account. After doing this, your application will automatically collect some events.\nWe\u0026rsquo;ll add a simple select_content event, which will be logged in Google Analytics for Firebase.\nFinally, we\u0026rsquo;ll create a Google Tag Manager container, and have it listen for the select_content event, and then duplicate its content to a Universal Analytics endpoint.\nThe application we\u0026rsquo;ll create will be hideous and feature-bare. Creating a flashy app is not the point here. I want to show you how easy it is to get started, and you can use even this simplest of applications to play around with Firebase in order to learn how it works.\nStep 1: Create a project in Xcode Let\u0026rsquo;s get started!\nGo to the Applications folder of your Mac and launch Xcode.\n  Click on File -\u0026gt; New -\u0026gt; Project to create a new Xcode project.\nChoose the Single View App as the template, and click Next.\n  In the next screen, give the product a name such as My Test Application. If you want to run the application on a physical device, choose yourself from the Team drop-down. If the Team dropdown doesn\u0026rsquo;t show your name (as configured on your Apple ID), follow the options to create a personal team with your Apple ID.\n Note! You only have to do this if you want to run the application on your physical device. We\u0026rsquo;ll be using a simulator in the examples so you can ignore the Team drop-down as well.\n You can give a custom Organization Identifier, as that makes it easier to remember your bundle identifier (which you\u0026rsquo;ll need when creating the Firebase project).\nMake sure Swift is selected as the language, and when ready click Next.\n  Finally, navigate to the directory where you want to create the project in. In my example, it\u0026rsquo;s a directory named ios-test.\nClick Create when done.\n  Xcode will now proceed to create your project. Once done, it will open the project\u0026rsquo;s general settings and show you the project structure in the left.\n  Before we run our app for the first time, let\u0026rsquo;s add some heart to it. Click Main.storyboard to open the visual editor for the application\u0026rsquo;s user interface.\nClick the Object Library icon in the top bar of Xcode, and type text in the search bar that appears. Drag-and-drop the Text Field item into the middle of the view.\n  Double-click the Text Field in the view, and type in Hello World!. You can reposition the field if you wish - though it won\u0026rsquo;t make it any prettier. This is what it should look like:\n  Now that we\u0026rsquo;ve created a beautiful user interface, we can run the application. Choose a phone model, e.g. iPhone 6 from the device menu, and click the Play button to build the application and run the simulator.\n  It\u0026rsquo;s going to take a while for the application to start, but once it does, you\u0026rsquo;ll be able to see it in the emulator window (it\u0026rsquo;s a different application that starts so remember to switch to it).\n  Congratulations! You\u0026rsquo;ve built the iOS application we\u0026rsquo;ll use as the testbed in this article. In the next step, we\u0026rsquo;ll configure Firebase!\nStep 2: Create a Firebase project Here, I\u0026rsquo;ll recommend you follow the exact same steps as in the Android article. Open this link in a new browser tab to learn how to create the Firebase project. Once the project is created, switch back to this article.\nStep 3: Deploy Firebase SDK in application Once you have the project created, you\u0026rsquo;ll need to integrate it into your iOS application.\nClick the iOS icon in the middle of the Project Overview dashboard.\n  In the iOS bundle ID field, copy-paste the Bundle identifier you can find in Xcode by first selecting your project, and then looking in the Identity section of the General tab of your project settings.\n  Give the app a nickname such as My iOS App if you wish, and then click Register app.\n  In the next screen, download the GoogleService-Info.plist file, and then drag-and-drop it to the project directory in Xcode (the yellow folder). You should see a screen like this, so make sure to check all the options as in the example.\n  You should end up with the file within the project directory.\n  In the web browser, click Next when done.\nNow you need to add the Firebase SDK to your project. To do that, you need to initialize a Podfile to manage your application\u0026rsquo;s dependencies.\nOpen the Terminal application, and browse to your project root, i.e. the directory which has the My Project Name.xcodeproj file. In this directory, type this and hit enter:\nsudo gem install cocoapods It\u0026rsquo;s a super-user command, so you\u0026rsquo;ll need to input your admin password. This installs the latest version of Cocoapods, the dependency manager for iOS.\nOnce done, type this and hit enter:\npod init If you now list the contents of the directory with ls, you should see a new Podfile listed among the files and folders.\n  Open the Podfile in a plain text editor. You can open it in Mac\u0026rsquo;s TextEdit by typing open Podfile in the console while in the directory with the Podfile.\nUnder the # Pods for \u0026lt;my project\u0026gt;, add the following two lines:\npod \u0026#39;Firebase/Analytics\u0026#39; pod \u0026#39;GoogleTagManager\u0026#39; This is what it should look like:\n  Once done, save the file and close it.\nNow, in the Terminal, while in the same directory as the Podfile, type the following command and press Enter.\npod install You should see a bunch of dependencies being installed, including Firebase and Google Analytics / Tag Manager modules.\n  You should now see a new file in this directory, named Your Project.xcworkspace. From now on, you\u0026rsquo;ll need to use this instead of the Your Project.xcodeproj file when working on the project in Xcode. The workspace bundles the Pods you downloaded together with your project, and lets you build an application that includes all the modules you have indicated as dependencies.\nSo go ahead and close Xcode and the iOS simulator, and then re-open Xcode by opening the Your Project.xcworkspace file.\nIf everything works as it should, you\u0026rsquo;ll now see both your project root and the Pods group in the project navigator.\n  Now, go back to the web browser with your Firebase SDK setup. As you\u0026rsquo;ve now added the SDK as a dependency, click the Next button.\n  The next step is to initialize the SDK in your application. Open Xcode, and then click the AppDelegate.swift class file to open it for editing.\nHere, right after the import UIKit statement, add the following line:\nimport Firebase This imports the Firebase set of modules. To initialize the Firebase SDK, add the following line of code into the application method, just before the return true statement:\nFirebaseApp.configure() This is what your AppDelegate.swift should look like after the changes.\n  Next thing to do is to save your changes and run the application in a simulator. So pick a phone model from the drop-down and then click the Play button again.\nThe application itself won\u0026rsquo;t show anything has changed, but you might see some new, Firebase-related items in the Xcode logs.\n   Note! You can also filter for GoogleTagManager to see evidence that the GTM dependency has loaded. However, since you don\u0026rsquo;t (yet) have a container added to the project, the only log will be a simple entry telling you that no container was found.\n Once you\u0026rsquo;ve run the app and verified that Firebase is initialized, go back to the web browser to finish the SDK setup.\nFrom the Add initialization code step, click Next to proceed to verification.\nNow, if you\u0026rsquo;ve run your app in Xcode on a computer with internet access, this final step should shortly show a green checkmark to confirm you\u0026rsquo;ve initialized the Firebase SDK.\n  Nice work! Click Continue to console to finish the setup.\nYou now have a working Firebase installation running in your app. You can now actually visit Google Analytics, browser to your Firebase App + Web property for this project, and see your data in the StreamView report within.\n  However, let\u0026rsquo;s not get ahead of ourselves. In the next chapter we\u0026rsquo;ll add some Firebase event logging to the application!\nStep 4: Add basic analytics to the application Now, let\u0026rsquo;s add a simple Firebase event to our application.\nOpen the ViewController.swift class. This class controls the view (d\u0026rsquo;oh) where you added the \u0026ldquo;Hello World!\u0026rdquo; text to.\nIn this class, load the relevant Firebase dependency with the following import command added to the beginning of the file:\nimport FirebaseAnalytics Next, at the end of the viewDidLoad() function, add the following lines of code:\nAnalytics.logEvent(AnalyticsEventSelectContent, parameters: [ AnalyticsParameterItemID: \u0026#34;my_item_id\u0026#34; ]) When you want to send events to Firebase Analytics, you use the Analytics singleton and its logEvent() method.\nThis method takes two arguments: the event name and a list of key-value pairs in a parameters list.\nFor the event name, you can provide a custom name such as \u0026quot;my_custom_event\u0026quot;, or you can utilize the standard events suggested by Firebase by accessing them through the AnalyticsEvent.* enumerated namespace.\nFor the key-value pairs in parameters, you can again use a custom key name such as \u0026quot;my_custom_key\u0026quot;, or you can use the suggested parameter names by accessing them through the AnalyticsParameter.* enum.\nWe are using the AnalyticsEventSelectContent enum, which returns select_content at runtime, and we are using the AnalyticsParameterItemID key, which returns item_id at runtime.\nHere\u0026rsquo;s what the modified ViewController.swift should look like:\n  Step 5: Debug and verify everything works Before you run the app again, you\u0026rsquo;ll need to add some verbose logging to the console. Choose Product -\u0026gt; Scheme -\u0026gt; Edit scheme from the menu bar.\n  In the Arguments Passed On Launch, click the plus (+) symbol to add a new argument, and name it -FIRAnalyticsDebugEnabled. This is what it should look like:\n  Now, run the app again. In the console logs, you should find the select_content event by either scrolling to it or filtering for the event name.\n  You can now also visit your Google Analytics reporting view for the App + Web property created for this project, and scroll to the DebugView report. This will automatically include all devices for which the -FIRAnalyticsDebugEnabled flag has been set. You should find your device (maybe with an odd name, though) in the Debug Devices list, and you should see a stream of events in the DebugView, together with the new select_content event you just created.\n  Before we\u0026rsquo;re done, let\u0026rsquo;s fork this Firebase Analytics hit using Google Tag Manager, and send the copy to a Universal Analytics endpoint!\nStep 6: Create and download a Google Tag Manager container First, make sure you have a valid Universal Analytics endpoint. You need to create a Web property with a Mobile App view. The latter is what collects hits sent from your application.\nOnce you have the tracking ID (UA-XXXXX-Y) at hand, you can head on over to https://tagmanager.google.com/ to create your new iOS container.\nIn Google Tag Manager, create a new container in one of your accounts, or create a new GTM account first.\nGive the container a name, e.g. My Test Application, and choose iOS as the type. Click Create when ready.\n  Click New Tag to create a new tag. Choose Google Analytics: Universal Analytics as the tag type.\nSelect Event from the Track Type list.\nAdd something to the Category field, e.g. Test Event.\nIn the Action field, start typing {{ and then choose New Variable from the list.\n  Choose Event Parameter as the variable type.\nKeep Suggested Firebase Parameter checked, and choose item_id from the Event Parameter list.\nGive the variable a name, e.g. {{item_id}} and then click Save.\n  Back in the tag, check Enable overriding settings in this tag, and add the Google Analytics tracking code for the web property that will be receiving these hits (UA-XXXXXX-Y).\n  Finally, click the Triggering panel to add a new trigger.\nClick the blue plus (+) sign in the top right corner to create a new trigger.\nChoose Custom as the trigger type.\nCheck Some Events under the This trigger fires on heading.\nSet the condition to:\nEvent Name equals select_content\nGive the trigger a name (e.g. select_content), and when happy with it, click Save.\n  Double-check to make sure your tag looks like the one below, and then click Save to return to the Google Tag Manager dashboard.\n  At this point, you are ready to publish the container, so click Submit in the top right corner, and then Publish in the overlay that opens.\n  Once the container has been published, you should see a version overview of the published version. Click the Download button in this screen to download the container JSON file.\n  Now, create a new folder named container in the root of your project (the directory with the .xcodeproj and .xcworkspace files as well as the Podfile). Move the container JSON into this directory.\n  Next, open Xcode. With your project root selected in the navigator, choose File \u0026ndash;\u0026gt; Add files to Your Project.\n  Find and select the container folder from your project root directory, and make sure the other options are checked as in the screenshot below. Click Add when ready.\n  Once ready, you can run your application, and it should load the default container you just added to the application, and then shortly after fetch the most recent container version over the network.\n Note! It\u0026rsquo;s always a good idea to keep as fresh a container version as the default container stored with the application itself. Thus if there\u0026rsquo;s network lag impacting the fetch of a more recent container version, the fallback (the default container) would be as up-to-date as possible.\n To be sure, you\u0026rsquo;ll need to debug the setup a little.\nStep 7: Debug and test that Universal Analytics receives the hits Xcode\u0026rsquo;s logs are a bit of a mess. However, if you\u0026rsquo;re not averse to scroll-and-search, you can find what you\u0026rsquo;re looking for. You can also type in text in the filter field to parse the logs, but it only returns matching rows and is not thus very helpful.\nHowever, we can quickly see that Google Tag Manager found our container and was able to load it in the application,\n  In addition to that, by scrolling down we can see that a Universal Analytics hit was dispatched via Google Tag Manager, using the parameters we set for the select_content event.\n  Finally, we can check the Real Time report of our mobile app view to confirm data is flowing in:\n  Debugging a mobile application is notoriously difficult. If you want to do it seriously, you might want to use a tool like Charles proxy, which creates a log stream of all the network requests dispatched by your mobile application. You don\u0026rsquo;t even need source code access to make it work!\nSummary As with my Android guide, this article should really be an introduction to iOS development, and not a full-blown tutorial.\nThe purpose is to give you the tools and confidence to start working with mobile app development. Understanding the capabilities and limitations of Firebase is fundamental to being able to fluently work with the ecosystem.\nWorking with mobile application development is far removed from web development. For starters, the IDEs, SDKs, and programming languages at your disposal are far more restrictive than the wild west that is the web browser.\nKotlin and Swift do not let you do all the crazy stuff you can do with JavaScript, and there are also restrictions what types of shenanigans can be executed at runtime (so no Custom HTML tags in mobile app containers).\nNevertheless, iOS development has its charms, and Xcode can be a great ally in the times when it\u0026rsquo;s not a complete pain in the butt.\nThe beauty of mobile application development is how quickly you can get started. You don\u0026rsquo;t need anything extravagant - just a machine capable of handling the virtual devices, some good tutorials, and a solid system such as Firebase picking up some of the slack.\nAs always, I\u0026rsquo;m looking forward to your feedback in the comments! I\u0026rsquo;m sure I\u0026rsquo;ll revisit Firebase many times in upcoming articles.\n"
},
{
	"uri": "https://www.simoahava.com/tags/swift/",
	"title": "swift",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/android/",
	"title": "android",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/android-quickstart-google-analytics-firebase-google-tag-manager/",
	"title": "Android Quickstart With Google Analytics For Firebase And Google Tag Manager",
	"tags": ["google tag manager", "firebase", "android", "app+web"],
	"description": "Quickstart guide for building a simple Android application that runs Google Analytics for Firebase and Google Tag Manager.",
	"content": "With the release of Google Analytics: App + Web, Firebase is suddenly all the rage. The new App + Web property can combine data from your website and mobile apps, as long as the latter uses Google Analytics for Firebase, formerly known as Firebase Analytics.\nIn this guide, I\u0026rsquo;ll walk you through the steps of creating an extremely simple Android application, and we\u0026rsquo;ll then create a Firebase project, and for good measure add Google Tag Manager to the mix.\n UPDATE 21 August 2019: I have now published a similar guide for iOS, too. Enjoy!\n   To follow along, all you need is a computer. You don\u0026rsquo;t need an Android device, even if debugging is nicer with an actual phone rather than an emulator.\nI\u0026rsquo;ve also recorded a video if you prefer to see the guide rather than read it.\n  What you\u0026rsquo;ll build We\u0026rsquo;ll create an Android application using Android Studio. This application can be run on your Android device or in an Android emulator (a virtual device).\nWe\u0026rsquo;ll create a Firebase project and deploy Firebase into the application. We\u0026rsquo;ll fire a basic event so we\u0026rsquo;ll know how to debug the hits sent from Firebase to Google Analytics.\nWe\u0026rsquo;ll create a Google Tag Manager container for Firebase, and we\u0026rsquo;ll install that in the application. We\u0026rsquo;ll use that container to take the event we configured for Firebase and send the data to a (non-Firebase) Google Analytics property as well.\nThe purpose of this guide is to show you how each of the components listed above works. It will do so in a very superficial manner, but you should be able to see how easy it is to create a mobile application and add some basic analytics to it.\nStep 1: Create a project in Android Studio First of all, visit this link to download Android Studio. Android Studio is an IDE (integrated development environment) for building on top of the Android operating system.\n  We\u0026rsquo;ll be using Android Studio to develop and debug our application. So, to get things started, fire up Android Studio, and choose Start a new Android Studio project from the splash screen.\n  Choose Empty Activity as the template and click Next.\n  Give the project a Name - can be anything you like. Take note of the Package name generated for the project (you can edit this if you want), as we\u0026rsquo;ll need it when configuring Firebase. Make sure the Save location target exists - Android Studio will automatically create the directory for the project, but the location where the project directory is created must be a valid one.\nCheck the box next to Use androidx. artifacts*, as we\u0026rsquo;ll need them for the latest releases of Firebase.\nClick Finish when ready.\n  Android Studio will build your application and load the basic dependencies while at it. Once the bottom bar says something like Gradle build finished in NNN ms, you can click the Play button in the header bar to run your application.\n  At this point, if you have an Android phone plugged into the USB port of your computer, you should see the phone in the list of connected devices. If the phone is running an Android OS version higher than the minimum set for the project (19 by default), you should be able to run the application on the phone by choosing it and clicking OK.\nOtherwise, you\u0026rsquo;ll need to create a Virtual device, so click Create New Virtual Device to launch the Android Virtual Device (AVD) Manager.\n  In the next screen, choose a phone model such as the Nexus 5 and click Next.\n  You need to choose a System image to format the device with. Download one of the recommended releases (I\u0026rsquo;m using Pie), as they should all have an API level higher than the one configured for the application. When the download is complete, finalize the configuration by clicking Next.\n  If you want, you can change the device name here, but you can leave all the other settings with their default values. Click Finish to create the device.\nNow you should see the device in the Available Virtual Devices of the deployment target selection. Make sure the device is selected and click OK to fire it up and run the application.\n  It\u0026rsquo;s going to take a while to boot up the virtual device, install the application (APK) in it, and then fire it up. But once it\u0026rsquo;s done, you should see this:\n  Congratulations, you\u0026rsquo;ve built an Android application! The virtual device functions just like a real device, and you can use the device controls to move around the settings and applications menus just as you would with a real device.\nNext, we\u0026rsquo;ll need to add Firebase to the application.\nStep 2: Create a Firebase project To be able to leverage Google Analytics for Firebase and to implement Google Tag Manager in the application, we need to create a Firebase project and configure its SDK in our application. Fortunately, this is very easy to do.\nBrowse to https://console.firebase.google.com/, login to Google services if necessary, and click to Create a project.\n  Give the project a name. Make note of the project ID given to the project (you can edit this if you wish, but it must be unique). It\u0026rsquo;s the name of the project that will be created in the Google Cloud Platform automatically for you.\n  In the next screen, make sure Set up Google Analytics for my project is selected and click Continue.\nNow you can select a Google Analytics account you have access to, or you can create a new account. The latter option is mandatory if you don\u0026rsquo;t have access to any Google Analytics accounts.\nIf you need to create a new account, configure its data sharing settings first.\nThen, click Create project to link the Firebase project to the Google Analytics account you selected (or created). Note, this will create a new [App + Web property]({\u0026lt; relref \u0026ldquo;getting-started-with-google-analytics-app-web.md\u0026rdquo; \u0026gt;}) with an App Stream in the target Google Analytics account.\n  You\u0026rsquo;ll see a spinner while the service creates your project. Wait patiently.\n  Once done, it will tell you that your new project is ready, so click Continue to configure the SDK.\nStep 3: Deploy Firebase project in application In the dashboard, click the little Android mascot head to configure the Firebase project to work in your application.\n  The first thing you need to do is Register the app, so type the package name configured for your application in the respective field.\nIf you can\u0026rsquo;t remember the package name, you can open the AndroidManifest.xml in the /app/manifests/ directory, and check the value of the package attribute in the \u0026lt;manifest\u0026gt; tag.\n  Once you\u0026rsquo;ve filled the fields, click Register app.\n  In the next screen, you are prompted to download the google-service.json file. Click the button to do so, and then switch to Android Studio.\n  In Android Studio, first click the \u0026ldquo;Android\u0026rdquo; label to open the drop-down menu for the project navigator view, and choose Project to see the full directory structure of the project.\n  Next, expand the project directory and the /app/ folder within, and drag-and-drop the google-services.json from your downloads into the /app/ directory. Click OK if you see a dialog asking if it\u0026rsquo;s ok to move the file. It should end up looking like this:\n  Once you\u0026rsquo;ve done this, go back to the web browser and click Next to proceed to the next step in your Firebase project creation.\nIn this step, you are asked to add the Firebase dependencies to your project- and app-level Gradle files. Gradle is the build tool used by Android Studio applications, and it governs, among other things, the dependencies that the app needs to load in order to compile and run. Firebase is one such dependency.\nThe first file you need to edit is the project-level build.gradle file. You can find this in the root of your project folder, so double-click it in Android studio to open it for editing.\n  Find the dependencies block, and add this line to the end:\ndependencies { ... classpath \u0026#39;com.google.gms:google-services:4.3.0\u0026#39; }  Note! At the time of writing, the Firebase console recommends google-services:4.2.0, but you can upgrade this to 4.3.0 as in the code example above. If a newer version is available, this will be indicated by a yellow highlight around the classpath code.\n Save the file, and then open the /app/build.gradle file.\n  Here, edit the dependencies block again to add two new lines to the end:\ndependencies { ... implementation \u0026#39;com.google.firebase:firebase-core:17.0.1\u0026#39; implementation \u0026#39;com.google.android.gms:play-services-tagmanager:17.0.0\u0026#39; }  Note! Again, if there are newer versions of the Firebase core and Google Tag Manager dependencies available, this will be indicated with a yellow highlight around the implementation statements.\n As you can see, we are already adding GTM as a dependency. It won\u0026rsquo;t do anything until we configure the container, though.\nFinally, add the following line to the end of the build.gradle file:\napply plugin: \u0026#39;com.google.gms.google-services\u0026#39;   Save the file. Now you can click the Sync now button in the blue header on top of the code editor to have the application download and build the dependencies you just added.\n  You might see a warning about deprecated methods or features in the sync log, but the sync should still complete successfully.\n  Now, in the web browser where you are creating the Firebase application, you need to click Next to enter the final stage. Here, Firebase waits for your application to send a request to Firebase servers. To do so, make sure you are connected to the internet, and run your app (by clicking the Play button in the Android Studio interface). If all goes well, the application will run, send a request to Firebase servers, and you\u0026rsquo;ll see the project creation view change to this:\n  Click Continue to console to go back to the Firebase dashboard.\nCongratulations! Firebase is now running in your app. You can scroll through the different screens in the Firebase console, but you won\u0026rsquo;t find anything useful, yet. You still need to make Firebase do something. And that something, in this guide, will be to collect data.\nStep 4: Implement basic analytics To implement some analytics logging, we need to load the Firebase Analytics dependency, and then initialize it in our application.\nSo, open the build.gradle file under /app/ again, and add the following new dependency:\ndependency { ... implementation \u0026#39;com.google.firebase:firebase-analytics:17.0.1\u0026#39; } Click the Sync now button in the editor to synchronize your application with this latest change to its dependencies.\nThen, open the MainActivity.kt file for editing. You can find it in /app/src/main/java/_project_/. You can also hit CMD + Shift + O and type MainActivity.kt in the selector to find and open the file. In any case, you should end up with the file open in the editor.\n  Activities correspond to views and screens in the application. They are so much more than that (read here), but as an analogy this will suffice for now. The MainActivity.kt file contains code that is run when the application is opened. This is because the application comprises just one activity.\nIf the application had different screens or views, we would create additional activities for them.\nIn the MainActivity.kt file, add the following code right after the class MainActivity heading.\nBefore hitting enter after typing FirebaseAnalytics, you\u0026rsquo;ll see a little tooltip appear that identifies the class you are trying to create an instance of. Hit enter to automatically add the import command for the FirebaseAnalytics class.\nclass MainActivity : AppCompatActivity() { // ADD THIS:  private lateinit var firebaseAnalytics: FirebaseAnalytics // DON\u0026#39;T TOUCH  override fun onCreate(savedInstanceState: Bundle?) { ... } } If you can\u0026rsquo;t get the tooltip to appear, or if the automatic import doesn\u0026rsquo;t work, you can expand the import statements in the top of the MainActivity.kt file, and manually add the FirebaseAnalytics module.\n  With this code, we are initializing a variable named firebaseAnalytics of the type FirebaseAnalytics. We are using the lateinit keyword to avoid having to initialize it with a value (as we\u0026rsquo;ll only have the value once the activity has loaded).\nNext, in the onCreate method we can actually initialize the variable. Add this line to the end of the onCreate method block:\nfirebaseAnalytics = FirebaseAnalytics.getInstance(this) This initializes the firebaseAnalytics local variable as an instance of the FirebaseAnalytics class. We can now use the firebaseAnalytics variable within this MainActivity class to send data to Firebase Analytics.\n  To send an event to Firebase Analytics, you need to call the logEvent() method of the firebaseAnalytics instance. The call takes two parameters: an event name, and a bundle of parameters (key-value pairs).\nTo keep things simple, we\u0026rsquo;ll send the event to Firebase from the same onCreate method, which means it will be sent as soon as the application has loaded (and Firebase with it).\nLet\u0026rsquo;s add a simple key-value pair using the recommended parameter Item ID. These prescribed parameters can be found as properties of the FirebaseAnalytics.Param enum.\nEdit the onCreate method to look like this:\noverride fun onCreate(savedInstanceState: Bundle?) { ... firebaseAnalytics = FirebaseAnalytics.getInstance(this) val bundle = Bundle() bundle.putString(FirebaseAnalytics.Param.ITEM_ID, \u0026#34;my_item_id\u0026#34;) } We create a new Bundle instance, to which we add a single parameter with the name item_id (stored in the enum FirebaseAnalytics.Param.ITEM_ID) and value \u0026quot;my_item_id\u0026quot;.\n  Next, we need to send the bundle to Firebase Analytics together with an event name. Prescribed event names can be found within FirebaseAnalytics.Event. We\u0026rsquo;ll use the SELECT_CONTENT event name for this exercise.\nAdd the following line to the end of the onCreate method:\noverride fun onCreate(savedInstanceState: Bundle?) { ... firebaseAnalytics.logEvent(FirebaseAnalytics.Event.SELECT_CONTENT, bundle) }   Once you\u0026rsquo;ve made the changes, you can run the application to have it log events to Firebase. Once the app is running, you can enter the StreamView report in your Firebase Analytics console, and you should see the console collecting data from your application.\nNote that Firebase processes dispatched events in batches, and it\u0026rsquo;s possible your select_content event doesn\u0026rsquo;t make it to the first batch. So at this point just make sure that StreamView is showing some data so that you can verify your Firebase Analytics setup works. Firebase collects certain events by default (e.g. user_engagement, first_open, screen_view), so you should see these in the StreamView.\nLuckily we have the DebugView for getting more Real Time data from our debugging devices. Read on!\nStep 5: Debug To have data appear in DebugView, you need to first register your virtual device as a valid Debug Device. To do that, click to open the Terminal tab in Android Studio with the virtual device running.\n  In the terminal console, type this and press Enter. Substitute com.example.mytestapplication with your application\u0026rsquo;s package name.\nadb shell setprop debug.firebase.analytics.app com.example.mytestapplication You might see a warning about mismatched server-client versions, but if you see the text * daemon started successfully, you\u0026rsquo;ll know it worked.\n  Now, run the application again and open the DebugView report in Firebase. Your device should appear in the Debug Device list (it might take a moment to appear), and your debug events should start appearing in the stream. You can click the select_content event to see the custom parameter being sent.\n  Nice! You\u0026rsquo;re collecting analytics data from your application to Firebase.\nThe last thing we\u0026rsquo;ll do is use that select_content hit to send a request to Universal Analytics, and we\u0026rsquo;ll use Google Tag Manager to do so.\nStep 6: Download Google Tag Manager container First, if you want to see the data in an actual Google Analytics report, you\u0026rsquo;ll need to create a Mobile View in Google Analytics. It\u0026rsquo;s not very intuitive, but you need to create a new Web Property, and then a new Mobile App View under that Property.\nNOTE! This is not necessary to test the integration. We can inspect the requests sent by the application to verify they are being sent to Google Analytics, even if the target property hasn\u0026rsquo;t been configured to accept mobile app data.\n  Go to https://tagmanager.google.com/ and create a new Container under an account of your choosing (if necessary, create the GTM account first).\nChoose Android as the container type, give the container a name, and then click to Create it.\n  Next, create a new Tag and choose Google Analytics: Universal Analytics as the type. Select Event as the Track Type, and add something like Test Event as the Event Category.\nIn Event Action, type {{ in the field and click New Variable\u0026hellip; from the drop-down menu.\n  Choose Event Parameter as the variable type.\n  From the Event Parameter drop-down, find item_id. Name the variable e.g. Firebase - item_id, and click to Save it.\n  Back in the tag, check Enable overriding settings in this tag, and type the UA-XXXXXX-Y number for your GA property in the Tracking ID field. If you only care about testing, you can type a dummy ID like UA-12345-1 here.\n  Now, click the Triggering box to add a new trigger to the tag. In the flyout, click the blue plus icon to create a new trigger.\n  Choose Custom as the trigger type, check Some Events and modify the trigger to check if Event Name equals select_content. Give the trigger a name and save it.\n  Once back in the tag editor, give your tag a name. It should now look like this:\n  If happy, save the tag. Next thing to do is to Publish the container, so click the big blue SUBMIT button in the top right corner of the Google Tag Manager user interface.\nGive the version a name and click Publish.\n  Once done, a summary screen should open, and you\u0026rsquo;ll see a big blue Download button in the top right corner. Click that to download the container JSON.\n   Note! Even though the GTM container is loaded over the network (as with web containers), the application loads with this downloaded container first, to account for scenarios where the user fails to download a fresh container e.g. due to network lag. Thus it\u0026rsquo;s a good idea to always have the latest Google Tag Manager container version also included in the app assets.\n With the container JSON downloaded, you\u0026rsquo;ll now need to create the directory in Android Studio. Expand the /app/src/main/ directory in the Project view, and with main selected, click File -\u0026gt; New -\u0026gt; Directory from the Android Studio menu bar.\n  Name the directory assets and click OK.\n  Next, select this new assets directory, and repeat the process of creating a new directory. This time, name the directory containers. You should end up with a tree structure that looks like this.\n  Finally, drag the container JSON file from your downloads into this new containers directory, and click OK to approve the move. You should see the file in its correct place.\n Note! It\u0026rsquo;s important that the container JSON be located in this exact directory!\n   Now, let\u0026rsquo;s test it works!\nStep 7: Debug Google Tag Manager and Google Analytics First thing you\u0026rsquo;ll need to do is enable some extra logging in Android Studio. With the Terminal tab open once more, type the following commands pressing Enter after each one. Note that you need to have your (virtual) device running.\nadb shell setprop log.tag.GoogleTagManager VERBOSE adb shell setprop log.tag.GAv4 DEBUG   Next, click to open the LogCat tab, and re-run your application. Once the app is running, filter the LogCat view for the string GoogleTagManager. You should see logging that your container was loaded, and then a bunch of logging about processing Firebase events and such.\n  Then, filter the LogCat for the string D/GAv4. You should see a line with Hit delivery requested with a bunch of parameters matching those you configured in the Universal Analytics tag of Google Tag Manager, and then another line with Hit sent to the AnalyticsService for delivery.\n  If you see all that, then everything works, woohoo! If you\u0026rsquo;re sending the data to an actual Google Analytics property where you have a Mobile App view running, you should see your event in the Real Time report.\n  There you have it! You\u0026rsquo;ve built an Android application, which collects data to Google Analytics for Firebase, runs a Google Tag Manager container, and duplicates the Firebase event into a Universal Analytics endpoint.\nSummary This guide should serve as a very basic introduction to Android development. I wrote it because I think everyone even remotely interested in application analytics should understand the mechanisms of how Android application development works.\nWhen you integrate Google Analytics into your Firebase Project, a Google Analytics: App + Web is automatically created in the Google Analytics user interface. Thus you can leverage the new reports immediately, and if you end up having a website to complement the app, you could add website data to the property as a Web stream, too.\nGoogle Tag Manager for apps is quite far removed from its web counterpart. You have far less to work with due to how application\u0026rsquo;s restrict the type of ad hoc code that can be run within.\nFirebase is also still a work-in-progress, and it still has many limitations that need to be sorted out before it\u0026rsquo;s feasible to even think about replacing Universal Analytics with it.\nHowever, I hope you were bitten by the Android bug. It\u0026rsquo;s a really fun platform to develop against, and Kotlin is a relatively simple language to learn, especially if you come from a JavaScript background. Here are some resources for further education:\n  Google\u0026rsquo;s Android Training Courses\n  Kotlin tutorials\n  I hope you enjoyed the guide! Let me know in the comments if something was left unclear.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/get-latest-exchange-rates-client-side-conversions/",
	"title": "#GTMTips: Get Latest Exchange Rates For Client-side Conversions",
	"tags": ["google tag manager", "api", "gtmtips", "custom html"],
	"description": "Get the latest currency rates in Google Tag Manager using the exchangeratesapi.io service.",
	"content": "It\u0026rsquo;s been a while since I\u0026rsquo;ve last written a bona fide Google Tag Manager trick, so here comes. This was inspired by Bart Gibby\u0026rsquo;s question in Measure Slack.\nThe purpose is to fetch the latest currency exchange rates from the exchangeratesapi.io service, cache them using sessionStorage, and push the results into dataLayer. From dataLayer, they can then be utilized in Custom JavaScript variables and custom variable templates to perform client-side conversions. I\u0026rsquo;ll show you how!\nTip 103: Get latest currency exchange rates in GTM   The API request itself is run in a Custom HTML tag (as custom templates do not support arbitrary HTTP requests yet), so go ahead and create one. In the tag, add the following code:\n\u0026lt;script\u0026gt; (function() { // Set expiration to stored cache. Default 24 hours.  var cacheExpiresHours = 24; // Set currency to get rates against  var base = \u0026#39;EUR\u0026#39;; // Do not edit below  var rates = JSON.parse(window.sessionStorage.getItem(\u0026#39;currencyRates\u0026#39;) || \u0026#39;{}\u0026#39;); var now = new Date().getTime(); var xhr; if (!rates.timestamp || rates.timestamp + cacheExpiresHours * 60 * 60 * 1000 \u0026lt;= now) { xhr = new XMLHttpRequest(); xhr.open(\u0026#39;GET\u0026#39;, \u0026#39;https://api.exchangeratesapi.io/latest?base=\u0026#39; + base); xhr.onreadystatechange = function() { if(xhr.readyState === XMLHttpRequest.DONE) { rates = JSON.parse(xhr.responseText).rates; rates.timestamp = now; sessionStorage.setItem(\u0026#39;currencyRates\u0026#39;, JSON.stringify(rates)); window.dataLayer.push({ event: \u0026#39;exchangeRates\u0026#39;, rates: rates, ratesType: \u0026#39;fresh\u0026#39; }); } }; xhr.send(); } else { window.dataLayer.push({ event: \u0026#39;exchangeRates\u0026#39;, rates: rates, ratesType: \u0026#39;cached\u0026#39; }); } })(); \u0026lt;/script\u0026gt; Modify the cacheExpiresHours to determine how long the currency rates should be cached in the browser for. If you don\u0026rsquo;t want to cache them, set it to 0. This would fetch the latest rates with every page load.\nI would recommend using a cache. The API is a free service, and it\u0026rsquo;s free for as long as it\u0026rsquo;s not abused. It makes no sense to continually fetch the latest rates, so set the cache to something you can live with.\nYou\u0026rsquo;ll also need to set a base currency against which the rates are calculated. In the example, I\u0026rsquo;m using 'EUR' as I want the rates to be calculated against euros.\nThe rest of the code follows this pattern:\n  If the currency rates are found in browser storage, and the cache expiration duration has not been met yet, then fetch the rates from storage and push them into dataLayer.\n  If the rates are not found in browser storage, or if the cache expiration duration has been met, fetch the latest rates using an HTTP request to https://api.exchangeratesapi.io/latest, using the base currency rate you provided in the script configuration. Push the results into dataLayer. Cache the results in browser storage.\n  I also included a key to reflect whether or not the results were cached (ratesType: 'fresh/cached'), which you can use to determine the reliability of the results. This is what the dataLayer would look like:\n{ event: \u0026#34;exchangeRates\u0026#34;, rates: { AUD: 1.6467, BGN: 1.9558, BRL: 4.4099, CAD: 1.4785 CHF: 1.0919 CNY: 7.8521 CZK: 25.727 DKK: 7.4644 GBP: 0.9183, ... timestamp: 1565172369394 }, ratesType: \u0026#39;fresh\u0026#39; }  Set this tag to fire on the All Pages trigger. That way it will fire as soon as the GTM container has loaded.\nNow that the data is in dataLayer, you can create some variables that will let you use the results.\nTo start off, a Data Layer variable for rates will fetch the corresponding object from GTM\u0026rsquo;s data model.\n  Now, to perform a conversion, you need the source variable (the value you want to convert) and this Data Layer variable. For example, if your source value is in Australian dollars, you can convert that to euros with a Custom JavaScript variable that has the following code:\nfunction() { // Update these if necessary  var convertFrom = \u0026#39;AUD\u0026#39;; var sourceVariable = {{Source value in AUD}}; var rates = {{DLV - rates}}; // Don\u0026#39;t change anything below  if (!rates || !rates[convertFrom] || isNaN(sourceVariable)) { return sourceVariable; } return sourceVariable / rates[convertFrom]; }  This variable pulls in the source value and the rates object. In case the rates object doesn\u0026rsquo;t exist, OR it doesn\u0026rsquo;t contain the currency symbol you want to convert from, OR the source variable does not return a number, the source variable is returned untouched.\nOtherwise, the variable divides the source variable with the exchange rate to give you the source variable value in the base currency you configured in the Custom HTML tag.\nYou can also create a custom variable template where the user indicates which source variable to use and what the currency symbol would be. The template would then return the result of the conversion.\n  You can download the template I created here.\n NOTE: The currency conversion will only work if the source variable is available when then conversion is made AND if the exchange rates have been successfully fetched. The request for the exchange rates is asynchronous, so you need to be careful not to fall into a race condition. You can use the exchangeRates event name in a Custom Event trigger if you want to fire a tag only after the exchange rates have been successfully retrieved.\n I hope this tip proves useful to someone, or at the very least gives you ideas for how to pull in data from public APIs, through GTM, into the user\u0026rsquo;s web browser.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/getting-started-with-google-analytics-app-web/",
	"title": "Getting Started With Google Analytics: App + Web",
	"tags": ["google tag manager", "app+web", "firebase"],
	"description": "How to get started with implementing tags for Google Analytics: App + Web if using Google Tag Manager.",
	"content": "Google recently released a new version of Google Analytics called App + Web. Clumsy name aside, this really is for all intents and purposes Google Analytics V2 or Firebase Analytics for the Web. We\u0026rsquo;re not talking about a charming way to do roll-up reporting between Google Analytics for Firebase and Universal Analytics, nor are we talking about an enhancement to Universal Analytics.\n  No, we\u0026rsquo;re talking about a new measurement model for web traffic, which has the convenience of being compatible with Google Analytics for Firebase, which you might already have running in your apps.\nIt\u0026rsquo;s still, ahem, slightly unpolished, and the documentation is still, ahem, slightly lacking, so I thought I\u0026rsquo;d give you a head start by showing how to set up data collection using Google Tag Manager. I fully intend to release a proper, comprehensive guide once the feature rolls out of beta, but this article should serve those of you who are dying to get your hands on the new property and all the goodness within.\nThe name Google Analytics: App + Web is a bit awkward. I suppose it\u0026rsquo;s just a matter of time before we\u0026rsquo;ll call it Firebase for the Web or something similar, since that\u0026rsquo;s what it essentially is. In this article, I\u0026rsquo;ll refer to it as GAv2 in places where I just can\u0026rsquo;t be bothered to spell it out (yes, I\u0026rsquo;m lazy).\n  Before I start, open Krista Seiden\u0026rsquo;s amazing blog in a new tab, and focus especially on the three articles she\u0026rsquo;s published on the reporting capabilities of Google Analytics: App + Web.\n  New App + Web Properties In Google Analytics.\n  Pathing in Google Analytics.\n  Streamview in Google Analytics.\n   UPDATE: Just after I published this article, Krista released yet another excellent guide, and the best thing is that it\u0026rsquo;s complementary to this one, as it covers the steps of creating the Firebase project in much more detail. So check it out: Step by Step: Setting up an App + Web Property.\n I\u0026rsquo;ll start off with some introductory words, so if you want, you can just skip right to the implementation steps.\nCaveats First of all, Google Analytics: App + Web is in beta. That means that it\u0026rsquo;s not ready yet. Seriously, it\u0026rsquo;s not ready yet. Many of the things I\u0026rsquo;m scratching my head about will no doubt enter the platform at some point, and many of the questions you have will certainly be answered by upcoming feature releases.\nSome of the things I found to be missing from the reporting UI and the Google Tag Manager setup include:\n  No specialized reports, e.g. \u0026ldquo;Site Search report\u0026rdquo;, or \u0026ldquo;Ecommerce\u0026rdquo; reports.\n  No way to distinguish between automatically collected, recommended, and custom events in the reporting UI.\n  No way to set user properties in the GTM tags. UPDATE: You can set User Properties, see below.\n  No way to send the items array (or any other multi-dimensional object) as an event parameter to Google Analytics when using GTM.\n  Setting persistent values in the configuration tag in GTM did not seem to work.\n  No way to upgrade an existing Firebase Analytics property to an App + Web property. UPDATE: You can upgrade existing Firebase Analytics properties per this support doc. When you open the property in Google Analytics (assuming you\u0026rsquo;ve linked the property in Google Analytics), you\u0026rsquo;ll see a blue banner in the top of the screen prompting you to upgrade.\n  Be sure to follow the official blog as well as Krista, as they will be your go-to channels for more information including releases that address the concerns listed in this chapter.\n  Then there\u0026rsquo;s the obvious thing. This is not Universal Analytics. Many of the things you\u0026rsquo;re used to seeing in Universal Analytics are not present in Firebase. Personally, I hope that feature parity is not in the horizon, as now Google have the chance to create something new and better. But naturally, we\u0026rsquo;d want the platform to replace Universal Analytics at some point, so at the very least it should cater to the same use cases.\nHere are, in my view, the main differences between Universal Analytics and the new platform.\n   Universal Analytics Google Analytics V2     Sessions in the foreground. Users and events in the foreground.   Hit-, session-, and user-scoped custom definitions. User properties and custom event parameters.   Superficial Real Time reports. StreamView and DebugView permit more comprehensive drill-down.   Segments. Audiences.   No semantic structuring. Automatically collected and recommended events.   Fairly forgiving quotas and limits (apart from sampling). Strict quotas and limits (no sampling).    The limits are especially painful if you compare them with what you had in Universal Analytics.\nAnyway, I\u0026rsquo;m sure these concerns will be addressed before the Google Analytics: App + Web rolls out of beta, but one thing is for certain: this is not Universal Analytics. Try to look at the new measurement model as an opportunity to go beyond what Universal Analytics ever could, rather than just another way to populate GA\u0026rsquo;s ancient data model.\nDue to these caveats, I strongly recommend against cutting off your Universal Analytics data collection and solely collecting data to the Google Analytics: App + Web property. If anything, this new measurement model should be run in parallel to whatever setup you already had on the site, so that you can compare parity between the two data sets.\nCreate a new Google Analytics: App + Web property To create a new Google Analytics: App + Web property, you will need to follow these steps.\nNote! Based on released documentation, it\u0026rsquo;s probable these steps will be greatly simplified in the future.\nStep 1: Create a new Firebase project Head on over to the Firebase console, and create a new project. This will be the underlying Firebase project for your new property.\n  Then, give the project a name and an ID (the ID must be unique, and one is generated for you from the project name).\nRead and accept the Firebase terms, and click Continue.\n  In the next step, make sure the \u0026ldquo;Set up Google Analytics for my project\u0026rdquo; is checked, and click Continue.\n  Depending on if you already have a Google Accounts or not, you can now select in which account to create the property in or you can create a new account altogether. If you don\u0026rsquo;t have any Google Analytics accounts associated with your login, you will need to accept GA\u0026rsquo;s terms of service and a new account and property will be created for you.\n  Once you hit Create project and it finishes processing, you can head on over to Google Analytics to find your new App + Web property in the account you selected.\n  Step 2: Create a new web stream The new measurement model revolves around streams. Streams can be incoming from your apps (iOS and Android), or from the web. I hope we\u0026rsquo;ll see new streams in the future, such as a \u0026ldquo;Measurement Protocol\u0026rdquo; stream that would accept HTTP requests from any device connected to the internet.\nAnyway, click the Data Streams option in the property column and choose Web.\n  To configure the stream, enter your site URL and give the stream a descriptive name. Next, click the little cogwheel to configure the Enhanced Measurement settings.\n  Enhanced Measurement basically adds some automatic tracking capabilities to the site, activated when you create a base configuration tag.\n  The list of things that can be automatically tracked are fairly clear and explained well in the Enhanced Measurement configuration (see screenshot above).\nOnce you\u0026rsquo;re ready, create the data stream and you should see a bunch of Tagging instructions and other stuff in the screen. The most important thing to note is the Measurement ID. Keep this tab open so that you can copy the Measurement ID into your Google Tag Manager tags when required.\n  Done? OK, let\u0026rsquo;s move to Google Tag Manager.\nCreate a base configuration tag In Google Tag Manager, when creating a new tag, you\u0026rsquo;ll see two new tag templates in the selector.\n  Google Analytics: App + Web Configuration is analogous to the config command used in a gtag.js implementation. You use it to configure the tracker, send the initial Page View, and configure persistent values for all your events.\nWhen you open the template, you\u0026rsquo;re not exactly greeted with an abundance of options. Basically, you have a field to set the Measurement ID in, a toggle to choose whether to send the initial Page View or not, and configuration fields you can set in the tag.\n  To configure the base tag, set the Measurement ID from the web stream you created in the previous chapter in its field. You can set the user_id field to some value that is linked to the user\u0026rsquo;s authentication ID, in case you want to pave the path for cross-device and cross-application measurement.\nAccording to the documentation, you should be able to define any event parameters in the Fields to set of the configuration tag, if you want those event parameters to be sent with every subsequent hit that uses the same Measurement ID. Unfortunately, at the time of writing this does not seem to work.\nNeither does there seem to be an option to send user properties, so it looks like we\u0026rsquo;re waiting for updates to the template for those, too.\nSet user properties To set user properties, you first need to create them in the reporting UI.\n  Next, in your configuration, go to Fields to Set, and add a new field. The name of the field needs to be user_properties.\n  As the value of the field, you need to use a Custom JavaScript variable (or a custom variable template) that returns an object, where each key of the object corresponds to a User Property name you created in the reporting UI, and the value is what you want to send as the user property in question.\nFor example, to set the user_type property (see the screenshot above), I can create a Custom JavaScript variable that looks like this:\nfunction() { return { user_type: \u0026#39;Premium user\u0026#39; }; }  Then you need to add this variable as the value of the user_properties field in the configuration tag.\nAn alternate method is to use a custom variable template (see my guide if you are unsure what custom templates are). Here\u0026rsquo;s a template I created for returning a user properties object. You can use it to create a variable where you set the individual user property keys and values, and then set that variable as the value of the user_properties key in the configuration tag.\n  Once you\u0026rsquo;ve created a configuration tag, set it to fire on the All Pages trigger and enter preview mode. Let\u0026rsquo;s take a look at how this new measurement model works on the website!\nTest it! When you enter the site and load the preview container, open your browser\u0026rsquo;s developer tools, and take a look at the cookies used by GAv2.\n  The _ga cookie is almost the same as with Universal Analytics, with one minor difference. The _ga cookie encodes the number of domain parts in the cookie value as the number after GA1.. For example, if the cookie is written on simoahava.com, the domain comprises two parts, and the cookie value prefix would be GA1.2.. However, with GAv2, this does not seem to be the case when using the default cookie_domain value (automatic configuration).\nI think this might be an oversight, as if you set the domain manually to e.g. www.simoahava.com, the cookie has the regular GA1.3. prefix.\nThe other cookie, _ga_MEASUREMENT-ID**, seems to maintain a **session state**. The first long number is a timestamp (in seconds) of when the cookie was created, and the second long number is a timestamp of when a hit was last sent to GA. In other words, it looks like GAv2 is at the very least keeping score of whether or not a session is currently active. It remains to be seen if it makes more use of client-side persistence for session information, and whether this cookie could be used to infer session state in the web browser.\nThe cookies are set with JavaScript, so they are impacted by measures to reduce the effectiveness of client-side cookies, such as Safari\u0026rsquo;s Intelligent Tracking Prevention.\nNext, spy on the Network requests sent by the browser, or use the Google Analytics debugger extension to see how the library sends hits to Google Analytics.\n  First of all, you can see that the request is sent to a familiar endpoint, /collect, with the exception that it\u0026rsquo;s located in the /g/ path instead.\nThe request type is POST, and the endpoint is still an image pixel. In other words, it looks like the hit defaults to using the Beacon API, which is an excellent improvement to Universal Analytics, where you had to manually enable this feature.\nThe Beacon API protects asynchronous requests that are sent when the page is unloaded, allowing them to complete even if the browser is closed. This lets you avoid the nuisance of missing out on hits (such as link click events) because the user had navigated away from the page before the request had completed.\n  Taking a look at the actual parameters sent to GA, there\u0026rsquo;s a lot of familiar stuff here if you remember how Measurement Protocol requests are built.\n  The v parameter is set to 2, whereas with Universal Analytics it was 1. This is the version of the protocol.\n  The tid parameter is set to your Measurement ID.\n  The gtm parameter is hashed from your GTM container ID.\n  The cid parameter is the Client ID, stored in the _ga cookie.\n  Other parameters that have stayed practically the same include things like uid (user ID), dl / dr / dt (document metadata), and sr (screen resolution).\nThere is no longer a \u0026ldquo;Hit Type\u0026rdquo; dimension, but rather the en parameter (event name) is used to distinguish different types of hits from each other. This is a major difference to Universal Analytics, as in GAv2 you have one single event stream, where the event parameters you send with the event name determine how the data shows up in the reports.\nEvent parameters are prefixed with ep. or epn. for text parameters and number parameters, respectively. In the screenshot, I\u0026rsquo;ve created an event parameter named country and set its value to US.\nUser Properties are prefixed with up. or upn. for user properties in text or number format, respectively. The screenshot shows a single text user property named user_type set to value Premium user.\nFinally, the sid parameter seems to contain the timestamp when the current session started (taken from the _ga_MEASUREMENT-ID cookie).\nIt\u0026rsquo;s still somewhat unclear what the other parameters do.\nOne thing you might have noticed is the delay it takes for the hit to be sent. When you load the page, you can see how the browser waits a few seconds before dispatching the hit to GA. This is because GAv2 batches requests automatically, which, again, is a great feature upgrade.\nBy batching multiple hits into a single request, browser resources are saved for other things. The delay is due to the browser waiting to see if other hits should be dispatched together with the Page View.\nBefore creating your first event tag, take a look at how the Enhanced Measurement stuff works. First, scroll down to the bottom of the current page and take a look at the event sent to Google Analytics.\n  Then, click a link that leads away from your site (use CTRL/CMD + Click to open it in a new tab). Take a look at the parameters.\n  Finally, if you have a YouTube embed on your site which has the enablejsapi=1 parameter set, check out automatic video tracking!\n  This stuff is ridiculously cool, but I do hope we get some controls to customize how Enhanced Measurement works.\nNext, let\u0026rsquo;s create our own event tag!\nCreate an event tag The thing about events in the new Google Analytics: App + Web property is that they function similarly to how gtag.js and Firebase use events. In other words, there\u0026rsquo;s a combination of automatically collected events, recommended events, and custom events.\nWhen creating new events, first make sure that the event is not already collected automatically.\nNext, check if there\u0026rsquo;s a recommended event structure you could use instead of a custom event. If you find a corresponding recommended event, make sure you use the suggested event name and parameters when configuring the event tag.\nFinally, if you want, you can still send a completely customized event name with totally arbitrary event parameters. Just note that you\u0026rsquo;ll lose some flexibility in the reporting UI if doing so, as there are some strict limitations to how many custom properties you can define for reporting in the UI.\nLet\u0026rsquo;s start with a simple custom event.\nCreate a new Google Analytics: App + Web Event tag, and populate it like this:\n  Set the Configuration Tag setting to the tag you created in the previous chapter. This works similarly to the Google Analytics Settings variable in that you only need to configure the tracker settings once, after which they\u0026rsquo;ll apply to all tags that use the configuration tag.\nGive the event a custom name (i.e. not one of the automatically collected or recommended events). In this case, we\u0026rsquo;re using data_loaded as the event name.\nThen, add some parameters.\n  all_data is a custom text parameter set to the value true.\n  debug_mode is a parameter which, when set in the tag, makes the hit appear in the DebugView stream of the reporting UI (more on this below).\n  Set the tag to fire on a Window Loaded trigger, refresh Preview mode, and load the container on the website. Take a look at the network requests to /collect.\nOnce the page has loaded, you should see a single request with two hits bundled into one (batching).\n  As you can see, there\u0026rsquo;s the page_view hit on the first row, and then your custom data_loaded event on the second row.\nNow, jump into the Google Analytics reporting UI, and open DebugView from the report navigator.\n Don\u0026rsquo;t forget to read Krista\u0026rsquo;s excellent article on the stream reports to understand how to interpret the following data. You can also check out Firebase\u0026rsquo;s own DebugView documentation for more information.\n From the Debug Device selector in the top of the screen, you should be able to see how many devices (e.g. browser instances) are currently sending debug hits to GA. From the selector, go through the devices listed until you find one that has recently sent the data_loaded event.\n This Debug Device selector is really unusable at the moment - hopefully it\u0026rsquo;ll be easier to find the debug devices in the near future.\n Once you find the data_loaded event, you might see other hits included in the debug view, too. This is because if the batch contained other hits than the data_loaded hit, they will automatically be enumerated in the DebugView as well.\n  You can click through the parameters in the data_loaded event to see what type of information is dispatched to Google Analytics with the hit.\nBefore finishing here, let\u0026rsquo;s try sending a recommended event, too. We\u0026rsquo;ll send a search event to simulate a site search.\nIf you check out the instructions in the recommended events list, you can see that the search event name is search, and it takes a single parameter: search_term. So, let\u0026rsquo;s create a new event tag and populate these values! This is what the tag would look like:\n  As you can see, I\u0026rsquo;ve just hard-coded the search term into the event parameters. I\u0026rsquo;m firing the tag with a Custom Event trigger set to the event name search.\nAfter refreshing preview mode and reloading my website, I can fire the event with a simple window.dataLayer.push({event: 'search'}) executed in the JavaScript console of the page.\nIn DebugView, I can see that the event was registered:\n  And\u0026hellip; that\u0026rsquo;s about it. There\u0026rsquo;s nothing else that distinguishes this event from custom events. There\u0026rsquo;s no report yet that would collate site search data in a similar fashion to Universal Analytics. At some point I\u0026rsquo;m certain search events will be recorded in their own report, where search data is combined with the automatically collected view_search_results event (a Page View recorded with query parameters corresponding to a site search).\nSummary With this article, my purpose was to show you how to set up the new Google Analytics: App + Web property when using Google Tag Manager as the implementation tool of choice.\nI hope I\u0026rsquo;ve made it very clear that the feature is still in beta, and is thus purposefully incomplete. New features will be rolled out over time, until some day in the future when we break out of beta with a fully functional platform.\nWhether this platform is intended to replace Universal Analytics remains to be seen, but conceptually it would be difficult to justify future maintenance or feature releases to Universal Analytics, when there\u0026rsquo;s this shiny new measurement model to contend with.\nI\u0026rsquo;m cautiously excited about bringing Firebase to the web. It\u0026rsquo;s something I\u0026rsquo;ve been looking forward to ever since I began to work with Firebase (and Firebase Analytics) a long time ago. I think the measurement model of users and events is superior to the heavily aggregated and sessionized model used by Google Analytics.\nHowever, it remains to be seen just how much of a context switch we need to do mentally in order to understand how the use cases addressed by Google Analytics: App + Web are comparable to those of Universal Analytics.\nI will definitely put more resources to writing content for Firebase now, as this new way of bridging together application and website analytics is too interesting to ignore.\nLet\u0026rsquo;s discuss Google Analytics: App + Web in the comments, but please remember that we are talking about a beta product. You are free to sound off your frustrations about development decisions and missing features, but do note that providing your criticism in a constructive manner could end up with changes addressing those criticisms in the final product, as the purpose of a beta release is to pull in feedback from the community.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/enable-preview-mode-safari-browser/",
	"title": "#GTMTips: Enable Preview Mode In The Safari Browser",
	"tags": ["google tag manager", "gtmtips", "itp", "safari"],
	"description": "How to enable Google Tag Manager&#39;s Preview mode in the Safari browser.",
	"content": "With Intelligent Tracking Prevention, the Safari browser is on a crusade against cross-site tracking. One of the most obvious and long-standing ways to battle cross-site tracking has been to block third-party cookies in the web browser, and this is exactly what Safari does by default.\nHowever, Google Tag Manager\u0026rsquo;s Preview mode relies on a third-party cookies, so that it can serve you the draft version of the container while serving the regular, live container to your site visitors.\nThus, to be able to use the Preview mode in the Safari browser, you will need to relax the cross-site tracking blockers in the browser, or just not use Safari for debugging the container.\nTip 102: Enable Preview mode in the Safari browser   When you enter Preview mode in the Google Tag Manager UI, GTM actually sends you to the googletagmanager.com domain, where a first-party cookie is set in your browser with an authentication token and details about the container version to be previewed. Then, when you visit your site and fetch the gtm.js container from the same domain, the cookie originally set on googletagmanager.com is now used in a third-party context to serve you the correct, draft container file.\nIf your browser is blocking third-party cookies, the request for gtm.js will not have access to the cookies written on googletagmanager.com, and thus you get served with the live container instead.\nIn Safari, to enable third-party cookies and thus enable GTM\u0026rsquo;s preview mode, you need to do the following steps.\n1. Enable cross-site tracking in Safari The new \u0026ldquo;Cross-site tracking\u0026rdquo; toggle in Safari\u0026rsquo;s Privacy settings is essentially the ITP switch. By unchecking it, you are giving the browser authorization to read, write, and make use of cookies in the third-party context. If this sounds unappealing, you will need to use some other browser for previewing GTM\u0026rsquo;s containers.\nTo find this switch, click the Safari menu, choose Preferences, expand the Privacy tab, and uncheck the Prevent cross-site tracking checkbox.\n2. Restart the browser This is important - you need to restart the browser for the change to come into effect.\n3. Go to Preview mode in Google Tag Manager Now, browser to your container in the GTM UI, and click the PREVIEW button. Contrary to how the other browsers work, you will be taken to a page on the googletagmanager.com domain, where you will now explicitly need to click that START PREVIEW button.\n  If you\u0026rsquo;re wondering why that button is there, it\u0026rsquo;s a good question. Most likely it has to do with the early versions of ITP (1.x), where you could use third-party cookies as long as there was a meaningful interaction on the domain where the cookies are written. Clicking a button is interpreted as a meaningful interaction.\nHowever, ITP 2.x, this is no longer enough, so the button doesn\u0026rsquo;t really do anything if you have cross-site tracking prevention toggled on.\nIf everything worked, you should see the familiar orange preview bar in your GTM UI. If you don\u0026rsquo;t see it, make sure you actually restarted the browser.\n4. Go to the website and clear the cache Finally, you can browse to the website itself. If you don\u0026rsquo;t see the Preview panel, it\u0026rsquo;s most likely because your site is caching the non-preview version, and you need to clear the cache. To clear the cache, hit CMD + OPT + E, or choose the \u0026ldquo;Empty Caches\u0026rdquo; option in the browser\u0026rsquo;s Develop menu. If you don\u0026rsquo;t see a Develop menu, follow this link for instructions.\nAfter clearing the cache, simply reload the page, and you should see your debug panel.\nSummary Just a quick tip this time, in response to a number of queries around this topic every since ITP really started having an impact on browsing behavior.\nLet me know in the comments if you still can\u0026rsquo;t get it to work, but you should be fine by dutifully following the steps outlined above.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/introducing-gtm-templates-library/",
	"title": "#GTMTips: Introducing The GTM Templates Library",
	"tags": ["google tag manager", "gtmtips", "custom templates", "open source"],
	"description": "The GTM Templates library is a open-source user interface for viewing and downloading custom templates created by the community for Google Tag Manager.",
	"content": "When Custom Templates were released in Google Tag Manager, many of us active in the GTM communities started doing two things: 1) creating our own custom templates, and 2) waiting patiently for Google to release a \u0026ldquo;gallery\u0026rdquo; or \u0026ldquo;library\u0026rdquo; for distributing these community contributions.\nWhile I have full faith in the latter happening some time in the future, I thought it would be fun to create something similar to a library, and then open-sourcing it for the community to help out with or to download locally for their own purposes.\nThe end result can be seen at \u0026ndash;DEPRECATED, see below\u0026ndash;.\n UPDATE 9 July 2020: Because Google now has the official community gallery, the resource described in this article has been shut down.\n The open-source GitHub repository can be found here: https://github.com/sahava/GoogleTagManagerTemplates_site.\nTip 101: Introducing the GTM Templates service   First, let\u0026rsquo;s get one thing out of the way. This is a hobby project. The time and devotion put into it has been entirely dependent on what amounts of these resources have been available at any given time. Thus there are plenty of unpolished corners and places to improve, which is one of the reasons I decided to open-source the whole thing in the first place.\nThe library uses templates that community members have kindly uploaded to the GoogleTagManagerTemplates repository, with metadata added by me (so any mistakes are my fault, and my fault alone. Please let me know in article comments or via email to simo at simoahava.com if you want me to correct any mistake you find).\nThere are basic search, sort, and filter options available, but as we are starting with a relatively small inventory of templates, it\u0026rsquo;s as of yet unknown how much optimization needs to be done with these features.\n  By clicking a template name, you can view detailed information about the template, and you can also download the template TPL file for importing it into Google Tag Manager. You can also view the permissions tab and even leave a review of a template (using Disqus).\n  There are lots of features that could be added to the library, so feel free to contribute ideas as issues in the GitHub repository.\nIf you want to upload templates to the library, you\u0026rsquo;ll need to provide them in the original GoogleTagManagerTemplates repository, or by contacting Simo Ahava via email.\nIf you want to contribute to development work on the service, the best way to do so is to ping Simo Ahava in Measure Slack for an invitation to the private channel where the service design and development collaboration takes place. To collaborate, you will need access to the Google Cloud project of the service, and some other information and secret keys, too.\nI am working on improving the documentation in the repository for the service, so that users can download the repository locally and run it with success. It basically requires you to create your own Google Cloud Project and configure Cloud Datastore to work with your local installation.\nThank you to all the collaborators who have worked on the service thus far (you can find a list of all contributors in GitHub).\nI\u0026rsquo;ll just sign off by repeating that this is a hobby project. If Google decides to release a library or gallery for custom templates, it\u0026rsquo;s very likely that its feature set will be richer and more comprehensive than anything that our community could create or manage as a hobby project, so at that point we might just look at merging our current templates into the official library, and leaving this GTM Templates site to survive in the web as a curiosity.\nFeel free to suggest new features in the comments, or let me know if you are having problems with the site or the templates contained within!\n"
},
{
	"uri": "https://www.simoahava.com/tags/open-source/",
	"title": "open source",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/google-cloud-platform/",
	"title": "google cloud platform",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-monitor/",
	"title": "How To Build A Google Tag Manager Monitor",
	"tags": ["google tag manager", "custom templates", "monitor", "google cloud platform", "bigquery"],
	"description": "How to build a monitor system for Google Tag Manager, where the rate and success of tags firing on your website is logged into BigQuery for analysis and anomaly detection.",
	"content": "Google Tag Manager is strictly a tag delivery system, and it\u0026rsquo;s very careful not to collect any analytics data on its own. This is most likely a deliberate choice, because if GTM was to start collecting data, it would introduce additional barriers to adoption.\nNevertheless, being a tool that consolidates the design, development, deployment, and testing of all the marketing and analytics pixels, code snippets, and utilities running on a website or a mobile app, lacking the necessary features for auditing and monitoring has always seemed like an oversight.\n  Recently, Google Tag Manager introduced a new API for custom templates: addEventCallback. This API lets you create your own monitoring system for your container. With community effort and support from Google, the new API will introduce a number of solutions around tag monitoring and auditing in the near future.\nIn this article, Mark Edmondson and I will help you get started. We’ll introduce how the new API works and how you can use it to build the necessary tags and triggers in GTM for monitoring purposes. We’ll also show you how to use powerful, cheap, and scalable features of the Google Cloud Platform to create the backend for the monitoring system, and finally we’ll give you some ideas for how to visualize the data in a Google Data Studio report.\nYou can also check out this video, where I go through (almost) the same steps in video format.\n  Guest author: Mark Edmondson Mark Edmondson is a former colleague of mine, a fellow Google Developer Expert for Google Analytics, and the owner of one of the most versatile skill sets in our industry. He\u0026rsquo;s currently working as a data engineer at IIH Nordic (a great company full of nice folks in its own right!).\nMark is fluent in bridging together the worlds of marketing and analytics, and his reach extends far beyond, as evidenced by his prolific contributions to the open-source space. He’s the author of the popular googleAnalyticsR library for R, and his blog is full of great walkthroughs for data engineering, data science, web analytics, and software development.\nBeing one of the most humble people I’ve met, he’s probably royally embarrassed by this introduction. So I’ll just wrap up this introduction by making it clear how grateful I am for his contributions not just to this article but to our whole industry.\nWhat we\u0026rsquo;ll build: The Google Tag Manager Monitor The key deliverable of this little project is a BigQuery view that collects data from your website. This data will be sent by way of a new Google Tag Manager custom template, and it will comprise statistics of all the tags that have fired on your site for any given dataLayer event.\n  In this proof-of-concept, the metadata collected from each dataLayer event includes:\n  Event name and timestamp (to uniquely distinguish events with the same name from each other).\n  Tag ID, name, firing status, and execution time for every tag that fired for that event (or null if no tags fired for the event).\n  It\u0026rsquo;s extremely easy to extend this solution to include things like Container ID or tag categorization information - we\u0026rsquo;ll explore these later in the guide.\nWhy do we need a monitor? There aren’t that many ways to monitor the consistency of tags firing in your containers. Obviously, you can run reports against the various endpoints, such as Google Analytics or Facebook, and detect anomalous data collection through their reporting interfaces.\nBut this has some problems:\n  Not all endpoints have reports readily available.\n  With endpoints collecting data from multiple different sources, it’s difficult to identify those that come from GTM and those that do not.\n  It’s difficult to know if the data issues in the endpoints are due to data collection or something else (e.g. a broken filter in Google Analytics).\n  In other words, if you want to be alerted to issues in data collection, the best way to do this is to monitor the data collection itself.\nFor this to be possible, the data collection mechanism requires the possibility to introduce a side effect. The side effect in this case is the addEventCallback API, and its purpose is to collect metadata about the data collection preferably to an endpoint that is separated from the tags that are being monitored.\n  Why have a separate endpoint for monitoring? Well, it wouldn’t really make sense to collect data about Google Analytics’ data collection in Google Analytics itself (a custom reporting property, for example), because if the analytics data collection fails due to not being able to connect to the GA endpoint, then the monitoring data collection would fail as well.\nThat is why this article utilizes an endpoint in the Google Cloud Platform to collect the data. It’s not failsafe, either, since if the data collection fails due to a network outage, the collection to GCP would fail, too. Even though there are ways to tackle this problem as well, we’ll try to keep things at an MVP (minimum viable product) level, and you can then extend the solution to fit your more elaborate needs.\nStep 1: Build the custom template The custom tag template will create a callback that is called every time a dataLayer event reaches conclusion. This is basically the same thing as the eventCallback of Google Tag Manager, just wrapped in a template API.\n  When the callback is invoked, an array of tags is passed to the callback in a data object. Each tag is reported with the following metadata:\n  id: Tag ID\n  status: Firing status (e.g. success, failure)\n  executionTime: Execution time in milliseconds\n  We’ll augment this data set with these values:\n  eventName and eventTimestamp (to uniquely identify each event)\n  name: Tag name\n  It’s easiest to show by doing.\nIn Google Tag Manager, open up the Templates view, and create a new tag template.\nYou can also download the template from here and import it directly into your container.\n1.1. Fields Feel free to add whatever you want into the Info tab of template. We went with this:\n  In the Fields tab, we’re going with two default fields. One for the endpoint URL, and one to let the user decide whether to batch the hits or not.\nThere is a third field as well, which becomes available if the user decided to batch the hits.\n1.1.1. endPoint field The first field is a Text input field named endPoint.\n  It has the following Field configurations toggled on:\n  Value hint: e.g. https://track.com/collect\n  Display name: GET request endpoint\n  Help text: Provide the URL to which the GET request with tag data is sent.\n  Always in summary: checked\n  Validation rules\n  There are two validation rules:\n  This value cannot be empty\n  This value must match a regular expression: ^https://.+\n  If you want, you can add some customized error messages to these validation rules by opening the Advanced settings for each rule.\n  1.1.2. batchHits field The second field is a Radio button field named batchHits.\n  It has the following Field configurations toggled on:\n  Display name: Batch hits\n  Help text: If you select No, details of all the tags that fired for any given hit are sent in a single GET request. If you select Yes, you can choose the maximum number of tags per request, and the tag will automatically send multiple requests if necessary.\n  There are two radio buttons.\nThe first one is named No, and has the value no.\nThe second one is named Yes, and has the value yes.\nThe Yes button has a nested field (found after showing advanced settings for the button), which is a Text input field named maxTags.\n  1.1.3. maxTags field The nested maxTags field is a Text input field where the user can set a limit of tags sent per request.\nThis is useful if you have tags with long names or dataLayer events that can fire dozens of tags at a time. Sending the requests in batches should reduce the effort required in the endpoint to parse the data into the BigQuery table.\n  The field has the following Field configurations toggled on:\n  Display name: Maximum number of tags per request\n  Help text: Enter the maximum number of tags per request that will be dispatched to the endpoint. If necessary, multiple requests will be made.\n  Default value: 10\n  Validation rules\n  The sole validation rule is This value must be a positive integer.\n1.2. Code In the Code tab, add the following code:\n// Require the necessary APIs const addEventCallback = require(\u0026#39;addEventCallback\u0026#39;); const readFromDataLayer = require(\u0026#39;copyFromDataLayer\u0026#39;); const sendPixel = require(\u0026#39;sendPixel\u0026#39;); const getTimestamp = require(\u0026#39;getTimestamp\u0026#39;); // Get the dataLayer event that triggered the tag const event = readFromDataLayer(\u0026#39;event\u0026#39;); // Add a timestamp to separate events named the same way from each other const eventTimestamp = getTimestamp(); const endPoint = data.endPoint; const batchHits = data.batchHits === \u0026#39;yes\u0026#39;; const maxTags = data.maxTags; // Utility for splitting an array into multiple arrays of given size const splitToBatches = (arr, size) =\u0026gt; { const newArr = []; for (let i = 0, len = arr.length; i \u0026lt; len; i += size) { newArr.push(arr.slice(i, i + size)); } return newArr; }; // The addEventCallback gets two arguments: container ID and a data object with an array of tags that fired addEventCallback((ctid, eventData) =\u0026gt; { // Filter out tags that have the \u0026#34;exclude\u0026#34; metadata set to true  const tags = eventData.tags.filter(t =\u0026gt; t.exclude !== \u0026#39;true\u0026#39;); // If batching is enabled, split the tags into batches of the given size  const batches = batchHits ? splitToBatches(tags, maxTags) : [tags]; // For each batch, build a payload and dispatch to the endpoint as a GET request  batches.forEach(tags =\u0026gt; { let payload = \u0026#39;?eventName=\u0026#39; + event + \u0026#39;\u0026amp;eventTimestamp=\u0026#39; + eventTimestamp; tags.forEach((tag, idx) =\u0026gt; { const tagPrefix = \u0026#39;\u0026amp;tag\u0026#39; + (idx + 1); payload += tagPrefix + \u0026#39;id=\u0026#39; + tag.id + tagPrefix + \u0026#39;nm=\u0026#39; + tag.name + tagPrefix + \u0026#39;st=\u0026#39; + tag.status + tagPrefix + \u0026#39;et=\u0026#39; + tag.executionTime; }); sendPixel(endPoint + payload, null, null); }); }); // After adding the callback, signal tag completion data.gtmOnSuccess();  The functionally most significant part of this code is the addEventCallback() method. This invokes the API of the same name.\nThe API checks the dataLayer event that triggered the tag created from this template, and then updates the dataLayer eventCallback with the function defined in the template.\nIn other words, when whatever event that triggers this monitor tag is fully resolved (i.e. all tags have signalled completion), the callback function will execute with data about each of the tags that fired on the event.\nThere\u0026rsquo;s a provision in place to exclude any tags from the monitoring based on a tag metadata field you can set (more on this below). For some, it might make sense to exclude the monitoring tag itself from being monitored, even though there might be value in measuring its execution time along with all the other tags\u0026rsquo;.\nBy far the most significant code is run when the payload is built:\nbatches.forEach(tags =\u0026gt; { let payload = \u0026#39;?eventName=\u0026#39; + event + \u0026#39;\u0026amp;eventTimestamp=\u0026#39; + eventTimestamp; tags.forEach((tag, idx) =\u0026gt; { const tagPrefix = \u0026#39;\u0026amp;tag\u0026#39; + (idx + 1); payload += tagPrefix + \u0026#39;id=\u0026#39; + tag.id + tagPrefix + \u0026#39;nm=\u0026#39; + tag.name + tagPrefix + \u0026#39;st=\u0026#39; + tag.status + tagPrefix + \u0026#39;et=\u0026#39; + tag.executionTime; }); sendPixel(endPoint + payload, null, null); });  Here, every tag that fired on the event is parsed and concatenated into a URL query string payload. For example, a dataLayer event named gtm.js that fired two tags might have the following payload:\n ?eventName=gtm.js \u0026eventTimestamp=1562402273899 \u0026tag1id=12 \u0026tag1nm=GA-PageView \u0026tag1st=success \u0026tag1et=124 \u0026tag2id=29 \u0026tag2nm=Facebook \u0026tag2st=failure \u0026tag2et=422  This is then joined with the endpoint URL, and a GET request is dispatched.\nThis is the payload that is captured by the endpoint in the Google Cloud, which Mark will show later how to create.\nIt\u0026rsquo;s easy to extend this with additional metadata. Just update the payload += concatenation with each additional metadata key you want to access from the tags.\n1.3. Permissions If you’ve added the code into the code editor without errors, you should see three permissions predefined in the Permissions tab.\n    Reads Data Layer: Add the key event into the text area.\n  Reads Event Metadata: No input necessary.\n  Sends Pixels: Add the URL of the endpoint you will be sending the data to (you can use wildcards and partials matches, too).\n  Naturally, you won’t know the endpoint until you’ve created it, so if you want to test the template before creating the endpoint, you can just a placeholder in the permissions such as https://placeholder.com/collect.\nYou can Test the template if you wish, but it won’t really do anything, as the callback requires an event to be pushed into dataLayer to trigger.\nSave the template once you’re done.\nStep 2: Tags and triggers The first thing we’ll need to do is create the monitor tag itself.\n2.1. Create the monitor tag In GTM, go to Tags and create a new tag. Select the template you just created from the tag type selector.\n  The GET request endpoint field takes the endpoint you’ll create in the next section, so you can just use the placeholder value of https://placeholder.com/collect there for now, we’ll return to this shortly.\nIf you want to send the hits in batches, choose the Yes radio button, but you might want to start without batches and just use the default settings.\nNext, expand Advanced Settings and scroll down to a new setting named Additional Tag Metadata.\nClick it open, and check the box named Include tag name. Set the Key for tag name value to name.\nAdd a new metadata by clicking the + Add Metadata button, and set these values:\n  Key: exclude\n  Value: true\n    Because this monitor should monitor every single event that fires in your container, add a new Custom Event Trigger to the tag that looks like this:\n  With that regular expression for the event name, the monitor tag will fire for every single dataLayer event.\nSave the trigger and tag when ready.\n2.2. The Additional Tag Metadata setting As you can see, there’s a new advanced setting for Google Tag Manager tags.\nThis Additional Tag Metadata setting lets you add metadata to every single tag in the container. This metadata is added to the tags passed to the eventCallback of the dataLayer.push() (such as the one added by the addEventCallback custom template API).\nOne predefined metadata is already given to you: the tag name. By checking the Include tag name box, the eventCallback data object will always include the name key (or whatever key you assign), automatically set to the tag’s name.\n  You can add additional metadata too, such as category or type of tag.\nJust remember that the solution as established in this article only makes use of the tag name and exclude metadata keys. Any other metadata you add will require a code change in the template, as the new metadata will need to be concatenated into the payload string as well.\n2.3. Update all your tags The last thing you’ll need to do in GTM (apart from updating the endpoint URL of the monitor tag when you have one), is to update every single one of your tags to include the tag name metadata. The tag name is easier to interpret than the tag ID when building your reports and visualizations.\nFor each tag, expand the Advanced Settings and check the Include tag name checkbox under Additional Tag Metadata. Remember to set the key name to name as well.\nHopefully, the tag name would be automatically included as a metadata in the future - it seems like a sensible feature to have always on. Or, at the very least, flip the checkbox around and require it to be checked if you want to exclude the tag name from the tag’s metadata.\nAt this point, you should have your monitor tag with almost all the settings in place, and you should have now edited all your tags to include the tag name. The monitor tag should have a Custom Event trigger that fires it for every single dataLayer event.\n  Feel free to test in Preview mode to see if the endpoint is invoked with a proper query string. You should see something like this in the Network logs of your browser, assuming you used the placeholder URL of https://placeholder.com/collect.\nNext up, Mark will show us how to get the Google Cloud Platform endpoint up and running!\nStep 3: Creating the Google Cloud Platform endpoint The GCP endpoint needs to process the GET requests sent from the site, insert them into a BigQuery table, parse them into columns and rows, and finally connect the data to Google Data Studio.\n3.1. Choosing the GCP solution for the endpoint Boiling down the use case requirements here, we are looking to turn an API call containing the GTM tag data into useful data for analysis. There are many ways to solve this particular use case on GCP - we\u0026rsquo;ll go through some and show why we eventually went for Cloud Functions.\nThe general solution requirements were:\n  Serverless: so we don\u0026rsquo;t babysit a server but are able to deal with high traffic and/or scale down to zero cost when no data is collected.\n  Low-code: easy to deploy and maintain.\n  Low-config: minimal maintenance so it can be generalized and can deal with varying data schemas easily.\n  The final output is to be a dashboard in Data Studio, so the natural choice for the eventual destination was BigQuery due to the ability to connect via its native Data Studio connector.\n Quick aside: We are wary of dashboards being a final solution for data flows since they rely on a human being to react to give you ROI. It’s fine for a proof-of-concept, but for your own project consider your actual use case - perhaps you could instead implement automatic alerts if certain thresholds on data quality were broken or something similar.\n With the above requirements in mind, some solutions included:\n3.1.1. Pixel tracking via a load balancer We thought about a Pixel Tracking Solution first, since in theory the only steps needed are to upload a pixel to Cloud Storage, make it publicly available, and then stream the access logs into BigQuery.\nHowever the resulting API endpoint is an HTTP IP address (e.g. http://111.222.333.444) and the Google Tag Manager API requires communication over HTTPS. It’s absolutely possible to assign a custom domain and SSL certificate to the load balancer, but to make the demo more accessible for everyone we wanted to find a solution that provided its own domain and HTTPS endpoint.\nHowever, if you do have a custom domain name at hand, the load balancer is probably the most robust solution to utilize, since it doesn’t require the overhead of running a programmed application.\n3.1.2. App Engine The next stop was App Engine, which up to last year would have been the one we’d pursue. In fact, here is some code on how to set up streaming from GTM to BigQuery that Mark wrote three years ago.\nHowever, since then Cloud Functions have arrived which have even less configuration and code required to set up the service. You may still want to consider App Engine if you have more requirements such as integration with Cloud Endpoints or other GCP APIs, for which Cloud Functions do not have support yet.\n3.1.3. Technology of choice: Cloud Functions This is the one we went for. For getting something quickly up and running, it is hard to beat. If using Python, you only need to write code, create a requirements.txt file, set the function to trigger with a public endpoint (via HTTP requests), and you have everything we need - HTTPS, scalable infrastructure and almost no configuration necessary.\n  It may cost more per call than the solutions above, but it scales down to zero meaning you can deploy and wait for traffic without incurring any costs for downtime.\nNote that Cloud Functions can take 100 million calls per 100 seconds in hits, but BigQuery itself accepts only (!) up to 100,000 rows per second per project which equates to 10 million calls per 100 seconds. If this is a limitation you might find yourself struggling with, you\u0026rsquo;ll need to start batching the data to BigQuery instead of streaming it.\n3.2. BigQuery setup With the API endpoint technology selected, we turn to configuring BigQuery.\nLoading data into BigQuery requires a schema for the data (STRING, INT, TIMESTAMP, etc.) which can be tricky if you are sending in a lot of data with untidy types (most data, that is) - one mistake and the data won\u0026rsquo;t load. We\u0026rsquo;re accepting data from an open HTTP endpoint so we are almost certain to get messy data and thus want to be able to modify sent data without reconfiguring everything.\nTo account for the above, we favor loading data into BigQuery with as few restrictions as possible, but then use BigQuery\u0026rsquo;s SQL to parse or transform the raw data into tidier forms. If you don\u0026rsquo;t know SQL but are better with JavaScript, you may prefer to do that data tidying logic in the Google Tag Manager code sending the data, or if better at Python perhaps in the Cloud Function collecting it. We still recommend to do this kind of data cleaning in BigQuery since otherwise changes need code updates in harder-to-reach places.\nTo achieve this, the BigQuery table accepting the GTM data will have as simple a data schema as possible: it will accept the entire URL as a string and not try to parse out the tag names or GTM events quite yet. We shall then create a BigQuery View on top of that data which will split the URL into a workable, tidy format.\n  The final raw data schema then is simply the URI and a timestamp. We consider a timestamp always useful to have, and we will stream the data into a partitioned table which automatically aligns the datasets with the date of data ingestion to make time-based analysis easier.\nTo create the table, you need a Google Cloud Platform project with billing enabled. Once you have that in place, browse to the BigQuery web UI and create a dataset for your project.\nWith the dataset in place, you can then create a table in it.\nPutting all of the above into place, the BigQuery table settings look like this:\n  The partition will put each hit\u0026rsquo;s data into a partition of today\u0026rsquo;s date, e.g. 20190703, which you can use to query particular time periods via BigQuery\u0026rsquo;s _PARTITIONDATE field.\nMake note of the project name, dataset name, and table name. You are naturally free to choose whatever you want for these, but you need the values when configuring the Cloud Function code next.\n3.3. The Cloud Function Code Now let us get some data into the table. This is the Python code we\u0026rsquo;ll use to update the Cloud Function with. You don\u0026rsquo;t need to store any code locally - you can use the web UI. The code below is based on the examples given using the Python SDK for BigQuery.\nSee the next chapter for information on where to copy-paste this code.\nfrom google.cloud import bigquery import datetime import logging PROJECT=\u0026#39;me-gtm-monitoring\u0026#39; # Update to match your project name DATASET=\u0026#39;gtm_monitoring\u0026#39; # Update to match your dataset name TABLE=\u0026#39;raw_data\u0026#39; # Update to match your table name def stream_bq(uri): client = bigquery.Client() table_ref = client.dataset(DATASET).table(TABLE) table = client.get_table(table_ref) # Stream the URI of the request errors = client.insert_rows(table, [{\u0026#39;URI\u0026#39;:uri, \u0026#39;timestamp\u0026#39;: datetime.datetime.now()}]) if errors: logging.error(errors) def gtm_monitor(request): if request.url: stream_bq(request.url) 3.3.1. Code walkthrough We first import the libraries for connecting to BigQuery (google.cloud.bigquery), creating the timestamp (datetime), and logging errors (logging).\nThen you need to alter the code to set the project, dataset, and table for the destination BigQuery data.\nfrom google.cloud import bigquery import datetime import logging PROJECT=\u0026#39;your-project\u0026#39; # Update to match your project name DATASET=\u0026#39;your_dataset\u0026#39; # Update to match your dataset name TABLE=\u0026#39;your_table\u0026#39; # Update to match your table name As you can see, you actually need to create the table first - you can see the settings for the table in the previous chapter. Once you have the project, dataset, and table created, update the Cloud Function code above with the correct values.\nBelow is the function stream_bq() to stream data into BigQuery. As we are deploying the Cloud Function in the same project that hosts the BigQuery table, Cloud Functions will handle authentication for us. All we need is the bigquery.Client() call, and the Cloud Function will handle authentication and identity management using the default service account set for the project.\nWe then create the table object, and stream the data into the table using the client.insert_rows() method. This will return errors if it fails, in which case we’ll log them.\ndef stream_bq(uri): client = bigquery.Client() table_ref = client.dataset(DATASET).table(TABLE) table = client.get_table(table_ref) # Stream the URI of the request errors = client.insert_rows(table, [{\u0026#39;URI\u0026#39;:uri, \u0026#39;timestamp\u0026#39;: datetime.datetime.now()}]) if errors: logging.error(errors) Next, gtm_monitor() is the main function that will be the entry point to the Cloud Function. We define this in the web UI (see below). The function checks if the request had a URL, and if it did, it sends this to the stream_bq() method described above.\ndef gtm_monitor(request): if request.url: stream_bq(request.url) Finally we also need to modify the requirements.txt file in the Cloud Function - we can do this in the web UI as well. Cloud Functions automatically parses this file to install any necessary python libraries through pip.\nThe only external dependency we need is the BigQuery SDK. The beauty of Cloud Functions is that you don\u0026rsquo;t need to install this locally and then deploy the whole project at once - you can simply edit the requirements.txt file in the online editor to have the Cloud Function automatically install the dependency before running any code.\n  The line you need to add is this:\ngoogle-cloud-bigquery==1.5.1 3.3.2. Create and deploy the Cloud Function With all the theory out of the way, we can now look how to actually write and deploy the code in the Cloud Function.\n    From a Google Cloud Project (with billing enabled) go to https://console.cloud.google.com/functions/add.\n  Give the function a descriptive name.\n  Select HTTP as the trigger.\n  Tick the Allow unauthenticated invocations box to make it accessible from anywhere.\n  Select the Python 3.7 runtime.\n  Paste in the main function code from the previous chapter into main.py, and make the appropriate changes where required (the project, dataset, and table names).\n  Copy-paste the BigQuery dependency into the requirements.txt tab of the online editor.\n  Set the Function to execute field to gtm_monitor, as this is the entry point of the Cloud Function.\n  Hit Create at the bottom of the screen.\n  Note that you can expand the More link to change the region from us-central-1 to something closer to where most of your traffic comes from. This should improve performance and decrease latency.\nAfter hitting Create, you should be able to see the URL the Cloud Function has assigned to your new function. This would be something like https://us-central1-my-project.cloudfunctions.net/my-function.\nAfter all that you should have a function that looks a little like this:\n    3.4. Update the Google Tag Manager Monitor tag With your cloud function deployed, you should now have a URL endpoint for your Google Tag Manager Monitor tag looking something like https://us-central1-your-project.cloudfunctions.net/your-function.\n  In your Google Tag Manager Monitor tag, update the GET request endpoint to reflect the correct Cloud Function HTTP endpoint and save the tag. You\u0026rsquo;ll then need to edit the Custom Template and open its Permissions tab. Update the Sends Pixels permission with the correct endpoint URL.\n  Make sure to add the * at the end of the permission URL to account for query parameters.\nYou can continue in Preview mode for a while to ensure that everything works before publishing the container for all your visitors.\nIf everything works, you should see GCP logs of your Cloud Function (accessible in the Cloud Function UI) showing HTTP calls are being triggered.\n  Within BigQuery, you should see data is being populated with the URI requested. Make sure to turn the cache off and query via SELECT * FROM dataset.table_name to test live data, as the Preview may show no data even when there is some. Nevertheless, after doing the SELECT * query, or after patiently waiting for the Preview to update, you should see your raw data being logged into BigQuery like this:\n  If you got this far, congratulations! You now have a live stream of data from your Google Tag Manager setup into BigQuery. Just imagine all the other use cases for this pipeline!\n3.5. Parsing BigQuery raw data into a view Now we do the parsing of this data into something useful. Simo took a jab at writing the necessary SQL for the proof-of-concept (M: Good job, Simo! S: Why, thank you!).\nThere is probably some more elegant way to do this with arrays, but this is what we went with in that it uses regular expressions to parse out the URI string into usable columns. It is here you can do adjustments to the output data to put it in a format you want.\nMake sure to update the FROM statement to pull the data from the correct project/dataset/table combination.\nSELECT timestamp, eventName, eventTimestamp, (SELECT REGEXP_EXTRACT(tagString, \u0026#34;\u0026amp;tag\\\\d+id=([^\u0026amp;]+)\u0026#34;)) as tagId, (SELECT REGEXP_EXTRACT(tagString, \u0026#34;\u0026amp;tag\\\\d+nm=([^\u0026amp;]+)\u0026#34;)) as tagName, (SELECT REGEXP_EXTRACT(tagString, \u0026#34;\u0026amp;tag\\\\d+st=([^\u0026amp;]+)\u0026#34;)) as tagStatus, (SELECT REGEXP_EXTRACT(tagString, \u0026#34;\u0026amp;tag\\\\d+et=([^\u0026amp;]+)\u0026#34;)) as tagExecutionTime FROM ( SELECT timestamp, eventName, eventTimestamp, tagStringUnnest as tagString, pt FROM ( SELECT timestamp, (SELECT REGEXP_EXTRACT(URI, \u0026#34;eventName=([^\u0026amp;]+)\u0026#34;)) as eventName, (SELECT REGEXP_EXTRACT(URI, \u0026#34;eventTimestamp=([^\u0026amp;]+)\u0026#34;)) as eventTimestamp, (SELECT REGEXP_EXTRACT_ALL(URI, \u0026#34;\u0026amp;tag\\\\d+id=[^\u0026amp;]+\u0026amp;tag\\\\d+nm=[^\u0026amp;]+\u0026amp;tag\\\\d+st=[^\u0026amp;]+\u0026amp;tag\\\\d+et=[^\u0026amp;]+\u0026#34;)) as tagStringRegex, DATE(_PARTITIONTIME) as pt FROM `your-project.gtm_monitoring.raw_data` ) LEFT JOIN UNNEST(tagStringRegex) as tagStringUnnest ) Once you have the SQL you want running in the BigQuery Web UI, select Save View to save the query as a table that will be used day-to-day.\n  Here’s what the data looks like when parsed into the view.\n  Step 4: Visualize the data For most use cases, having the data in a BigQuery view is already more than enough. That\u0026rsquo;s the data set you can query for anomaly detection, or plug into a pipeline for automated alerts.\nNevertheless, we can see how useful a proper visualization of the data could potentially be. For this purpose, Google\u0026rsquo;s Data Studio is an excellent project for quickly delivering useful visualizations. Best of all, it already has a built-in BigQuery connector for querying your BQ data with.\n  As a demo, we\u0026rsquo;ve created a starter dashboard for you to check out. Feel free to copy it and modify it to suit your own purposes.\nVisit the dashboard here.\nSummary Maybe we haven\u0026rsquo;t said this enough yet, but this is a proof-of-concept. This article serves a number of purposes:\n  You\u0026rsquo;ll learn how to set up a pipeline of data collection from the website, via Google Tag Manager, to a data warehouse located in the Google Cloud.\n  You\u0026rsquo;ll learn about the new addEventCallback API in Google Tag Manager\u0026rsquo;s custom templates.\n  You\u0026rsquo;ll learn about the new Additional Tag Metadata field in GTM\u0026rsquo;s tags.\n  You\u0026rsquo;ll learn how to efficiently monitor the success rates and execution times of your Google Tag Manager tags.\n  We can only hope that others build much cooler and more useful solutions than this simple MVP. Some features we considered useful but not in the scope of this introductory article:\n  Report that compares data from the respective end points with that of GTM\u0026rsquo;s monitor system to see if tags are firing but not sending the correct data.\n  Utilize a pixel file stored in Google Cloud Storage, behind a load balancer, to achieve a zero-code setup with superior performance compared to a Cloud Function.\n  Training a machine learning model on the data set to identify anomalies and automatically alert to them when detected.\n  Let us know in the comments what you think of this, and how you would improve it if you had the chance!\nFinal note from Simo: I am truly grateful to get the great Mark Edmondson to co-author this article and design the monitor solution with me. As the editor, I take full responsibility for any errors and mistakes in the final draft of the article.\n"
},
{
	"uri": "https://www.simoahava.com/tags/monitor/",
	"title": "monitor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/enhanced-ecommerce/",
	"title": "enhanced ecommerce",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/enhanced-ecommerce-builder-custom-variable-template/",
	"title": "Enhanced Ecommerce Object Builder Template For Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "variable templates", "enhanced ecommerce"],
	"description": "Introducing a Custom Variable Template with which you can generate Enhanced Ecommerce payloads without dataLayer in Google Tag Manager.",
	"content": "One of the cool things about Enhanced Ecommerce deployments in Google Tag Manager is that you can use a Custom JavaScript variable to generate the necessary data.\nThere are many reasons to do so, with the biggest one being the flexibility it offers for manipulating the dataLayer object in case quick changes are required on the site, and it would take too long to wait for a new release of the site JavaScript.\nHowever, there\u0026rsquo;s also the chance that you don\u0026rsquo;t have access to a dataLayer implementation of Enhanced Ecommerce at all. This is not a very enjoyable situation, as you\u0026rsquo;ll have to rely on things like DOM scraping to populate the necessary values.\n  To help with some of the pains of manipulating the ecommerce object in dataLayer and for generating an entire Enhanced Ecommerce implementation from scratch without dataLayer, I want to introduce a new custom template of mine: the Enhanced Ecommerce Object Builder variable template.\nPurpose The template allows you to generate an Enhanced Ecommerce object from scratch. You can, of course, use Google Tag Manager variables to populate parts of the ecommerce object from dataLayer in the template itself (see in the screenshot above how the Currency Code field is populated with a variable).\nAny variable created with this template will then return a syntactically valid ecommerce object as its return value, which means that you\u0026rsquo;ll need to have a tag that has the Enable Enhanced Ecommerce Features setting toggled on, and the respective variable selected from the list:\n  Now, when this tag fires, it will pull in the ecommerce object generated by the variable (and the template code), and as a result, populate the hit to GA with the correct Enhanced Ecommerce parameters.\n  Download and import template To download the template, save the following file with the .tpl extension somewhere in your computer:\nEnhanced Ecommerce Object Builder.tpl\nThen, use the instructions found here to import the template into your container.\n  How to use the template First, you need to create a new User-defined Variable using the template. So head on over to Variables, click New, and choose the Enhanced Ecommerce Object Builder when prompted for the variable type.\n  An important thing to note is that all the text fields in the template accept Google Tag Manager variables. So if you want to populate some fields dynamically, you can.\nEnhanced Ecommerce data type The first selection you need to make is Type. The options are:\n  Promotion View\n  Promotion Click\n  Impression View\n  Impression Click\n  Product Detail View\n  Add To Cart\n  Remove From Cart\n  Checkout\n  Checkout Option\n  Purchase\n  Refund\n  If these choices look odd or unfamiliar, I urge you to read about them in my Enhanced Ecommerce guide.\nIn case you selected Promotion View, Promotion Click, or Impression View, the Action Data and Product Data groups will disappear, as those three Enhanced Ecommerce data types do not support the sending of action and product data.\nCurrency Code If you want to set a local currency for the hit, you can set it by toggling the Set Currency Code checkbox and typing the currency code into the field that appears.\nAction Data Action Data is only available if you selected an Enhanced Ecommerce action as the tag type. This rules out Promotion Views, Promotion Clicks and Impression Views, which have their own settings in the template.\nUnder Action Data, you\u0026rsquo;ll see some options, depending on which type you selected.\nIf you selected Checkout or Checkout Option, you\u0026rsquo;ll see the fields for checkout step and option:\n  If you selected Purchase or Refund, you\u0026rsquo;ll see the fields for a transaction:\n  Regardless of which action you selected, you will always see the field where you can add the Product List:\n  Product Data Product Data is only available if you selected an Enhanced Ecommerce action as the tag type. This rules out Promotion Views, Promotion Clicks and Impression Views, which have their own settings in the template.\nThe Product Data group is split into three options. First, you have the products themselves. Second, you have the possibility to add Product-scoped Custom Dimensions. Finally, you can optionally add Product-scoped Custom Metrics.\nProducts are added by clicking the Add Product button. This opens an overlay where you can configure individual product settings.\n  You can add as many products as you wish, keeping in mind that there\u0026rsquo;s an 8192 byte limit to the payload size of Google Analytics hits.\nNote also that not all the fields are necessary for all the action types.\nProduct-scoped Custom Dimensions are added by enabling Product-scoped Custom Dimensions by checking the relevant checkbox, and then inserting new rows into the table that appears.\n  Add the index number of the Custom Dimensions to the Index column, and the value of the Custom Dimension to the Value column.\nIn the Apply to SKUs field, you add the product IDs/SKUs to which the custom dimension should apply. If you want the dimension to apply to all the fields in the hit, leave the Apply to SKUs field blank.\nAs you can see from the screenshot above, you can add multiple rows with the same index as long as the Apply to SKUs field does not overlap. Otherwise, only the last value for any given index is sent to Google Analytics.\nNote also that you can add more than one SKU to the field. You can do this by separating the values with a comma: shirts123,shirts1234.\nYes, this is a rather convoluted way to add custom dimensions to the products, but unfortunately there\u0026rsquo;s no way to have a table of dynamic size (or really of any kind) as one of the fields in the Add Products table.\nThe same process outlined above applies to Product-scoped Custom Metrics, too.\n  Impressions Viewed You can add Impressions to any Enhanced Ecommerce hit, regardless of hit type.\n  Adding impression views and any Product-scoped Custom Dimensions and/or Metrics related to them follows the exact same process as the one for regular products, so check out the previous chapter for a refresher. The main difference is that list is one of the fields you need to add to the impression data, so that the impression can be assigned to a specific product list.\nPromotions Viewed You can add Promotion Views to any Enhanced Ecommerce hit, apart from Promotion Click.\n  Promotion views are added simply by clicking the Add Promotion button, and then inputting values for ID, Name, Creative, and Position.\nPromotions Clicked You can add Promotion Clicks if you\u0026rsquo;ve selected Promotion Click as the action type.\nThe clicked promotions are added exactly the same way as you would add viewed promotions, so see the previous chapter for reference.\nTesting everything Once you\u0026rsquo;ve created the variable and added it to a tag (see the chapter titled Purpose for a refresher on how to do that), you\u0026rsquo;ll need to test it works.\nThe easiest way to test is by using a browser extension like GTM/GA Debug or dataLayer Inspector+. If everything works, you should see all the data added to the hit generated by the tag whose ecommerce settings you modified.\n  Summary With this variable template, you should be able to build your custom Enhanced Ecommerce settings without the benefit of a fully-formed, syntactically valid dataLayer object. Naturally, having access to dataLayer for formatting the Enhanced Ecommerce data is almost invariably more reliable than building the payloads from scratch like this, but I do believe there to be use cases for a custom setup as well.\nAlso, since you can add other GTM variables to almost all the fields of this template, you can use the Enhanced Ecommerce Object Builder to format and manipulate the data already in dataLayer.\nLet me know in the comments if you think there are features missing. One thing I\u0026rsquo;d like to work on at some point is to let you provide an ecommerce object from dataLayer as the foundation for the variable, but then override individual fields and values of that object with this template.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/send-page-visibility-state-to-ga/",
	"title": "#GTMTips: Send Page Visibility State To Google Analytics",
	"tags": ["google tag manager", "gtmtips", "javascript"],
	"description": "Use a simple JavaScript variable to send the document hidden state (true/false) to Google Analytics as a custom dimension. This can be used to qualify the hits being collected.",
	"content": "One of my pet peeves about Google Analytics has to do with nomenclature. For example, a User isn\u0026rsquo;t really a user but a browser instance, and Direct traffic isn\u0026rsquo;t necessarily \u0026ldquo;direct\u0026rdquo; at all, but rather just traffic that has no discernible source. But being so invested in content analytics, my biggest gripe has to do with Pageviews.\nA Pageview in Google Analytics is collected when a hit with the hit type pageview is received successfully by the Google Analytics endpoint. This hit is dispatched most often with a JavaScript library such as Google Tag Manager or gtag.js.\nThe problem with the name is that it doesn\u0026rsquo;t really tell you at all whether the page was viewed or not. It just tells you that a line of JavaScript was executed on the page.\nLuckily, pageviews aren\u0026rsquo;t typically at the top of the list of Most Actionable KPIs, but you still see them being referenced especially in journals that rely on clicks rather than engagement.\nWith this #GTMTips post (my one hundredth!), I want to show you an extremely simple way of adding this crucial metadata of page visibility to your Google Analytics hits, so that you can segment your data for qualified pageviews, and maybe even ignore events that happen when the page is not visible.\nTip 100: Send page visibility state as a custom dimension to GA   The steps are simple:\n  Create a Hit-scoped Custom Dimension in Google Analytics, and make note of its index.\n  In Google Tag Manager, create a new JavaScript variable for document.hidden (see below).\n  Then, still in GTM, edit your Google Analytics Settings variable, and add a new Custom Dimension with the index from step (1) and the variable from step (2).\n  This is what the JavaScript variable looks like:\n  Note the Format Value settings. You need to cast the Boolean true and value (returned by document.hidden) into their string counterparts \u0026quot;true\u0026quot; and \u0026quot;false\u0026quot;. Otherwise analytics.js has the annoying habit of converting Booleans to 1 and 0 which makes the reports difficult to intepret.\nThe document.hidden property is nicely supported by all the major browsers.\nYou don\u0026rsquo;t have to add the Custom Dimension to all your hits, but I personally think it makes sense. Why qualify just the pageviews? Events can happen \u0026ldquo;off-viewport\u0026rdquo; too, and it might make sense to focus only on the ones that happen in an active browser window. This is what the Google Analytics Settings variable would look like, assuming the index of the Custom Dimension is 13 and the variable name is {{document.hidden}}:\n  Finally, I\u0026rsquo;m not illusioned enough to think that this somehow \u0026ldquo;fixes\u0026rdquo; anything. Collecting visibility state doesn\u0026rsquo;t guarantee that the user is actually viewing anything, as they might be fetching their cup of coffee or dozing off in front of the screen. But I\u0026rsquo;m pretty convinced that this is an improvement especially when you want to consider pageviews to be true to their name.\nSo, that\u0026rsquo;s one hundred GTM tips all done. And it took me almost five years. I promise to get the next hundred done much, much faster!\n"
},
{
	"uri": "https://www.simoahava.com/tags/app-engine/",
	"title": "app engine",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/google-cloud/create-cookie-rewrite-web-service-google-cloud/",
	"title": "Create A Cookie Rewrite Web Service Using The Google Cloud Platform",
	"tags": ["google analytics", "itp", "google cloud", "app engine"],
	"description": "Create a simple web service on Google Cloud App Engine, which digests POST requests and uses the data to return new cookies in a Set-Cookie header.",
	"content": " Last updated 11 September 2020: Added important note about how the custom domain should be mapped with A/AAAA DNS records rather than a CNAME record.\n Ah, Safari\u0026rsquo;s Intelligent Tracking Prevention - the gift that keeps on giving. Having almost milked this cow for all it\u0026rsquo;s worth, I was sure there would be little need to revisit the topic. Maybe, I thought, it would be better to just sit back and watch the world burn.\nBut then Mr. Charles Farina, a good friend and so amicable he\u0026rsquo;s practically Canadian, tempted me into testing out a quick proof-of-concept web service that would give us back our first party cookies.\nAs it turns out, it\u0026rsquo;s not all that time-consuming to do and it works pretty nicely. Want proof? Visit this site with Safari, and check the expiration of the _ga cookie. Cool, huh?\n  In this article, I\u0026rsquo;ll show you how to set it up.\nThe article will focus on the _ga cookie used by Google Analytics, but the same method can be used for any first-party cookie set by JavaScript libraries running on your site.\n  You can also follow the steps outlined in this article by watching the video above.\nBut why? With ITP 2.1, Safari caps first party cookies written with JavaScript to a maximum 7-day expiration. The Google Analytics library writes its cookie with JavaScript. Ergo, on Safari, the Google Analytics cookie persists user data for only 7 days at a time, meaning if the user visits the site with more than 7 days between visits, they will be considered a new user in their return visit.\nWith ITP 2.2, expiration of some cookies is set to 24 hours.\n From https://webkit.org/blog/8828/intelligent-tracking-prevention-2-2/  However, Safari does not (currently) cap cookies set with the Set-Cookie HTTP header. Why? Because creating a setup that makes use of the Set-Cookie header requires developer resources and is always a deliberate decision how to handle first-party persistence.\nITP\u0026rsquo;s main beef is against third-party JavaScript libraries executing code that repurposes first-party storage for something other than first-party data collection (namely, cross-site tracking).\nSo when you have a site deliberately building or using a web service which requires the extension of resources to things like the creation of new DNS records, I guess Safari thinks that by doing all this work, you\u0026rsquo;re accepting responsibility for any of the scummy things that can be done on top of such a setup.\nBut I don\u0026rsquo;t care about any of that. I just want to tackle the problem itself and then share the solution.\nHow does it work?   I\u0026rsquo;ve been at this blog for more than six years now, and my skills at drawing a process diagram or flowchart are just getting worse and worse. Sorry about the mess above.\nHere\u0026rsquo;s how the setup works:\n  When the user loads a page, a Google Analytics tracker is generated with the first GA event.\n  This tracker checks if the browser has the _ga cookie. If not, a new one is generated.\n  If the URL has cross-domain linker parameters and the tracker has the allowLinker:true field, then the tracker will use those parameters to update the _ga cookie.\n  Since this cookie is generated with JavaScript, it\u0026rsquo;s susceptible to ITP 2.1. Thus, in the hitCallback of the Page View tag, the web service is called with a request to return a cookie named _ga with the value from the cookie generated by analytics.js.\n  The web service returns this in the Set-Cookie header, and thus the _ga cookie is updated as an HTTP cookie, ignoring the impact of ITP 2.1 and ITP 2.2.\n  The only way the above works smoothly is by adding the new (and poorly documented) cookieUpdate field to the tag settings. This field ensures that the tracker does not update the expiration of the cookie if one is found. This is instrumental, as otherwise each time a tracker is initialized the cookie is updated from an HTTP cookie back to a JavaScript cookie.\nWhat you\u0026rsquo;ll need For the solution to work, you\u0026rsquo;ll need the following:\n  A credit card, since you won\u0026rsquo;t be able to efficiently create a Google Cloud Project without using a billing account.\n  A domain for which you have full access to DNS settings. This domain must share the same root/parent as the domain from where you\u0026rsquo;ll be sending requests to the web service. So if your site is at www.website.com, the domain you\u0026rsquo;ll set up as the web service must be a subdomain of (or the actual domain) website.com.\n  Necessary access to modify the settings of every single Google Analytics tag that fires on the site.\n  The domain mapping is important. Since the web service uses the Set-Cookie header to set the cookie, it can only do so in a first-party context if the header sets the cookie on a hostname shared by both the domain from where the request is originating (e.g. www.website.com) and the domain you have mapped to the web service (e.g. tracker.website.com).\nStep 1: Set up Google Cloud First, visit Billing management with your favorite Google ID and create a new Billing Account for your Google Cloud organization or project. If you already have billing accounts setup, you can skip this step. You\u0026rsquo;ll need to add your credit card details for situations where you blow past the free quotas. Don\u0026rsquo;t worry! You can add budgets to your billing account to avoid surprises.\nNext, visit the Google Cloud console with the same login, and click to Create project.\n  Give the project a name and make sure to EDIT the ID so that it\u0026rsquo;s easier to remember / handle. You\u0026rsquo;ll be prompted to add a billing account to the project, so choose the one you just created, or whatever account you want to use.\nNext, visit https://console.cloud.google.com/apis/api/cloudbuild.googleapis.com/overview and click Enable API to enable the Cloud Build API. We\u0026rsquo;ll need this to upload our configurations to the cloud.\nFinally, go to https://cloud.google.com/sdk/install and follow the steps. You\u0026rsquo;ll want to install the Google Cloud SDK so that you can run most of the necessary commands from the command line.\nAfter installing the SDK, run gcloud init in your terminal or command-line program. This will initialize the Google Cloud SDK for you, and it will let you choose which project to activate (choose the one you created earlier in this chapter).\nStep 2: Clone the GitHub repository First, make sure you have the git client installed. I\u0026rsquo;m using the command-line client in these examples, but feel free to use a GUI (graphical user interface) client if you wish.\nOnce the git client is installed, run git clone https://github.com/sahava/cookie-bouncer-service.git in a directory of your choice to clone the source code to your computer.\n  Let\u0026rsquo;s recap.\n  You have created a Google Cloud Platform project, linked to a Billing account with the Cloud Build API enabled.\n  You have installed the Google Cloud SDK, and you have initialized it with the same login you used to create the Google Cloud project, and you\u0026rsquo;ve configured it to set your new project as the active project.\n  You have cloned the GitHub repository.\n  Step 3: Create and deploy the App Engine application Your next step is to create an App Engine application. App Engine is Google\u0026rsquo;s fully managed, serverless application platform. It\u0026rsquo;s like a light-weight virtual machine environment, where Google handles things like scaling and instance generation for you.\nThe web service we\u0026rsquo;ll create in this exercise will run on the App Engine Standard environment.\nWhile in the folder where you cloned the Git repository, run the following command:\ngcloud app create\nThis will initiate the App Engine creation process. It will ask you for a region, so choose one that is geographically close to your location, and make sure it supports the standard environment.\n  Next, open the file app.js for editing, and find the lines containing const allowedHosts = [...]. In this array, you need to add all the hostnames from which you expect to call the web service. Remember, they must share the root domain with all the custom domains you\u0026rsquo;ll map to the web service. For example, on my site I only intend to call the web service from https://www.gtmtools.com and any of its pages, so I would simply add:\nconst allowedHosts = [ \u0026#39;https://www.gtmtools.com\u0026#39; ];    If I also wanted to map something like service.simoahava.com to the App Engine service (yes, you can add multiple domains), I could also add https://www.simoahava.com to the list of allowed origins.\nNext, run:\ngcloud app deploy\n  It will deploy the App Engine application using the source code from the repository. Note! You must be in the folder with the app.yaml file from the repository for this command to work.\nSoon it will tell you that everything is running. You can then type gcloud app browse to automatically open a browser window in the application root. If it works, you should see a warning that GET requests are not supported.\n  The reason it complains about GET is because the endpoint isn\u0026rsquo;t configured to handle GET requests. It\u0026rsquo;s only meant to handle POST requests.\nYou\u0026rsquo;re almost done with the web service. The one piece of the puzzle that\u0026rsquo;s missing is to map the web service to your custom domain, so that it can set cookies on your site with the Set-Cookie header.\nStep 4: Map the custom domain to the endpoint Browse to https://console.cloud.google.com/appengine/settings/domains and make sure you\u0026rsquo;ve got your App Engine project selected.\nClick Add a custom domain to get the domain verification process started.\nFrom the Select the domain you want to use, choose Verify a new domain, and type the domain name into the respective field. Click Verify.\n NOTE! It\u0026rsquo;s important that you verify the subdomain that you want to use, rather than add a new subdomain to an already verified \u0026ldquo;naked\u0026rdquo; domain. The difference is that with the first option you need to set A/AAAA DNS records, and with the second option you need to set a CNAME record. With upcoming changes to Safari, it\u0026rsquo;s important that you do not use a CNAME record for this, as it negates the benefit of the entire solution.\n   It will shuttle you off to something that looks very much like Google Search Console\u0026rsquo;s domain verification screen. You\u0026rsquo;ll need to verify that you own the domain you are trying to map to App Engine.\n  For example, I\u0026rsquo;m using GoDaddy as my service provider, so it\u0026rsquo;s easy to just follow the instructions and add the necessary TXT record.\n  Once you\u0026rsquo;ve done the change, you\u0026rsquo;ll need to wait for the DNS record to propagate. Sometimes it happens really fast, sometimes it can take hours. So just stay in the verification page, and click Verify every now and then to see if the TXT records have been updated.\n  Once the domain is verified, you can click forward in the steps to add a custom domain to the App Engine project. Google will issue a free SSL certificate for the domain, securing the endpoint itself behind HTTPS.\nIn the last step, Update your DNS records to enable security, it will list you a number of A and AAAA records you\u0026rsquo;ll need to add to your DNS settings for the host you want to map to App Engine. For example, in my example of tracker.gtmtools.com, I get the following records:\n  In GoDaddy, I\u0026rsquo;ve now added them as instructed:\n  Now, again, it\u0026rsquo;s a question of just waiting for the records to propagate. Once everything works, typically well within 24 hours of making the changes, you should be able to visit https://tracker.yourdomain.com, i.e. the custom domain you just mapped, and it should return the similar GET error you saw when visiting the App Engine endpoint directly.\nStep 5: Test in the browser Now it\u0026rsquo;s time to do a quick browser test. Browse to one of the domains that you configured in app.js in the allowedHosts array (see this chapter for a refresher). Use the Safari browser.\nThen, open the JavaScript Console - that\u0026rsquo;s CMD-OPT-C on a Mac, but you can also find it in the Develop menu.\nIn the console, copy-paste this code. You need to change the value of the endpoint variable to the web service domain you configured in the App Engine domain mappings, and you need to change the value of the domain variable to match the root domain your site and the endpoint have in common.\nconst endpoint = \u0026#39;https://tracker.gtmtools.com\u0026#39;; const domain = \u0026#39;gtmtools.com\u0026#39;; const xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, endpoint); xhr.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/json\u0026#39;); xhr.withCredentials = true; xhr.send(JSON.stringify({name: \u0026#39;testCookie\u0026#39;, value: \u0026#39;testValue\u0026#39;, options: {domain: domain, maxAge: 1000*60*60*24*365*2}}));  This generates a POST request to the endpoint you have configured, and it instructs the web service to return a cookie named testCookie with value testValue, written on the root domain. Since you don\u0026rsquo;t provide an expiration, the cookie will have an expiration of two years.\nIf all worked out, you should see the POST request come back with a response saying the cookie was processed. Also, the response should have a proper Set-Cookie header. Finally, the cookie should appear in the Storage section of Safari\u0026rsquo;s developer console.\n      So that\u0026rsquo;s how you create first-party cookies with no expiration limitations in Safari post-ITP-2.1!.\nBut let\u0026rsquo;s check out the final step - how to enable this in Google Analytics.\nStep 6: Modify Google Analytics tags To make the _ga cookie work with the web service, you need to do two things.\n  Add the cookieUpdate field with value false to every single Google Analytics tag firing on the page. This is absolutely necessary. If there\u0026rsquo;s even a single GA tag that doesn\u0026rsquo;t have this field (and shares the cookie settings with your tags that DO have the field), the JavaScript cookie will keep on overwriting the cookie set in the HTTP response.\n  Add a hitCallback field and variable to your Page View tag (or whatever tag fires on every single page) that handles the actual logic.\n  The first step is easy. Just use a Google Analytics Settings variable or, alternatively, add the field manually to every single tag.\nThe key is to browse to More Settings -\u0026gt; Fields to Set, and add a new field with:\nField name: cookieUpdate\nValue: false\n  If you\u0026rsquo;re using analytics.js, you set the field on the tracker like this:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {cookieUpdate: false});  And if you\u0026rsquo;re using gtag.js, it\u0026rsquo;s like this:\ngtag(\u0026#39;config\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, { cookie_update: false });  Once that is done, the next step is to create the hitCallback function. I\u0026rsquo;ll show you how to do it in Google Tag Manager, but something similar could easily be done with analytics.js and gtag.js - it\u0026rsquo;s just JavaScript, after all.\nIn GTM, create a new Custom JavaScript variable.\n What? Why not use custom templates? Well, unfortunately they don\u0026rsquo;t support creating POST requests yet. You could reconfigure the endpoint to work with GET requests, but POST is more suitable for handling structured data.\n In the Custom JavaScript variable, add this code:\nfunction() { return function() { // Change these  var endpoint = \u0026#39;https://tracker.gtmtools.com\u0026#39;; var domain = \u0026#39;gtmtools.com\u0026#39;; // If your GA cookie name is something different than the default _ga, change this accordingly  var gaCookieName = \u0026#39;_ga\u0026#39;; // From https://www.w3schools.com/js/js_cookies.asp  function getCookie(cname){var name=cname+\u0026#34;=\u0026#34;;var decodedCookie=decodeURIComponent(document.cookie);var ca=decodedCookie.split(\u0026#34;;\u0026#34;);for(var i=0;i\u0026lt;ca.length;i++){var c=ca[i];while(c.charAt(0)==\u0026#34; \u0026#34;){c=c.substring(1)}if(c.indexOf(name)==0){return c.substring(name.length,c.length)}}return\u0026#34;\u0026#34;} // Don\u0026#39;t touch anything below \t// Using getCookie() because we need to fetch the cookie value when the variable is executed, not when its added to the tag  var gaCookie = getCookie(gaCookieName); var safariApiPoll = {{Cookie - _safari_api_poll}}; // Only run if GA cookie is set and if Safari hasn\u0026#39;t been polled in the last 6 days  if (!gaCookie || safariApiPoll) { return; } var data = JSON.stringify([{ name: \u0026#39;_ga\u0026#39;, value: gaCookie, options: { path: \u0026#39;/\u0026#39;, domain: domain, maxAge: 1000 * 60 * 60 * 24 * 365 * 2 } }]); var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, endpoint, true); xhr.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/json\u0026#39;); xhr.withCredentials = true; xhr.send(data); document.cookie = \u0026#39;_safari_api_poll=true; domain=\u0026#39; + domain + \u0026#39;; path=/; max-age=\u0026#39; + 60*60*24*6; } }  Remember to change the endpoint and domain to match the service endpoint and the root domain, respectively.\nGive the variable name such as {{hitCallback - Cookie rewrite}}.\nThe hitCallback function does two things.\n  If the _ga cookie has been set, and if it\u0026rsquo;s been more than 6 days since the web service was last polled, it requests the web service to write a new cookie named _ga, using the value of the actual _ga cookie written by Google Analytics.\n  It sets a new cookie with a 6 day expiration to avoid polling the web service any more than necessary.\n  The reason we have the 6-day-expiration cookie is to prevent the API from being constantly polled by your tags. Since the API sets a cookie with a two-year-long expiration, it\u0026rsquo;s OK to only poll it once every six days.\nThere\u0026rsquo;s a complicated logic here, so bear with me.\nThere\u0026rsquo;s no way to know using JavaScript what the expiration of the _ga cookie is. Thus, we need to poll the API every now and then to make sure the cookie stays written without a short expiration.\nThe maximum length of time we can persist information about when the API was last polled is 7 days. Why? Because this information is stored in a cookie written with JavaScript. The 6 days is just a precaution - you could easily modify the max-age parameter to make the cookie last a while longer.\nAnd if you don\u0026rsquo;t mind the API being polled constantly, feel free to remove this cookie check or to make it poll the API once a day, for example.\nAnyway. Next you\u0026rsquo;ll need to create two First Party Cookie variables:\n    Finally, you need to find your Page View tag (or whatever tag fires consistently on every page), and scroll down to its More Settings -\u0026gt; Fields to Set (check \u0026ldquo;Enable overriding settings in this tag\u0026rdquo; first). Add a new field:\nField name: hitCallback\nValue: {{hitCallback - Cookie rewrite}}\n  Now that you\u0026rsquo;re all set, it\u0026rsquo;s time to test!\nTake the container to Preview mode, and visit the site with Safari. You should see a request to tracker.domain.com in the Network tab, and you should see your _ga cookie with a full two-year-long expiration. Reload the page and you should see no more requests to the web service, nor should the expiration of the _ga cookie change.\n  Troubleshooting I don\u0026rsquo;t see any requests to tracker.mywebservice.com Make sure you configure the hitCallback variable correctly and that you add it to the tag.\nI see a request to tracker.mywebservice.com but it fails Check the JavaScript console. There should be details about why the request failed. Most likely you\u0026rsquo;ve misconfigured the allowedHosts array in app.js, and thus the cross-origin request isn\u0026rsquo;t going through.\nI see the Set-Cookie header but the cookie expiration doesn\u0026rsquo;t change Make sure the domain is correct - it needs to be a root domain shared by your website and the web service.\nMake sure you\u0026rsquo;ve set the cookieUpdate field to false in all your GA tags.\nI have problems setting up the App Engine application or something else Leave a question in the comments and I\u0026rsquo;ll try to help you out.\nFinal thoughts I hope you find this exercise useful - it should give you some idea about what the technical steps are for creating some type of cookie routing service.\nThere are other ways to handle this, listed in this article. However, the beauty of an API service like this is that it\u0026rsquo;s as scalable as you like (or as your budget allows).\nUsing App Engine Standard environment isn\u0026rsquo;t too expensive. For example, the modest amount of hits my site is generating for the API (around 7000 requests per day) is well within the free quota. App Engine charges based on instance hours, and you\u0026rsquo;re given 28 instance hours for free per day. I\u0026rsquo;m barely hitting 25 with this load.\nNaturally, there are ways to optimize this even further. For example, you could check if the user\u0026rsquo;s browser is Safari and only use the API then. Similarly, you could extend the API poll cookie from six days to far longer expirations by setting THAT cookie with the web service, too. Though it might create some sort of temporal cosmic paradox, I don\u0026rsquo;t know.\nRegardless, let me know in the comments what you think of this solution!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/create-facebook-pixel-custom-tag-template/",
	"title": "#GTMTips: Create Facebook Pixel Custom Tag Template",
	"tags": ["google tag manager", "gtmtips", "custom templates", "tag templates"],
	"description": "How to create a Facebook Pixel template in Google Tag Manager. This guide uses the Templates feature of Google Tag Manager.",
	"content": "After the recent release of Custom Templates for Google Tag Manager, my mind has been occupied by very little else. However, I have a nagging feeling that due to how involved the feature set is, there\u0026rsquo;s still a lot of demystifying that needs to take place before templates are fully embraced by the GTM user base.\nIn this article, I want to show you a concrete example of template creation. It\u0026rsquo;s going to be much more ambitious than the simple walkthrough I explored in the main guide. This time, we\u0026rsquo;ll step through creating an actual, functional template that caters to a very specific use case: the Facebook Pixel.\n This article is not endorsed by Facebook or supported by them in any capacity, official or unofficial.\n We\u0026rsquo;ll stop short of having a perfect representation of the Facebook pixel. However, you\u0026rsquo;ll have all the tools necessary to extend the template to cover all the features you might need and to replicate it with some other third-party vendor code.\nTo sweeten the deal, I\u0026rsquo;ve also created a video that goes through the motions, in case it\u0026rsquo;s easier to follow the steps that way.\n  I want to give special thanks Eric Burley from Google for walking me through some of the intricacies of template APIs.\n OK, OK, I have actually created the full, feature-ready Facebook Pixel Template for you to enjoy. You can find it in this GitHub repository.\n Tip 99: Create a Facebook Pixel template   You can download the template export file here. Read this to find out how to import it into your custom templates. You don\u0026rsquo;t have to use the export file, but it might make it easier to walk through the rest of the article.\nWhat you\u0026rsquo;ll end up with   The template has the following features:\n  Ability to add multiple Pixel IDs, comma-separated. The tag hit will be sent to all the Pixel IDs you specify in the list.\n  Support for four events: PageView, Lead, CompleteRegistration, and Custom (the Custom event will let you specify the event name in a text field that appears).\n  Possibility to add any Object Properties to the hit.\n  You can check a box to disable the Automatic Configuration of the pixel.\n  Step 1: Add template information After creating a new template, this is what the Info screen looks like. Remember to enable Advanced Settings for the editor so that you can edit the Brand Name field.\n  Step 2: Add fields The template will have four fields.\nField 1: Text Input named pixelId   Make sure the following field configurations are enabled for the Text Input field:\n  Always in summary: Checked\n  Display name: Facebook Pixel ID(s)\n  Validation rules: (see below)\n  Value hint: e.g. 12345678910\n  For the validation rules, add two different rules:\n  This value cannot be empty, with Error message You must provide a Pixel ID.\n  This value must match a regular expression, with value ^[0-9,]+$, and Error message Invalid Pixel ID format.\n  You can find the Error message by clicking to show Advanced Settings for each Validation rule.\nField 2: Drop-down Menu named eventName   Make sure the following field configurations are enabled for the Text Input field:\n  Always in summary: Checked\n  Display name: Event Name\n  Nested fields: (see below)\n  Add the following menu items:\n  Item name: PageView, Value: PageView\n  Item name: Lead, Value: Lead\n  Item name: CompleteRegistration, Value: CompleteRegistration\n  Item name: Custom, Value: Custom\n  Click Add field under Nested fields, and choose a Text Input field. Name the field customEventName, and choose the Display name and Enabling conditions field configurations for it.\n  Display name: Custom Event Name\n  Enabling conditions: eventName equals Custom\n    The enabling condition ensures the Custom Event Name is only shown in case \u0026ldquo;Custom\u0026rdquo; is chosen from the drop-down menu. Cool, huh?\nField 3: Group named objectProperties   The default field configurations should be all you need for this Group field.\n  Group style: Collapsible section - Collapsed\n  Display name: Object Properties\n  Nested fields: (see below)\n  Click Add field under Nested fields, and choose a Simple Table field. Name the field propertyList and make sure the \u0026ldquo;New row\u0026rdquo; button text field configuration is toggled on.\nAdd two columns to the table. Both should be Text field columns.\nThe first column should have the following settings:\n  Column name: Property Name\n  Internal name: name\n  Require column values to be unique: Checked (this option becomes available when you choose to show Advanced Settings for this column)\n  The second column should have the following settings:\n  Column name: Property Value\n  Internal name: value\n    Finally, set the \u0026ldquo;New row\u0026rdquo; button text option to Add property, and leave the Display name setting blank.\nField 4: Group named moreSettings   Set the following settings for the group:\n  Group style: Collapsible section - Collapsed\n  Display name: More Settings\n  Nested fields: (see below)\n  Click Add field under Nested fields, and choose a Checkbox field. Name the field disableAutoConfig and make sure the Help text field configuration is toggled on.\n  Checkbox text: Disable Automatic Configuration\n  Help text: Facebook collects some metadata (e.g. structured data) and user interactions (e.g. clicks) automatically. Check this box to disable this automatic configuration of the pixel.\n  Step 3: Edit code Step on over to the Code editor tab, and replace the contents with the following JavaScript:\nconst createQueue = require(\u0026#39;createQueue\u0026#39;); const callInWindow = require(\u0026#39;callInWindow\u0026#39;); const aliasInWindow = require(\u0026#39;aliasInWindow\u0026#39;); const copyFromWindow = require(\u0026#39;copyFromWindow\u0026#39;); const setInWindow = require(\u0026#39;setInWindow\u0026#39;); const injectScript = require(\u0026#39;injectScript\u0026#39;); const makeTableMap = require(\u0026#39;makeTableMap\u0026#39;); const initIds = copyFromWindow(\u0026#39;_fbq_gtm_ids\u0026#39;) || []; const pixelIds = data.pixelId; // Utility function to use either fbq.queue[] // (if the FB SDK hasn\u0026#39;t loaded yet), or fbq.callMethod() // if the SDK has loaded. const getFbq = () =\u0026gt; { // Return the existing \u0026#39;fbq\u0026#39; global method if available  const fbq = copyFromWindow(\u0026#39;fbq\u0026#39;); if (fbq) { return fbq; } // Initialize the \u0026#39;fbq\u0026#39; global method to either use  // fbq.callMethod or fbq.queue)  setInWindow(\u0026#39;fbq\u0026#39;, function() { const callMethod = copyFromWindow(\u0026#39;fbq.callMethod.apply\u0026#39;); if (callMethod) { callInWindow(\u0026#39;fbq.callMethod.apply\u0026#39;, null, arguments); } else { callInWindow(\u0026#39;fbq.queue.push\u0026#39;, arguments); } }); aliasInWindow(\u0026#39;_fbq\u0026#39;, \u0026#39;fbq\u0026#39;); // Create the fbq.queue  createQueue(\u0026#39;fbq.queue\u0026#39;); // Return the global \u0026#39;fbq\u0026#39; method, created above  return copyFromWindow(\u0026#39;fbq\u0026#39;); }; // Get reference to the global method const fbq = getFbq(); // Build the fbq() command arguments const props = data.propertyList ? makeTableMap(data.propertyList, \u0026#39;name\u0026#39;, \u0026#39;value\u0026#39;) : {}; const command = data.eventName !== \u0026#39;Custom\u0026#39; ? \u0026#39;trackSingle\u0026#39; : \u0026#39;trackSingleCustom\u0026#39;; const eventName = data.eventName !== \u0026#39;Custom\u0026#39; ? data.eventName : data.customEventName; // Handle multiple, comma-separated pixel IDs, // and initialize each ID if not done already. pixelIds.split(\u0026#39;,\u0026#39;).forEach(pixelId =\u0026gt; { if (initIds.indexOf(pixelId) === -1) { // If the user has chosen to disable automatic configuration  if (data.disableAutoConfig) { fbq(\u0026#39;set\u0026#39;, \u0026#39;autoConfig\u0026#39;, false, pixelId); } // Initialize pixel and store in global array  fbq(\u0026#39;init\u0026#39;, pixelId); initIds.push(pixelId); setInWindow(\u0026#39;_fbq_gtm_ids\u0026#39;, initIds, true); } // Call the fbq() method with the parameters defined earlier  fbq(command, pixelId, eventName, props); }); injectScript(\u0026#39;https://connect.facebook.net/en_US/fbevents.js\u0026#39;, data.gtmOnSuccess, data.gtmOnFailure, \u0026#39;fbPixel\u0026#39;);  At this point, it\u0026rsquo;s a good idea to take a short breather.\nIf you take a close look at the code, you\u0026rsquo;ll see that it\u0026rsquo;s far more complex than what the Facebook pixel snippet is. The reason for this is the sandboxed JavaScript that custom templates use. For example, the Facebook snippet creates the global fbq() method with something like:\nwindow.fbq = function() { window.fbq.callMethod ? window.fbq.callMethod.apply(window.fbq, arguments) : window.fbq.queue.push(arguments); }  It\u0026rsquo;s a very simple piece of code, which simply passes the arguments you provide to fbq() (e.g. 'track', 'PageView') to one of two places, depending on whether the SDK has loaded yet or not.\nTo do this in a custom template is far more complicated. You can\u0026rsquo;t just set a global variable, you need to use an API for that. You can\u0026rsquo;t just check if a global method exists, you need an API for that. And you can\u0026rsquo;t just create the queue property for the fbq() method, you need an API for that.\nSo, let\u0026rsquo;s go over block by block to understand what the code does.\nInitialize the necessary APIs This script needs a handful of APIs, which are initialized with the require() API:\nconst createQueue = require(\u0026#39;createQueue\u0026#39;); const callInWindow = require(\u0026#39;callInWindow\u0026#39;); const aliasInWindow = require(\u0026#39;aliasInWindow\u0026#39;); const copyFromWindow = require(\u0026#39;copyFromWindow\u0026#39;); const setInWindow = require(\u0026#39;setInWindow\u0026#39;); const injectScript = require(\u0026#39;injectScript\u0026#39;); const makeTableMap = require(\u0026#39;makeTableMap\u0026#39;);  I\u0026rsquo;ll explain how they function when we encounter them in the code.\nFetch list of initialized IDs const initIds = copyFromWindow(\u0026#39;_fbq_gtm_ids\u0026#39;) || []; const pixelIds = data.pixelId;  Here we fetch the list of initialized IDs (stored in a custom _fbq_gtm_ids global array), and we also pull in the value, input by the user, of the pixelId field from the template itself.\nUtility to fetch the proper global fbq method const getFbq = () =\u0026gt; { // Return the existing \u0026#39;fbq\u0026#39; global method if available  const fbq = copyFromWindow(\u0026#39;fbq\u0026#39;); if (fbq) { return fbq; } ...   NOTE! The arrow function is a feature of ES6 supported by Custom Templates. const getFbq = () =\u0026gt; { translates to var getFbq = function() { in the older flavor of JavaScript.\n The purpose of getFbq is to return a representation of the global fbq method, which passes the arguments to the correct place, similar to how the regular Facebook snippet works.\nThe first lines check if fbq has already been created globally, and returns the global method in that case.\nIn case the global method does not exist, it needs to be created.\n... setInWindow(\u0026#39;fbq\u0026#39;, function() { const callMethod = copyFromWindow(\u0026#39;fbq.callMethod.apply\u0026#39;); if (callMethod) { callInWindow(\u0026#39;fbq.callMethod.apply\u0026#39;, null, arguments); } else { callInWindow(\u0026#39;fbq.queue.push\u0026#39;, arguments); } }); aliasInWindow(\u0026#39;_fbq\u0026#39;, \u0026#39;fbq\u0026#39;); ...  Here, the global fbq method is initialized as a new function. This function first checks if the fbq.callMethod method already exists (which means the FB SDK has loaded), and if it does, it passes the arguments sent to the fbq method (e.g. 'track', 'PageView') to this built-in method.\nIf the callMethod method has not been created yet, then the method passes its arguments to fbq.queue as an array push. The queue is basically a waiting list for pixel request, queued up for the Facebook SDK as it loads over the network. Once the SDK has loaded, it processes the messages in this queue and dispatches them to Facebook.\nThe last line makes an alias of the fbq method in another global variable, _fbq. I\u0026rsquo;m not certain why this is necessary, but it is what the Facebook snippet does as well.\n... createQueue(\u0026#39;fbq.queue\u0026#39;); return copyFromWindow(\u0026#39;fbq\u0026#39;); };  The last lines of the setInWindow API call create the fbq.queue global array, before finally returning the current content of the global fbq variable, which is the function you created above.\nPrepare the fbq command const fbq = getFbq(); const props = data.propertyList ? makeTableMap(data.propertyList, \u0026#39;name\u0026#39;, \u0026#39;value\u0026#39;) : {}; const command = data.eventName !== \u0026#39;Custom\u0026#39; ? \u0026#39;trackSingle\u0026#39; : \u0026#39;trackSingleCustom\u0026#39;; const eventName = data.eventName !== \u0026#39;Custom\u0026#39; ? data.eventName : data.customEventName;  These lines first fetch the latest representation of the fbq global method by invoking the getFbq function you just created.\nNext, the contents of the fbq command are built. The makeTableMap API takes your propertyList Simple Table field, and converts each row to a key-value pair, where the key is the first column value (e.g. content_ids), and the value is the second column value (e.g. 123456). It\u0026rsquo;s a really handy API for converting the template table format into what many JavaScript libraries expect.\nThe command variable depends on whether you\u0026rsquo;re using a standard event (e.g. PageView or Lead), in which case it is set to trackSingle, or whether you\u0026rsquo;re using the Custom event, in which case it\u0026rsquo;s set to trackSingleCustom.\nThe eventName takes either the value of the drop-down menu selection if a standard event is selected, or the value of the customEventName text input field if a Custom event is selected.\nCycle through all Pixel IDs defined in tag, and dispatch the commands pixelIds.split(\u0026#39;,\u0026#39;).forEach(pixelId =\u0026gt; { if (initIds.indexOf(pixelId) === -1) { if (data.disableAutoConfig) { fbq(\u0026#39;set\u0026#39;, \u0026#39;autoConfig\u0026#39;, false, pixelId); } fbq(\u0026#39;init\u0026#39;, pixelId); initIds.push(pixelId); setInWindow(\u0026#39;_fbq_gtm_ids\u0026#39;, initIds, true); } fbq(command, pixelId, eventName, props); });  The whole command process is wrapped in an iterator, which loops through all the Pixel IDs the user has added to the tag. The commands are run identically for every single Pixel ID in the tag.\nFirst, the code checks if the Pixel ID has already been initialized by looking at the contents of the initIds array you created at the very beginning of the code. You don\u0026rsquo;t want to initialize any Pixel ID more than once, or you\u0026rsquo;ll risk running into problems with Facebook\u0026rsquo;s SDK.\nIf the pixel hasn\u0026rsquo;t been initialized, then first the autoConfig parameter is set to false, if the user has checked the respective checkbox in the template.\nNext, the fbq('init', pixelId) command is run, after which the Pixel ID is pushed into the array of initialized pixels.\nFinally, the fbq() command is run with the parameters you created previously.\nLoad the Facebook SDK The very last line in the code editor loads the Facebook SDK, and signals either data.gtmOnSuccess() or data.gtmOnFailure, depending on whether the SDK load was successful or not.\ninjectScript(\u0026#39;https://connect.facebook.net/en_US/fbevents.js\u0026#39;, data.gtmOnSuccess, data.gtmOnFailure, \u0026#39;fbPixel\u0026#39;);  Step 4: Permissions Because you use all these APIs and mess so much with the global namespace, you\u0026rsquo;ll need to add some permissions if you want the template code to run.\n  As you can see, every single global variable you interact with, either directly (via copyFromWindow, callInWindow, etc.) or indirectly (via aliasFromWindow) must be specified in the Permissions list.\nSimilarly, the script injection of the SDK itself must be allowed using the appropriate permission.\nTest it! To test it, save the template, then browse to your container\u0026rsquo;s tags, and create a new tag. You should see the Facebook Pixel in the tag menu.\n  Next, fill in the fields. Try a couple of different things, such as passing various object properties, using custom events, and adding more than one Pixel ID.\n  Try also creating more than one tag with the same Pixel ID, to make sure the initialization is done just once per ID.\nUsing GTM Preview mode and Facebook\u0026rsquo;s Pixel Helper, you should be able to verify that everything is working as it should.\n  Final thoughts I hope this article has helped demystify custom templates.\nAs you can see, working with the sandboxed JavaScript isn\u0026rsquo;t just a question of copy-pasting some original code and rewriting some method calls. It calls for a different approach completely, especially when working with global variables.\nThere are some things I think custom templates should do to improve flexibility. For example, the function wrapper that GTM automatically adds whenever you create a global function is problematic, since there are use cases where you might want to be able to add properties to the global function itself. In its current format, custom templates do not permit this, so you need to use an API like createQueue to establish fbq.queue as an array.\nIt would be better if I could just run something like setInWindow('fbq.queue', []), but right now, setInWindow only allows you to set the variable and not its individual properties.\nOther than that, the benefit of using this over a Custom HTML tag is huge: you\u0026rsquo;re minimizing the risk of code errors due to operating through the template, and you don\u0026rsquo;t need the problematic unsafe-eval directive in your Content Security Policy.\nThanks for reading, and perhaps watching! Let me know in the comments if you have questions about how this whole thing works.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/custom-templates-guide-for-google-tag-manager/",
	"title": "Custom Templates Guide For Google Tag Manager",
	"tags": ["google tag manager", "custom templates", "guide", "tag templates", "variable templates"],
	"description": "A comprehensive guide to building custom templates (both tag templates and variable templates) in Google Tag Manager.",
	"content": " Last updated 12 August 2020: Added details about server-side tagging.\n As I have finally managed to pick up my jaw from the floor, it\u0026rsquo;s now time to tell you what\u0026rsquo;s got me so excited. Google Tag Manager recently released a new feature called Custom Templates. Actually, it\u0026rsquo;s not fair to call it a feature. It\u0026rsquo;s a full-blown paradigm shift in how we use Google Tag Manager. It\u0026rsquo;s a suite of features designed to help brands, companies, and users create and share their own custom JavaScript and HTML setups with ease, while taking care that the code is optimized for delivery in the web browser.\n  Custom Templates, in short, are tag, variable, and Client templates that you can create and configure. In other words, if you have a cool idea for a tag (e.g. an analytics tracking tag for a vendor not natively supported by GTM), a variable (e.g. a Custom JavaScript variable that does something with a string), or a Client (e.g. a server-side endpoint for some new analytics tool), you can now turn them into reusable templates which can, in turn, be shared with other users and containers via template export and import. You can also use the Community gallery to distribute your templates.\n  Templates use a customized, sandboxed version of JavaScript, which has its own idiosyncratic vernacular that you must learn (with the help of this guide, of course). The reason for this added complexity is that with templates you can ensure that the code being executed is safe, unintrusive, and optimized.\nFurthermore, templates you create will define certain permissions that are required for the template code to be able to run. An additional level of governance is provided by way of policies defined on the web page itself where the template code might be run. The interplay between these permissions and policies is a core feature of template security.\nThere are lots and lots of things to cover in this guide, so let\u0026rsquo;s just get started.\nHow to read this guide This is a long guide. It has to be - there\u0026rsquo;s so much about custom templates that needs to be addressed in any document whose purpose is to provide a comprehensive treatment of the subject matter.\nHowever, don\u0026rsquo;t interpret my inability to write concise prose as indicative of how complex custom templates are. I can assure you - they\u0026rsquo;re absolutely manageable by anyone who\u0026rsquo;s been using Google Tag Manager for a while.\nThis guide is a reference. Its purpose is to offer you documentation to support your work with custom templates.\nBecause of this, I want to suggest some different ways to approach this guide.\n  Everyone should read the chapters Custom Templates in a nutshell and Core concepts.\n  I really recommend that everyone take a look at the two walkthroughs in the Getting started chapter.\n  Keep the Official documentation handy at all times, particularly the API references for web templates and for server-side templates.\n  When working with templates, the Fields editor (with a deep-dive into Field configurations) chapter should be very useful - same as the one on Permissions.\n  If you\u0026rsquo;re a site admin, you might want to read through the Policies reference to get an idea of how you can further restrict the execution of custom code on your site.\n  If you suffer from insomnia, start from the beginning and don\u0026rsquo;t stop until you fall asleep. Should happen by the 10,000 word mark.\n  Be sure to check out my other guide on how to create a Facebook pixel template - it should shed more light on how templates work. You can also check the corresponding video if you prefer watching rather than reading.\nYou can also view all the custom templates I have created and/or collected in this GitHub repository and in the Templates section of this site.\nCustom Templates in a nutshell Google Tag Manager\u0026rsquo;s Custom Templates offer a way to build a user interface around the custom code you might want to run on the site using Google Tag Manager. The user interface is what you\u0026rsquo;ve come accustomed to when using GTM\u0026rsquo;s tags and variables. It comprises text input fields, settings, tables, labels, drop-down menus, and so forth.\n  Obviously, the UI itself is already a huge asset. Being able to offer a user interface in lieu of a complicated code block will minimize problems arising from input errors, and will help keep the code stable.\nHowever, the templates have another, less apparent (but no less impactful) function. They add layers of protection and security to the code they abstract. Templates use a custom JavaScript framework which introduces a handful of APIs (application programming interfaces) that you must use if you want the code to actually do anything.\nThis introduces a steep learning curve, because you can\u0026rsquo;t just copy-paste code from Stack Overflow any more. If you want to set a global window property, you need to use an API for that. If you want to log to console, you need to use an API for that. If you want to check the value of a cookie, guess what, you need to use an API for that.\n  Basically any code that tries to access the global state of the page or run any native JavaScript functions defined on the global level requires an API call.\nSo why this added complexity? Well, for one, these APIs make sure that potentially dangerous and/or intrusive modifications to the global state are done in a controlled manner.\nWhenever you want to use an API, you must require() it in the template code. And when you introduce an API like that, the template automatically generates a set of configurable permissions for that API call.\n  In a nutshell, templates encapsulate the logic you would otherwise introduce with custom code. By introducing APIs with permissions, the templates can be configured to work in a secure and easily managed context.\nAn added level of security is the introduction of policies, where you as the site owner can add some code to the web page itself, which can have additional levels of control over how template permissions are resolved.\nFor example, if I have a tag configured to send hits to some endpoint, I can write a policy on the page that only allows pixel requests to one of the many endpoints configured in the tag.\nwindow.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;policy\u0026#39;, \u0026#39;send_pixel\u0026#39;, function(container, policy, data) { if (data.url !== \u0026#39;https://snowplow.simoahava.com/i\u0026#39;) { throw(\u0026#39;Invalid pixel endpoint!\u0026#39;); } else { return true; } });  With that policy in place, the image request will only be executed if the endpoint URL is https://snowplow.simoahava.com/i. Otherwise, the tag will fail in an error, and you can see the error message in the Errors tab of Preview mode.\n  An additional perk of using templates is that you don\u0026rsquo;t have to add the nasty unsafe-eval keyword to your Content Security Policy. Any code run through a template is compiled into JavaScript when the container is written, and thus doesn\u0026rsquo;t require the use of eval().\nConversely, with Custom HTML tags and Custom JavaScript variables, the code is written into a string which is then compiled with eval() at runtime. This is a bad practice and requires a huge compromise in security if using a Content Security Policy.\nI hope you can see the usefulness of Custom Templates. Imagine a library of Custom Templates, where anyone can share their own work for others to download and use in their containers. Smaller brands and companies could finally get their tools and platforms out there for the masses using Google Tag Manager.\n Update 2 October 2019: You no longer have to imagine such a library as it now exists. Check out the Community Template Gallery as well as my introduction to it.\n Getting started Before exhausting you with all the details about custom templates (and trust me, there\u0026rsquo;s a lot to digest in the feature), I want to start by walking you through creating a tag template and a variable template. We won\u0026rsquo;t use all the most complex features for this, but it should serve as a nice intro to how custom templates work in Google Tag Manager.\nBe sure to check out this guide for a walkthrough of building a Client template for server-side tagging.\nDon\u0026rsquo;t forget to check out my other article, which covers the creation of a Facebook pixel template. It should provide a more comprehensive (and more overwhelming) look at how templates are created.\nTag template walkthrough In this walkthrough, we\u0026rsquo;ll go through how to create a simple script injection tag. This is how many of the third-party vendors out there want their scripts to be loaded.\nWe\u0026rsquo;ll use a wonderful company called Conductrics as an example. They have developed a tool with which you can do A/B-testing and personalization, using ML-driven targeting logic, dynamic goals, server-side and client-side deployment options, and a whole host of other features to help you answer those difficult business questions you have with data.\n  Note! You can of course replace the Conductrics-specific stuff with some other vendor source URL, if you want. The steps you take in this guide would still be identical.\nThe tag template is simple, by design. Conductrics offers the option to host the required JavaScript for you, so all you need to do is add the \u0026lt;script\u0026gt; tag to the page that loads the JavaScript library from Conductrics\u0026rsquo; server.\nHowever, I\u0026rsquo;ve added some UI sugar to make the setup slightly more interesting.\nBasically, all templates comprise four components:\n  The details, which determine the template name, logo, and description.\n  The user interface, which governs the fields and field configurations the template has.\n  The code editor, which makes use of whatever the user input into the fields to run the actual template code.\n  Permissions, which determine what type of code can be run by the tag.\n  We\u0026rsquo;ll walk through each of these steps here.\nStep 1 - Create and set the details of the tag template The first thing you need to do is create a new tag template:\n  Next, fill in some details. For the image, I used the logo created by Conductrics\u0026rsquo; brand designer Joshua McCowen.\n  You can Save the template by clicking the respective icon in the top-right corner now. You should see the logo and the template title in the preview window.\n  There! Almost done. Well, not quite.\nStep 2 - Create the UI Now we need to create the bells and whistles of the tag\u0026rsquo;s actual user interface.\nFirst, click the Fields tab to open the Fields editor. Then, click the blue Add Field button.\n  In the overlay that opens, select Text input.\n  You should see a text input field appear in the Fields editor. Rename the field to sourceUrl, and edit the Display name setting. The field name is used in the code editor, and the Display name setting is what the user sees above the field when editing the tag settings.\n  The UI could be ready now - it\u0026rsquo;s really the only field and the only configuration you\u0026rsquo;d need.\nBut let\u0026rsquo;s make it a bit more robust by adding some validation.\nClick the cogwheel icon associated with the field to open the Field configuration overlay. In the overlay, make sure Display name, Help text, and Validation rules are all toggled ON.\nEach field type has its own set of configurations you can edit to make the field more versatile.\n  Close the overlay by clicking the X in the corner, or anywhere outside the overlay.\nNext, add some Help text. Help text can be seen by hovering over the little question mark next to the Display name when editing the tag.\n  You can Save the template periodically to refresh the template preview and see the changes there.\nNext, let\u0026rsquo;s add some Validation rules. These can be used to ensure the user adds valid values to the fields.\nClick Add rule and edit the rule to match a regular expression where the expression is ^https://.*. Validation regular expressions look for full matches only, so you need to add leading and/or trailing .* to use an open-ended pattern.\n  Next, click the little action menu in the top corner of the validation rule box, and select Show advanced settings.\n  Validation rules have two advanced settings. The first is where you can provide a custom error message (which we\u0026rsquo;ll use). The second is one where you can establish conditions for when this validation rule is active (or inactive).\nIn this case, let\u0026rsquo;s add a descriptive error message. Set the Error message field to The URL must start with \u0026quot;https://\u0026quot;..\n  You can now quickly test how this works. Click Save in the top right corner, then click the tag template in the preview mode to enter edit mode. Add some string to the text input field that doesn\u0026rsquo;t start with \u0026ldquo;https://\u0026rdquo;, and click the Test button in the top-right corner. You should see your error message.\n  Cool, right?\nLet\u0026rsquo;s add two more validation rules. Make them look like this:\n  The first rule ensures that the script is loaded from a *.conductrics.com domain, and the second rule requires that the script URL has the apikey parameter.\nOne final thing we\u0026rsquo;ll add to the user interface is a simple debug toggle.\nClick the Add field button, and select Checkbox from the overlay.\n  Set the field name to debug, and set the Checkbox text field to Log debug messages to console. Feel free to save the template to see how the checkbox looks like in the wild.\n  We\u0026rsquo;re all done with the UI, finally!\nStep 3 - Add some code Now, select the Code tab from the template editor. You should see some boilerplate code that will help you get started (we\u0026rsquo;ll replace this with some other stuff soon enough).\n  For tag templates, the code you write has three components you need to be aware of.\n  The code format itself, which utilizes a special, sandboxed JavaScript. This sandboxed JS offers a bunch of template APIs you can use to work with JavaScript outside the scope of the code itself (access dataLayer, write cookies, and so forth).\n  The data object, which comprises the contents of the tag fields the user might have interacted with.\n  The data.gtmOnSuccess() and data.gtmOnFailure() methods, which indicate success or failure, respectively, of the tag execution.\n  With these three in mind, we need to do the following.\n  We need the tag to load Conductrics\u0026rsquo; JavaScript from their network, which means we\u0026rsquo;ll utilize the injectScript API to do so.\n  We\u0026rsquo;ll take the URL for the injectScript API from the text field in the tag, and if the user has chosen to write debug messages to the console, we\u0026rsquo;ll respect that choice.\n  If the script loads fine, we\u0026rsquo;ll signal this with data.gtmOnSuccess(), but if there\u0026rsquo;s a failure (such as a 404 error), we\u0026rsquo;ll call the data.gtmOnFailure() method.\n  Success and failure are relevant for Preview mode output and also for tag sequencing.\nWithout further ado, here\u0026rsquo;s the complete code you should replace the contents of the code editor with:\n// Require the necessary APIs const logToConsole = require(\u0026#39;logToConsole\u0026#39;); const injectScript = require(\u0026#39;injectScript\u0026#39;); const queryPermission = require(\u0026#39;queryPermission\u0026#39;); // Get the URL the user input into the text field const url = data.sourceUrl; // If the user chose to log debug output, initialize the logging method const log = data.debug ? logToConsole : (() =\u0026gt; {}); log(\u0026#39;Conductrics: Loading script from \u0026#39; + url); // If the script loaded successfully, log a message and signal success const onSuccess = () =\u0026gt; { log(\u0026#39;Conductrics: Script loaded successfully.\u0026#39;); data.gtmOnSuccess(); }; // If the script fails to load, log a message and signal failure const onFailure = () =\u0026gt; { log(\u0026#39;Conductrics: Script load failed.\u0026#39;); data.gtmOnFailure(); }; // If the URL input by the user matches the permissions set for the template, // inject the script with the onSuccess and onFailure methods as callbacks. if (queryPermission(\u0026#39;inject_script\u0026#39;, url)) { injectScript(url, onSuccess, onFailure); } else { log(\u0026#39;Conductrics: Script load failed due to permissions mismatch.\u0026#39;); data.gtmOnFailure(); }  The code comments should help you understand how the code works. Here are some key things about the code:\n  The APIs are loaded with the require() method. You must use these APIs - their counterparts in browser JavaScript have been suppressed. For example, console.log() would not work, nor would document.createElement().\n  The data object has keys matching the field names you edited in the field editor. Any value those fields have will be the values stored in the data object. Thus, if the user typed hello into the sourceUrl field, that value would be available via data.sourceUrl.\n  Using queryPermission() is a good way to ensure the user input matches the permissions set in the template. The permissions are updated automatically when you require() a specific API in the code editor (we\u0026rsquo;ll get to the permissions in the next chapter).\n  Key thing to note is that this editor is a JavaScript editor. Thus, unlike with Custom HTML tags, you should not wrap the code with \u0026lt;script\u0026gt; and \u0026lt;/script\u0026gt;.\nOnce you\u0026rsquo;re done, click Save and move on to the next chapter.\nStep 4 - Modify permissions The final thing to edit are the permissions. When you require() APIs, you\u0026rsquo;ll also automatically enable their permission configurations in the Permissions tab. See the chapter on permissions for more details on how these work.\nAnyway, since you are using the logToConsole and injectScript APIs, their permissions are now available for editing. The queryPermission and require APIs don\u0026rsquo;t have any permissions associated with them.\nClick to the Permissions tab and expand the two permissions you find.\n  The Injects Scripts permission is, surprise surprise, for the injectScript API. It expects URL match patterns that the value input by the user into the text field must match.\nAdd the value https://*.conductrics.com/ into the text field.\nThis value basically means that the script URL injected in the page must be a subdomain of conductrics.com, and it can have any path structure (the / after the hostname acts as a wildcard). Thus it will match, for example:\n  https://conductrics.com/tracker\n  https://de.cdn-v3.conductrics.com/ac-aBcDeFgH/v3/agent-api/js/f-aBcDeFggg/dt-b1234567?apikey=api-w123456\n  The Logs to Console permission should be self-explanatory. It governs how the logToConsole API works. You can check the Always log option, because we manage logging to console in the template code itself.\nRight now, the permissions should look like this:\n  Click Save to save the current template.\nStep 5 - Preview and test You can now click to edit the tag in the Template Preview window. When you\u0026rsquo;ve added some text, you can click the Test button to see what happens.\nYou can also choose to show a test page from the template menu:\n  Test different things:\n  Try with a random, non-URL string to see the warning about the missing https://.\n  Try with https:// but with a hostname that doesn\u0026rsquo;t contain conductrics.com to see the error for incorrect URL.\n  Try with https://domain.conductrics.com/loader to see the error for missing apikey.\n  Try with https://domain.conductrics.com/loader?apikey=12345 to see the code pass with flying colors.\n  Try with and without the Debug checkbox checked, and see how this impacts what you see in the Console.\n    If you already have a Conductrics account, you can test with a real deployment URL to see the success message in the console.\nFinally, you can dig deep into the iframe of the Test page to find your script tag there.\n  Once done testing to your satisfaction, you can do one final Save and then close the template editor - you are ready to create your first tag off of this template!\nStep 6 - Create tag and preview In the GTM UI, go to Tags and click New. In the overlay that opens, find your new template and click it.\n  This screen should be familiar to you. The only difference to native templates in GTM is the Tag permissions bar. Click it to preview what permissions have been set for the template.\n  Then, fill in the tag fields as you would with a regular template. For testing purposes, just set it to fire on the All Pages trigger.\nYou can try with invalid URLs to see the error messages. However, to test how the tag actually works, use a correct (but ultimately invalid) URL like https://domain.conductrics.com/loader?apikey=12345, and the tag should look like this:\n  Then, go the Preview mode and enter your site.\nYou should see the tag having fired, but in Failed state because the endpoint returned a 404 error.\n  Check out the JavaScript console, too. You should see some relevant output there.\n  Finally, since we\u0026rsquo;re injecting a script, you can drill into the page elements to find it in the \u0026lt;head\u0026gt; of the page.\n  Step 7 - You\u0026rsquo;re done! And that\u0026rsquo;s it!. This was an extremely simple use case, and it might seem like a really complicated way to go about it. However, remember that you\u0026rsquo;re providing controls for governance and responsible code injection here.\nThe use of permissions, sandboxed JavaScript, and, if you choose to use them, policies help you run a tight ship on what the custom tags can and can\u0026rsquo;t do on the site.\nThe template itself does a simple script injection, but I hope I convinced you how powerful the UI editor can be, and we barely scratched the surface!\nVariable template walkthrough Variable templates differ from tag templates in that they only have a singular purpose: to return a value. Optimally, they should not have any side effects, which include e.g. setting variables in the global scope, writing cookies, pushing to dataLayer, injecting scripts, or firing pixels.\nIn this example, we\u0026rsquo;ll create a simple variable which is intended to work with the hitCallback field of your Google Analytics tags.\nThe idea is that the variable will return a function (hitCallback always requires a function as its value), which when executed will write a cookie into the browser. This cookie can then be used to check if less than 30 minutes (the default expiration) have passed since the last Google Analytics hit to GA, thus giving you a very rough estimate of whether a GA session is currently active.\nSo yes, we\u0026rsquo;re breaking the \u0026ldquo;no side effects\u0026rdquo; rule I literally just mentioned, but this is a bit different. It\u0026rsquo;s not an uncontrollable side effect, which it would be if it fired every single time the variable is called. Instead, the functionality is restricted to the hitCallback function, which is only executed once, when the Google Analytics request has been dispatched.\n  Note, there are more elegant ways to handle this, but for the purposes of this guide I wanted to start off with something simple.\nStep 1 - Create and set the details of the variable template Browse to Templates, and click New in the corner of the Variable Templates area.\n  In the view that opens, make sure the Info tab is selected, and add a Name and Description to the variable.\n  As you can see, you can add HTML styling to the description field. The text I wrote was this:\nUse this variable to write (and update) a session cookie after each Google Analytics hit by using the \u0026lt;code\u0026gt;hitCallback\u0026lt;/code\u0026gt; field in the GA tags. Once done, click Save in the top right corner and you should see the Template Preview area show the new template name.\n  Next, let\u0026rsquo;s create the user interface for this template!\nStep 2 - Create the UI Click over to the Fields tab in the template editor.\nThis template will comprise three text input fields. One for the cookie\u0026rsquo;s maximum age, one for the cookie path, and one for the cookie domain.\nClick the Add Field button, and choose the Text Input field from the overlay.\n  Do this two more times, so that you end up with three text input fields.\nName the first field maxAge, the second cookiePath, and the third cookieDomain.\n  Expand the first field, maxAge, and click the cogwheel icon to open its field configurations. Toggle on Always in summary, Default value, and Display name.\n  Edit the maxAge settings to be like this:n\nDisplay name: Maximum age in seconds\nDefault value: 1800\nAlways in summary: Checked\nThe Display name is what appears above the field in the editor.\nWe\u0026rsquo;ll use a Default value of 1800 seconds (that\u0026rsquo;s 30 minutes), so if the user doesn\u0026rsquo;t touch the field, that\u0026rsquo;s the value that will be used.\nAlways in summary means that the field contents will show up when the variable is opened in the UI but not in edit mode. It\u0026rsquo;s a convenience thing, not vital in any way.\n  Next, click the cogwheel for the cookiePath field, and toggle the same configurations on (Display name, Default value, and Always in summary).\nSet them to these values:\nDisplay name: Cookie path\nDefault value: /\nAlways in summary: Checked\nFinally, click the cogwheel for the cookieDomain field, and toggle the same conditions on, together with the Help text configuration.\nSet them to these values:\nDisplay name: Cookie domain\nHelp text: Set to \u0026lsquo;auto\u0026rsquo; to write the cookie on the highest possible domain name.\nDefault value: auto\nAlways in summary: Checked\nThe fields should look like this:\n  The Help text configuration is pretty cool. If you refresh the Template Preview, you\u0026rsquo;ll see how there\u0026rsquo;s a little question mark next to the Cookie domain field. By hovering over it, you\u0026rsquo;ll see the help text.\n  Now you\u0026rsquo;re ready to add the code!\nStep 3 - Add some code Click the Code tab, and replace the contents with the following:\n// Require the necessary APIs const setCookie = require(\u0026#39;setCookie\u0026#39;); // Build the options object from user input const options = { domain: data.cookieDomain, \u0026#39;max-age\u0026#39;: data.maxAge, path: data.path }; // Return the hitCallback function return () =\u0026gt; { // Set the cookie when the hit has been dispatched  setCookie(\u0026#39;_ga_session\u0026#39;, \u0026#39;true\u0026#39;, options); };  As you can see, it\u0026rsquo;s a very simple variable. It uses a single API, setCookie (for obvious reasons), and it has a return statement at the end, which returns a function wherever the variable is called.\nIf you read the setCookie specification, you can see that it takes three arguments.\nsetCookie(name, value, options)\nTo make things a bit easier, we\u0026rsquo;re building the options object before calling the method. The object has three properties, domain for the domain name, max-age for the maximum age, and path for the cookie path.\nThe setCookie API has a nice feature where if you set the value of domain to 'auto', it automatically finds the highest possible (i.e. shortest) domain name it can use. Thus, if the variable is called on sub.domain.simoahava.com, the API writes it on simoahava.com.\nAs you can see, there are no validations or permission checks going on here. After reading this guide, you\u0026rsquo;ll be able to extend the UI to do the validations directly on the fields, or you can also check the field values in code, falling back to some valid values in case the user input is not perfect.\nOnce you\u0026rsquo;ve added the code, make sure to save the template, and then click on over to the Permissions tab of the editor. You\u0026rsquo;ll need to make sure the template has permissions to write the cookie.\nStep 4 - Modify permissions In the Permissions tab, you should now see a permission for Sets A Cookie Value. It\u0026rsquo;s added automatically when you run require('setCookie') in the template code. Pretty sweet!\n  Expand the permission, and click the + Add allowed cookie button. In the overlay that opens, configure the cookie as follows:\nCookie name: _ga_session\nDomain: *\nPath: *\nSecure: Any\nSession: Any\nClick Save in the overlay when done. This is what you should see in the permissions area:\n  You\u0026rsquo;re done with the template! Remember to Save it one last time.\nThere\u0026rsquo;s not much sense in previewing or testing it in the template editor itself. The variable returns a function, so the only logical place to test it is in an environment where the function is actually executed in context.\nThus, it\u0026rsquo;s time to create a variable from the template and add it to a tag!\nStep 5 - Create variable, add to tag, and preview Click to Variables in the GTM UI, and click New in the User-Defined Variables section.\nIn the variable type picker, choose your newly created template from the list.\n  You can now configure the variable to your liking. The default settings are more than fine for most cases, though.\n  Next, find your Google Analytics Page View tag (or create one if you don\u0026rsquo;t have it), check its Enable overriding settings in this tag, and browse to More Settings \u0026gt; Fields to Set. Click +Add Field.\nSet the Field Name to hitCallback, and Value to the variable you just created.\n  Save the tag, and go to Preview mode. Then, load the page where the Page View tag will fire.\nIf all goes as planned, you should now find a new browser cookie named _ga_session, with an expiration set to the number of seconds you set in the Maximum age in seconds field. Easiest way to find your browser cookies is to use the developer tools of your browser.\nIn Chrome, press CMD + OPT + I (Mac) or CTRL + SHIFT + I (Win/Linux) to open the developer tools. Then, activate the Application tab, and select the Cookies option for your domain. You should see the _ga_session cookie with value true, and an expiration in the future (or in the past if you set its maximum age to 0).\n  If you see the cookie then congratulations, your variable template works as intended!\nStep 7 - You’re done! The purpose of this guide was to walk you through the steps how to create a variable template.\nYes, we used an extremely simple example, but the idea was to get familiarized with the routine of template creation, rather than jump in the deep end with all the APIs, field configurations, and permissions available.\nThe main difference to tag templates is that the variable needs to return something. Whatever happens before that final return statement is up to you, but you might want to avoid too many side effects unless you are absolutely certain they are only invoked in a specific, predictable context (such as hitCallback).\nThat\u0026rsquo;s it for the tutorial part! Now it\u0026rsquo;s time to explore with more detail what custom templates are, how they work, and how they are such a game-changer for tag management with GTM.\nBut first, take a break. You\u0026rsquo;ve earned it.\nCore concepts Welcome back!\nBefore we get to the good stuff, let me go over some of the core concepts of Custom Templates. These concepts will emerge and re-emerge in much of the discussion below, and they are fundamental to understanding what Custom Templates are and what they can do.\nSandboxed JavaScript Custom Templates are written with JavaScript. If you are unfamiliar with the language but still aching to start working with templates, I recommend taking a look at some learning materials before trying your hand with the template code. Check out these (free) learning portals, too:\n  freeCodeCamp - Excellent and comprehensive JavaScript and web technology tracks that span everything from the very basics to creating stateful web services.\n  Codecademy - Great tracks for all sorts of programming languages and disciplines offered in a nice, interactive way.\n  A further complication is that Custom Templates don\u0026rsquo;t actually use just any old browser JavaScript. They use a special, sandboxed version of the JavaScript language.\nBasically, any code you write in the code editor will be automatically wrapped in a function that provides a single argument named data.\nfunction(data) { // Start of code editor code  const log = require(\u0026#39;logToConsole\u0026#39;); const copyFromWindow = require(\u0026#39;copyFromWindow\u0026#39;); const ga = copyFromWindow(\u0026#39;GoogleAnalyticsObject\u0026#39;); if (typeof ga === \u0026#39;undefined\u0026#39;) { log(\u0026#39;Google Analytics not loaded!\u0026#39;); } log(data.userInputText); data.gtmOnSuccess(); // End of code editor code }  The data object is really important. Each field in the template will be accessible as a property of the data object, and the value of that property will be the result of the user\u0026rsquo;s interaction with the field. The data object also has the gtmOnSuccess() and gtmOnFailure() methods you must use to signal the tag\u0026rsquo;s successful completion or its failure. Variable completion is signalled by a return statement.\nIn addition to being automatically wrapped as a function body, the set of JavaScript methods and accessors available to you is limited. Basically, you will have no access to the global window object. This includes things like location, document (and document.cookie), console, and any constructors (such as new Date().\nTo access these global methods and properties, you\u0026rsquo;ll need to use the template APIs exposed by the template editor.\nES6+ syntax ES6 (ECMAScript 6) is one of the most significant updates the JavaScript standard, originally released in 2015, with prominent support in all the major browsers. The code editor in GTM\u0026rsquo;s templates supports some ES6 features. These features include:\n  const and let keywords. These two keywords are offered as alternatives to JavaScript\u0026rsquo;s var keyword. The main difference is that const and let have block-level scope, meaning they are scoped to the context of the wrapping { and }, rather than the whole surrounding function context as with var. Furthermore, there\u0026rsquo;s no variable hoisting - const and let variables cannot be referenced before their declaration. Finally, const variables cannot be reassigned.\n  Arrow functions. Arrow functions are a new way to write JavaScript function expressions. It\u0026rsquo;s syntactically more compact, so pursuers of \u0026ldquo;elegant code\u0026rdquo; rejoice. For example, var multiplyByTwo = function(a) { return a*2; }; becomes const multiplyByTwo = a =\u0026gt; a*2;. Unlike \u0026ldquo;regular\u0026rdquo; JavaScript functions, there\u0026rsquo;s no this binding and no access to the arguments object.\n  You are of course free to use the \u0026ldquo;old\u0026rdquo; style of JavaScript. In the templates\u0026rsquo; code editor, I will try to adhere to ES6 syntax purely out of habit, but also because using const and let actually has some functional weight in the code, too.\nRestrictions to what type of data you can handle To prevent users from circumventing template permissions and from running code that breaks through the templates\u0026rsquo; sandbox, Google Tag Manager has a number of suppression mechanisms built in.\n  You can\u0026rsquo;t access window or document directly. Access to the global namespace is restricted to template APIs. Even if you tried something clever such as returning window.setTimeout in a JavaScript variable, so that it\u0026rsquo;s technically not in the code editor itself, the template editor automatically parses all variables and user-based input brought into the template before it is made accessible to the code editor.\n  You can\u0026rsquo;t pass non-plain objects, such as instances created with construtors, to the template code. It\u0026rsquo;s difficult to contain security of these, because the prototype chain can be manipulated by code outside the template, making the template itself vulnerable and less secure. This basically means that things like customTask will not be possible to do with variable templates for now, because it requires access to a constructed, non-plain model object to function.\n  You can\u0026rsquo;t access DOM elements. This is kind of the same as (2), because a DOM element represents a complex, non-plain object type. Nevertheless, it means that you can\u0026rsquo;t access things like {{Click Element}} in template code.\n    In time, I\u0026rsquo;m certain the proliferation of APIs in the template will solve many of the restrictions above, but first and foremost the point of templates is to add a layer of security to prevent scripts outside the template from messing with the code within.\nAnother one of the idiosyncracies of the templates\u0026rsquo; sandboxed JavaScript is how function expressions exposed outside the template code are encapsulated in a wrapper by GTM.\nThe purpose of this wrapper is to make sure functions set in the window or returned by the variable template can\u0026rsquo;t be used to circumvent permissions set in the template.\nThe wrapper itself is almost always inconsequential, because it passes the parameters collected by the wrapper to the function created in or handled by the template. However, if any one of the parameters violates the restrictions listed above, the parameter is suppressed and replaced with undefined.\nTemplate APIs The most common global properties and methods you\u0026rsquo;ll need to use are abstracted behind custom APIs. We\u0026rsquo;ve covered a number of API examples in the preceding chapters. For example, there\u0026rsquo;s logToConsole for logging items to the JavaScript console of the browser, and there\u0026rsquo;s copyFromWindow for accessing global variables defined in the window object.\nEach API comes with its own configurable set of permissions. The copyFromWindow API, for example, requires that you define the global keys the code has access to. As soon as you type the require('copyFromWindow') statement and refresh the template, the Accesses Global Variables permissions will become available in the respective tab.\n  As you can see, for these particular permissions you can define the keys and whether the code has read and/or write and/or execution permissions for them. You might have guessed now that Access Global Variables is not limited to just copyFromWindow (which is read-only), but also determines the usefulness of e.g. setInWindow (for writing/setting global variables) and callInWindow (for calling global methods).\nThe purpose of this sandboxed JavaScript is to give you a safe and secure JavaScript environment to write and run your code in. The interplay between your code and the permissions model is designed to produce code that is as conservative as possible without compromising the runtime logic of what you want to do with the code.\nThis doesn\u0026rsquo;t mean that you can\u0026rsquo;t cause havoc with your custom template - quite the contrary. However, the user interface guides you to make the correct choices when choosing the APIs and the permissions thereof. Hopefully the guide you are currently reading will further help you understand how to make your code run in the most efficient and responsible way possible.\nServer-side tagging Server-side tagging has its own set of APIs, which mainly revolve around parsing HTTP requests and compiling HTTP responses. The purpose of a Client is to read the incoming requests, launch virtual container processes for tags and triggers, and then build a response back to the source of the original request.\nThis guide is mostly focused on Custom Templates in web containers, but all the main features apply to Server-side tagging as well.\nPolicies The counterpart of permissions are policies. Policies give the site owner the ability to block potentially hazardous code from running. It\u0026rsquo;s somewhat similar to Google Tag Manager\u0026rsquo;s whitelist/blacklist feature, except with policies you have extremely granular access to how individual permissions are handled.\nPolicies are basically dataLayer commands which specify what happens when a Custom Template for which some permissions have been set tries to execute on the page.\nIf you don\u0026rsquo;t define any policies then the tag or variable will run its course unimpeded.\nHowever, you can create a policy which is invoked whenever some specific permission is accessed by the template. Let me show you an example:\nconst getCookie = require(\u0026#39;getCookieValues\u0026#39;); const log = require(\u0026#39;logToConsole\u0026#39;); log(getCookie(\u0026#39;_user_id\u0026#39;)); data.gtmOnSuccess();  The template code above uses the getCookieValues API to log the value of a cookie named _user_id. When you save that code, a new permission appears in the editor, and you need to whitelist the cookie names the code is allowed to access:\n  Now, if you didn\u0026rsquo;t specify a policy, the code would always log the cookie value into the console when the tag runs, because the _user_id cookie has been whitelisted in the tag permissions.\nHowever, let\u0026rsquo;s say the page has the following code in the HTML:\nwindow.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;policy\u0026#39;, \u0026#39;get_cookies\u0026#39;, function(container, policy, data) { if (data.name === \u0026#39;_user_id\u0026#39;) { throw(\u0026#39;You are not allowed to query _user_id on this page!\u0026#39;); } return true; });   Don\u0026rsquo;t be confused by the gtag() command - policies use the gtag() method, which is essentially a helper to push the arguments into dataLayer. You do not need to download the gtag.js library to be able to run policies on the site template.\n Now when the tag is run and the getCookie('_user_id') code is executed, the policy you created will activate. This policy checks if the requested cookie was named _user_id, and reacts by throwing an error. Perhaps the developers have deemed it too risky to allow GTM to query the _user_id cookie like this.\nYou don\u0026rsquo;t need policies to work with Google Tag Manager\u0026rsquo;s Custom Templates. However, given how popular Custom Templates are likely to become, I do expect to see more and more policies defined, too.\nBe sure to check out the policy reference at the end of this guide for more details on how individual policies should be set.\nTests Tests allow you to write unit tests for your template code directly in the editor. The tests feature comprises a number of APIs you can use to, for example, mock the data object and write assertions against the compiled and executed template code.\n  For more information about how to write and run tests with custom templates, see this guide:\nWriting Tests For Custom Templates In Google Tag Manager\nOfficial documentation When working with Custom Templates, you should have two documents at hand:\n  Custom template APIs reference - This document introduces the various APIs you\u0026rsquo;ll need to use if you want your code to run beyond the boundaries of the sandbox.\n  Custom template permissions reference - This reference lists the permissions the APIs require, and which are used to create policies with which web pages can deploy an additional layer of governance for running custom templates.\n  **Custom template APIs for Server-side tagging - This document lists the APIs available for Client and tag templates in Server-side tagging.\nKeep those documents open when working with Custom Templates. I won\u0026rsquo;t repeat their contents in this guide, but I do refer to them where necessary.\nThe editor Let\u0026rsquo;s jump right in! To start creating a new template, click the Templates menu option.\n  Next, click New in the right corner of the Tag Templates card.\n  What you see now is the template editor. There are several views and menus here, so let\u0026rsquo;s start with a quick overview.\n  1 - The main navigation of the editor. If you have Show advanced settings checked in the editor menu, you\u0026rsquo;ll also see the Notes tab here.\n2 - The editor view for the selected tab. If you have Show advanced settings checked in the editor menu, you might see extra options here.\n3 - Template Preview shows what the template will look like when saved. If you\u0026rsquo;ve made changes to the template, you\u0026rsquo;ll see a Refresh link here, which will update the preview when clicked.\n4 - The Console will log information about template performance when you Test the template. Additionally, any calls to the logging API will be output here, too.\nIf you select Show Test Page from the editor menu, you\u0026rsquo;ll see the Test Page window between the Template Preview and Console.\n5 - Click Test to execute the template code with the settings you have entered in the preview. If you\u0026rsquo;ve chosen Show Test Page from the editor menu, then any code that modifies the page will impact what the Test Page shows.\n6 - Save the template. The template cannot be saved if the code has syntax errors or invalid JavaScript. You need to fix the errors before saving. Note that the template can be saved even if you have incorrect permissions for the APIs you use.\n7 - The editor menu. The editor menu has options for managing the template and for customizing the editor.\nThe editor for tag templates and variable templates is largely the same. The main difference is the absence of an icon/logo selector in the info view and how the code editor is utilized. With tags, the code editor needs data.gtmOnSuccess() for successful execution and data.gtmOnFailure() (if necessary) for a failure. Variables, on the other hand, don\u0026rsquo;t use this. You just need a return statement that returns whatever the variable is supposed to return.\nInfo view The Info view is where you\u0026rsquo;ll customize what the template looks like. Verbosity is not a sin - try to be as clear and descriptive in the template name and description as possible.\n  The Name of the template is what appears in the tag/variable selection menu as well as in the template itself when you create a new instance from it.\nThe Description will appear in the tag/variable selection menu below the template name.\nTag templates allow you to choose an image for the template. This image will appear in the tag selection menu and the template itself.\n  At the bottom of the Info view is the checkbox for approving the terms of service for the Community Template Gallery. You only need to check this if you want to submit your template to the gallery.\n  If you have Show advanced settings selected, you\u0026rsquo;ll also see the template version as well as the contexts in which the template works.\nYou\u0026rsquo;ll also see a Source toggle in the top of the view. By clicking the toggle, you\u0026rsquo;ll see the JSON representation of the template info, which you can edit (if you want to change the context, for example).\n  Fields editor The Fields editor is where you\u0026rsquo;ll spend a lot of your time. You use this editor to establish what the tag or variable user interface will look like, and how individual fields interact with each other.\nFor each field you create, you need to specify (at least) a name with which you can refer to the field value in the code editor. The field name must be unique, since all fields (even those nested within other fields) will be directly accessible using the field name as a property of the data object. For example, to access the value of a field whose name is gaTrackingId, you\u0026rsquo;d use this in the code editor:\nconst ua = data.gaTrackingId; log(ua); // logs UA-12345-1 or whatever the user set the value of the field to.  Furthermore, each field has a field configuration that you\u0026rsquo;ll use to establish how the field functions in the user interface of the template itself.\n  Below the field name are the options and settings for the field. You\u0026rsquo;ll see more of these as you add additional configurations.\nIn the top right corner of each field row you\u0026rsquo;ll see a trash can for deleting the field.\nThere\u0026rsquo;s also a cogwheel which opens the field configuration options.\nThe more options menu opens options for manipulating the positioning of the field (see screenshot above), and the caret at the very right lets you collapse and expand the field in the editor.\nIf you have Show advanced settings enabled, you can toggle the Source toggle in the top corner of the view to see a JSON representation of the fields.\n  Field configuration Whenever you add a field in the editor, you have the option of configuring field-specific rules and settings by clicking the cogwheel icon next to the field.\n  Depending on the field you opened the configuration for, you\u0026rsquo;ll see a set of toggles that you can toggle on or off. Some toggles are on by default, again depending on the field you are configuring.\n  As you read through the field descriptions below, I\u0026rsquo;ve added the configuration options for each field. Furthermore, at the end of this guide there\u0026rsquo;s a field configuration reference which lists all the possible configuration options in more detail.\nNote that some fields can be nested, and some fields actually include nested fields by default (e.g. the Param table field). In these cases, the nested fields are treated as their own fields with their own configurations. The only difference between an isolated and nested field is that the latter is subservient to the field configurations of its parent. For example, if the parent is disabled due to enabling conditions not validating, the nested fields will be disabled, too.\nText input Description The Text input field is a simple input box to which the user can write text.\n Text input field in the editor   Text input field in the UI  Code editor output The property data.fieldName will resolve to whatever the user typed in the field, or whatever a GTM variable used in the field resolves to.\n// Using the example from the screenshot above const textFieldValue = data.textToWriteToConsole;  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  Allow empty strings - Convert and unfilled field into an empty string. OFF by default. Always in summary - Show the field and its current value in the summary view of the tag or variable. OFF by default. Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display line count - Line count of greater than 1 turns the field into a text area. OFF by default. Display message when not set - When the field is untouched, show this text in the summary view. OFF by default. Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Text as list - When the text field is a text area (see Display line count above), store the value of the field as an array, where each item represents a row of input. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default. Value hint - Text shown as a placeholder help text in the field before the user starts editing it. OFF by default. Value unit - Text shown next to the field. Useful to specify a format or type, for example. OFF by default.  Drop-down menu Description The Drop-down menu field provides a menu where only a single item can be selected. It\u0026rsquo;s often a combination of predefined items (such as True and False), and all the Google Tag Manager variables in the container.\n Drop-down menu in the editor   Drop-down menu in the UI  Code editor output The data.fieldName property will resolve to the value of the item selected from the drop-down menu. If the item was one of those you defined in the template editor, then what you wrote in the Value field is what the property resolves to. If the user selected a GTM variable, then the value returned by that variable is what the property resolves to.\nconst dropDownFieldValue = data.dropDownMenu1; log(dropDownFieldValue); // logs true if an item whose Value is true is selected.  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  \u0026ldquo;Not set\u0026rdquo; option - Show a placeholder value in the menu before the user has selected anything. If menu is left untouched, the field returns an empty string. OFF by default. Always in summary - Show the field and its current value in the summary view of the tag or variable. OFF by default. Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Include variables - Check this box if you want to include all GTM variables in the drop-down menu. ON by default. Nested fields - Nested fields are useful if you want to show fields related to a specific value of the parent field only under certain conditions, for example. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default. Value unit - Text shown next to the field. Useful to specify a format or type, for example. OFF by default.  Checkbox Description The Checkbox field should be self-explanatory. The checkbox doesn\u0026rsquo;t have a display name by default. Instead, text you type in the Checkbox text field will be shown next to the box in the UI.\n Checkbox in the editor   Checkbox in the UI  Code editor output The data.fieldName property resolves to true if the checkbox was checked and false if not.\nconst checkBoxValue = data.useDataLayer; log(checkBoxValue); // true  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  Always in summary - Show the field and its current value in the summary view of the tag or variable. OFF by default. Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display name - The label of the field shown in the GTM UI. OFF by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Nested fields - Nested fields are useful if you want to show fields related to a specific value of the parent field only under certain conditions, for example. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default.  Radio buttons Description You can add one or more radio buttons into a single radio button group. The radio button group is treated as a single field. Each radio button has a name (displayed next to the button) and a value (what is returned if the button is selected). The user can only select one button from the group. You can expand advanced settings to add a help text and to add nested fields under each radio button selection.\n Radio buttons in the editor   Radio buttons in the UI  Code editor output The data.fieldName property, where fieldName is the name of the entire radio button group, will resolve to the value of the selected radio button.\nconst selectedButton = data.gaEventType; if (selectedButton === \u0026#39;pageView\u0026#39;) { const gaId = data.trackingId; // Do something with gaId }  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Nested fields - Nested fields are useful if you want to show fields related to a specific value of the parent field only under certain conditions, for example. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default.  Simple table Description With a simple table, you can define columns (either text input fields or drop-down menus), and the users can add and remove rows to and from the table as they wish.\nYou can specify that all values in a column must be unique (i.e. the user can\u0026rsquo;t add multiple rows with the same value in such a column), and you can add things like default values and validation rules to each column, once you have selected to Show advanced settings for the column.\n Simple table in the editor   Simple table in the UI  Code editor output The data.fieldName property will resolve to an array of objects, where each object represents a row the user has added in the UI. Furthermore, each row object will have key-value pairs for every column, where the key is the column name and the value is the value of the input (the selection value if a drop-down menu or the input text if a text field).\nlog(data.cookieSettings); /* Logs: [ {\u0026#34;cookieOption\u0026#34;:\u0026#34;cookieDomain\u0026#34;,\u0026#34;optionValue\u0026#34;:\u0026#34;simoahava.com\u0026#34;}, {\u0026#34;cookieOption\u0026#34;:\u0026#34;cookieName\u0026#34;,\u0026#34;optionValue\u0026#34;:\u0026#34;myCookie\u0026#34;} ] */  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  \u0026ldquo;New row\u0026rdquo; button text - Change the value of the \u0026ldquo;Add row\u0026rdquo; button text. OFF by default. Always in summary - Show the field and its current value in the summary view of the tag or variable. OFF by default. Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display message when not set - When the field is untouched, show this text in the summary view. OFF by default. Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default.  Param table Description A Param table is a more complicated table. Instead of being able to edit individual column values in the table itself, a param table requires you to input all the values of the row in a special overlay menu that pops up when you click to add a row. The individual columns can be any of the supported field types, and thus you can add far more complexity into the table than you could with a regular simple table field.\n Param table in the editor   Param table in the UI  Code editor output Similar to the simple table, the data.fieldName property resolves to an array of objects, where each object represents a row in the table. Each row object has key-value pairs for each column, and the value is whatever the column field returns. For example, the screenshot above would render as:\nlog(data.userSelection); /* Logs: [ {\u0026#34;genderSelect\u0026#34;: \u0026#34;male\u0026#34;, \u0026#34;nameInput\u0026#34;: \u0026#34;Simo Ahava\u0026#34;}, {\u0026#34;genderSelect\u0026#34;: \u0026#34;female\u0026#34;, \u0026#34;nameInput\u0026#34;: \u0026#34;Simona Ahava\u0026#34;} ] */  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  \u0026ldquo;Edit row\u0026rdquo; dialog title - You can change what the heading of the \u0026ldquo;Edit row\u0026rdquo; overlay is. OFF by default. \u0026ldquo;New row\u0026rdquo; button text - Change the value of the \u0026ldquo;Add row\u0026rdquo; button text. OFF by default. \u0026ldquo;New row\u0026rdquo; dialog title - You can change what the heading text is in the overlay you see when adding a new row. OFF by default. Always in summary - Show the field and its current value in the summary view of the tag or variable. OFF by default. Clear on copy - Prevent filled field values from being copied when a copy is made of the tag or variable. OFF by default. Default value - The value of the field until the user decides to change it. OFF by default. Display message when not set - When the field is untouched, show this text in the summary view. OFF by default. Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default. Validation rules- One or more rules against which the field must validate before the user can save the tag or variable. OFF by default.  Group Description A Group is simply a logical way to group different fields together. The main benefit is that it offers you a Group style selection.\n  Simple section - This style simply shows the nested fields in the group without any collapsing.\n  Collapsible section \u0026ndash; Open - This style shows the nested fields in a collapsible section which is open initially.\n  Collapsible section \u0026ndash; Closed - This style shows the nested fields in a collapsible section which is collapsed initially.\n  Collapsible section \u0026ndash; Open if not default - This style shows the nested fields in a collapsible section which is open if the nested field(s) don\u0026rsquo;t have default values (i.e. the user has changed the value of the fields).\n  A Group is useful if you want to section a set of fields separately, because you can also control the entire groups visibility with the Enabling condition field configuration.\n Collapsible group in the editor   Collapsible group in the UI  Code editor output The Group doesn\u0026rsquo;t bring anything extra to the code editor. The nested fields are accessed directly as properties of the data object - the group itself is not present in the object in any way.\nlog(data); /* Logs: [ {\u0026#34;cookieSettings\u0026#34;: [{\u0026#34;cookieOption\u0026#34;: \u0026#34;cookieDomain\u0026#34;, \u0026#34;optionValue\u0026#34;: \u0026#34;auto\u0026#34;}]}, {\u0026#34;trackerName\u0026#34;: \u0026#34;_ga_tracker\u0026#34;} ] */  Available field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default. Group style - How the group section is rendered in the GTM UI (simple vs. collapsed vs. open). ON by default. Help text - Add text to a question mark tooltip shown next to the field. OFF by default.  Label Description The Label field is extremely simple. It\u0026rsquo;s just text that you show in the GTM UI when the instance is opened. The Display name configuration determines the text that is shown. Additionally, a Label field has only one other configuration: an Enabling condition which you can use to conditionally show the text.\n Label in the editor   Label in the UI  Code editor output There is no way to access the label in the code editor.\nAvailable field configurations  Click the configuration name to jump to details about the configuration in the reference at the end of this guide.\n  Display name - The label of the field shown in the GTM UI. ON by default. Enabling conditions - Establish conditions (based on other field inputs) for showing this particular field. OFF by default.  Utilizing APIs Template APIs are methods in the Google Tag Manager sandboxed JavaScript which let you invoke commonly used utilities of the browser. GTM restricts direct access to global methods to improve the stability and reliability of the code, and to optimize the way the code is delivered in the browser.\nWhen working with the code editor, remember to always have the API reference document open in your browser.\nBecause the official documentation is pretty thorough, I won\u0026rsquo;t do an exhaustive overview of all the APIs. Instead, I\u0026rsquo;ll point out quirks or things you should know about some of the APIs, listed below in their separate chapters.\nThe require API To invoke a template API, you must first require it in the editor. If you\u0026rsquo;ve been working with Node.js, you\u0026rsquo;ll recognize the require() method as a way to include JavaScript modules into your code.\nIn GTM templates, require() is used to gain access to template APIs, so the functionality is similar even if quite a bit more restricted, as the list of APIs you can integrate is limited.\nTo utilize an API, you simply run const method = require(name);, where name is the name of the API you want to use, and method is the name of the variable to which you will locally scope the API function.\nFor example, to make it possible to log to the console and to set cookies in the template code, you would need to require the respective APIs:\nconst log = require(\u0026#39;logToConsole\u0026#39;); const setCookie = require(\u0026#39;setCookie\u0026#39;);  When you require an API that is governed by a set of permissions, those permissions will automatically appear in the Permissions tab of the template UI. When you use the API, you need to make sure that the permissions allow you to perform the tasks you are trying to perform.\nThe queryPermission API Especially if you\u0026rsquo;ve written the template for public use, you might want to utilize the queryPermission API to wrap your code in a validator that only runs if the required permissions have been set.\nFor example, if you want to make sure you can actually set some cookie, you might want to use this type of syntax:\nconst queryPermission = require(\u0026#39;queryPermission\u0026#39;); const setCookie = require(\u0026#39;setCookie\u0026#39;); const options = { domain: \u0026#39;www.simoahava.com\u0026#39; }; // Check whether it\u0026#39;s possible to write the cookie before writing it if (queryPermission(\u0026#39;set_cookies\u0026#39;, \u0026#39;_gaClientId\u0026#39;, options)) { setCookie(\u0026#39;_gaClientId\u0026#39;, \u0026#39;abc123\u0026#39;, options); }  The setCookie() API code is only run if the permissions allow you to write a cookie named _gaClientId on the domain www.simoahava.com. This is a good way to avoid your tag running into errors.\nThe copyFromDataLayer API This API is relatively straightforward - it fetches the item from GTM\u0026rsquo;s data model that you requested. For example, to fetch the current value for gtm.elementUrl, you\u0026rsquo;d run:\nconst copyFromDataLayer = require(\u0026#39;copyFromDataLayer\u0026#39;); const clickUrl = copyFromDataLayer(\u0026#39;gtm.element\u0026#39;);  This code would fetch the value from the data model at the time that the tag was run.\nHowever, there\u0026rsquo;s a very important catch here.\nIf the template code fetching the dataLayer value is run asynchronously (e.g. with the callLater API), or if the tag built from the template is part of a tag sequence, the value fetched from dataLayer will be whatever is currently stored in the data model. In all other scenarios, the code is run synchronously, relative to the dataLayer.push(), so the value returned by copyFromDataLayer will reflect what was included in the pushed object.\nTo illustrate, consider the following code executed on the site:\nwindow.dataLayer.push({ event: \u0026#39;fire\u0026#39;, key: \u0026#39;value\u0026#39; }); window.dataLayer.push({ event: \u0026#39;fire\u0026#39;, key: \u0026#39;otherValue\u0026#39; });  These two dataLayer.push() calls are run one after the other. If you\u0026rsquo;ve built a tag that fires on the \u0026ldquo;fire\u0026rdquo; event, and the template for that tag uses copyFromDataLayer to fetch the value for key, then in most cases it will always return whatever the value of key was during the trigger push.\nIn other words, the first time the tag fires, key will be set to value, and the second time the tag fires, key will return otherValue. This is understandable, and it\u0026rsquo;s how Google Tag Manager has always worked.\nHowever, if the template uses copyFromDataLayer in an asynchronous method, or if the tag is part of a sequence, then when the first tag fires, it\u0026rsquo;s possible that key will actually return otherValue, because by the time the tag resolves that code, the second push will have happened and the values stored in GTM\u0026rsquo;s data model will have been updated.\nThis is something to be mindful of. I hope that we get the chance to control this behavior by providing a flag that lets us choose whether to use this asynchronous behavior or to fall back to the original, synchronous process.\nThe \u0026ldquo;global variable\u0026rdquo; APIs Templates offer you a handful of APIs that all interact with the global namespace (namely, the window object). These APIs are:\n  aliasInWindow for creating a copy of a global variable in another global variable.\n  callInWindow for executing a global function.\n  copyFromWindow for creating a local copy (a proper copy, NOT a reference to the original) of the global variable.\n  createArgumentsQueue for creating an array as well as a helper function that passes its arguments to the array.\n  createQueue for creating an array as a global variable.\n  setInWindow for setting a global variable.\n  These APIs have a number of permissions associated with them, so you need to make sure you make the necessary modifications to the template permissions.\nAnother thing to keep in mind is that when you access these global variables in the context of the templates, GTM creates a local copy of each and does NOT copy objects by reference, which is the typical way of handling JavaScript objects.\nSee this example:\nconst copy = require(\u0026#39;copyFromWindow\u0026#39;) const obj = copy(\u0026#39;someObject\u0026#39;); obj.someProperty = true;  This sets someProperty on the object to true only in the template code. It doesn\u0026rsquo;t change it to true in the global object itself. That\u0026rsquo;s because GTM creates a clone of the global variable rather than a reference to it.\nFinally, GTM handles functions in a special way. When you try to run setInWindow('someVariable', someFunction), where someFunction is a function you have created, what gets set in the global variable someVariable is not the actual function, but rather a wrapper created by GTM which ends up calling the function.\nThis shouldn\u0026rsquo;t be a big deal - since the end result is always the same. Whatever you call the global variable with gets executed in the function you created.\nHowever, it does mean that you won\u0026rsquo;t be able to set individual properties to that function. Take this example:\nconst func = str =\u0026gt; str + \u0026#34; Simo\u0026#34;; func.loaded = true; setInWindow(\u0026#39;someFunction\u0026#39;, func);  If you now call window.someFunction('Hello');, the code ends up returning \u0026quot;Hello Simo\u0026quot;, so it works. However, if you check someFunction.loaded, you\u0026rsquo;ll notice it\u0026rsquo;s undefined when it should be true.\nThe makeTableMap API The makeTableMap API makes the simple table field more manageable.\nThe simple table field itself returns an array of objects, where each object represents a row of the table and is comprised of key-value pairs. Each key-value pair corresponds to a column.\nFor example, if the simple table had two columns:\nColumn 1: fieldToSetName\nColumn 2: fieldToSetValue\nThe resulting data object would look like this:\n[ { \u0026#34;fieldToSetName\u0026#34;: \u0026#34;page\u0026#34;, \u0026#34;fieldToSetValue\u0026#34;: \u0026#34;/home/\u0026#34; }, { \u0026#34;fieldToSetName\u0026#34;: \u0026#34;userId\u0026#34;, \u0026#34;fieldToSetValue\u0026#34;: \u0026#34;abc123\u0026#34; } ] The makeTableMap API turns this into a single object whose contents are mapped from the values the user input. Naturally, this means that the column you use as the \u0026ldquo;key\u0026rdquo; of this new map must have unique values.\nTo continue the example from above, if you run makeTableMap against the array above, this is the result:\nconst makeTableMap = require(\u0026#39;makeTableMap\u0026#39;); const log = require(\u0026#39;logToConsole\u0026#39;); const data = data.properties; // This is the array from the example above const newMap = makeTableMap(data, \u0026#39;fieldToSetName\u0026#39;, \u0026#39;fieldToSetValue\u0026#39;); log(newMap); /* LOGS: { \u0026#34;page\u0026#34;: \u0026#34;/home\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;abc123\u0026#34; } */  The data object If you want to access values the user has input into the template fields, you need to use the data object. Furthermore, to signal tag template completion (or failure), you will also need to use the data object in your template code.\nEvery single user input in the template fields is encoded in the data object as properties, where the property name matches the field name you gave in the editor.\n  To access the values the user input into the cookieSettings field above, you would use this syntax:\nconst input = data.cookieSettings;  You don\u0026rsquo;t need to require() any API to access the user input - the data object is always available for fetching the values the user has entered. Read through the Fields editor chapter to see how each different field type is encoded into the dataobject.\nVariable templates in the code editor Variable templates have just one requirement, similar to Custom JavaScript variables in GTM: they must return a value. If you don\u0026rsquo;t have a return statement in the template, the variable will always return undefined (not very useful).\nconst userInput = data.someNumber; return userInput * 2;  The variable template above would take the value entered by user into the someNumber field and return it multiplied by 2.\nTag templates in the code editor With tag templates, you must invoke one of two methods in the code:\n  data.gtmOnSuccess() to indicate that the tag was a success.\n  data.gtmOnFailure() to indicate that the tag execution failed.\n  I recommend to always have data.gtmOnSuccess() (after all, why create a tag that doesn\u0026rsquo;t indicate successful completion). If there is a clear point of failure, such as something you want to block a tag sequence with, you should also add a data.gtmOnFailure() call into the code.\nTry to avoid paths in the code that do not lead to either data.gtmOnSuccess() or data.gtmOnFailure(), as the tag will be in \u0026ldquo;Still running\u0026rdquo; status for perpetuity.\nHere\u0026rsquo;s an example. Let\u0026rsquo;s say the template\u0026rsquo;s purpose is to write a cookie into the browser storage. If the template permissions allow the cookie to be written, data.gtmOnSuccess() is called after the write. If the permissions prevent this, a warning is logged into the console and data.gtmOnFailure() is run instead.\nconst log = require(\u0026#39;logToConsole\u0026#39;); const setCookie = require(\u0026#39;setCookie\u0026#39;); const queryPermission = require(\u0026#39;queryPermission\u0026#39;); if (queryPermission(\u0026#39;set_cookies\u0026#39;, \u0026#39;_gaClientId\u0026#39;)) { setCookie(\u0026#39;_gaClientId\u0026#39;, \u0026#39;abc123\u0026#39;); data.gtmOnSuccess(); } else { log(\u0026#39;Unable to write cookie due to missing permissions!\u0026#39;); data.gtmOnFailure(); }  Tag success/failure/incompletion status is shown in Preview mode, too.\n  Client templates in the code editor With Client templates, you need to invoke the claimRequest API when you want the Client to parse the HTTP request and not let other Clients use it any longer. For example, if you have a Client that is designed to handle requests coming to /my-pixel/, you\u0026rsquo;d build a Client like this:\nconst getRequestPath = require(\u0026#39;getRequestPath\u0026#39;); const claimRequest = require(\u0026#39;claimRequest\u0026#39;); if (getRequestPath() === \u0026#39;/my-pixel/\u0026#39;) { claimRequest(); }  When the Client has done its work, it must return a response back to the source of the request. This is done with the returnResponse API.\nrunContainer(event, () =\u0026gt; { // onComplete callback called, return the response.  returnResponse(); });  I do recommend to read this article for an overview of how Client templates work.\nPermissions When you add one of the supported APIs using the require method in the code editor, the associated permissions for that API are automatically displayed in the Permissions tab.\n  Read this document for a comprehensive list of permissions used by Server-side tagging templates.\nPermissions are described at length in the official documentation. Nevertheless, in this chapter I\u0026rsquo;ll show what the UI for each permission looks like, and what the different settings are used for.\nNote that you can save a template with code that conflicts with a permission. It\u0026rsquo;s not until the tag is run that an error is thrown, and this error is surfaced in the Errors tab in Preview mode, signalling that there was a permissions conflict within the tag.\nAccesses Global Variables   The Accesses Global Variables permission allows the code to Read (see what value is assigned to the variable), Write (update the value assigned to the variable), and Execute (if the variable is a function, execute it) global variables. The Key is the name of the global variable, accessed via window[key].\nHere are the APIs and the relevant permissions for them:\n   API example Permission     aliasInWindow('copyTo', 'copyFrom') Write on copyTo, Read on copyFrom.   callInWindow('someFunction') Execute on someFunction.   copyFromWindow('copyFrom') Read on copyFrom.   createArgumentsQueue('helper', 'queue') ReadWrite on helper, ReadWrite on queue.   createQueue('someArray') ReadWrite on someArray.   setInWindow('someVariable', 'someValue', true) ReadWrite on someVariable (regardless of third parameter).    In other words, if your code needs to access any global variable using e.g. setInWindow or copyFromWindow, you need to add those variables into these permission settings, and you need to specify if the code can read, write, and/or execute the variable in question.\n Name used with the queryPermission API and policies: access_globals.\n Accesses Local Storage   The Accesses Local Storage permission allows the code to Read (get items from localStorage) and Write (set items in localStorage) items in the browser\u0026rsquo;s localStorage, by utilizing the localStorage template API.\nIf your code needs to access localStorage, you need to specify Read and/or Write permissions for every key in localStorage the template needs to access.\n Name used with the queryPermission API and policies: access_local_storage.\n Accesses Template Storage   The Accesses Template Storage permission allows the code to Read and Write to a temporary storage which exists for the current page load. It\u0026rsquo;s useful if you need to persist information that persists across template executions.\nFor example, if you want to make sure your template doesn\u0026rsquo;t run some code more than once per page load, you could use templateStorage like this:\nconst templateStorage = require(\u0026#39;templateStorage\u0026#39;); const ids = templateStorage.getItem(\u0026#39;ids\u0026#39;) || []; if (!ids.includes(data.id)) { // Run code  ... ids.push(data.id); templateStorage.setItem(\u0026#39;ids\u0026#39;, ids); }  The permission has no configuration - read, write, and delete operations are always permitted.\n Name used with the queryPermission API and policies: access_template_storage.\n Reads Cookie Value(s)   The Reads Cookie Value(s) permission lists all the first-party cookies the tag or variable code can access with the getCookieValues API. To allow the code to read from a first-party cookie, you need to list all the cookie names that can be accessed on their own line in the text box in the permission settings.\n Name used with the queryPermission API and policies: get_cookies.\n Reads Referrer URL   The Reads Referrer URL permission allows template code to access any or only parts of the referring page URL (from document.referrer. You can restrict access to any combination of its URL components. Since the permission model is the same as for the Reads URL permission, please read the very next section for more details.\n Name used with the queryPermission API and policies: get_referrer.\n Reads URL   The Reads URL permission allows template code to access any or only parts of the current page URL. You can restrict access to any combination of the following URL components:\n   Component Example     protocol http, https, or file   host blog.simoahava.com   port 443   path /analytics/articles/   extension html   fragment about-us   query gclid=1.2.3.4    After selecting query, You can specify which query keys the getUrl API is allowed to access, or you can leave it to its default setting which is any query keys.\n Name used with the queryPermission API and policies: get_url.\n Injects Hidden Iframes   You can list a number of URL patterns, each on its own row, that must match those in the code editor used to inject hidden iframes on the page. The src attribute of the iframe the editor wants to inject must match one of these parameters.\nThe URL patterns must include https://, a valid hostname, and a valid path. Hostnames can use asterisk to wildcard match any subdomains, and paths can use asterisk to wildcard match any characters. Path ending with a / is also a wildcard match for anything that follows.\nFor example, given these three patterns:\n  https://*.simoahava.com/tracker.html\n  https://www.gtmtools.com/\n  https://www.tracksimo.com/*tracker.html\n  Any of these will be valid matches:\n  https://simoahava.com/tracker.html\n  https://blog.tracker.simoahava.com/tracker.html\n  https://www.gtmtools.com/track/\n  https://www.gtmtools.com/tracking/this/\n  https://www.tracksimo.com/tracker.html\n  https://www.tracksimo.com/track/superdupertracker.html\n   Name used with the queryPermission API and policies: inject_hidden_iframe.\n Injects Scripts   You can list a number of URL patterns, each on its own row, that must match those in the code editor used to inject scripts on the page. The src attribute of the script the editor wants to inject must match one of these patterns.\nThe URL patterns must include https://, a valid hostname, and a valid path. Hostnames can use asterisk to wildcard match any subdomains, and paths can use asterisk to wildcard match any characters. Path that only consists of a / is also a wildcard match for anything that follows.\nFor example, given these three patterns:\n  https://*.simoahava.com/tracker.js\n  https://www.gtmtools.com/\n  https://www.tracksimo.com/*tracker.js\n  Any of these will be valid matches:\n  https://simoahava.com/tracker.js\n  https://blog.tracker.simoahava.com/tracker.js\n  https://www.gtmtools.com/tracking/gtmtracker.js\n  https://www.tracksimo.com/tracker.js\n  https://www.tracksimo.com/track/superdupertracker.js\n   Name used with the queryPermission API and policies: inject_script.\n Logs To Console   You can choose whether the logToConsole API can log into the browser console only when in preview/debug mode, or whether it can log to console whenever the tag fires, regardless of debug context.\n Name used with the queryPermission API and policies: logging.\n Reads Data Layer   In the permission configuration, add the Data Layer keys the code has access to, each on its own row.\nYou can use wildcards to allow the code access to any subproperties of the key. For example, a permission like this:\n ecommerce.*  Will allow the code editor to read ecommerce, ecommerce.purchase.actionField.id, ecommerce.purchase.products, and any other key nested under ecommerce.\n Name used with the queryPermission API and policies: read_data_layer.\n Reads Document Character Set   An extremely simple permission for an extremely simple API. This permission governs whether or not the code editor can use the readCharacterSet API, which, in turn, returns the value of document.characterSet.\nThere are no configuration options you can pass to the permission, so it\u0026rsquo;s always permitted (unless a policy is used to block it).\n Name used with the queryPermission API and policies: read_character_set.\n Reads Container Data   Very simple permission (nothing to configure), which is used when the template code needs to use the getContainerVersion API.\n Name used with the queryPermission API and policies: read_container_data.\n Reads Event Metadata   This permission allows the code to use the addEventCallback API, which updates the eventCallback of the dataLayer.push() that triggered the tag created from this template. A data object of tags that fired for the dataLayer event is passed as an argument to the callback.\nTake a look at this article for more details on how this API works.\nThis permission doesn\u0026rsquo;t take any configuration options, so it\u0026rsquo;s always permitted (unless a policy is used to block it).\n Name used with the queryPermission API and policies: read_event_metadata.\n Reads Document Title   Another really simple permission for a really simple API. This permission allows the code to use the readTitle API, which returns the value of document.title.\nThis permission doesn\u0026rsquo;t take any configuration options, so it\u0026rsquo;s always permitted (unless a policy is used to block it).\n Name used with the queryPermission API and policies: read_title.\n Sends Pixels   You can provide a list of URL patterns (each on its own row), and when using the the sendPIxel API, the URL the pixel is dispatched to must match one of these patterns.\nFor example, given these three patterns:\n  https://*.simoahava.com/collect\n  https://www.gtmtools.com/\n  https://www.tracksimo.com/*/track\n  Any of these will be valid matches:\n  https://simoahava.com/collect\n  https://blog.tracker.simoahava.com/collect\n  https://www.gtmtools.com/tracking/collect\n  https://www.tracksimo.com/tracker/track\n  https://www.tracksimo.com/collect/analytics/track\n   Name used with the queryPermission API and policies: send_pixel.\n Sets A Cookie Value   This permission lets you configure which cookies the template code is allowed to set. You can also configure the following parameters per cookie:\n  Domain - on which domain the cookie can be written on or * for any.\n  Path - on which path the cookie can be written on or * for any.\n  Secure - whether the cookie must be set with the secure flag, without the secure flag, or either.\n  Session - whether the cookie must be a session cookie, a cookie with an expiration, or either.\n   Name used with the queryPermission API and policies: set_cookies.\n Tests For more information on writing and running tests against your template code, see this chapter, and read this article.\nTemplate preview The Template Preview window is where you can see what your template user interface looks like in its current state, and you can also try filling in the inputs before testing the template.\n  When you make changes to the template, whether in the Info, Fields, Code, or Permission tabs, the Refresh icon appears. By clicking this icon, the template preview is updated to reflect the changes you have made to the template in the editor.\nOther than that, the template preview should perform exactly as the real thing. In the summary view, i.e. before you click it for editing, the template will show you only those fields that are included in summary. You can manually toggle this with the Always in summary field configuration.\nThe template preview is the perfect place to try out your template user interface before testing how the template actually runs, and finally saving it into the template library of your container.\nImporting and exporting Importing and exporting templates will almost certainly be one of the most useful things you can do with templates. By exporting your custom templates, you can create a store or library of Google Tag Manager templates for others to use. By importing templates, you can add custom templates created by others into your own template library.\nTo export a template, click open the editor menu and choose Export.\n  Your browser should automatically download a file named \u0026lt;your template name\u0026gt;.tpl. The TPL suffix is a custom file format for Google Tag Manager templates. If you open the file in a text editor, you can see that it\u0026rsquo;s a combination of JSON objects and plain text.\n  To import a template, click open the editor menu and choose Import. When importing a template, it\u0026rsquo;s a good idea to first create a new template which will host the imported item. Unless of course you are deliberately updating an existing template with a newer version of the import.\nOnce you\u0026rsquo;ve chosen the file, a pop-up will warn you that importing the file will overwrite the template currently being edited.\nWhy is this significant? Well, an import completely overwrites the template to which it is imported. So if you expect some sort of \u0026ldquo;merging\u0026rdquo; being an option, well, at the time of writing there\u0026rsquo;s no such feature.\nAdvanced settings When you select Show Advanced Settings from the editor menu, a number of changes takes place in the UI.\n   A Notes option becomes visible in the editor navigation. You can write anything you want into this text area, such as developer documentation, instructions for use, etc. The Notes field contents are included with the template export/import.    The Info tab will now show the Version (of the templating system itself) and the Container Context (whether the template is for web containers or for app containers). You can\u0026rsquo;t change either of these values - they are set when you start creating the template. The Info tab will also show the Brand Name field.    In the Info and Fields tabs, you can directly edit the JSON source. Note that even though this lets you edit anything you want in the source, trying to change things that can\u0026rsquo;t be changed (such as Version and Container Context from above) will result in the template issuing a warning when you try to refresh it in the preview or save it.    Running the template code Whenever you want to test how the template actually runs, you can click the Run code button in the Template Preview window. This executes the tag/variable template code itself, outputting any debug messages into the console.\n  For tag templates, the console will tell you when the preview was last refreshed. It will also output any test logs, such as test start and completion (with elapsed time), as well as any errors.\nIf you don\u0026rsquo;t change anything in the code editor, the default code snippet will output the contents of the data object into the console.\nGenerally, the console will display any strings you write into the console using the logToConsole API.\n  Furthermore, when you choose to Show test page in the editor menu, a content area will pop up which you can inspect to find any changes the tag code has made to the page on which the tag runs. This is useful if you want to check if and how your hidden iframe was added to the page, for example.\n  To find your modifications to the test page, right-click it in your browser and choose Inspect (this may vary by browser). Start drilling down the DOM until you find an iframe with GTM\u0026rsquo;s sandbox HTML file. The \u0026lt;body\u0026gt; of that iframe should contain any modifications your tag did to the page.\n  Variable templates differ in that they don\u0026rsquo;t modify the underlying page (or, they SHOULDN\u0026rsquo;T modify the underlying page). Instead, when you test the template, the console outputs what the variable will return.\n  Templates in GTM\u0026rsquo;s Preview mode In Preview mode, tag and variable templates work just like their built-in template counterparts.\nFor tags, you\u0026rsquo;ll see the tag name and properties.\nTwo default properties are always included:\n  Type - which is the template name, basically.\n  Firing Status - which will show \u0026ldquo;Succeeded\u0026rdquo; if data.gtmOnSuccess() was reached in the tag code, \u0026ldquo;Failure\u0026rdquo; if data.gtmOnFailure() was reached in the tag code, and Still running if neither was reached or if the tag is, actually, still running (due to e.g. the endpoint timing out).\n  The rest of the properties mirror the fields and field configurations you have generated. In the example below, a text input field named Iframe URL has been populated with the value https://www.gtmtools.com/, and a param table field named Iframe parameters has been populated with the values you see.\n  For variables, the output can be found in the Variables tab of Preview mode, and you can see the return value of each custom variable in exactly the same way you can see the return values for all predefined variable templates.\nThe variable template type will be the name of the custom template (\u0026ldquo;Multiply\u0026rdquo; in the example below).\n  The Errors tab will surface any errors thrown by policies or if the template failed a permission check.\n  Field configuration reference This chapter lists all the Field configuration options you can configure for individual fields. Remember to check out the Fields editor chapter for a detailed description of the fields and the configurations available to them.\n  \u0026ldquo;Edit row\u0026rdquo; dialog title   Description: The \u0026ldquo;Edit row\u0026rdquo; dialog title is available in fields where the user adds rows to a table and can edit those rows in an overlay. Defaults to \u0026ldquo;Edit row\u0026rdquo;.\nHow it works: The text you write into the configuration field will appear as the title of the overlay which pops out when the user edits a row they have already added to the table.\nUsed in: Param table.\n\u0026ldquo;New row\u0026rdquo; button text   Description: The \u0026ldquo;New row\u0026rdquo; button text determines the button text with which the user can add a new row to a table field.\nHow it works: The text you type into the configuration field will be the text of the button below the table, which the user can use to add new rows to the table. Defaults to \u0026ldquo;Add row\u0026rdquo;.\nUsed in: Param table, Simple table.\n\u0026ldquo;New row\u0026rdquo; dialog title   Description: The \u0026ldquo;New row\u0026rdquo; dialog title determines the text you see as the heading of the overlay that pops out when you choose to add a new row into a table. Defaults to \u0026ldquo;New row\u0026rdquo;.\nHow it works: Type the new title into the configuration field, and it will show as the heading of the overlay the user sees when adding a new row to a table that uses overlays for data input.\nUsed in: Param table.\n\u0026ldquo;Not set\u0026rdquo; option   Description: Before the user chooses an item in the drop-down list, you can use the \u0026ldquo;Not set\u0026rdquo; option to show a placeholder value.\nHow it works: The \u0026ldquo;Not set\u0026rdquo; option is a selectable option in the drop-down list, which also shows up if the user hasn\u0026rsquo;t made any selection yet. If the user leaves the \u0026ldquo;Not set\u0026rdquo; option as the selected item, the value of the field in the code editor will be a blank string.\nUsed in: Drop-down menu.\nAllow empty strings Description: The Allow empty strings checkbox lets you determine whether or not an empty text input field shows up as an empty string when accessing the field value in the data object.\nHow it works: If the checkbox is checked, the text input field value in the code editor will be an empty string. If unchecked or if the configuration hasn\u0026rsquo;t been added to the field, the value will be undefined for empty text fields.\nUsed in: Text input\nAlways in summary   Description: The Always in summary configuration determines whether the field name and current value will show up in the summary view of the item.\nHow it works: When the checkbox is toggled, the user will see the name (if Display name is configured) and current value of the field in the summary view if the item. The summary is view what you see when the tag or variable is not in edit mode.\nUsed in: Text input, Drop-down menu, Checkbox, Simple table, Param table.\nClear on copy Description: With Clear on copy, you can toggle whether or not the field will retain or clear its value if a copy is made of the tag or variable.\nHow it works: If Clear on copy is checked, then when the user makes a copy of an item created with this template, the field will have its value cleared (returned to its initial state) in the copy. If Clear on copy is unchecked or missing, the value of the field from the original item will be preserved in the copy.\nUsed in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table.\nDefault value   Description: The Default value option determines what the value of the field is before the user inputs anything into the field.\nHow it works: Default value represents the initial value of the field. It is considered a true value, meaning if the user doesn\u0026rsquo;t delete or change it, the default value will be what the field returns in the code editor.\nUsed in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table.\nDisplay line count   Description: Set the height of the text input area, and whether or not a multi-line value can be given.\nHow it works: If Display line count is set to 1, then only one row of data can be input into the field. Anything larger than 1, and the height of the text area grows as a result, and the user can type on multiple rows in the text input field.\nUsed in: Text input.\nDisplay message when not set   Description: The Display message when not set allows you to show a \u0026ldquo;default value\u0026rdquo; for the field when in the summary view and the field doesn\u0026rsquo;t have a value.\nHow it works: The text will show up only in the summary view when the field is not set, i.e. has no determinable value. It will not impact what the code editor returns as the field value.\nUsed in: Text input, Simple table, Param table.\nDisplay name   Description: The Display name configuration lets you set a label for the field.\nHow it works: The text you input in the Display name is shown as a label for the field when both in edit mode and the summary view.\nUsed in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table, Group, Label.\nEnabling conditions   Description: You can establish a dependency with Enabling conditions. The field will only be visible in the UI if the enabling condition validates.\nHow it works: An Enabling condition is essentially a check against some other field\u0026rsquo;s value. You can use it to check whether some checkbox is unchecked, for example (as in the screenshot above).\nUsed in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table, Group, Label.\nGroup style   Description: Use the Group style configuration to determine how the Group field works.\nHow it works: The options you can choose are:\n  Simple section: the fields are simply grouped without UI impact. This is useful if you simply want to have the field configurations for the group impact the nested fields within.\n  Collapsible section \u0026ndash; Open: The fields are in a section that can be collapsed, and the section defaults to being open.\n  Collapsible section \u0026ndash; Closed: The fields are in a section that can be collapsed, and the section defaults to being closed.\n  Collapsible section \u0026ndash; Open if not default: The fields are in a section that can be collapsed, and if the fields have not been edited, the section is closed.\n  Used in: Group.\nHelp text   Description: Use Help text to show a little tooltip when hovering over the question mark icon next to the field in the UI.\nHow it works: The text you type into the Help text configuration field will appear in the UI when the user hovers the mouse cursor over the little question mark icon.\nUsed in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table, Group.\nInclude variables   Description: When checked, Include variables will make the full list of GTM variables available for selection as the field value.\nHow it works: The drop-down menu will include all the GTM variables as selectable options if this field configuration is checked.\nUsed in: Drop-down menu.\nNested fields   Description: When you add Nested fields to a field, those fields become dependent on the enabling condition set for the parent field. Also, visually they will be placed closer to the parent field compared to if they were regular, non-nested fields.\nHow it works: Toggle Nested fields on, and you can add any available field type as a nested field of the current field. After that, if the parent field is disabled due to an invalid Enabling condition, for example, the nested fields will be disabled, too.\nUsed in: Drop-down menu, Checkbox, Radio buttons.\nText as list   Description: Use this with Line count to access the field value as an array of strings (where each row corresponds to an item in the array).\nHow it works: When Text as list is checked, then each row in a Text input field (when Line count is also configured) will be an item in the array of the resulting data object. Without Text as list, a multi-line input will result in a single string, where each row is separated with the newline character (\\n).\nUsed in: Text input.\nValidation rules   Description: Use the Validation rules configuration to establish validation criteria for the field.\nHow it works: When you add a Validation rule to the field, the field must pass the validation, or the user won\u0026rsquo;t be able to save the tag or variable. The available rules are:\n   Rule How to fail validation     \u0026hellip;cannot be empty User does not input anything into the field.   \u0026hellip;must be a string of the required length User inputs a string that is less than the minimum or more than the maximum length.   \u0026hellip;must be a number User inputs a value that is not an integer number.   \u0026hellip;must be a positive number User inputs an integer number that is equal to or less than 0.   \u0026hellip;must be a positive number or 0 User inputs an integer number that is less than 0.   \u0026hellip;must be an integer between 0 and 100 (inclusive) User inputs an integer number that is less than 0 or more than 100.   \u0026hellip;must be a number between 0 and 1 (inclusive) User inputs a number (integer or floating point) which is less than 0.0 or more than 1.0.   \u0026hellip;must match a regular expression User inputs a value that does not match the given regular expression. The RegEx accepts full matches only, so don\u0026rsquo;t forget to add leading and/or trailing .* if you want it to be an open-ended match.   \u0026hellip;must be a valid GA tracking ID User inputs a value that does not match the GA tracking ID format (UA-11111-1).   \u0026hellip;must be a list of the required length User inputs rows into a table fewer than the minimum length or more than the maximum length. Applies also to a Text input field where Text as list is being used.     NOTE! For fields that are governed by enabling conditions, the validation rule will only apply if the field has been enabled.\n You can click the action menu for the Validation rule configuration to Show advanced settings.\n  The advanced settings include:\n  Error messages: You can customize the error message that is shown if the field does not pass validation.\n  Enabling conditions: You can add enabling conditions to the validation rule itself, meaning if the enabling conditions do not pass, the validation rule is ignored.\n  Used in: Text input, Drop-down menu, Checkbox, Radio buttons, Simple table, Param table.\nValue hint   Description: The text you enter here will be shown as a placeholder value in the field before the user has added any input.\nHow it works: The text will simply be a placeholder - it will not be the value of the field if saved without any input.\nUsed in: Text input.\nValue unit   Description: Use this to guide the users on what type of value is expected in the field.\nHow it works: The text will be displayed to the right of the field, giving users indication what type of value is expected in the field. The text can be anything (i.e. isn\u0026rsquo;t restricted to e.g. JavaScript types).\nUsed in: Text input, Drop-down menu.\nPolicies reference Policies are directives the site admin adds to the page template (or, if necessarily, dynamically with JavaScript), which dictate the type of API permissions each custom template can use.\nThey differ from Permissions in one critical detail: where permissions are built into the template, and specify a broader set of API configurations the template can run with, policies are implemented by the site where these templates are run.\nIn other words, template permissions come from the vendor, template policies come from the site owner. Both have the capability to delimit or restrict the types of templates that can run on any given page.\nTo provide a policy, you need to use some gtag.js code (note, you do not have to install the gtag.js snippet!) where you specify the permission requests your site will listen to.\nFor example, if you have a template that is trying to send a pixel to an endpoint whitelisted in the template\u0026rsquo;s permissions, you can use a policy to make sure that the template can actually only send the request to one specific endpoint. See the example below.\nwindow.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;policy\u0026#39;, \u0026#39;send_pixel\u0026#39;, function(container, policy, data) { // Only restrict the policy to one specific container by returning true  // for all other containers  if (container !== \u0026#39;GTM-ABCDE\u0026#39;) { return true; } // Check if the URL of the pixel request is a specific endpoint,  // and if it isn\u0026#39;t throw an error and prevent the tag from working.  if (data.url !== \u0026#39;https://snowplow.simoahava.com/i\u0026#39;) { throw(\u0026#39;Invalid pixel endpoint!\u0026#39;); } else { return true; } });  Note: The policy must return true explicitly if you want the check to pass. The policy will always default to returning false and preventing the template from working.\nYou can add more than one policy to the page, each in its own gtag() command, or you can check against ALL permissions requests by setting the policy name to all as below:\nwindow.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;policy\u0026#39;, \u0026#39;all\u0026#39;, function(container, policy, data) { // Prevent sending pixels and injecting iframes  if (policy === \u0026#39;send_pixel\u0026#39; || policy === \u0026#39;inject_hidden_iframe\u0026#39;) { throw \u0026#39;Sending pixels and injecting hidden iframes blocked!\u0026#39;; } else { return true; } });  Please read the official policy documentation for more details on how policies work.\nBelow are listed all the policy names and what the data object comprises for each.\naccess_globals Triggered by APIs:\n  aliasInWindow (once for the toPath key and once for the fromPath key).\n  callInWindow\n  copyFromWindow\n  createArgumentsQueue\n  createQueue\n  setInWindow\n  data object composition:\n{ \u0026#34;key\u0026#34;: \u0026#34;some_global_var\u0026#34;, // name of the key the permission tries to access  \u0026#34;read\u0026#34;: true || false, // does the key have read access  \u0026#34;write\u0026#34;: true || false, // does the key have write access  \u0026#34;execute\u0026#34;: true || false // does the key have execute access }  get_cookies Triggered by APIs:\n getCookieValues  data object composition:\n{ \u0026#34;name\u0026#34;: \u0026#34;some_cookie_name\u0026#34; // name of the cookie being accessed }  get_referrer Triggered by APIs:\n  getReferrer\n  getReferrerQueryParameters\n  data object composition:\n{ \u0026#34;component\u0026#34;: \u0026#34;query\u0026#34; // the component of the referrer URL being accessed }  get_url Triggered by APIs:\n getUrl  data object composition:\n{ \u0026#34;component\u0026#34;: \u0026#34;protocol\u0026#34; // the URL component name being accessed }  inject_hidden_iframe Triggered by APIs:\n injectHiddenIframe  data object composition:\n{ \u0026#34;url\u0026#34;: \u0026#34;https://some-iframe.com/\u0026#34; // the URL of the iframe being injected }  inject_script Triggered by APIs:\n injectScript  data object composition:\n{ \u0026#34;url\u0026#34;: \u0026#34;https://some-script.com/script.js\u0026#34; // the URL of the script being loaded }  logging Triggered by APIs:\n logToConsole  There is no data object associated with this permission.\nread_character_set Triggered by APIs:\n readCharacterSet  There is no data object associated with this permission.\nread_data_layer Triggered by APIs:\n copyFromDataLayer  data object composition:\n{ \u0026#34;key\u0026#34;: \u0026#34;some.variable.name\u0026#34; // name of the Data Layer Variable being accessed }  read_event_metadata Triggered by APIs:\n addEventCallback  There is no data object associated with this permission.\nread_title Triggered by APIs:\n readTitle  There is no data object associated with this permission.\nsend_pixel Triggered by APIs:\n sendPixel  data object composition:\n{ \u0026#34;url\u0026#34;: \u0026#34;https://some-endpoint.com/endpoint\u0026#34; // the URL of the pixel request endpoint }  set_cookies Triggered by APIs:\n setCookie  data object composition:\n{ \u0026#34;name\u0026#34;: \u0026#34;some_cookie_name\u0026#34;, // the name of the cookie  \u0026#34;options\u0026#34;: { \u0026#34;domain\u0026#34;: \u0026#34;somedomain.com\u0026#34;, // the domain of the cookie  \u0026#34;path\u0026#34;: \u0026#34;/some-path/\u0026#34;, // the path of the cookie  \u0026#34;max-age\u0026#34;: \u0026#34;15000\u0026#34;, // the maximum age of the cookie (in seconds)  \u0026#34;expires\u0026#34;: \u0026#34;Sun, 11 Aug 2019 10:00:00 GMT\u0026#34;, // UTC date string of the cookie\u0026#39;s expiration  \u0026#34;secure\u0026#34;: true || false, // Secure cookie  \u0026#34;sameSite\u0026#34;: true || false // sameSite cookie  } }  Final thoughts It\u0026rsquo;s difficult to compose any sort of summary to what I consider to be the most extensive and fundamental Google Tag Manager update since the programmatic API was released almost five years ago.\nCustom templates are, for now, a completely optional feature. No one is forcing you to use them, and you can continue as a happy GTM user without having to bother about the new Templates menu.\nHowever, I have a hunch that there is a huge incentive for the GTM team to get rid of the Custom HTML tag and the Custom JavaScript variable. When talking about governance and the existing prejudices towards GTM, especially from developers, it often boils down to being able to inject any arbitrary JavaScript code on the site, using the outdated and questionable eval() method, no less.\nWith custom templates, many of the problems around governance are presented with a solution:\n  The template code is compiled into JavaScript when the container is created, so eval() is no longer used to run the code.\n  Template-specific permissions (created by the template author) can delimit what the template does and what types of user input are valid and accepted.\n  Page-specific policies (created by the site admin) can be used to further restrict what any template can do on the site.\n  But these are mostly how templates address negative qualities of GTM - it would be foolish to ignore their net positive effect.\nGoogle Tag Manager is a UI-driven tool. Brands can now use custom templates to encapsulate their complicated JavaScript activation mechanisms under a user interface, thus minimizing the possibility of human error, and demystifying how the vendor JavaScript runs. Thus, instead of sharing a JavaScript snippet the user has to copy-paste to a Custom HTML tag, the brand can share a template export that can be added to the container directly, with all the code in the correct place.\nI can\u0026rsquo;t wait to see what the community comes up with, too! I\u0026rsquo;m waiting for a library of custom templates to emerge, hopefully sanctioned by Google but moderated by the community (with brands being able to add verified templates, too).\nSo now, brave traveller. You have reached the end, though I can only assume you skipped most of the above. What do you think about custom templates? Is my hyperbolic exuberance (yet again) unwarranted?\n"
},
{
	"uri": "https://www.simoahava.com/tags/guide/",
	"title": "Guide",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/fetch-latest-value-data-layer-variable/",
	"title": "#GTMTips: Fetch The Latest Value Of A Data Layer Variable",
	"tags": ["google tag manager", "gtmtips", "custom javascript", "data layer variable"],
	"description": "Use a custom JavaScript function in Google Tag Manager to fetch the latest value of any Data Layer Variable - regardless of when the tag itself fired.",
	"content": "One of Google Tag Manager\u0026rsquo;s oldest and most reliable features is that it freezes the state of Data Layer variables to the moment when the trigger event occurred. Thus, any tags firing on this trigger (and any variables resolved on this trigger event) will always have access to the same value of each Data Layer variable.\nHowever, there are situations where this is not a good thing. One is tag sequencing, and the other is a scenario where you want to run some custom code that should access the latest value of the Data Layer variable at a moment in time after the tag has already fired. Read on for an example!\nTip 98: Access the latest value of a Data Layer variable   For example, let\u0026rsquo;s say your site has a custom event listener built to detect when the user is about to leave a page (using the beforeunload custom event). However, you only want to send that event if the user is logged out, for some obscure reason.\nThis is what your first attempt might look like. It\u0026rsquo;s a Custom HTML tag set to fire when the page is first loaded with the All Pages trigger.\n\u0026lt;script\u0026gt; (function() { window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if ({{DLV - loginStatus}} === \u0026#39;logged-out\u0026#39;) { window.dataLayer.push({ event: \u0026#39;userLeavesPages\u0026#39; }); } }); })(); \u0026lt;/script\u0026gt; Now, when the Custom HTML tag fires, the beforeunload listener is created, and the {{DLV - loginStatus}} variable is resolved to whatever value it had when the tag fired.\nThen, when the user is about to leave the page by closing the browser or clicking a link, the beforeunload callback is executed, and it checks if this original state of loginStatus is \u0026ldquo;logged-out\u0026rdquo;, in which case it runs the dataLayer.push().\nDo you see the problem here? The beforeunload event is not fired until the user tries to leave the page, but the {{DLV - loginStatus}} is resolved to whatever value it had when the tag itself was initially run. If the loginStatus changes while the user is on the page, it wouldn\u0026rsquo;t make a difference. The initial value is what\u0026rsquo;s used in the if... condition, meaning you\u0026rsquo;ll risk losing valid hits because of that.\nSo we need a mechanism that fetches the value of the Data Layer variable when the relevant code is run. We want the if... condition to evaluate against what the value of loginStatus is at the time of the beforeunload event and not when the tag is first run.\nFor this, we use a JavaScript method that is built into Google Tag Manager and allows us to do exactly this.\nwindow.google_tag_manager[{{Container ID}}].dataLayer.get('variableNameHere');\nRemember to enable the Container ID built-in variable for this.\nThis method polls GTM\u0026rsquo;s internal data model and fetches the latest value of the variableNameHere Data Layer variable. So, to modify the original example, here\u0026rsquo;s what you end up with:\n\u0026lt;script\u0026gt; (function() { window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if (window.google_tag_manager[{{Container ID}}].dataLayer.get(\u0026#39;loginStatus\u0026#39;) === \u0026#39;logged-out\u0026#39;) { window.dataLayer.push({ event: \u0026#39;userLeavesPage\u0026#39; }); } }); })(); \u0026lt;/script\u0026gt; Now the if... condition checks what the latest value of loginStatus is, and it will work nicely if the status has changed while the user is on the current page.\nIt\u0026rsquo;s a simple trick, but can come in handy when working with GTM\u0026rsquo;s idiosyncractic way of freezing the variable state for the duration of each trigger event.\n"
},
{
	"uri": "https://www.simoahava.com/tags/data-layer-variable/",
	"title": "data layer variable",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/adwords/",
	"title": "adwords",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/web-development/monitor-google-ads-accounts-r-adwords-api-slack-api/",
	"title": "DIY Google Ads Account Monitor With R, AdWords API, And Slack API",
	"tags": ["adwords", "api", "r", "slack", "guest post"],
	"description": "Use the AdWords API and Slack API together with R to build a poweful monitoring system for your Google Ads accounts.",
	"content": "Every now and then I\u0026rsquo;m fortunate enough to be able to publish guest posts by illustrious people in the analytics and digital marketing industries. This time, I get to work with an old colleague of mine who\u0026rsquo;s a veritable wizard when it comes to building solutions that make our daily work in the digital industry so much smoother.\nErik Grönroos works as an analyst in MarkkinointiAkatemia, a Finnish digital customer acquisition agency. Erik\u0026rsquo;s job is to utilize data and analytics to help grow the agency as well as the companies of the clients he works with.\nIn this guide, Erik will share with you how to build an application which helps you automate the monitoring of your Google Ads accounts using the R programming language, the AdWords API and the Slack API.\n  I\u0026rsquo;m particularly excited to publish this article, as it\u0026rsquo;s the first time (if I recall correctly) that Google Ads and R are covered in the blog.\nAnyway, as the editor, all the mistake and errors that you can find in the article are 100% mea culpa, so let me know in the comments if errors persisted to the final draft. Thank you!\nIntroduction In this article I\u0026rsquo;ll show you how to implement an automated monitoring application for keeping an eye on your Google Ads accounts. Since the nature of the implementation is a Proof of Concept (PoC), I\u0026rsquo;ll try to keep it simple by using just one practical use case for demonstrating the end-to-end pipeline:\nZero-cost detection in Ads accounts, for monitoring credit card expirations or other issues that prevent your ads from rolling.\nThis implementation utilizes R and the Google AdWords API for checking the data, Slack API for realtime alerting via your Slack bot if something critical is detected, and good ol\u0026rsquo; cron for scheduling the script.\nIf you\u0026rsquo;re a marketing agency, a typical situation is that you have plenty of Google Ads accounts to take care of. When the number of accounts breaches the hundreds or the thousands, manually managing, monitoring, and maintaining them isn\u0026rsquo;t really an option anymore.\nWhat you can do, of course, is set up Google Ads\u0026rsquo; built-in alerts one by one for each of your accounts. You could even manage these alerts via the Google Ads API or utilize Google Ads scripts. But the problem with these is that they involve email alerts by default. Hey, it\u0026rsquo;s 2019. Email is probably not the only channel you should use for building automation on.\nSo, we decided to harness the force of APIs and, as a result, saved the galaxy!\nHow the solution works The script is populated with the list of Google Ads account IDs we want to monitor. The monitoring system itself queries the cost data of each ID using the Google AdWords API, and parses the results for those accounts that have accumulated no cost at all, signalling potential issues with your budgets or your configurations.\nWhen this information has been parsed, the account IDs that were flagged are written to a database for further analysis, and a notification is dispatched to the Slack bot so that the team responsible can be immediately notified when problems arise.\nRequirements  R CLI on a Linux server (for cron to run your R script) RStudio for developing \u0026ndash; Required R packages: RAdwords, httr, jsonlite \u0026ndash; Optional R packages: DBI, RMariaDB Google API Client ID and Client Secret Google AdWords Developer Token Slack webhook URL Crontab access (on the same Linux server)  About R R is a statistical programming language that today supports pretty much the same Machine Learning (ML) tricks that Python does. R vs. Python is a long-standing debate, so it really boils down to language preference: which one do you want to use for cool data stuff! Okay, I\u0026rsquo;ll put my pants back on, hold on.\nWe don\u0026rsquo;t need ML in this simple monitoring example, but we do use it for more advanced solutions. That\u0026rsquo;s why R is the tool of choice for us.\nRStudio is an IDE for R, it makes R easier to use. Quote from RStudio\u0026rsquo;s website:\n RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\n  RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux) or in a browser connected to RStudio Server or RStudio Server Pro (Debian/Ubuntu, RedHat/CentOS, and SUSE Linux).\n If you don\u0026rsquo;t have RStudio installed yet, grab yours from RStudio downloads.\n  We won\u0026rsquo;t get into the RStudio installation details in this article. RStudio\u0026rsquo;s website provides comprehensive documentation and step-by-step how-to\u0026rsquo;s for that.\nOnce you have your RStudio setup up and running, let\u0026rsquo;s script!\nEpisode I: An R script We are using MariaDB to store the Google Ads Customer IDs of our clients, where they can be retrieved from for several other integration and automation purposes we need.\nIf you\u0026rsquo;re using some other database server, you need to modify the DB part of the code to suit your needs. R has comprehensive support for a wide variety of databases, perhaps even all of them. Basically, if it\u0026rsquo;s something to do with data, R can read from it.\nIf you\u0026rsquo;re not using a database, you can skip the DB part. Optionally, R can also read the IDs from e.g. Google Sheets or a CSV file. How to do this in R is your homework!\nYou\u0026rsquo;re probably thinking \u0026ldquo;But why aren\u0026rsquo;t you reading the IDs directly from AdWords API\u0026rdquo;?\nWell, the unfortunate thing is that RAdwords, the R AdWords package, implements the Adwords API Reporting Service only, and the Reporting Service does not support querying Google Ads Customer IDs on the Manager Account level (formerly known as My Client Center or Google MCC). Instead, you\u0026rsquo;d need access to AdWords API Managed Customer Service to do that.\nIn our case, this isn\u0026rsquo;t a problem though, because we want to persist the IDs in our own custom environment and have different classifications and levels for different sort of contracts and clients. This way we are able to do the desired actions to those clients that need them, and we can also add monitoring capabilities to the setup.\nIf you need to read the IDs directly from the AdWords API, you can access the Managed Customer Service via a newer R package called adwordsR.\nNow, let\u0026rsquo;s script for real! Create a new R file in your RStudio with the following content, set up your DB user and pass, and save the file as db_credentials.R in /YOUR_R_USER_ROOT_DIR/auth/. This is your DB auth file.\ndb_username = \u0026#34;YOUR_USERNAME\u0026#34; db_password = \u0026#34;YOUR_PASSWORD\u0026#34; Create another file including the following code and save the file as connect_db.R in /YOUR_R_USER_ROOT_DIR/. This is your DB connector.\n# Include DB credentials from your /auth/ dir source(\u0026#34;~/auth/db_credentials.R\u0026#34;) # Check if the required packages are installed, install if not if(!require(DBI)) install.packages(\u0026#34;DBI\u0026#34;) if(!require(RMariaDB)) install.packages(\u0026#34;RMariaDB\u0026#34;) # Load packages library(DBI) library(RMariaDB) # Create a DB connection handle con \u0026lt;- dbConnect( drv = RMariaDB::MariaDB(), username = db_username, password = db_password, host = \u0026#34;YOUR_DB_HOST\u0026#34;, port = 3306, \u0026#34;YOUR_DB_NAME\u0026#34; ) And finally the actual Ads R script, include the following code and save it as google-ads-zero-cost-monitor.R in /YOUR_R_USER_ROOT_DIR/.\n# Init the DB connection source(\u0026#34;~/connect_db.R\u0026#34;) # Check if the required packages are installed, install if not if(!require(RAdwords)) install.packages(\u0026#34;RAdwords\u0026#34;) # Load packages library(RAdwords) # Define the file path where your AdWords API OAuth RData file should be stored to, we\u0026#39;ll need this next google_ads_auth_file \u0026lt;- \u0026#34;~/auth/google_ads_auth.RData\u0026#34; # Check if the google_auth RData file already exists, create if not if(!file.exists(google_ads_auth_file)) { # Auth steps: # 1) You\u0026#39;ll need Google API Client ID, Client Secret and AdWords Developer Token here for authenticating, see https://github.com/jburkhardt/RAdwords for details # 2) Once you\u0026#39;ve provided those in your R console when R requests them, you\u0026#39;ll get redirected to OAuth screen in your browser # 3) Authenticate with your Google Account (the same that has access to your AdWords account) # 4) You\u0026#39;ll be redirected to a OAuth success screen that provides a final token code and asks you to copy/paste it to your application (to your R console in this case) google_auth \u0026lt;- doAuth(save = FALSE) # Save the auth credentials, to be used the auth from a file from now on save(google_auth, file = google_ads_auth_file) } # Load Google auth credentials load(file = google_ads_auth_file) # Init date vars dayBeforeYesterday \u0026lt;- Sys.Date() - 2 yesterday \u0026lt;- Sys.Date() - 1 # Fetch Google Ads Customer IDs from your DB ads_accounts \u0026lt;- dbGetQuery(con, \u0026#34;SELECT ads_customer_id FROM ads_accounts\u0026#34;) # Create a list of the IDs for later looping account_list \u0026lt;- ads_accounts$ads_customer_id # Alternatively, if you want to skip the DB stuff, you can define your Google Ads Customer IDs manually by uncommenting the following line and listing the IDs here: # account_list \u0026lt;- c(\u0026#34;123-123-1234\u0026#34;, \u0026#34;234-234-2345\u0026#34;) # Init AdWords API request # We\u0026#39;re checking the cost data from two previous days body \u0026lt;- statement(select = c(\u0026#39;Cost\u0026#39;), report = \u0026#34;ACCOUNT_PERFORMANCE_REPORT\u0026#34;, start = dayBeforeYesterday, end = yesterday) # Init a data frame for collecting the data and and an index variable for keeping track of progressing during the for loop adsdata \u0026lt;- data.frame() progressIndex \u0026lt;- 0 # Get the Ads cost data for (account in account_list) { # Get cost data ads_api_response \u0026lt;- getData(clientCustomerId = account, google_auth = google_auth, statement = body) # Proceed only if the API doesn\u0026#39;t return an error (happens when there\u0026#39;s no data to return or when the API fails), this is for dropping off false zero cost alerts if(!is.null(dim(ads_api_response))) { # Init empty data frame adsdata_temp \u0026lt;- setNames(data.frame(matrix(ncol = 2, nrow = 1)), c(\u0026#34;Cost\u0026#34;, \u0026#34;Account\u0026#34;)) # Include account id adsdata_temp$Account \u0026lt;- account # Include cost data # If there\u0026#39;s no data, the result is doubled for some reason # Fix this by always reading the 1st value only with [1] adsdata_temp$Cost \u0026lt;- ads_api_response$Cost[1] # If cost is zero, NA is returned, replace it with zero if(is.na(adsdata_temp$Cost)) adsdata_temp$Cost \u0026lt;- 0 # Collect during each iteration adsdata \u0026lt;- rbind(adsdata, adsdata_temp, stringsAsFactors = FALSE) } # Print progress to R console progressIndex \u0026lt;- progressIndex + 1 cat(paste(\u0026#34;Query \u0026#34;, progressIndex, \u0026#34; of \u0026#34;, NROW(account_list), \u0026#34; done\\r\\n\u0026#34;, sep = \u0026#34;\u0026#34;)) } # Keep accounts with zero cost only ads_accounts_with_zero_cost \u0026lt;- subset(adsdata, Cost == 0) # Count nbr of alerted accounts nbr_of_accounts_with_alerts \u0026lt;- nrow(ads_accounts_with_zero_cost) # Save all alerts to DB, if there was any # This assumes you have DB table named \u0026#34;alerts\u0026#34; existing with fields:  # ads_customer_id VARCHAR # category VARCHAR # status_id INT # created_at DATETIME # updated_at DATETIME if(nbr_of_accounts_with_alerts \u0026gt; 0) { for (i in 1:nrow(ads_accounts_with_zero_cost)) { row \u0026lt;- ads_accounts_with_zero_cost[i, ] ads_customer_id \u0026lt;- row$ads_customer_id category \u0026lt;- \u0026#34;ALERT_ZERO_COST\u0026#34; # For categorizing different types of alerts status_id \u0026lt;- 0 # To be set to eg. 1 when the alert is handled via a custom UI # Save each item to db query \u0026lt;- paste(\u0026#34;INSERT INTO alerts ( ads_customer_id, category, status_id, created_at, updated_at) VALUES (\u0026#34;, \u0026#34;\u0026#39;\u0026#34;, ads_customer_id, \u0026#34;\u0026#39;, \u0026#34;, \u0026#34;\u0026#39;\u0026#34;, category, \u0026#34;\u0026#39;, \u0026#34;, status_id, \u0026#34;, \u0026#34;, \u0026#34;now(), \u0026#34;, \u0026#34;now())\u0026#34;, sep = \u0026#34;\u0026#34; ) dbExecute(con, query) } # If you want to check/debug what was stored, uncomment and run these lines # db_alerts \u0026lt;- dbGetQuery(con, \u0026#34;SELECT * FROM alerts\u0026#34;) # View(db_alerts) } Now you have your Ads zero cost monitor script ready and you can move on to setting up your Slack bot.\nEpisode II: The Slack bot strikes back Because the guys at Slack are awesome, they have written an excellent guide on how to set up your bot. They even serve you virtual cookies! Slack\u0026rsquo;s guides are a textbook example about how documentation should be written - they\u0026rsquo;re always a pleasure to read and follow! Thanks guys!\nSo, follow the Slack bot users guide. The required steps for our alert notifications are:\n  Create your Slack app\n  Create your bot user\n  Install your bot to your Slack workspace\n  Once you manage to get your bot installed into your Slack workspace, you will get a webhook URL that you\u0026rsquo;ll need in the following script.\n  The bot script part is here:\n### SLACK NOTIFICATIONS ### # Replace the default value with the webhook URL you got from Slack webhook_url \u0026lt;- \u0026#34;https://hooks.slack.com/services/12345\u0026#34; # Check if the required packages are installed, install if not if(!require(jsonlite)) install.packages(\u0026#34;jsonlite\u0026#34;) if(!require(httr)) install.packages(\u0026#34;httr\u0026#34;) # Load packages library(jsonlite) library(httr) # Nbr of accounts for the Slack notification nbr_of_accounts \u0026lt;- NROW(account_list) # Setup the message you want to send to Slack msg_desc \u0026lt;- paste(\u0026#34;Number of Google Ads accounts checked: \u0026#34;, nbr_of_accounts, sep = \u0026#34;\u0026#34;) msg_title \u0026lt;- paste(\u0026#34;ALERT - Number of zero cost accounts detected: \u0026#34;, nbr_of_accounts_with_alerts, sep = \u0026#34;\u0026#34;) # This places a channel ping if there were occurrences with zero cost, and if not, defaults to empty message if(nbr_of_accounts_with_alerts \u0026gt; 0) { msg_body \u0026lt;- \u0026#34;Alerts saved to database! \u0026lt;!channel\u0026gt;\u0026#34; } else { msg_body \u0026lt;- \u0026#34;\u0026#34; } # Alert to Slack - POST the msg request \u0026lt;- POST(webhook_url, body = paste( \u0026#39;{\u0026#34;attachments\u0026#34;: [{\u0026#39;, \u0026#39;\u0026#34;title\u0026#34;: \u0026#34;\u0026#39;, msg_title, \u0026#39;\u0026#34;,\u0026#39;, \u0026#39;\u0026#34;pretext\u0026#34;: \u0026#34;\u0026#39;, msg_desc, \u0026#39;\u0026#34;,\u0026#39;, \u0026#39;\u0026#34;text\u0026#34;: \u0026#34;\u0026#39;, msg_body, \u0026#39;\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#ee0000\u0026#34;\u0026#39;, \u0026#39;}]}\u0026#39;, sep = \u0026#39;\u0026#39; ) ) Once you\u0026rsquo;ve set this up, include the above Slack part into your google-ads-zero-cost-monitor.R script. Your monitoring script is now done!\nThis is what a sample notification would look like in Slack:\n  Test it carefully in your RStudio and make sure everything is working correctly. After that, you\u0026rsquo;re ready to move your script to production by scheduling it via cron.\nEpisode III: Return of the cron job Open crontab in your Linux shell for editing:\ncrontab -e Add a cron job that suits your needs. Here\u0026rsquo;s an example syntax with a scheduler that runs the R monitoring script daily at 10 AM.\n# Daily at 10 AM 0 10 * * * Rscript /home/YOUR_USERNAME/google-ads-zero-cost-monitor.R Summary You have now created a potentially powerful Google Ads monitoring pipeline that has endless opportunities for extending. Other useful use cases for utilizing the pipeline are for example anomaly detection, high spend tracking, campaign-level stuff, automation of ads placement optimizing, and so forth.\nIt goes without saying that automation is absolutely necessary in this day and age when it can save you and your team from countless hours of manual labor. By automating trivial processes such as monitoring and anomaly detection, you are freeing up your most valuable resources (people) to work on more ambitious tasks.\nAs always, we\u0026rsquo;re looking forward to your comments and questions. Let us know what you think of the solution, if it could be improved, and if you already have extensions in mind!\n"
},
{
	"uri": "https://www.simoahava.com/tags/guest-post/",
	"title": "guest post",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/r/",
	"title": "r",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/css-selector-guide-google-tag-manager/",
	"title": "CSS Selector Guide For Google Tag Manager",
	"tags": ["google tag manager", "css selectors", "guide"],
	"description": "Guide to CSS selectors in Google Tag Manager. You can use CSS selectors to identify and work with elements on the page. This is especially useful when using Click triggers.",
	"content": "Some four years ago, Google Tag Manager released a new trigger predicate named matches CSS selector. Slowly but surely, it has evolved into one of the most useful little features in GTM.\n  Even though I\u0026rsquo;ve written about CSS selectors many times before, I wanted to compile all the relevant information into a single guide. For an external resource, I recommend bookmarking the w3schools.com CSS Selector Reference. But for your day-to-day use of CSS selectors in GTM, this guide will hopefully prove useful.\nRemember to read my article on the 10 selectors I consider most useful in GTM - that list has more concrete use cases for you to use. If you want a concrete way to test CSS selectors with (outside GTM), check out the w3schools.com CSS selector tester.\nWhat are CSS selectors? You can use CSS selectors to target any element on a web page. Every single element is uniquely positioned in the document object model (DOM), and thus selectors can be used to find even the most generic elements.\nSometimes selectors are very simple while being extremely robust. For example, when using a selector like div#author, you\u0026rsquo;re selecting an element that looks like \u0026lt;div id=\u0026quot;author\u0026quot;\u0026gt;. Since it has the id attribute, it\u0026rsquo;s a reasonable expectation that it\u0026rsquo;s the only such element on the page, thus even that short selector is very powerful.\nOn the other hand, sometimes all you can do is use a really long, complex chain, because there are so few uniquely identifying features in the element itself or its closest ancestors. Thus, the selector might end up looking like this:\n#main \u0026gt; article \u0026gt; div.post-content.markdown \u0026gt; div \u0026gt; p:nth-child(12) \u0026gt; a:nth-child(2)\nIt\u0026rsquo;s not pretty, and it carries the additional weight that the longer and more complex the CSS selector is, the more fragile it becomes. The selector above will be invalidated when any one of the elements in the chain changes position or form.\nIn other words, always strive to create a selector that is as simple as possible without compromising its accuracy to target the exact element you want.\nCSS selectors originated, surprise surprise, in Cascading Style Sheets (CSS). Style sheets are sets of rules and declarations that govern how HTML elements are displayed (and sometimes interacted with) on a web page. Here\u0026rsquo;s what a typical style declaration might look like:\na:hover, a:active { text-decoration: none; font-weight: bold } The two items preceding the { and separated by a comma are the selectors, and the two rows contained within the curly braces are the declarations. This is how you would read the rule:\n Select all links (a) that are currently being hovered over by the mouse (:hover) AND select all links (a) that are currently being clicked by the mouse (:active) THEN remove all text decorations (e.g underline) from them AND set their font weight to bold  CSS is its own, wonderfully complex discipline in web design and development. There are so many weird and magnificent things you can do with it, from replacing JavaScript functionality to running complex transitions using hardware acceleration.\nWell, in this guide we\u0026rsquo;ll try to be a bit more modest and focus on how CSS selectors can be used with Google Tag Manager\u0026rsquo;s JavaScript to make the most out of triggers, tags, and variables, where selecting, querying, or parsing the correct element is of utmost importance.\nCSS selectors in JavaScript In JavaScript, you\u0026rsquo;ll often run into CSS selectors in two scenarios:\n  You need to retrieve a specific element or elements from the page.\n  You need to check if a given element matches a CSS selector.\n  Both of these scenarios are very relevant in Google Tag Manager. Before we jump into how they work in GTM, let\u0026rsquo;s take a look at JavaScript\u0026rsquo;s handling of CSS selectors.\nUsing document.querySelector and document.querySelectorAll The two JavaScript methods you\u0026rsquo;ll most often use with CSS selectors are document.querySelector(selector) and document.querySelectorAll(selector).\nThe first returns the first element on the page that matches the given selector.\nThe second returns a list of all the elements on the page that match the given selector.\n// Get the first outbound link element on the page var firstOutbound = document.querySelector(\u0026#39;a:not([href=\u0026#34;mydomain.com\u0026#34;])\u0026#39;); // Get all the checked checkbox and radio button elements on the page var allChecked = document.querySelectorAll(\u0026#39;input[type=\u0026#34;checkbox\u0026#34;]:checked,input[type=\u0026#34;radio\u0026#34;]:checked\u0026#39;);  As you can see, the selector is passed as a string argument to the method.\nRemember that HTML elements are essentially objects, and just capturing them makes often little sense. Instead, you\u0026rsquo;ll want to do something with those objects, such as parse some property from them.\n// Get the first outbound link element on the page var firstOutbound = document.querySelector(\u0026#39;a:not([href=\u0026#34;mydomain.com\u0026#34;])\u0026#39;); // Push the URL of this outbound link into dataLayer window.dataLayer.push({ firstOutboundLinkURL: firstOutbound.getAttribute(\u0026#39;href\u0026#39;) });  In the example above, we use document.querySelector to fetch the first outbound link (i.e. link that does NOT have mydomain.com in its href), and then we push it into dataLayer.\nOne thing to remember is that document.querySelectorAll doesn\u0026rsquo;t return an array but rather a NodeList. This means that you can\u0026rsquo;t use regular array methods with whatever the method returns. You\u0026rsquo;ll have to resort to some workarounds if you, for example, want to map() all elements in the list to get a modified array as a result.\n// Get ALL outbound links on the page var allOutbound = document.querySelectorAll(\u0026#39;a:not([href=\u0026#34;mydomain.com\u0026#34;])\u0026#39;); // Create an array of all their href values var allHrefValues = Array.prototype.map.call(allOutbound, function(link) { return link.href; }); // Remove all duplicates var uniqueHrefValues = allHrefValues.filter(function(href, index) { return allHrefValues.indexOf(href) === index; });  As you can see, in order to run the map() method against the list returned by document.querySelectorAll, you need to invoke the method from the array prototype. It\u0026rsquo;s a good thing to keep in mind if you ever find yourself working with document.querySelectorAll.\nUsing element.matches If you want to check if any given element matches a specific CSS selector, you can use the matches() method like this:\n// Check if the clicked element is an outbound link function checkIfClickedElementIsOutbound() { var element = {{Click Element}}; if (element.matches(\u0026#39;a:not([href=\u0026#34;mydomain.com\u0026#34;])\u0026#39;) { return true; } else { return false; } }  You invoke the matches() method on the element itself (the element has to be an HTML element), and like querySelector / querySelectorAll, you pass the selector as a string argument. The method returns true for a match, and false otherwise.\nThe thing about matches() is that it didn\u0026rsquo;t use to have stellar browser support, so if you want to use these methods with Internet Explorer in mind, you might need to implement a polyfill to extend the support (see here for inspiration).\nLuckily, the matches() method is abstracted in Google Tag Manager when using triggers, which is by far the most common use case. So let\u0026rsquo;s dive right in!\nCSS selectors in Google Tag Manager In Google Tag Manager, you\u0026rsquo;ll find CSS selectors in a number of places.\nYou can use them as the selection method in the DOM Element variable:\n  Using a CSS selector here lets you target specific elements that might not have the ID attribute (the only other selection method provided).\nYou can also find the same option in the Element Visibility trigger:\n  Whereas the DOM Element variable only returns the first matching element (thus being similar to document.querySelector), the Element Visibility trigger can be set to target all the matching elements, making CSS selectors super powerful in this particular trigger type.\nThe third, and by far the most useful scenario for CSS selectors in Google Tag Manager is the matches CSS selector predicate when used together with the {{Click Element}} built-in variable.\nThe Click Element variable The Click Element variable is a Built-in variable that you need to enable before you can use. The easiest way to enable it is to go to Variables via GTM\u0026rsquo;s main navigation, and click the blue Configure button in the top corner of the content.\n  In the overlay that opens, check the box next to Click Element and you\u0026rsquo;re done. Now you can choose the Click Element variable in all the variable drop-downs of GTM.\nClick Element returns the HTML element that was the target of the auto-event trigger action. In other words:\n  It returns the clicked element when using the Click / All Elements and Click / Just Links triggers.\n  It returns the submitted form element when using the Form Submission trigger.\n  It returns the matched element that became visible when using the Element Visibility trigger.\n  Because it returns an HTML element, you can use it in your Custom HTML tags and Custom JavaScript variables together with typical HTML element methods such as getAttribute, appendChild, and yes, matches.\nIn GTM, you\u0026rsquo;ll use it most often, I\u0026rsquo;m confident enough to say, with the matches CSS selector predicate in your Click and Form triggers. It\u0026rsquo;s not that useful with the Element Visibility trigger, since you already specify the set of matching elements with the CSS selector setting in the trigger.\nThe Matches CSS Selector predicate You can use the matches CSS selector trigger predicate together with the Click Element variable to check if the element matches a specific selector (d\u0026rsquo;oh).\nThis is particularly useful with the All Elements trigger, since it truly fires when anything on the page is clicked, and you can use CSS selectors to delimit the tag to which the trigger is attached from firing all too often.\nTo make it work, you need to first check the This trigger fires on\u0026hellip;Some Clicks/Some Link Clicks/Some Forms option, then select the Click Element variable from the variable selector, the matches CSS selector predicate from the predicate selector, and then type the selector in the field to the right, like so:\n  One thing to keep in mind is that the All Elements truly captures the exact element the user clicked. Thus it might be something nested within the element you actually drafted the selector for. With the All Elements trigger, you should make liberal use of the wildcard (*) selector. Read this article for more information.\nIn the reference chapter below, I have included what a Click trigger would look like for all the selectors listed in the reference.\nCSS selector reference for Google Tag Manager The reference below is a modified (slimmed down) version of the w3schools\u0026rsquo; excellent resource. I\u0026rsquo;ve tailored the different selectors with imaginary Google Tag Manager use cases in mind.\nRemember that you can combine selectors to identify different aspects of the element. When combining selectors, put them one after the other with no whitespace in between. For example, to target all \u0026lt;a\u0026gt; elements that have the author class, simo as the ID, and that are also outbound links, use something like this:\na.author#simo:not([href=\u0026quot;mydomain.com\u0026quot;])\nThe order is inconsequential, but it\u0026rsquo;s customary to add pseudo-classes (e.g. :checked, :not) to the end for readability.\n.class Matches elements that have the given class as one of the class names in their class attribute.\nSample HTML structure\n\u0026lt;a class=\u0026#34;highlight author\u0026#34; href=\u0026#34;/author-page/\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Simo Ahava\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; Sample trigger\n  Selector\n.author\nOutcome\nThe trigger fires if the link is clicked. Clicks on the \u0026lt;span\u0026gt; work too, since the Just Links trigger automatically retrieves the closes wrapping \u0026lt;a\u0026gt; element of whatever was actually clicked.\n#id Matches elements that have the given ID as the value of their id attribute.\nSample HTML structure\n\u0026lt;div class=\u0026#34;date\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/archives\u0026#34;\u0026gt; \u0026lt;span id=\u0026#34;date\u0026#34;\u0026gt;2019-04-09\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\n#date\nOutcome\nThe trigger fires if the \u0026lt;span id=\u0026quot;date\u0026quot;\u0026gt;...\u0026lt;/span\u0026gt; element is clicked.\nelement Matches elements that are the given element (e.g. a for links, img for images).\nSample HTML structure\n\u0026lt;a class=\u0026#34;author\u0026#34; href=\u0026#34;/author-page\u0026#34;\u0026gt; \u0026lt;img id=\u0026#34;simo\u0026#34; src=\u0026#34;simo.jpg\u0026#34;/\u0026gt; \u0026lt;/a\u0026gt; Selector\nimg#simo\nSample trigger\n  Outcome\nThe trigger fires if the clicked element is the \u0026lt;img id=\u0026quot;simo\u0026quot;.../\u0026gt;. As you can see, the selector combines both element and id, meaning the matched element must be both an image and have the ID simo.\nelement,element Matches elements that can be selected with ANY of the selectors separated by a comma (you can add multiple selectors, each separated by a comma).\nSample HTML structure\n\u0026lt;a class=\u0026#34;author\u0026#34; href=\u0026#34;/author-page\u0026#34;\u0026gt; \u0026lt;img id=\u0026#34;simo_img\u0026#34; src=\u0026#34;simo.jpg\u0026#34;/\u0026gt; \u0026lt;p id=\u0026#34;simo_name\u0026#34;\u0026gt;Name: Simo Ahava\u0026lt;/p\u0026gt; \u0026lt;/a\u0026gt; Sample trigger\n  Selector\nimg#simo_img, p#simo_name\nOutcome\nThe trigger fires if the click lands on EITHER \u0026lt;img id=\u0026quot;simo_img\u0026quot;.../\u0026gt; OR \u0026lt;p id=\u0026quot;simo_name\u0026quot;\u0026gt;...\u0026lt;/p\u0026gt;.\nelement element Matches the rightmost element when it is within the leftmost element in the DOM tree. The relationship does not have to be parent-child - it\u0026rsquo;s enough that the left element wraps the right element at some point.\nSample HTML structure\n\u0026lt;section id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;article\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/archives\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; Sample trigger\n  Selector\nsection#main h1\nOutcome\nThe trigger fires if the click lands on \u0026lt;h1\u0026gt;Hello world!\u0026lt;/h1\u0026gt; because one of its wrapping elements matches the first selector (section#main).\n* (wildcard) Matches any element. Extremely useful when used with the Click / All Elements trigger.\nSample HTML structure\n\u0026lt;div id=\u0026#34;navi\u0026#34;\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/home\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/products\u0026#34;\u0026gt;Products\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/contact\u0026#34;\u0026gt;Contact us\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv#navi, div#navi *\nOutcome\nThe trigger fires when the \u0026lt;div id=\u0026quot;navi\u0026quot;\u0026gt; element or any of its nested elements (\u0026lt;ul\u0026gt;, \u0026lt;li\u0026gt; and \u0026lt;a\u0026gt;) are clicked.\nThe selector is div#navi, div#navi *. Based on what you have learned above, this is actually two selectors, div#navi and div#navi *, and the trigger will fire if either is clicked.\nThe first selector matches clicks directly on the \u0026lt;div id=\u0026quot;navi\u0026quot;\u0026gt; element. The second selector matches clicks on any element that is wrapped by \u0026lt;div id=\u0026quot;navi\u0026quot;\u0026gt;. The whitespace is important here. If the second selector were div#navi \u0026gt; *, it would only match clicks on the \u0026lt;ul\u0026gt;, as it\u0026rsquo;s the only direct child of \u0026lt;div id=\u0026quot;navi\u0026quot;\u0026gt;.\nelement\u0026gt;element Matches the rightmost element when its direct parent is the element to the left.\nSample HTML structure\n\u0026lt;section id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;article\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/archives\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Hello world!\u0026lt;/h2\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;section id=\u0026#34;end\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Goodbye world!\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; Sample trigger\n  Selector\ndiv\u0026gt;h2\nOutcome\nThe trigger fires if the click lands on \u0026lt;h2\u0026gt;Goodbye world!\u0026lt;/h1\u0026gt; because its direct parent is \u0026lt;div\u0026gt;. It will not fire on \u0026lt;h2\u0026gt;Hello world!\u0026lt;/h2\u0026gt; because that element\u0026rsquo;s direct parent is \u0026lt;a\u0026gt;.\nelement+element Matches the rightmost element if it comes directly after the leftmost element. They must share the same parent.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/author.jpg\u0026#34; id=\u0026#34;author_image\u0026#34;/\u0026gt; \u0026lt;span\u0026gt;Author name\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\nimg#author_image+p\nOutcome\nThe trigger fires if the click lands on the \u0026lt;span\u0026gt;, because it is right after the \u0026lt;img id=\u0026quot;author_image\u0026quot;/\u0026gt;.\nelement1~element2 Matches the rightmost element if it is preceded by the leftmost element. They must share the same parent. In other words, it\u0026rsquo;s slightly less strict than the previous selector, in that the preceding element does not have to be immediately next to the targeted element.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/author.jpg\u0026#34; id=\u0026#34;author_image\u0026#34;/\u0026gt; \u0026lt;span\u0026gt;Author name\u0026lt;/span\u0026gt; \u0026lt;a href=\u0026#34;https://www.simoahava.com\u0026#34;\u0026gt;Author home page\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\nimg#author_image~a\nOutcome\nThe trigger fires if the click lands on the \u0026lt;a\u0026gt;, because it is preceded by \u0026lt;img id=\u0026quot;author_image\u0026quot;/\u0026gt;.\n[attribute] Matches if the element has the given attribute.\nSample HTML structure\n\u0026lt;div id=\u0026#34;main_content\u0026#34;\u0026gt; \u0026lt;div data-name=\u0026#34;gtm_example\u0026#34;\u0026gt; \u0026lt;span\u0026gt;GTM examples\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv[data-name]\u0026gt;span\nOutcome\nThe trigger fires if the click lands on the \u0026lt;span\u0026gt;, because its direct parent is a \u0026lt;div\u0026gt; with the data-name attribute.\n[attribute=value] Matches if the element has the given attribute with the exact value.\nSample HTML structure\n\u0026lt;div id=\u0026#34;main_content\u0026#34;\u0026gt; \u0026lt;div data-name=\u0026#34;gtm_example\u0026#34;\u0026gt; \u0026lt;span\u0026gt;GTM example\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div data-name=\u0026#34;second_gtm_example\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Another GTM example\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv[data-name=\u0026quot;second_gtm_example\u0026quot;]\u0026gt;span\nOutcome\nThe trigger fires if the click lands on the second \u0026lt;span\u0026gt;, because only it has a direct parent whose data-name attribute has the value second_gtm_example.\n[attribute^=value] Matches if the element has the given attribute whose value starts with the provided string.\nSample HTML structure\n\u0026lt;div id=\u0026#34;main_content\u0026#34;\u0026gt; \u0026lt;span id=\u0026#34;product_12345\u0026#34;\u0026gt; Product 12345 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;product_23456\u0026#34;\u0026gt; Product 23456 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;attachment_12345\u0026#34;\u0026gt; Attachment 12345 \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\nspan[id^=\u0026quot;product\u0026quot;]\nOutcome\nThe trigger fires if the click lands on either of the two \u0026lt;span\u0026gt; elements whose ID starts with product. It will not fire if the click lands on the third span, because that element ID starts with attachment.\n[attribute$=value] Matches if the element has the given attribute whose value ends with the provided string.\nSample HTML structure\n\u0026lt;div id=\u0026#34;main_content\u0026#34;\u0026gt; \u0026lt;span id=\u0026#34;product_12345\u0026#34;\u0026gt; Product 12345 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;product_23456\u0026#34;\u0026gt; Product 23456 \u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;attachment_12345\u0026#34;\u0026gt; Attachment 12345 \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\nspan[id$=\u0026quot;_12345\u0026quot;]\nOutcome\nThe trigger fires if the click lands on either the first \u0026lt;span\u0026gt; or the last \u0026lt;span\u0026gt;, since both elements\u0026rsquo; ID attributes end with _12345. It will not fire if the click lands on \u0026lt;span id=\u0026quot;product_23456\u0026quot;\u0026gt;, since that ID does not end with _12345.\n[attribute*=value] Matches if the element has the given attribute whose value contains the provided string.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;span id=\u0026#34;simo_ahava_profile\u0026#34;\u0026gt;Simo\u0026#39;s profile\u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;profile_benjamin_ahava\u0026#34;\u0026gt;Benjamin\u0026#39;s profile\u0026lt;/span\u0026gt; \u0026lt;span id=\u0026#34;derek_anderson\u0026#34;\u0026gt;Derek\u0026#39;s profile\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv#author \u0026gt; span[id*=\u0026quot;ahava\u0026quot;]\nOutcome\nThe trigger fires if the click lands on either of the first two \u0026lt;span\u0026gt; elements, since they have IDs that contain the string ahava. It will not fire for the third \u0026lt;span\u0026gt; because that element\u0026rsquo;s ID attribute does not contain the string ahava.\n:checked Matches if the given element is checked (radio buttons, checkboxes, and \u0026lt;option\u0026gt; elements within select menus).\nSample HTML structure\n\u0026lt;form id=\u0026#34;contact-us\u0026#34;\u0026gt; My name: \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34; /\u0026gt;\u0026lt;br /\u0026gt; I consent to everything \u0026lt;input type=\u0026#34;checkbox\u0026#34; name=\u0026#34;consent\u0026#34; /\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; Sample trigger\n  Selector\ninput[name=\u0026quot;consent\u0026quot;]:checked\nOutcome\nThe trigger fires if the click lands on the checkbox when it is checked (i.e. the user unchecks it).\n:first-child Matches if the given element is the first child of its parent.\nSample HTML structure\n\u0026lt;ul id=\u0026#34;main_navigation\u0026#34;\u0026gt; \u0026lt;li\u0026gt;Home\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/products\u0026#34;\u0026gt;Products\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/contact\u0026#34;\u0026gt;Contact\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; Sample trigger\n  Selector\nul#main_navigation \u0026gt; li:first-child\nOutcome\nThe trigger fires if the click lands on \u0026lt;li\u0026gt;Home\u0026lt;/li\u0026gt; since it is the first child element of its direct parent.\n:first-of-type Matches if the given element is the first element of its type (e.g. p, img, span) of its parent.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author_info\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/author\u0026#34;\u0026gt;Simo Ahava\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;Web Analytics Developer\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Ukulele and death metal enthusiast\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv#author_info \u0026gt; p:first-of-type\nOutcome\nThe trigger fires if the click lands on the \u0026lt;p\u0026gt;Web Analytics Developer\u0026lt;/p\u0026gt; since it is the first \u0026lt;p\u0026gt; under its parent.\n:last-child Matches if the given element is the last child element of its parent (the reverse of :first-child).\nSample HTML structure\n\u0026lt;ul id=\u0026#34;main_navigation\u0026#34;\u0026gt; \u0026lt;li\u0026gt;GTM\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Ukulele\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Death metal\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; Sample trigger\n  Selector\nul#main_navigation \u0026gt; li:last-child\nOutcome\nThe trigger fires if the click lands on \u0026lt;li\u0026gt;Death metal\u0026lt;/li\u0026gt; since it is the last child element of its parent.\n:last-of-type Matches if the given element is the last element of its type of its parent.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author_info\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/author\u0026#34;\u0026gt;Simo Ahava\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;Web Analytics Developer\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Ukulele and death metal enthusiast\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\ndiv#author_info p:last-of-type\nOutcome\nThe trigger fires if the click lands on \u0026lt;p\u0026gt;Ukulele and death metal enthusiast\u0026lt;/p\u0026gt; since it is the last \u0026lt;p\u0026gt; under its parent.\n:not(selector) Matches if the given element does not match the selector within the parentheses.\nSample HTML structure\n\u0026lt;div id=\u0026#34;author\u0026#34;\u0026gt; \u0026lt;p\u0026gt; \u0026lt;a href=\u0026#34;https://www.simoahava.com/\u0026#34;\u0026gt;Home page\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; \u0026lt;a href=\u0026#34;https://tagmanager.google.com/\u0026#34;\u0026gt;Favorite hobby\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Sample trigger\n  Selector\na:not([href*=\u0026quot;simoahava.com])\nOutcome\nThe trigger fires when the click lands on the second link (\u0026lt;a href=\u0026quot;https://tagmanager.google.com/\u0026quot;\u0026gt;Favorite hobby\u0026lt;/a\u0026gt;). This is because the CSS selector targets all link elements (\u0026lt;a\u0026gt;) that do not match the CSS selector [href*=\u0026quot;simoahava.com\u0026quot;]. In other words the link must not have a href attribute containing the string simoahava.com.\n:nth-child(n) Matches if the given element is the nth child of its parent, where n is the number passed in the parentheses.\nSample HTML structure\n\u0026lt;ul id=\u0026#34;main_navigation\u0026#34;\u0026gt; \u0026lt;li\u0026gt;GTM\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Ukulele\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Death metal\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; Sample trigger\n  Selector\nul#main_navigation \u0026gt; li:nth-child(2)\nOutcome\nThe trigger fires if the click lands on \u0026lt;li\u0026gt;Ukulele\u0026lt;/li\u0026gt; since it is the second child (nth-child(2)) of its parent.\nSummary I\u0026rsquo;ve said it many times, but I seriously think CSS selectors are high up there with JavaScript and regular expressions in the list of things you must learn if you want to master Google Tag Manager.\nMuch of browser-based web analytics, especially when tracking interactions, is rooted in the ability to track interactions with specific elements. Often, we\u0026rsquo;re recommended to use the id and class attributes these elements have, but it\u0026rsquo;s very common that they don\u0026rsquo;t actually have these attributes.\nIt\u0026rsquo;s in these situations that CSS selectors shine. They allow you to pinpoint every single and any element on any given page, since every element always has a unique position that can be targeted with some selector.\nJust remember my tip from earlier - always strive for a selector that is a simple as possible without compromising its ability to target the specific elements you want it to.\n"
},
{
	"uri": "https://www.simoahava.com/tags/css-selectors/",
	"title": "css selectors",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/trigger-groups/",
	"title": "trigger groups",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/trigger-groups-google-tag-manager/",
	"title": "Trigger Groups In Google Tag Manager",
	"tags": ["google tag manager", "triggers", "trigger groups"],
	"description": "Introducing the new Trigger Groups trigger feature in Google Tag Manager.",
	"content": "Trigger Group is the newest trigger type you can add to a tag in Google Tag Manager. It allows you to establish dependencies between multiple triggers, not firing the tag until every trigger in the group has fired at least once.\n  This establishes an interesting new paradigm in Google Tag Manager, because until now it wasn\u0026rsquo;t possible to create triggers that relied on earlier values of a given key (event in this case). With the Trigger Group, information about triggers that have fired on the page is persisted internally in GTM, and once the triggers configured in the Trigger Group have all signalled completion, the Trigger Group will fire any tag it\u0026rsquo;s attached to.\nHow to create a Trigger Group You\u0026rsquo;ll find the new Trigger Group within the trigger workflow. So, in the Google Tag Manager UI, select Triggers, then NEW, and finally click the \u0026ldquo;Choose a trigger type to begin setup\u0026rdquo; option.\n  In the overlay that opens, select the Trigger Group option.\n  Now you should see the configuration screen.\n  Trigger Groups have a very simple operational logic.\n  Click the Choose a trigger (or the blue plus) button in the middle of the configuration screen.\n  Choose one or more existing triggers from the overlay that opens.\n  Every single trigger you add to the Trigger Group will need to fire on the page as many times as it appears in the Trigger Group for the group itself to fire any tag to which it is attached.\nNote that you can add a trigger more than once. By adding a trigger more than once into the group, that particular trigger must fire as many times as it has been added to the group for the Trigger Group to work.\nThe order of triggers in the group is inconsequential. The Trigger Group will fire as soon as all the included triggers have fired once, regardless of order.\nOnce you\u0026rsquo;re done adding triggers, you can also add additional firing conditions by selecting Some Conditions from the \u0026ldquo;This trigger fires on\u0026rdquo; selection. Any condition you add here will need to pass in addition to the triggers of the group firing.\nWhen you\u0026rsquo;re done, you can add the Trigger Group to your tags. The Trigger Group works just like a regular trigger in that as soon as its conditions are met (i.e. all the triggers in the group have fired), the tag itself will fire.\nExample 1: Scroll depth and time spent Following the example of another recent article of mine, you can create a Trigger Group with a Scroll Depth trigger and a Timer trigger. The Trigger Group will not fire until both the Scroll Depth trigger and the Timer trigger have fired, meaning you can defer your tag from firing until the user has scrolled a sufficient amount and spent some time on the page.\nIn this example, I have a Scroll Depth trigger which fires at 50 percent scrolled, and a Timer trigger which fires at 30 seconds of dwell time on the page.\n  The Trigger Group combines the Timer and the Scroll Depth trigger into a decent engagement tracker.\nExample 2: Wait for the All Pages trigger to fire first This example should ring true if you want to establish a running order between two triggers (and, as a consequence, the tags that these triggers fire). A classic example is waiting for the Page View tag in Google Analytics to fire before any Event tag.\n Even though it\u0026rsquo;s a bit of a myth that collecting events before pageviews is somehow detrimental to analytics (GA is more than capable of stitching together a session where the entrance hit was not a pageview), the exercise is still illuminating.\n In this Trigger Group, we have the actual trigger we want to fire the tag with as well as the All Pages trigger. The idea being that the event trigger itself will do nothing until the All Pages trigger has also fired.\n  Remember that you can\u0026rsquo;t establish order in a Trigger Group. The Trigger Group will fire as soon as all the triggers listed within have fired - regardless of order. So the underlying assumption here is that the Event - nonIdle trigger will never fire before the All Pages trigger.\nIt might be more reasonable to use tag sequencing in most cases where you want to establish a sequence of items firing, but especially when you have a large amount of tags depending on a very similar dependency, it might be easiest to use a Trigger Group instead.\nExample 3: Consent given Another popular use case for Trigger Groups would be to make sure your other triggers don\u0026rsquo;t fire until the user has given consent or has opted in to your analytics and advertising tracking efforts.\nIt might not have the same elegance as Tag Sequencing or as having the event push the consent status into dataLayer (or writing it in a cookie), but it does let you fire tags only after a specific \u0026ldquo;consent granted\u0026rdquo; event has been pushed into dataLayer.\nIn any case, here\u0026rsquo;s what a Trigger Group would look like for a tag which I want to fire when the user logs in, but I also want to make sure the user has consented to them being tracked on my website:\n  Example 4: Form engagement Another example of engagement measurement (for which Trigger Groups are great), would be tracking if the user has interacted with a set amount of form fields before tagging them as being engaged with the form.\nFor this to work, you need a trigger that fires when the user changes the value of a form field. This is easy to do with a custom event listener, where the event name you are listening for is 'change':\nvar form = document.querySelector(\u0026#39;#someForm\u0026#39;); form.addEventListener(\u0026#39;change\u0026#39;, function(e) { window.dataLayer.push({ event: \u0026#39;formFieldChanged\u0026#39;, field: e.target }); });  And then you\u0026rsquo;d create a Custom Event trigger for the formFieldChanged event name. Once you\u0026rsquo;ve done that, creating a Trigger Group where the user must interact with form fields three times before the tag fires is simple:\n  Things to consider Here are some things to consider when working with Trigger Groups.\nTrigger Groups can\u0026rsquo;t be used as generic exceptions You can\u0026rsquo;t use a Trigger Group as a trigger exception, because the only thing that a Trigger Group can block from firing is itself.\nThis is a bit unfortunate, as it wouldn\u0026rsquo;t be too difficult to come up with use cases where you want to prevent the tag from firing if certain triggers have already fired.\nTrigger Groups will fire just once When a Trigger Group fires because all the triggers listed within have fired, the Trigger Group won\u0026rsquo;t fire again even if the triggers listed within fire again.\nThis might not be a big deal, but there are some repercussions. For example, if you have a Scroll Depth trigger that fires on the 25, 50, 75 and 100 percent depths, and then you combine this with a 30 second Timer trigger in a Trigger Group, the Trigger group will fire just once when both the Scroll Depth trigger and the Timer trigger have fired (at least once). Thus, it\u0026rsquo;s not possible to wait for the Trigger Group to fire again until the scroll depth threshold is a certain value, for example.\nThe Trigger Group doesn\u0026rsquo;t reset after it\u0026rsquo;s fired once.\nTrigger Groups don\u0026rsquo;t replace the grouped triggers This means that if you have a Trigger Group added to a tag, you might want to make sure you don\u0026rsquo;t inadvertently add the triggers within that group to the tag, too. In fact, the triggers you add to the group don\u0026rsquo;t need to be added to any tag at all - they can exist solely for the sake of the Trigger Group itself.\nThe dataLayer object is pretty bare The Trigger Group adds a new event into GTM\u0026rsquo;s default event dictionary: gtm.triggerGroup. The object that is pushed with this event is basically just the gtm.triggers key, matching the ID of the container coupled with the ID of the Trigger Group itself:\n  In other words, there\u0026rsquo;s no details about the triggers within the group in the object that is pushed into dataLayer.\nSummary Trigger Groups are a nice addition to the arsenal of Google Tag Manager. It\u0026rsquo;s certainly refreshing to have some extra logic into how triggers are added to a tag.\nWe\u0026rsquo;re still a long way from being able to establish complex logic between triggers (IF\u0026hellip;ELSE, NOT, etc.), but this is certainly a step in the right direction.\nI\u0026rsquo;m especially excited about the idea of persisting earlier values of Data Layer keys (in the form of knowing what triggers have fired before) on the page. Perhaps this idea will be extended further so that we can actually query the history of Data Layer on any given page. And, wild thought, maybe even persist this information across pages, so that a Trigger Group could fire based on multi-page conditions!\nLet me know if you have more examples where Trigger Groups might be useful!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/send-event-before-pageview-in-google-analytics/",
	"title": "Send Event Hits Before Pageview Hits In Google Analytics",
	"tags": ["google analytics", "session", "google tag manager"],
	"description": "This article explores what actually happens if you end up sending event hits before pageview hits when using Google Analytics.",
	"content": "One of the myths surrounding Google Analytics is that the first hit of a session should always be a pageview. It makes sense - sessions are initialized with a landing page, and thus need a page view to have one.\n  However, in this article I want to show you empirically how this myth is just that - a myth.\nThere is little discernible impact if the first hit of a session is an event, and GA is more than capable of stitching the first event together with the subsequent pageview into a session entity.\nThe three repercussions I could find were:\n  Entrances and Sessions not matching up with the Page dimension. See here for some details, too.\n  Behavior Flow report attributes the event page as the \u0026ldquo;Landing Page\u0026rdquo; and the page sent with the pageview as the \u0026ldquo;Starting Page\u0026rdquo;.\n  Goal Completion Location for an event goal set for the event in question shows (entrance) if the event was the first hit of the session.\n    I\u0026rsquo;ll explore these later on in the article.\nTest setup Here\u0026rsquo;s how I tested this. I created four test cases. Each pair of event-pageview hits was contained in its own session (using the sessionControl parameter).\nAll events were sent from page /, where as the pageview hits always had a different page parameter. This was mainly to see the impact of the event\u0026rsquo;s page parameter on the Goal Completion Location (there was none).\nI also created an Event goal which matched all the events sent with this test.\nHere are the test cases:\n  Non-interactive event as the 1st hit, pageview on /test-event-1 as the 2nd hit.\n  Interactive event as the 1st hit, pageview on /test-event-3 as the 2nd hit.\n  Pageview on /test-event-5 as the 1st hit, non-interactive event as the 2nd hit.\n  Pageview on /test-event-7 as the 1st hit, interactive event as the 2nd hit.\n  Here\u0026rsquo;s what a sample command looked like. This is for the first test case:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-XXXXXX\u0026#39;, {name: \u0026#39;test\u0026#39;}); ga(\u0026#39;test.send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;eventFirstHit\u0026#39;, \u0026#39;nonInteractive\u0026#39;, \u0026#39;noPath\u0026#39;, {nonInteraction: true, sessionControl: \u0026#39;start\u0026#39;, campaignSource: \u0026#39;test_event_1\u0026#39;, campaignMedium: \u0026#39;test_event_1\u0026#39;}); ga(\u0026#39;test.send\u0026#39;, \u0026#39;pageview\u0026#39;, {page: \u0026#39;/test-event-1\u0026#39;});  The first line creates the tracker.\nThe second line dispatches the event. You can ignore the noPath label and the campaignSource/Medium parameters - they were just used to control the test analysis in GA.\nThis is what the fourth test case code looked like:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-XXXXXX\u0026#39;, {name: \u0026#39;test\u0026#39;}); ga(\u0026#39;test.send\u0026#39;, \u0026#39;pageview\u0026#39;, {sessionControl: \u0026#39;start\u0026#39;, page: \u0026#39;/test-event-7\u0026#39;, campaignSource: \u0026#39;test_event_7\u0026#39;, campaignMedium: \u0026#39;test_event_7\u0026#39;}); ga(\u0026#39;test.send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;eventNotFirstHit\u0026#39;, \u0026#39;Interactive\u0026#39;, \u0026#39;noPath\u0026#39;);  Results Here are the results of the test.\nThe first thing to note is that not one of the test cases produced a session with Landing Page (not set). That\u0026rsquo;s a red herring I\u0026rsquo;ll discuss this briefly in the next chapter.\nTest case 1    Item Description     1st hit Non-interactive event on /   2nd hit Pageview on /test-event-1   Landing Page /test-event-1   Goal Completion Location (entrance)   Sessions metric /test-event-1: 0, /: 1   Entrances metric /test-event-1: 1, /: 0      Test case 2    Item Description     1st hit Interactive event on /   2nd hit Pageview on /test-event-3   Landing Page /test-event-3   Goal Completion Location (entrance)   Sessions metric /test-event-3: 0, /: 1   Entrances metric /test-event-3: 1, /: 0      Test case 3    Item Description     1st hit Pageview on /test-event-5   2nd hit Non-interactive event on /   Landing Page /test-event-5   Goal Completion Location /test-event-5   Sessions metric /test-event-5: 1   Entrances metric /test-event-5: 1      Test case 4    Item Description     1st hit Pageview on /test-event-7   2nd hit Interactive event on /   Landing Page /test-event-7   Goal Completion Location /test-event-7   Sessions metric /test-event-7: 1   Entrances metric /test-event-7: 1      Results overview As you can see, even if the event was sent as the first hit of a session, it didn\u0026rsquo;t cause the cosmos to implode. Google Analytics is able to build the session nevertheless, treating the event as the first hit of the session, and adding subsequent hits (other events, pageview) to the session normally.\nThe main impacts of sending an event as the first hit of a session are:\n  The Landing Page is associated with the first pageview of the session. The page sent with the event hit cannot be the Landing Page.\n  When querying against the Page dimension, the Entrance is always incremented for the page sent with the pageview, but the Sessions metric is incremented for the first hit - non-interactive or interactive.\n  Goal Completion Location will always be the URI of the most recent page view sent before the event goal was completed. If the event is the first hit of the session, there is no preceding pageview, so the goal completion happens at (entrance).\n  Landing page (not set) One red herring surrounding events and pageviews is that sending an event as the first hit of a session results in the Landing Page dimension showing (not set). This is something that Google Tag Assistant warns about.\n  Landing Page will be (not set) only if the session didn\u0026rsquo;t have ANY page views. Since the Landing Page is always the URI of the first pageview sent in the session, a session with no pageviews will have no Landing Page.\nSending an event as the first hit of a session does not automatically imply that the session will have no pageviews. Claiming otherwise is a non sequitur.\nNote that if you have any events firing on your page, you will always risk having sessions with only events. All it takes is for the user to take a break of 30 minutes after the pageview was dispatched, and then scroll down the page, firing a scroll event you\u0026rsquo;ve configured.\nThis will start its own session (since the previous one expired after the 30 minute timeout was reached). If this scroll event isn\u0026rsquo;t followed by a pageview within the session timeout period, the event will be contained in its own session with landing page (not set).\nBut, if the event is followed by a pageview before the session timeout expires, the landing page will be the path of the pageview. Remember that.\nSession with only non-interactive events Similarly, a session with only non-interactive events will have a Landing Page of (not set).\nHowever, unlike sessions with just interactive events (or a mix of interactive and non-interactive), sessions with just non-interactive events will not increment the Sessions metric, as that requires an interactive hit. They do increment the Users metric, which is why you\u0026rsquo;ll sometimes see more Users than Sessions.\n  But still, this is unrelated to having events fire before the pageview when a session starts.\nSummary As my tests above show, it\u0026rsquo;s OK to have an event precede the pageview hit when a session starts. You don\u0026rsquo;t have to rush to tag sequencing in Google Tag Manager to fix this \u0026ldquo;problem\u0026rdquo; - the implications are not severe at all.\nSo why is this still being warned about? Well, apart from misunderstanding how sessions work, there is a seed of truth in being cautious if you have events that can fire before the pageview on any given page. It\u0026rsquo;s potentially hazardous if you set campaign parameters with the pageview hit that are missing from the event hit.\nSince the pageview is the nexus of Google Analytics\u0026rsquo; session, it follows that most care is put into making sure the pageview hit has all the necessary session, campaign, and cookie modifications. This lopsided focus can be problematic in Google Tag Manager, where each hit has its own unique tracker object, and thus the hits don\u0026rsquo;t inherit anything from a common tracker, as with analytics.js and gtag.js.\nIn other words, you might modify your pageview hit to include campaign-defining fields such as campaignSource and campaignMedium, or you might modify its Document Location field to have UTM parameters manually appended to the URL string. Or, you might even rewrite the clientId field in the pageview hit alone.\nThe key thing here is consistency. Any modifications to campaign- or session-parameters in individual hits or tags should be applied to all hits or tags that can fire on the same page.\nSince an event hit is capable of creating and populating a session on its own, it\u0026rsquo;s vital to have that event hit mirror the campaign settings of the pageview hit, and vice versa.\nIf you do that, you\u0026rsquo;ll avoid having rogue sessions starting simply because an event hit had totally different campaign settings from the pageview hit. In Google Tag Manager, the best way to mitigate this is to use the Google Analytics Settings variable.\nDo you think I missed something crucial in this test? Please let me know in the comments.\n"
},
{
	"uri": "https://www.simoahava.com/tags/session/",
	"title": "session",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/create-utility-variables-returning-functions/",
	"title": "#GTMTips: Create Utility Variables By Returning Functions",
	"tags": ["google tag manager", "gtmtips", "custom javascript"],
	"description": "You can use Google Tag Manager&#39;s Custom JavaScript variables to create your own utilities. You do this by having these variables actually returning the function you want to invoke with parameters.",
	"content": "In Google Tag Manager, the Custom JavaScript variable is an anonymous function with a return statement. It does not take any parameters, and by default it\u0026rsquo;s impossible to pass any parameters to it, because the Custom JS variable is simply resolved to whatever value the function returns. If it returns a number, for example, passing a parameter to it would make no sense and would result in a TypeError since the variable resolves to a number, not a function.\nHowever, you can actually have the variable return a function. You can use this to create your own utilities, which take parameters and, in turn, return some new value based on the calculations done with the variables.\nThis isn\u0026rsquo;t a novel discovery - variables returning functions are at the core of how things like hitCallback and customTask work. In this article, I\u0026rsquo;ll explain a bit further what you can do with these utility variables.\nTip 97: Create utilities with the Custom JavaScript variable   A function which returns a function has plenty of use in JavaScript. It\u0026rsquo;s sometimes called a closure because it has access to the lexical scope of the wrapping function even after that function has ended execution.\nIn Google Tag Manager, using the function as a closure isn\u0026rsquo;t necessarily the biggest perk, because Custom JavaScript variables are designed to be ephemeral - simply processing things in scope (e.g. other variables), and then returning the result of this processing.\nHowever, when you return a function which accepts parameters in a Custom JavaScript variable, you can create a utility that can be invoked from Custom HTML tags and other Custom JavaScript variables without having to always rewrite the same code in all the places you need to the utility.\nFor example, let\u0026rsquo;s say you want to create a browser cookie in a Custom HTML tag. You have some options:\n\u0026lt;script\u0026gt; // Option 1: Create a global method  window._setCookie = function(name, value, ms, path, domain) { if (!name || !value) { return; } var d; var cpath = path ? \u0026#39;; path=\u0026#39; + path : \u0026#39;\u0026#39;; var cdomain = domain ? \u0026#39;; domain=\u0026#39; + domain : \u0026#39;\u0026#39;; var expires = \u0026#39;\u0026#39;; if (ms) { d = new Date(); d.setTime(d.getTime() + ms); expires = \u0026#39;; expires=\u0026#39; + d.toUTCString(); } document.cookie = name + \u0026#34;=\u0026#34; + value + expires + cpath + cdomain; }; window._setCookie(\u0026#39;someCookieName\u0026#39;, \u0026#39;someCookieValue\u0026#39;); // Option 2: Directly call the document.cookie API  document.cookie = \u0026#39;someCookieName=someCookieValue\u0026#39;; // Option 3: Use a Custom JavaScript variable  {{Set Cookie}}(\u0026#39;someCookieName=someCookieValue\u0026#39;); \u0026lt;/script\u0026gt; Option 1 creates a utility method, too, but it adds it to the global namespace. Yes, this has the added benefit of being available to all your Custom HTML tags and Custom JavaScript variables, and actually any other script running on your site, even outside GTM. Assuming these other tags and scripts run after the _setCookie global method has been created with this tag, that is.\nThe downside is that you\u0026rsquo;re polluting the global namespace. You\u0026rsquo;re reserving a key that might be overwritten or accessed by any other JavaScript running on the site. If only scripts within GTM need this method, you\u0026rsquo;d be better off with some other solution.\nOption 2 is simple in that you don\u0026rsquo;t need a function at all, you\u0026rsquo;re directly writing the cookie. With the example above it\u0026rsquo;s really the simplest way to go about it. However, as soon as you need to do some extra processing, such as determining on which domain or path to write the cookie or what its expiration should be, you\u0026rsquo;ll add lines and lines of code, and all of these lines need to be rewritten in any other tag where you also want to write a cookie.\nOption 3 is what this article is about. Instead of adding the _setCookie method into the global scope of the page, you can use Google Tag Manager\u0026rsquo;s Custom JavaScript variables to handle the logic for you. The variable would look like this:\nfunction() { return function(name, value, ms, path, domain) { if (!name || !value) { return; } var d; var cpath = path ? \u0026#39;; path=\u0026#39; + path : \u0026#39;\u0026#39;; var cdomain = domain ? \u0026#39;; domain=\u0026#39; + domain : \u0026#39;\u0026#39;; var expires = \u0026#39;\u0026#39;; if (ms) { d = new Date(); d.setTime(d.getTime() + ms); expires = \u0026#39;; expires=\u0026#39; + d.toUTCString(); } document.cookie = name + \u0026#34;=\u0026#34; + value + expires + cpath + cdomain; } }  As you can see, it\u0026rsquo;s almost the same as Option 1, except instead of assigning the function to a global variable, you\u0026rsquo;re returning it in the Custom JavaScript variable. And because you return the function, you can then invoke it with the parameters name, value, ms, path, domain, and have the function handle writing the cookie for you.\nAssuming you\u0026rsquo;ll name that Custom JavaScript variable as Set cookie variable, this is how you would invoke it from a Custom HTML tag or a Custom JavaScript variable:\n{{Set cookie variable}}(\u0026#39;cookieName\u0026#39;, \u0026#39;cookieValue\u0026#39;, 100000, \u0026#39;/\u0026#39;, \u0026#39;simoahava.com\u0026#39;);  This is just one example. You could also\u0026hellip;\n  Create a SHA-256 hashing function (thanks Johan Terpstra for the link to the source code), which encrypts any string passed to it.\n  Create simple utilities such as string reversal, array equality checks, and converters.\n  Write callbacks for custom event listeners.\n  In summary, a Custom JavaScript variable which returns a function is useful if you want to\u0026hellip;\n  Avoid polluting the global namespace.\n  Avoid rewriting the same code over and over again. and/or c) having a number of similar variables doing almost the same thing.\n  Avoid having a number of similar variables doing almost the same thing.\n  The last point is especially powerful, as instead of having five different Custom JavaScript variables, you could use a single Custom JavaScript variable, returning a function which takes a parameter, and this function would then return one of five different outcomes, depending on the parameter.\nJust remember that you can only run JavaScript in two contexts within GTM: Custom HTML tags and Custom JavaScript variables. So you can\u0026rsquo;t add parameters to a Custom JavaScript variable in regular tag, trigger, or variable fields.\nDo you have any cool utility functions you\u0026rsquo;d like to share with the readers?\n"
},
{
	"uri": "https://www.simoahava.com/tags/analytics/",
	"title": "analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/itp-2-1-and-web-analytics/",
	"title": "ITP 2.1 And Web Analytics",
	"tags": ["web development", "analytics", "privacy", "itp"],
	"description": "An overview of Safari&#39;s Intelligent Tracking Prevention 2.1 release and the potential impact it has on web analytics going forward.",
	"content": " Updated 1 October 2019 With ITP 2.3 it looks like Safari is reducing the usefulness of localStorage as well, so using that as an alternative fix to persistence issues should not be considered future-proof. this solution should not be considered future-proof.\n  Updated 12 March 2019 with some minor clarifications..\n On 21st February 2019, WebKit announced the release of the latest iteration of Safari\u0026rsquo;s Intelligent Tracking Prevention (ITP), known as ITP 2.1. For a while now, Safari has been targeting cross-site tracking with ITP, first starting with cookies in third-party contexts, then tightening the noose after a number of workarounds emerged, and finally with the latest iteration targeting cookies that were moved from a third-party context to a first-party context.\n  ITP 2.1 has one specific feature that will make us web analytics folks tremble in our boots:\n With ITP 2.1, all persistent client-side cookies, i.e. persistent cookies created through document.cookie, are capped to a seven day expiry.\n From the WebKit blog, emphasis mine.\nThis means that any JavaScript library wanting to store a cookie in the web browser will have that cookie capped to a seven day lifetime. After seven days since cookie creation, the cookie will expire and will be removed from the browser.\nIn this article, I want to explore the implications this has on web analytics, and what we can do to avoid losing the integrity of our data.\nBrief introduction to ITP In the following chapters, I will use these two terms:\n  Third-party context means that a resource is requested from a domain external from the one the user is currently on (i.e. they don\u0026rsquo;t share the same eTLD + 1), and this request tries to access cookies set on this external domain. Since the domain differs from the one the user is on (has to be a different root domain, basically), the interaction with this external domain happens in a third-party context.\n  First-party context means that a resource is requested from the current (parent) domain (eTLD + 1) the user is on. Since the user is in the same domain environment, any requests made happen in a first-party context.\n  ITP 1.0 When Safari first introduced Intelligent Tracking Prevention, its mission was fairly clear. Safari wanted to prevent domains classified as having tracking capabilities from tracking users across different sites using third-party cookies.\nA classic example is how advertising technology services might build an audience profile of their users by observing on which sites the user visits. This is done by having those sites call the AdTech domain with a pixel request or something similar, and a third-party cookie stored on the AdTech domain will thus be able to build a profile of the user based on the sites where the pixel was requested on.\n  ITP made this more difficult by requiring that users actually interact with the third-party domain in a first-party context in order for the domain to be allowed to harvest their data in a third-party context. If there was no meaningful interaction such as loading the page and clicking on a button, a machine learning algorithm would classify the domain as having cross-site tracking capabilities, and as a result partition the cookies on that domain, preventing them from being used for cross-site tracking.\nRemember: ITP\u0026rsquo;s main modus operandi is preventing cross-site tracking. It\u0026rsquo;s Intelligent Tracking Prevention, not Intelligent Cookie Prevention.\nNaturally, for AdTech this is awkward. The whole point of pixel tracking is that it\u0026rsquo;s transparent and unobtrusive. What would you think if you were suddenly redirected to e.g. doubleclick.net and asked if it\u0026rsquo;s ok that Google continues to build an audience profile out of your web browsing behavior?\n From https://webkit.org/blog/7675/intelligent-tracking-prevention/  But, third-party cookies can also be used for non-tracking purposes, such as maintaining a single sign-on (SSO) session. This is why the original ITP introduced a 24-hour grace period during which time the SSO cookie could be used in a third-party context. After that, the cookies should be stored in a first-party context or, preferably, by moving to HTTP cookies (more on this later) to avoid these issues altogether.\nThe cookies would be partitioned for 30 days. This meant that cookies used for tracking purposes get a unique storage double-keyed to the domain requesting the cookie (the first-party domain), and the domain setting the cookie (third-party domain). So, for example, a third-party cookie set by google.com while on simoahava.com would only be accessible when browsing simoahava.com. If the user went to younggoodmanahava.com, a separate cookie store would be created for that origin.\nPartitioning the cookies this way meant that third-party cookies could still be used for things like login state management. Since the partitions are unique to each origin combination, cross-site tracking would effectively be neutered.\nIf the user visited the tracking domain within 30 days of the last interaction, both the 24-hour grace period and the 30-day partition timer would be reset.\nIf the 30-day limit expired without interaction with the tracking domain, all cookies would be purged from the tracking domain.\nITP 1.0 wasn\u0026rsquo;t very impactful for us Google Analytics users. GA works in a first-party context, so ITP\u0026rsquo;s updates did not concern us.\nITP 1.1 and Storage Access API With ITP 1.1 and the Storage Access API, ITP made some concessions to third-party services serving embedded content, such as social logins or video services. It would be weird to have the user visit these services in a first-party context, because the whole purpose of embedding content is to do it smoothly while staying on the same site. The Storage Access API was the solution, where embedded content would be given access to the cookies stored on the third-party domain as long as the embed followed certain rules:\n From https://webkit.org/blog/8124/introducing-storage-access-api/  The Storage Access API did not prompt the user for anything - this was the major concession from WebKit.\nSince the Storage Access API allowed access to the third-party domain\u0026rsquo;s own, non-partitioned cookies, the partitioned cookies preserved for things like login state management were demoted to session cookies, so they would get purged once the browser was closed. This meant that the Storage Access API was the main way to access persistent data in a third-party context.\nStill no issues with first-party web analytics, phew!\nITP 2.0 Mid-2018, WebKit introduced ITP 2.0.\nITP 2.0 removed the 24-hour grace period altogether. Now cookies would be partitioned immediately after creation, if the domain was classified as having cross-site tracking capabilities.\n From https://webkit.org/blog/8311/intelligent-tracking-prevention-2-0/  Another major thing ITP 2.0 did was make Storage Access API prompt-based. Instead of allowing embedded content to access cookies set in a third-party context by virtue of following the rules established in the previous chapter, Safari would now explicitly prompt the user for access to the third-party context. If the user gave access, the consent would persist and the 30-day timeline would be refreshed.\nIn other words, the Storage Access API was now the main way for domains classified as having cross-site tracking capabilities to access their cookies when embedded or requested in a third-party context.\nFirst-party web analytics is still safe. But not for long.\nITP 2.1 ITP 2.1 was announced on February 21st, 2019, and it will come to effect as soon as iOS 12.2 and Safari 12.1 come out of beta.\n Update 8 March 2019: It looks like features that are now bundled as ITP 2.1 have been quitely rolling out since the beginning of this year already.\n ITP 2.1 removes partitioned cookies altogether. Now if a domain classified as having cross-site tracking capabilities needs to have access to its cookies in a third-party context, the Storage Access API must be used, even if used for things like login state management. The main purpose of this change is to reduce the amount of memory overhead that partitioned (session) cookies introduce on any given site.\nBut by far the biggest change in ITP 2.1, one that has direct consequences on first-party web analytics are the new measures placed on first-party cookies set with client-side JavaScript.\nNo longer is ITP just attacking AdTech companies who rely on cross-site tracking to build audience profiles. Now cookies set with document.cookie will also be targeted to prevent them from being harvested on different subdomains than the one on which they were set.\n  As in the above example, any one of the subdomains of simoahava.com can write a first-party cookie on that root domain, after which any subdomain of simoahava.com can access that cookie. This is nothing new - this is how cookies set on a root domain have always worked.\nHowever, ITP now targets cookies set with JavaScript\u0026rsquo;s document.cookie because it has become apparent that some vendors (e.g. one whose name rhymes with lacebook) have begun repurposing first-party cookies to help with their cross-site tracking intentions. They are also taking advantage of the scenario depicted in the image above. Even if you limited these vendors\u0026rsquo; pixels to fire only on a specific subdomain, they could still access cookies on higher-level domain names.\nThe impact on web analytics is brutal, depending of course on Safari\u0026rsquo;s share of your traffic. Take the following example:\n  Day 1: User visits www.simoahava.com, the _ga cookie is written on simoahava.com. It is set at a 7-day expiry (rather than the 2 years that analytics.js defaults to).\n  Day 3: User visits blog.simoahava.com. The _ga cookie is found on simoahava.com, so its value is available to blog.simoahava.com, and the 7-day expiry is reset.\n  Day 13: User visits www.simoahava.com. The _ga cookie has expired, so a new Client ID is generated in a new _ga cookie, and the visitor is treated as a new user in Google Analytics.\n  Google Analytics uses document.cookie to set the cookie in the browser. It really has no other choice. It\u0026rsquo;s a vendored JavaScript library, downloaded from Google\u0026rsquo;s servers, so it has no capability of setting e.g. HTTP cookies on simoahava.com, since they would be set in a third-party context.\nFunnily enough, ITP 2.1 removes support for the Do Not Track signal in Safari, denoting the end to this miserable experiment in WebKit. Had more sites respected DNT when determining should visitors be tracked or not, perhaps we wouldn\u0026rsquo;t have seen ITP 2.1 in its current shape.\nBut because sites can\u0026rsquo;t be trusted to handle privacy and tracking prevention on their own, Safari must now take the reins and do it for them.\nTechnical implications and solutions First of all, ITP 2.1 really only impacts browser cookies set with document.cookie. It will not impact other DOM storage (such as localStorage), because those are same-origin only, so lacking the capability of working across sub-domains. Similarly, it will not impact cookies set with HTTP responses, i.e. using the Set-Cookie header in the HTTP response.\nHTTP cookies require a server-side script (or an edge cache / serverless solution) to modify the HTTP response, so it\u0026rsquo;s a deliberate decision from the site owner to set the cookies in the first-party context rather than a JavaScript library downloaded from a CDN being able to, at whim, set and get any first-party cookies it wants to. I suppose this is why they are not (yet) impacted by ITP.\nlocalStorage for same-domain browsing  Update 1 October 2019: With ITP 2.3, localStorage should not be considered a viable solution for persisting client-side state anymore.\n Many suggested using localStorage to persist these anonymous identifiers used by Google Analytics, for example. Instead of dropping a cookie named _ga, use window.localStorage.setItem() instead.\nI was excited about this option, and even wrote an article that describes this possibility in detail.\nHowever, the main issue is that localStorage is same-origin only. The storage in www.simoahava.com will not be available on blog.simoahava.com.\nThus the only thing that localStorage actually solves is not having the 7-day expiration cap on data stored on a single domain. Since all my web traffic happens on www.simoahava.com, localStorage is a good option for me, since it persists the Client ID nicely (though with some caveats nevertheless).\nlocalStorage for cross-subdomain browsing  Thanks to Eike Pierstorff and Fujii Hironori for suggesting this solution.\n An alternative option to same-domain localStorage is to create a store on a page in one of your subdomains, and then load that page in an iframe element, utilizing the postMessage API to get and set persistent cookie values on your site.\n  Since the localStorage store shares the same parent domain eTLD + 1 with the pages making the requests through the iframe, the interaction happens in a first-party context, and the localStorage store can be accessed without the Storage Access API. Also, in third-party contexts, localStorage is transient in Safari, meaning it is purged after the browser is closed. In first-party context this is not the case.\nThe process would look like this.\n  The page wanting to access the store would load the tracker page in an iframe, and send a message to it requesting a stored Client ID.\n  The iframe checks if it has the Client ID stored. If it does, it returns it in a message back to the origin page. If it doesn\u0026rsquo;t, it returns a string indicating this.\n  On the origin page, a listener listens for the response. If the response has a Client ID, that is used in the trackers running on the page. If the response has a \u0026ldquo;null\u0026rdquo; indicator, the origin page builds the Client ID, and finally sends it with another message back into the iframe for storage.\n  Here\u0026rsquo;s what the parent page script would look like:\n// Create iframe var el = document.createElement(\u0026#39;iframe\u0026#39;); el.src = \u0026#39;https://www.maindomain.com/tracker.html\u0026#39;; el.setAttribute(\u0026#39;style\u0026#39;, \u0026#39;width:0;height:0;border:0;border:none\u0026#39;); document.body.appendChild(el); // Add listener to get the stored Client ID window.addEventListener(\u0026#39;message\u0026#39;, function(e) { if (e.origin !== \u0026#39;https://www.maindomain.com\u0026#39;) { return; } if (e.data === \u0026#39;null\u0026#39;) { generateTracker(null); } else { generateTracker(e.data); } }); // Send request to get the Client ID el.contentWindow.postMessage(\u0026#39;get\u0026#39;, \u0026#39;https://www.maindomain.com\u0026#39;); // Send message to set the Client ID after the tracker has generated one function trackerGeneratedCallback(clientId) { el.contentWindow.postMessage(clientId, \u0026#39;https://www.maindomain.com\u0026#39;); }  Here, the iframe is generated, after which a message is sent to the iframe to get the Client ID. A listener on the page then waits for the iframe to respond. If the iframe responds with a 'null' message, a new tracker is generated (generateTracker) with its default Client ID generation mechanism.\nIf the iframe responds with the stored Client ID, then a new tracker is generated with this stored Client ID instead.\nIf the tracker did generate a new Client ID, then the trackerGeneratedCallback is called afterwards, and this sends the Client ID to the tracker page for storage.\nAnd this is what the iframe page would look like:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; content=\u0026#34;noindex,nofollow\u0026#34;\u0026gt; \u0026lt;script\u0026gt; // Check the request comes from *.maindomain.com  var hostNameRegex = /^https:\\/\\/([^.]+\\.)maindomain\\.com/; function processMessage(event) { if (!hostnameRegex.test(event.origin)) { return; } // If request is to get the clientId, send it as a message back to the source page  // or send \u0026#39;null\u0026#39; if no Client ID is found.  if (event.data === \u0026#39;get\u0026#39;) { event.source.postMessage(window.localStorage.getItem(\u0026#39;ga_client_id\u0026#39;) || \u0026#39;null\u0026#39;, event.origin); return; } // Otherwise, set the Client ID in localStorage using the message content  window.localStorage.setItem(\u0026#39;ga_client_id\u0026#39;, event.data); return, } // Add a listener to listen for postMessage messages  window.addEventListener(\u0026#39;message\u0026#39;, processMessage); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This way you could persist any client-side cookie values in the localStorage store of the domain where the iframe rests. This example is just for an arbitrary Client ID, but you could extend it for any value passed in the message.\nJust remember the idiosyncracies of each platform. With Google Analytics, for example, additional measures need to be taken to handle cross-domain tracking, cookie expiration, etc.\nSet-Cookie headers in a server-side script  Update 1 October 2019: This is definitely the most effective way to persist client-side state. See this article for inspiration.\n Instead of setting the _ga cookie with document.cookie, you could instead set it with the HTTP response. For example, if running a node.js Express server, you could have the following lines in your code:\napp.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { // Check for existing cookie and use that if found  let ga = req.cookies[\u0026#39;_ga\u0026#39;]; // Check for linker and use that if valid  if (req.query[\u0026#39;_ga\u0026#39;]) { ga = getClientIdFromLinker(req.query[\u0026#39;_ga\u0026#39;]) || ga; } // Otherwise generate a new Client ID  if (!ga) { ga = generateGAClientId(); } res.cookie(\u0026#39;_ga\u0026#39;, ga, {domain: \u0026#39;simoahava.com\u0026#39;, path: \u0026#39;/\u0026#39;, secure: true, expires: new Date(Date.now() + 1000*60*60*24*365*2), }); res.render(\u0026#39;index\u0026#39;); });  In this example, when the home page is requested from the server, its cookies are parsed. If the _ga cookie is found, its value is stored as a local variable. Then, if the request also has the cross-domain linker parameter, the parameter validity is verified (using e.g. my Gist) and if it\u0026rsquo;s a valid parameter, the local variable is overwritten with the Client ID from the linker.\n NOTE! The allowLinker script builds the fingerprint using, among others, the browser-based APIs window.navigator.plugins and window.navigator.language. As these are not available in the HTTP headers, you\u0026rsquo;d need to pass these to the server with the payload of the request you\u0026rsquo;d use to route the cookies, and then modify the allowLinker script to use these payload parameters rather than the window.navigator APIs. Alternatively, you could build your own cross-domain linker solution.\n Finally, if the _ga cookie didn\u0026rsquo;t exist and if the cross-domain linker was missing or broken, a new Client ID is generated.\nIn the HTTP response, the _ga cookie is set with this value, and its expiration is reset to two years.\nThis type of server-side setup should be trivial to do in almost any web server environment (PHP/Apache, React, Node.js, IIS, etc.). But you do need access to the web server and its request handlers to be able to do this.\nSet-Cookie headers in an edge cache If you\u0026rsquo;re using a service such as Cloudflare, which caches your server content on \u0026ldquo;the edge\u0026rdquo;, you might also be able to run JavaScript to intercept and handle the HTTP requests between the browser and Cloudflare. On Cloudflare, this technology is called Cloudflare Workers. Amazon has a similar solution called Lambda@Edge.\nThe idea is that when the HTTP request for content comes in, a script running in the edge \u0026ldquo;rewrites\u0026rdquo; any cookies in the HTTP header with Set-Cookie, thus avoiding ITP 2.1 limitations.\nDustin Recko tackled this in a recent article on ITP 2.1.\nSince many sites are already leveraging Cloudflare or Amazon\u0026rsquo;s Cloudfront, this would be a fairly lightweight way to handle the cookie routing.\nShared web service referenced with a CNAME record  Thanks to Lars Gundersen for nudging me about this idea.\n This is super interesting. Apparently, it\u0026rsquo;s what Adobe has already been doing for a while.\nThe logic is similar to the server-side cookie router and/or the localStorage store described above. But instead of hosting the content on your own domain, you would create the endpoint in a virtual machine running in the Google Cloud, for example, and then create a CNAME record in your DNS to point tracker.mydomain.com to that virtual machine.\nThen, when a page is loaded, the first thing it does is call this endpoint (tracker.mydomain.com). The endpoint would grab the cookies from the HTTP headers and use Set-Cookie in the response to write the cookies on mydomain.com, thus avoiding the ITP 2.1 ban on client-side cookies.\nAlternatively, you could have tracker.html running in the service, and you\u0026rsquo;d load tracker.mydomain.com/tracker.html in an iframe, and use that as the localStorage store.\nNaturally, this incurs some extra costs, because you would have to route a lot of calls to this cloud endpoint to make sure all the relevant cookies get their expiration extended. This is why, I guess, Adobe offers it as a service.\nI\u0026rsquo;d be curious to see if this is something Google is looking at, too. Seems like it would be a somewhat elegant way to handle the problem with the _ga cookie. They could, for example, have analytics.js fetched from this CNAME redirected domain, but in addition to serving the library, it would also write the _ga cookie in a Set-Cookie response.\nI verified with John Wilander (a WebKit engineer working on ITP) that this should be OK:\nYou’re asking two questions. Let me respond to them separately:\n1) Same eTLD+1 is same-site, i.e. same “party.” ITP doesn’t look at subdomains.\n\u0026mdash; John Wilander (@johnwilander) March 2, 2019  The problem here is that it adds quite a bit of overhead to the web service providing the response. For one, it would need to do the calculations for the _ga ID, but also it would need to process the request and customize the response. It\u0026rsquo;s possible that even if Google did do this, they wouldn\u0026rsquo;t offer it as part of the free Google Analytics service.\nReverse proxy to third party service  Thanks to Plamen Arnaudov for this suggestion.\n A reverse proxy stands in front of web servers, masking their existence when the user requests for resources from a site. For example, a reverse proxy could forward a request for content to https://www.google.com/, but show https://www.simoahava.com/search-page in the URL.\nIt\u0026rsquo;s thus conceptually similar to the CNAME redirect, where the user is exploring content in their own domain namespace but the content is delivered from another origin.\n From https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/  With the reverse proxy, a service like Google Analytics should need to modify some endpoint to either rewrite the _ga cookie in the request header with a Set-Cookie response, or create the cookie upon the request and serve it with the HTTP header. It might make sense, for example, to have the request for analytics.js respond with the Set-Cookie header.\nWe have the same limitations here as with the CNAME option above. Google would need to process the request before returning the header. Considering how many millions of requests to their services are made daily, any type of extra processing they\u0026rsquo;d need to do would reflect in costs.\nThe main difference between this and the CNAME option is that a reverse proxy requires the site owners and developers to add the server-side logic for the proxy, whereas CNAME requires just a DNS record change.\nSetting up a reverse proxy isn\u0026rsquo;t complicated at all - with a Node.js server, you could do it with http-proxy-middleware in just a couple of lines:\nconst proxy = require(\u0026#39;http-proxy-middleware\u0026#39;); ... app.get(\u0026#39;/analytics.js\u0026#39;, proxy({ target: \u0026#39;https://www.google-analytics.com/\u0026#39;, changeOrigin: true });  This would essentially perform the GET request for the analytics.js library on Google\u0026rsquo;s domain, but serve its content under your domain\u0026rsquo;s /analytics.js URL. If Google plays along, it could serve as a valid solution especially in cases where you can\u0026rsquo;t add any more CNAME records to your DNS.\nServer-side analytics There have been an array of suggestions using terms like server-side analytics, but I think these have misunderstood what ITP 2.1 does.\nIf there is no authentication against a backend, the user\u0026rsquo;s browser is identified with an anonymous identifier set in a cookie. No matter what type of analytics server is used - third-party or first-party - it\u0026rsquo;s this browser cookie that tells the service the user is the same as the one who visited some other page 5 minutes ago.\nChanging the endpoint from www.google-analytics.com/collect to proxy.simoahava.com/collect, or changing from www.google-analytics.com/collect to snowplow.simoahava.com/track doesn\u0026rsquo;t change this dynamic. There\u0026rsquo;s no way to really align requests sent from a browser with requests sent by the same browser without having this identifier binding those two together somehow.\nAnd if that cookie is set with document.cookie, it will fall under the 7-day expiration schedule, meaning two requests spread more than 7 days apart will not be able to enjoy the same cookie value any more.\nThe only way any server-side solution would work is if cookies were set with HTTP requests instead of with document.cookie. At this point, it might be easier to setup a cookie router, a CNAME redirect, or with a reverse proxy.\nFinal thoughts I have many thoughts on this topic. Many, indeed. So let me list them here:\nThis is a good thing\u0026hellip; I think anything that questions the validity of browser cookies is a welcome disruption.\nIt\u0026rsquo;s not just those set with document.cookie. An overwhelming amount of sensitive information is still stored in cookies that do not use the HttpOnly and Secure attributes. See this piece by Mike West from Google if you want to be depressed (and there\u0026rsquo;s some cool suggestions for going forward, too).\nAnything that makes life even more difficult for AdTech gets a solid thumbs up from me. Turning third-party data leeching into a consent-based prompt via Storage Access API is a great way to give users the reins.\nFor sites with authentication, this makes it even more prudent to incentivize a login. If a user logs in to the service, no cookies are even needed, since a persistent User ID could be used as the user\u0026rsquo;s identifier sent to analytics platforms (as long as the user\u0026rsquo;s consent for this is requested and as long as the tracking only happens when they\u0026rsquo;re logged in).\n\u0026hellip;but perhaps the baby was thrown out with the bathwater I can\u0026rsquo;t help thinking that benevolent first-party analytics gets hit with collateral damage here.\nI had some exchanges in Twitter with the WebKit engineer working on ITP (John Wilander), and this is one of the things he responded to me with:\nOnly document.cookie cookies are affected by the 7-day cap in ITP 2.1. Cross-site tracking is bad for people and for the web. Therefore, ITP will evolve to protect users from being tracked.\n\u0026mdash; John Wilander (@johnwilander) February 27, 2019  I totally agree with cross-site tracking being generally bad. But there are ways to do web analytics without malicious intent. It would have made more sense to prevent cookies from being accessed outside the domain they were set on, thus forcing sites to append linker parameters to internal links, but expiring all document.cookie setters with a 7-day cap is pretty drastic.\nHaving Google Analytics running on your site does not make you a bad person - you are using it to optimize the performance and usability of your site, and for creating better landing pages and marketing campaigns. But now that the cookie is capped at 7 days, you basically lose all ability to build cohorts of users who only visit your site once a month, for example, unless you invest in setting HTTP cookies instead.\nJohn\u0026rsquo;s later tweet clarified this:\nThings would be easier if third-parties stayed third-parties. But since site owners bring in third-parties into their first-party context, ITP has to deal with it. Hopefully, we can get to a place where devs help prevent cross-site tracking.\n\u0026mdash; John Wilander (@johnwilander) February 28, 2019  So it does seem that ITP is taking blanket measures to best target the bad actors in the industry.\nAnd this is definitely not the last of it It\u0026rsquo;s not just ITP. Firefox will also start blocking cross-site third-party trackers, no doubt taking a leaf out of ITP\u0026rsquo;s book. They\u0026rsquo;ve already had a very strong-armed approach to blocking trackers (though in private browsing mode only, for now), so this doesn\u0026rsquo;t come as a big surprise.\n Update 8 March 2019: Firefox announced they will start experimenting with the 7-day expiration of JavaScript cookies, too.\n As workarounds for ITP are invented, new iterations of ITP will be introduced. Until now, each iteration has made ITP stricter. Even if they made a concession (Storage Access API without user prompt), they seem to not hesitate to pull it back in a later version.\nI don\u0026rsquo;t think they\u0026rsquo;ll let go of the 7-day cookie expiration for document.cookie. If anything, they\u0026rsquo;ll reduce it to just one day, or only allow session cookies. Or, perhaps eradicate JavaScript cookies altogethe. In any case, it\u0026rsquo;s up to vendors and their enterprise clients (since money talks) to consider how big a deal this is, and how to solve this going forward.\nAdobe has already introduced their own ways of tackling ITP but with the ban on document.cookie it remains to be see how they\u0026rsquo;ll react to ITP 2.1.\nVendors might be hesitant to take action because nothing is future-proofed. Any solution that caters to web analytics users could be misappropriated for cross-site tracking purposes, and at that point a future iteration of ITP would most certainly neuter it. And if that happens, the analytics vendor who encouraged enterprise clients to spend thousands of dollars in implementing the solution will now be blamed for not having enough foresight.\nGoing forward To sum up this article, here are the key takeaways:\n  If you are tracking just root domains, and do not need cross-subdomain tracking to work without explicit linker parameters, you can use the localStorage workaround. See this chapter for more information.\n  If you think Safari has a big enough share of your traffic to seriously impact your data quality, you should look into writing the _ga cookie using HTTP cookies. See here for a recap.\n  Follow John Wilander on Twitter.\n  Await an official announcement from Google (or whatever your favorite analytics vendor is) on how they intend to tackle ITP\u0026rsquo;s stranglehold on cookies written with document.cookie.\n  For a geeky tech dude like me, we\u0026rsquo;re living in some pretty cool and exciting times. Having to figure out workarounds for this concentrated attack on cookies by (most) web browsers is inspiring!\n"
},
{
	"uri": "https://www.simoahava.com/tags/web-development/",
	"title": "Web development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/use-localstorage-client-id-persistence-google-analytics/",
	"title": "#GTMTips: Use localStorage For Client ID Persistence In Google Analytics",
	"tags": ["google tag manager", "gtmtips", "customtask", "clientid", "itp"],
	"description": "Use this customTask trick to persist the Client ID in browser localStorage in situations where the Google Analytics cookie is not available or has expired due to Safari&#39;s Intelligent Tracking Prevention measures.",
	"content": " Updated 1 October 2019 With ITP 2.3 it looks like Safari is reducing the usefulness of localStorage as well, so this solution should not be considered future-proof. The only stable way to persist client-side data at the moment seems to be HTTP cookies.\n  Updated 7 March 2019 - Added some extra caveats to this solution. Also, be sure to read my article on ITP 2.1, which has far more detail on what Intelligent Tracking Prevention is and how to work with it.\n Looks like Safari is tightening the noose around browser cookies with the introduction of ITP 2.1 (Intelligent Tracking Prevention). Among other things, ITP 2.1 caps the expiration of client-side cookies to 7 days. Client-side cookies in this context refer to cookies set with the document.cookie API, which would cover pretty much all cookies set by vendored JavaScript libraries like Google Analytics\u0026rsquo; analytics.js.\nPersonally, I think anything that forces a rethink of the cookie model is welcome. It\u0026rsquo;s awkward and twisted that we still rely on such flimsy and fragile browser storage for persisting important data such as anonymous analytics identifiers, authentication flags, and basically anything that needs to persist from one page to the next.\nHaving said that, this can make potentially benevolent tracking such as that done with Google Analytics on a website quite difficult for users with Safari browsers. Since GA relies on a browser cookie capped at 2 years\u0026rsquo; expiration (by default), this 7 days max can really hurt data quality.\nIn this article, I want to show you how to use customTask with localStorage to serve as a backup of sorts for your cookie-based Client ID persistence.\nIt\u0026rsquo;s not perfect. Check out the caveats chapter for details. Please treat this as a technology demo rather than a be-all, end-all solution to cookie woes. We\u0026rsquo;ll need to rely on the vendors and browsers finding consensus for how to make life harder on the bad agents without compromising the work us \u0026ldquo;regular folks\u0026rdquo; need to do to improve our websites with first-party analytics data.\nTip 96: Persist the Client ID in localStorage   It\u0026rsquo;s not a novel trick. In fact, Google themselves show you how to do this in the developer documentation. The thing is, though, that Google only shows you how to replace cookie storage with localStorage. I still want to leverage cookies, because they make things like cross-domain tracking far easier to do.\nThe process is basically this:\n  When a Google Analytics tracker is created, check if Client ID is persisted in localStorage.\n  If this is the case, check if its expiration is in the future.\n  If this is the case, build the tracker with the Client ID from localStorage (the tracker will generate the _ga cookie with this data, too).\n  If there is no Client ID in localStorage use GA\u0026rsquo;s default Client ID creation and storage mechanisms.\n  When the tag fires, write the Client ID in localStorage.\n  It should be pretty smooth sailing. Since the _ga cookie and localStorage are kept in sync, this should work across origins which is something that localStorage by default doesn\u0026rsquo;t do.\nNOTE! Since you\u0026rsquo;re always checking localStorage first, this means that if the user deletes their cookies, they don\u0026rsquo;t actually delete the Client ID, because that would require localStorage to be flushed, too. You might want to discuss this solution with your legal team before going forward with it.\nHow to set the Client ID You\u0026rsquo;ll need two Custom JavaScript variables. One to set the Client ID, and one to get it.\nTo set the Client ID, go to the customTask Builder tool, select the option named Use localStorage To Persist ClientId, and then click Copy to clipboard.\n  In Google Tag Manager, create a new Custom JavaScript variable, and paste the clipboard content within. Then, do the following two maintenance operations:\n  Delete the following text from the start of the code block: var customTask = . Just that, nothing else.\n  Delete the last character of the entire code block, which should be a semi-colon.\n  Then, modify the configuration object, if you wish.\nvar localStorageCid = { objectName: \u0026#39;ga_client_id\u0026#39;, expires: 1000*60*60*24*365*2 };  If you want the name of the object written to localStorage to be something other than 'ga_client_id', change the respective string value. The expires value is a number in milliseconds denoting how long the object should be in storage. It\u0026rsquo;s updated every time a tag fires with this customTask script. If you want the storage to expire sooner or later than two years\u0026rsquo; time, change the value of expires accordingly.\n NOTE! There is no automatic expiration mechanism with localStorage. \u0026ldquo;Expires\u0026rdquo; here simply means that if the expiration of the item is in the past, it will be overwritten with the Client ID generated by GA.\n Finally, add this customTask to all your Google Analytics tags. Easiest way to do it is to use a Google Analytics Settings variable. Add it like this:\n  Remember that you can only add one customTask per tag or Google Analytics Settings variable. Use the customTask Builder tool to compile a customTask that incorporates multiple different functions.\nHow to get the Client ID The second Custom JavaScript variable you\u0026rsquo;ll need is something that will pull the Client ID from localStorage and use that instead of the value stored in the _ga cookie (if any).\nThis is what the Custom JavaScript variable code looks like:\nfunction() { var objectName = \u0026#39;ga_client_id\u0026#39;; if (window.localStorage) { var jsonObj = window.localStorage.getItem(objectName) || \u0026#39;{}\u0026#39;; var obj = JSON.parse(jsonObj); var now = new Date().getTime(); if (obj.clientId \u0026amp;\u0026amp; obj.expires) { if (now \u0026lt;= obj.expires) { return obj.clientId; } } } return; }  Change the value of objectName to match the name of the localStorage object you set in the customTask above. By default, it\u0026rsquo;s 'ga_client_id'.\nThis script checks if a Google Analytics Client ID is found in localStorage, and that if found, its expiration time hasn\u0026rsquo;t whizzed past yet. If both of these conditions pass, the Client ID is returned from localStorage.\nIf no item is found, or if object has expired, or if the browser doesn\u0026rsquo;t support localStorage, the variable returns undefined which basically means that GA falls back to its default method of Client ID generation, retrieval, and storage.\nYou need to add this variable to every single tag to which you added the customTask from above. Again, it\u0026rsquo;s imperative that every single tag uses this new localStorage method consistently, or you might end up with tags firing with the wrong Client ID, thus skewing your data.\nYou add this variable with the field name clientId, like this:\n  That\u0026rsquo;s it for this simple solution. There is one big problem with this approach, though. It will always take the Client ID from localStorage if it\u0026rsquo;s available and hasn\u0026rsquo;t expired. This is problematic in one specific scenario: cross-domain tracking.\nHow to respect GA\u0026rsquo;s cross-domain linker parameter Turns out, this isn\u0026rsquo;t totally trivial to do. Even if you set the allowLinker field to true in the tag, the Client ID from localStorage will always overwrite the respective field in the tag, no matter how valid the linker parameter was.\nSo, you need to replicate how allowLinker works, checking if the URL has a valid linker parameter. If it does, then you need to bypass the localStorage fetch, so that analytics.js can build the Client ID with the linker parameter, and then write the updated ID into localStorage in the customTask.\nUnfortunately, analytics.js doesn\u0026rsquo;t expose the allowLinker functionality as an API you could simply query to know whether the URL has a valid linker parameter or not.\nThis leaves us with very few options. Your best bet is to actually reproduce what allowLinker does. It\u0026rsquo;s not trivial, so I did the legwork for you (thanks to David Vallejo\u0026rsquo;s generous help). You can find the source code in this Gist.\nTo add it to your setup, open the Custom JavaScript variable you created in the previous chapter, and edit it to this:\nfunction() { var objectName = \u0026#39;ga_client_id\u0026#39;; var checkLinker=function(t){var n,e,i=function(t,n){for(var e=new Date,i=window.navigator,r=i.plugins||[],a=[t,i.userAgent,e.getTimezoneOffset(),e.getYear(),e.getDate(),e.getHours(),e.getMinutes()+n],s=0;s\u0026lt;r.length;++s)a.push(r[s].description);return o(a.join(\u0026#34;.\u0026#34;))},r=function(t,n){var e=new Date,i=window.navigator,r=e.getHours()+Math.floor((e.getMinutes()+n)/60);return o([t,i.userAgent,i.language||\u0026#34;\u0026#34;,e.getTimezoneOffset(),e.getYear(),e.getDate()+Math.floor(r/24),(24+r)%24,(60+e.getMinutes()+n)%60].join(\u0026#34;.\u0026#34;))},o=function(t){var n,e=1;if(t)for(e=0,n=t.length-1;0\u0026lt;=n;n--){var i=t.charCodeAt(n);e=0!=(i=266338304\u0026amp;(e=(e\u0026lt;\u0026lt;6\u0026amp;268435455)+i+(i\u0026lt;\u0026lt;14)))?e^i\u0026gt;\u0026gt;21:e}return e.toString()};if(\u0026#34;string\u0026#34;==typeof t\u0026amp;\u0026amp;t.length){if(!/_ga=/.test(t))return\u0026#34;Invalid linker format in string argument!\u0026#34;;e=t.split(\u0026#34;\u0026amp;\u0026#34;).filter(function(t){return\u0026#34;_ga\u0026#34;===t.split(\u0026#34;=\u0026#34;)[0]}).shift()}else e=(n=/[?\u0026amp;]_ga=/.test(window.location.search)?\u0026#34;search\u0026#34;:/[#\u0026amp;]_ga=/.test(window.location.hash)?\u0026#34;hash\u0026#34;:void 0)\u0026amp;\u0026amp;window.location[n].substring(1).split(\u0026#34;\u0026amp;\u0026#34;).filter(function(t){return\u0026#34;_ga\u0026#34;===t.split(\u0026#34;=\u0026#34;)[0]}).shift();if(void 0===e||!e.length)return\u0026#34;Invalid linker format in URL!\u0026#34;;var a,s,g,u,f=e.indexOf(\u0026#34;.\u0026#34;);return f\u0026gt;-1\u0026amp;\u0026amp;(e.substring(0,f),s=(a=e.substring(f+1)).indexOf(\u0026#34;.\u0026#34;),g=a.substring(0,s),u=a.substring(s+1)),void 0!==u?g===i(u=u.split(\u0026#34;-\u0026#34;).join(\u0026#34;\u0026#34;),0)||g===i(u,-1)||g===i(u,-2)||g===r(u,0)||g===r(u,-1)||g===r(u,-2):void 0}; if (checkLinker()) { return; } if (window.localStorage) { var jsonObj = window.localStorage.getItem(objectName) || \u0026#39;{}\u0026#39;; var obj = JSON.parse(jsonObj); var now = new Date().getTime(); if (obj.clientId \u0026amp;\u0026amp; obj.expires) { if (now \u0026lt;= obj.expires) { return obj.clientId; } } } return; }  That big block of code starting with var checkLinker= contains the code in the GitHubGist just minified to reduce clutter.\nThe checkLinker() method parses the URL for a valid linker parameter. If one is found, then the variable ignores the Client ID stored in localStorage and allows the GA tag to build the Client ID from the linker parameter instead.\nRead on for one major caveat in this approach.\nCaveats There are, naturally, some caveats to this workaround.\n  There\u0026rsquo;s no mechanism involved to handle multiple _ga cookies. So if you have a set of tags that need to have their Client ID handled separately (e.g. due to roll-up tracking, simply use a different localStorage object name for that set of tags.\n  Unlike typically with localStorage, you don\u0026rsquo;t have to worry about cross-origin tracking, since the _ga cookie would persist across subdomains (assuming it has the cookieDomain field set to auto), and thus when the user lands on a subdomain without the localStorage object, the _ga cookie is used instead, and this is then written into localStorage in the customTask.\n  Tracking across subdomains is nevertheless a problem, because the _ga cookie only survives 7 days without returning visits. Thus if the user visits one subdomain on day 1 and another subdomain on day 8, the localStorage solution described here will be of little help.\n  Cross-domain tracking, even with the linker trick mentioned in the previous chapter, is still a pain. Google is experimenting with different linker parameters, so the solution above might become outdated soon. I\u0026rsquo;ll update the code when the linker parameter format changes.\n  There are potential legal implications of disregarding cookie purges as described in this article (thanks to Brian Clifton) for pointing this out in the comments. I seriously recommend to only treat this solution as a technical demo of what could be done, not what should be done.\n  Safari is already blocking localStorage.setItem() in private browsing mode, and other browsers might follow suit.\n  And then, of course, the biggest caveat:\nThis isn\u0026rsquo;t the last we\u0026rsquo;ve seen of the concentrated attack against browser cookie storage. Other browsers will likely follow suit, after they let Safari take the blow for being the first one audacious enough to delimit first-party cookies in this way.\nFinal thoughts I want you to consider this a tech demo first and foremost. It\u0026rsquo;s not robust enough to carry your entire enterprise web analytics setup, but it should be usable in situations where you want to minimize data loss especially with Safari users.\nPlease do note that you are potentially making it more difficult for your visitors to clear their tracking identifiers. Make it obvious in your privacy statement that this type of persistence is happening, even if it\u0026rsquo;s not necessary per the regulations or laws of your region. It\u0026rsquo;s just good behavior. Users should be allowed to purge their tracking identifiers without having to be rocket scientists to do so.\nI\u0026rsquo;m also expecting this solution to be temporary. It\u0026rsquo;s still unclear how Intelligent Tracking Prevention evolves. One of the purposes of the 7-day cap to client-side cookies is to prevent third party JavaScript libraries from writing cookies on one domain and accessing them on another subdomain. If Apple considers this localStorage trick to be a hack around this limitation, it\u0026rsquo;s possible they\u0026rsquo;ll invest in measures to prevent this from working, too.\nAlso, I\u0026rsquo;m expecting big vendors who suffer most from this (such as Google and Facebook) to introduce their own solutions that help avoid the potential compromisation of data quality that\u0026rsquo;s at stake here. As such, I do hope that this article becomes outdated soon, when the platforms suggest an officially supported way of persisting data reliably.\nFinally, many might be now even more tempted to move towards server-side tracking due to the increased fragility of client-side persistence. You don\u0026rsquo;t need a full server-side proxy to cope with ITP 2.1. Since it only targets cookies written with document.cookie, you could simply generate anonymous identifiers like GA\u0026rsquo;s Client ID server-side, and set them in the Set-Cookies header of the HTTP response. Be sure to check out my more extensive article on ITP 2.1 and web analytics for more details.\nWhat do you think of this latest ITP 2.1 release? Feel free to join the discussion in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/tags/clientid/",
	"title": "clientid",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/customtask/",
	"title": "customtask",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-and-jquery/",
	"title": "Google Tag Manager And jQuery",
	"tags": ["google tag manager", "jquery", "javascript"],
	"description": "This article goes over the (non-existent) relationship between the jQuery library and Google Tag Manager, and offers you tips on how to integrate the two.",
	"content": "The jQuery JavaScript library is by almost any means of counting the most popular JavaScript library used in websites around the world. It\u0026rsquo;s so influential, in fact, that its evolution is tightly bound to the JavaScript standardization effort itself, and it\u0026rsquo;s an integral part of the JS Foundation\u0026rsquo;s efforts to build a community for JavaScript developers.\n  Google Tag Manager, similarly, is the most popular tag management system used in websites, globally. Thus, by way of weak correlation, it makes sense to expect some synergy between the two.\nIn this article, I want to explore the relationship between the two tools. Both serve a similar purpose - abstraction and facilitation of doing stuff with JavaScript on a web page. I also want to dispel a myth about Google Tag Manager bundling jQuery natively, and I want to explore when it might make sense not to use jQuery.\nNo, Google Tag Manager does not come bundled with the jQuery library Some people have peeked into the depths of the gtm.js library downloaded by the browser when the container snippet is executed. This library contains the GTM container in all its glory. Within the minified, obfuscated JavaScript code, sharp eyes can find the following comment:\n  This comment might tempt you to think that GTM bundles the full jQuery library within the container JavaScript.\nIt doesn\u0026rsquo;t.\nThe comment is there because Google Tag Manager uses one method that is heavily inspired by a similar functionality in the jQuery library. You can find the source of this in the data-layer-helper GitHub project, where the is_plain_object.js contains the explanation for why there is a licence notice like this:\n  By the way, the data-layer-helper project is a great way to get acquainted with how GTM\u0026rsquo;s internal data model works!\nCheck if jQuery is already running Chances are, your site is already running jQuery. You can test this by opening the JavaScript console in your favorite web browser. Once the console is open, type in the following text and press enter:\ntypeof window.jQuery !== \u0026#39;undefined\u0026#39; ? console.log(window.jQuery.fn.jquery) : \u0026#39;jQuery not found!\u0026#39;  If your site is running jQuery, you\u0026rsquo;ll see the version number output into the console. If your site is not running jQuery, you\u0026rsquo;ll see the text: jQuery not found!.\n  If jQuery was found, you can tentatively use it in your Custom HTML tags and your Custom JavaScript variables.\nHowever, jQuery is (or at least should be) loaded asynchronously, which means there might be a race condition where you try to call its methods using GTM before the jQuery library has actually loaded. Thus, whenever you want to use jQuery, you should hedge it with some safeguards.\nUse jQuery safely Basically, if you want to use jQuery, you should always check if it has been initialized with something like this:\nif (typeof window.jQuery !== \u0026#39;undefined\u0026#39;) { // Do something with jQuery } else { // Fallback in case jQuery hasn\u0026#39;t been loaded }  If jQuery is found, you can use it at will, and if it isn\u0026rsquo;t found, the else block is the fallback you\u0026rsquo;ll use in such cases. Here\u0026rsquo;s an example:\nfunction() { var el = {{Click Element}}; if (typeof window.jQuery !== \u0026#39;undefined\u0026#39;) { return window.jQuery(el).find(\u0026#39;h1\u0026#39;).text(); } else { return el.querySelector(\u0026#39;h1\u0026#39;).textContent; } }  The code in the if block and the code in the else block are essentially the same, though jQuery has some benefits, such as making sure that text() returns the text content regardless of browser type and version.\nLoad jQuery using Google Tag Manager If jQuery isn\u0026rsquo;t used by your site, and you still want to leverage it, you can always load it in a Custom HTML tag.\n First, discuss this with your developers. jQuery can introduce quite a bit of bloat into the page, and the developers might have had a good reason not to use jQuery at all. It\u0026rsquo;s possible they use another library that has the same functionality.\n The best way to load jQuery is to use a Custom HTML tag in a tag sequence with whatever tag(s) you have that might need to use jQuery.\nLet\u0026rsquo;s start with what the Custom HTML tag looks like:\n\u0026lt;script\u0026gt; (function() { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://code.jquery.com/jquery-3.3.1.min.js\u0026#39;; el.async = true; el.addEventListener(\u0026#39;load\u0026#39;, function() { window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}})}); el.addEventListener(\u0026#39;error\u0026#39;, function() { window.google_tag_manager[{{Container ID}}].onHtmlFailure({{HTML ID}})}); document.head.appendChild(el); })(); \u0026lt;/script\u0026gt; You\u0026rsquo;ll need to enable the Container ID and HTML ID built-in variables for this. Do not add any triggers to this tag.\nBasically, you create an asynchronous \u0026lt;script\u0026gt; load request for the jQuery library from jQuery\u0026rsquo;s own CDN (content distribution network). Because it\u0026rsquo;s asynchronous, you also add a load listener, which fires when the library has loaded successfully, and an error listener, which fires if the library failed to load due to an error.\nIt\u0026rsquo;s not perfect, since sometimes just because the library loaded doesn\u0026rsquo;t mean it managed to execute correctly (if there are conflicts in the page\u0026rsquo;s global namespace, for example), and an error isn\u0026rsquo;t always thrown if the library fails to load.\nOnce you\u0026rsquo;ve created that Custom HTML tag, scroll down to its Advanced settings, and set its Tag firing options to Once per page.\n  This is because regardless of how many tag sequences it\u0026rsquo;s in, you only want to load the jQuery library once per page.\nNext, you need to open every tag that needs jQuery, scroll down to their Advanced settings, and add the jQuery loader as a setup tag in the sequence:\n  If you want to block the tag from running in case jQuery did not load, you can check the \u0026ldquo;Don\u0026rsquo;t fire tag if setup_tag fails or is paused\u0026rdquo;. But this is a bit drastic. Instead, you might want to simply see the tip in the previous chapter to run a fallback in case jQuery is not found.\nI want to repeat what I wrote above: discuss with your developers first. jQuery isn\u0026rsquo;t big, but it\u0026rsquo;s still bloat if you only need it for a handful of things. Which leads me to the next point.\nYou don\u0026rsquo;t always have to use jQuery Check out this wonderful website: You Might Not Need jQuery. It\u0026rsquo;s a service that helps you rewrite jQuery methods using vanilla JavaScript.\nIf you find yourself only needing jQuery for a handful of things, it might be better to just use native JavaScript methods rather than load an entire library to perform simple tasks.\n How to write an HTTP POST request with and without jQuery  Here are my own, personal rules of thumb:\n  If the site already loads the jQuery library, use it at will. Just make sure to avoid race conditions if jQuery is loaded asynchronously (as it should).\n  If the site is not running jQuery, look at the complexity of the tasks you want to do. If you only need to do simple DOM traversal or trivial HTTP requests, look into using native JavaScript rather than loading the library.\n  If you must load jQuery, use a recent, minified version of the library. Load it asynchronously, and use tag sequencing to ensure the dependency has loaded before it is used by the main tag.\n  Summary jQuery is a great tool in web development. There\u0026rsquo;s a reason it\u0026rsquo;s the most popular JavaScript library on the web.\nHowever, native JavaScript is spectacularly useful, too, even if you might have type in some extra characters compared to jQuery\u0026rsquo;s helper methods.\nI\u0026rsquo;ve always tried to avoid using jQuery or, really, any helper library because I think it\u0026rsquo;s important to understand how the underlying JavaScript engine in the browser works. If you always use jQuery, you\u0026rsquo;ll easily ignore things like the nuances of browser support, performance optimization, and how to write good, readable code in general.\nSimilar to Google Tag Manager, jQuery is a great tool, but you have to earn the right to use it by understanding how it works on the web page. Ignorance to what jQuery actually does can lead to catastrophic results, when you write business critical code in your tags, relying on an outdated version of jQuery, or using methods that totally kill performance.\n"
},
{
	"uri": "https://www.simoahava.com/tags/jquery/",
	"title": "jquery",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/enhanced-ecommerce-facebook-pixel/",
	"title": "#GTMTips: Use Enhanced Ecommerce Data Layer With Facebook Pixel",
	"tags": ["google tag manager", "gtmtips", "facebook", "enhanced ecommerce"],
	"description": "Use the Enhanced Ecommerce object in the dataLayer to populate the required variables of a Facebook pixel in Google Tag Manager.",
	"content": "A recurring question in the Google Tag Manager communities (e.g. product forums) is how to use an Enhanced Ecommerce dataLayer object with the Facebook pixel code? It\u0026rsquo;s a common question since running a Facebook conversion pixel on a site that also collects data from the store into Google Analytics\u0026rsquo; Enhanced Ecommerce reports is probably a very typical scenario.\n Side note: Since Google+ is about to go the way of the dodo, I\u0026rsquo;ve created an archive of the entire community which you can browse and make text searches against.\n I\u0026rsquo;ve shared a tip on this before, but I think I need to be even more specific and give concrete examples how to populate the various properties of the Facebook pixel object.\nFor a guide on how to implement the Facebook pixel, check out Yehoshua\u0026rsquo;s amazing Facebook Pixel with Google Tag Manager guide.\nTip 95: Use the Facebook Pixel with an Enhanced Ecommerce Data Layer   The solution here is to create an API that you can pass your Enhanced Ecommerce products array to, and it will return the array all reduced and converted to the various formats required by the Facebook Pixel.\nFirst things first, you need to create the Custom JavaScript variable.\n  Create a new Custom JavaScript variable in the GTM UI.\n  Name it {{EECFB API}} (or something less of an abomination, if you wish).\n  Add the following code within:\n  function() { return function(products) { if (!Array.isArray(products)) { return; } var idList = products.map(function(prod) { return !!prod.id \u0026amp;\u0026amp; prod.id.toString(); }); var totalValue = products.reduce(function(acc, cur) { return !!cur.price \u0026amp;\u0026amp; !!cur.quantity ? acc + (cur.price * cur.quantity) : acc; }, 0); var totalQuantity = products.reduce(function(acc, cur) { return !!cur.quantity ? acc + parseInt(cur.quantity) : acc; }, 0); var contentsArray = products.map(function(prod) { return { id: !!prod.id ? prod.id.toString() : undefined, item_price: !!prod.price ? parseFloat(prod.price) : undefined, quantity: !!prod.quantity ? parseInt(prod.quantity) : undefined }; }); return { content_ids: idList, value: totalValue, num_items: totalQuantity, contents: contentsArray }; }; }  This code actually returns a function, so it\u0026rsquo;s basically a wrapper for a closure, which you can then call in your other tags using a parameter.\nWhen you invoke this API in a Custom HTML tag, for example, you need to have a variable reference to the products array at hand to pass to this API as a parameter (I\u0026rsquo;ll show you an example soon).\nThe API returns an object with four properties:\n   Property Description Sample output     content_ids Array of all the Product IDs. ['id123', 'id234', 'id345']   value The sum of all the product prices multiplied by their respective quantities. 63.55   num_items The sum of all the quantities in the array. 5   contents The products array converted to the format Facebook requires. [{id: 'id123', quantity: 2, item_price: 10.2}]    How to put it all together Let\u0026rsquo;s take the Facebook Purchase event as an example. Here\u0026rsquo;s what you\u0026rsquo;d need to do.\nFirst, let\u0026rsquo;s say this is what your Enhanced Ecommerce purchase object looks like:\necommerce: { purchase: { products: { actionField: {...}, products: [{ id: \u0026#39;shirt1\u0026#39;, name: \u0026#39;T-Shirt\u0026#39;, price: \u0026#39;15.99\u0026#39;, quantity: 2 },{ id: \u0026#39;pants2\u0026#39;, name: \u0026#39;Pants\u0026#39;, price: \u0026#39;9.99\u0026#39;, quantity: 3 }] } } }    First, create a Data Layer variable for variable name ecommerce.purchase.products and name it {{ecommerce.purchase.products}}. This is the reference to the products in your Enhanced Ecommerce purchase object.\n  Create a Custom HTML tag that fires after your Facebook pixel has been initialized.\n  This is what the contents might look like:\n  \u0026lt;script\u0026gt; (function() { // Get reference to the Enhanced Ecommerce products \tvar prods = {{ecommerce.purchase.products}}; // Poll the custom Facebook API with this array \tvar fbObj = {{EECFB API}}(prods); // Create the FB pixel call \tif (typeof window.fbq === \u0026#39;function\u0026#39;) { window.fbq(\u0026#39;track\u0026#39;, \u0026#39;Purchase\u0026#39;, { value: fbObj.value, content_ids: fbObj.content_ids, contents: fbObj.contents, num_items: fbObj.num_items }); } })(); \u0026lt;/script\u0026gt; It\u0026rsquo;s an imaginary situation - you wouldn\u0026rsquo;t typically send the Facebook Purchase event with that payload.\nAnyway, what happens here is that first you fetch a reference to the products array in the Enhanced Ecommerce purchase object. Then, you call the {{EECFB API}} passing that array as the parameter. The API returns an object, which you store in the fbObj local variable.\nThen, you simply access the properties of this object (listed in the table earlier in this article) in the fbq() call. That\u0026rsquo;s how you build the required pixel call with all the necessary parameters.\nSo, the window.fbq() method is actually called with these values:\nwindow.fbq(\u0026#39;track\u0026#39;, \u0026#39;Purchase\u0026#39;, { value: 61.95, content_ids: [\u0026#39;shirt1\u0026#39;, \u0026#39;pants2\u0026#39;], contents: [{ id: \u0026#39;shirt1\u0026#39;, item_price: 15.99, quantity: 2 },{ id: \u0026#39;pants2\u0026#39;, item_price: 9.99, quantity: 3 }], num_items: 5 });  Summary What you\u0026rsquo;re creating here is a helper function that automatically chews up your Enhanced Ecommerce products and spits out the values in a format required by the Facebook pixel.\nYou can freely extend the variable with other properties, and adapting it to other source object formats should be quite simple to do, too.\nIn any case, let me know in the comments if you think the API needs some other properties, or if you need help in extending it!\n"
},
{
	"uri": "https://www.simoahava.com/tags/facebook/",
	"title": "facebook",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/true-view-impressions-google-tag-manager/",
	"title": "True View Ecommerce Impressions With Google Tag Manager",
	"tags": ["google tag manager", "customtask", "enhanced ecommerce"],
	"description": "Measure Enhanced Ecommerce impressions in batches of items that were actually viewed (i.e. scrolled into the viewport). The solution is for Google Tag Manager.",
	"content": " Last updated 9 October 2020: customTask updated to a more stable version.\n I\u0026rsquo;m a big fan of Enhanced Ecommerce in Google Analytics. In fact, I think it\u0026rsquo;s the only valid way to deploy Ecommerce tracking today, especially when using Google Tag Manager. The ability to use a Custom JavaScript variable and the possibility to tackle the full ecommerce funnel are some of the benefits of using Enhanced Ecommerce.\nHowever, tracking certain view-based events, impressions in particular, has a significant problem when it comes to how Google Analytics processes events. In a nutshell, when implemented by the book, impressions are sent for all the product impressions on the page when the page is loaded. In other words, you are collecting hits from impressions the user might not actually have seen.\nThis also compounds into a number of problems:\n  Google Analytics has a maximum hits per session of 500. And this is a hard limit. Anything past 500 will simply not be recorded in the session (though if you\u0026rsquo;re using the paid GA360 you\u0026rsquo;ll still be billed for them!). If you send each impression separately, or even if you send them in batches but don\u0026rsquo;t discard the ones the user never saw, you might be approaching or going past this quota limit.\n  The maximum payload length is 8192 bytes for any GA request. This is easily surpassed if you\u0026rsquo;re sending 50-60 impressions in a single hit.\n  In this article, I want to tackle both of these problems by introducing a way of tracking only those impressions that were viewable in the browser viewport.\n  This solution is written for Google Tag Manager, as it utilizes a number of features that are built natively into the platform.\nI\u0026rsquo;m also going to use customTask, because I want to write the whole solution without using a Custom HTML tag.\nHow it works The solution relies on the Element Visibility trigger to identify impressions in the viewport and push them into a batch queue. A Timer trigger is also used to purge the batch queue in case the user is inactive. Finally, if a user clicks on an impression, all the items in the batch queue are sent, as are they if the user leaves the page.\nA batch queue is what we\u0026rsquo;ll use to group the viewed impressions together into units of certain size, so that the request to GA is neither too crowded (risking going past the payload length limit) or too sparse (risking having too many hits in the session).\n  Here\u0026rsquo;s a simple flow of how the solution works:\n  When the page is first loaded, an inventory of impression objects is built by identifying an ecommerce.impressions object in the dataLayer.\n  Every single product impression on the page itself will need to have an identifying attribute that links the product impression with the corresponding object in the ecommerce.impressions array in dataLayer. This should be something like data-productid=\u0026quot;impression1\u0026quot; where impression1 is the ID of the impression.\n  The Element Visibility trigger waits for items with this data-productid to become visible on the page, adding each into a batch array of viewed impressions.\n  When the array length reaches the batch maximum you define, the impressions in the batch array are sent to GA and then deleted from the array (so that you don\u0026rsquo;t end up sending them again).\n  If the user clicks a product impression on the page, a Product Click event is sent with that impression data together with all the items in the current batch queue.\n  Every 60 seconds (configurable), the batch queue is also emptied and sent to GA to avoid the impressions from being stuck in the queue for a long period of time if the user is inactive.\n  When the user leaves the current page, a custom beforeunload listener will also send any impressions left in the batch queue.\n  The end result is that as impressions are viewed by the visitor, items are added to the batch queue and sent whenever the queue fills up. There are safeguards (timer, beforeunload) which ensure that items are never forgotten in the queue.\nIt\u0026rsquo;s not trivial to set up, so let\u0026rsquo;s get to it.\nWhat you need First of all, you need a fairly solid understanding of what Enhanced Ecommerce impressions are, how they are (traditionally) tracked, and how they appear in your GA reports. This guide will not go over the basics. Check out my Enhanced Ecommerce guide, as well as Google\u0026rsquo;s official support documentation for more information.\nFor the solution to work \u0026ldquo;out-of-the-box\u0026rdquo;, you will need a properly formatted ecommerce.impressions object in the dataLayer deployed into the page template preferably before the GTM container snippet. This is what you should already have if you\u0026rsquo;ve implemented Enhanced Ecommerce impressions by the book.\n  It\u0026rsquo;s possible to modify the code so it works with objects with a different composition, and it\u0026rsquo;s also possible to modify it to work with impressions scraped from the page. However, this guide will not tackle these edge cases. If you want to make these modifications yourself, feel free to do so (let me know in the comments if you need help).\nAnother thing you\u0026rsquo;ll need is a way to tag all the HTML elements that contain impressions with the custom attribute that will link the impression to the corresponding dataLayer object. For example, this is what the first seven products on the page would look like if using the impressions data from the screenshot above:\n  As you can see, the value in data-productid corresponds with the individual id values in the ecommerce.impressions object.\nYou could add these dynamically using a Custom HTML tag and some DOM manipulation magic, but I do recommend doing this correctly from the start. Adding the attributes directly into template code prevent any race conditions from emerging, such as the Element Visibility trigger not identifying the items to which you dynamically added the new attributes.\nOnce you have the dataLayer object and the data attributes in place, you can get to work on the Google Tag Manager tags, triggers, and variables.\nCreate the variables We\u0026rsquo;re going to need a handful of variables. Two Lookup Table variables will handle some of the event tag logic. A Custom JavaScript variable for the customTask will be where most of the magic happens. Then, we\u0026rsquo;ll also need a Custom JavaScript variable to handle the Enhanced Ecommerce object compilation.\nThere are many moving parts here, so let\u0026rsquo;s get to work.\n{{Lookup - Get Impression Label}} This Lookup Table variable will be used to populate the Event Label field of the tag we\u0026rsquo;ll create label. As the label, i\u0026rsquo;m using either \u0026ldquo;Impression View\u0026rdquo; or \u0026ldquo;Impression Click\u0026rdquo;, depending on what type of event caused the tag to fire. As you can guess, the tag will fire for both impressions views and clicks.\nThis is what the Lookup Table variable should look like:\n  {{Lookup - Get Impression Interaction}} This Lookup Table variable will return either true or false, depending on whether the hit was non-interaction or not. We\u0026rsquo;ll create all impression views as non-interactive, and the product impression click will be interactive.\n  {{JS - Function - True view impression handler}} This is the main driving force of the solution. It\u0026rsquo;s a customTask variable, designed to build the batch queue, to create the beforeunload handler, to allow hits to fire (when the batch queue is full or a product is clicked), and to block GA hits from being dispatched (if the impression was simply added to the queue).\nHere\u0026rsquo;s what the code in the Custom JavaScript variable should look like:\nfunction() { // Configure the following  var maxBatch = 10, batchVariableName = \u0026#39;_impressions_batch\u0026#39;, productIdAttribute = \u0026#39;data-productid\u0026#39;; // Do not touch anything below this  var targetElement = {{Click Element}}, event = {{Event}}; if (event === \u0026#39;gtm.click\u0026#39;) { while (!targetElement.getAttribute(productIdAttribute) \u0026amp;\u0026amp; targetElement.tagName !== \u0026#39;BODY\u0026#39;) { targetElement = targetElement.parentElement; } } return function(customModel) { // Set up the beforeunload listener only when the tag is first run.  if (typeof window[batchVariableName] === \u0026#39;undefined\u0026#39;) { window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { window.dataLayer.push({ event: \u0026#39;sendFinalBatch\u0026#39; }); }); } var shouldFire = false, batch = window[batchVariableName] = window[batchVariableName] || [], impressionId, ost; if (event === \u0026#39;gtm.elementVisibility\u0026#39;) { impressionId = targetElement.getAttribute(productIdAttribute); batch.push(impressionId); if (batch.length === maxBatch) { shouldFire = true; } } if ([\u0026#39;sendFinalBatch\u0026#39;, \u0026#39;gtm.timer\u0026#39;].indexOf(event) \u0026gt; -1 \u0026amp;\u0026amp; batch.length \u0026gt; 0) { shouldFire = true; } if (event === \u0026#39;gtm.click\u0026#39;) { shouldFire = true; } if (shouldFire) { ost = customModel.get(\u0026#39;sendHitTask\u0026#39;); customModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { ost(sendModel); window[batchVariableName] = []; }); } else { customModel.set(\u0026#39;sendHitTask\u0026#39;, null); } }; }  First, modify the three variables in the beginning of the returned function.\n  Set maxBatch to the maximum length of the batch queue. The default value of 10 is probably enough in most cases.\n  Set batchVariableName to what the global variable that stores the batch queue should be named. The default value is, again, probably OK, but you\u0026rsquo;ll want to rename it in the rare case that a global variable named _impressions_batch already exists and is in use.\n  Set productIdAttribute to the HTML attribute name that stores the ID with each product HTML block on the page. The default value is data-productid, which means that the HTML structures that contain each product impression should have the attribute data-productid=\u0026quot;some_product_id\u0026quot; in one of the elements (preferably an element that wraps the entire impression).\n  Let me tear this code apart and show you what each individual part does.\nif (event === \u0026#39;gtm.click\u0026#39;) { while (!targetElement.getAttribute(productIdAttribute) \u0026amp;\u0026amp; targetElement.tagName !== \u0026#39;BODY\u0026#39;) { targetElement = targetElement.parentElement; } }  This is a simple iterator which climbs up the element tree from the clicked element until it finds the element with the aforementioned product ID attribute.\nif (typeof window[batchVariableName] === \u0026#39;undefined\u0026#39;) { window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { window.dataLayer.push({ event: \u0026#39;sendFinalBatch\u0026#39; }); }); }  When the customTask is fired for the first time on the page, a beforeunload listener is created. The idea is that when the user leaves the current page, the beforeunload event that is automatically dispatched triggers a window.dataLayer.push() call that we can then use to send the remaining items in the batch queue.\nvar shouldFire = false, batch = window[batchVariableName] = window[batchVariableName] || [], impressionId, ost;  By default, the customTask blocks the Google Analytics request from firing (shouldFire = false). Also, here we create the batch queue if one doesn\u0026rsquo;t already exist.\nif (event === \u0026#39;gtm.elementVisibility\u0026#39;) { impressionId = targetElement.getAttribute(productIdAttribute); batch.push(impressionId); if (batch.length === maxBatch) { shouldFire = true; } }  If the tag was fired due to a visibility event, the product ID that triggered the event will be pushed into the batch queue. If the batch queue thus reaches its maximum length, customTask will allow the hit to fire.\nif ([\u0026#39;sendFinalBatch\u0026#39;, \u0026#39;gtm.timer\u0026#39;].indexOf(event) \u0026gt; -1 \u0026amp;\u0026amp; batch.length \u0026gt; 0) { shouldFire = true; } if (event === \u0026#39;gtm.click\u0026#39;) { shouldFire = true; }  If the trigger event was sendFinalBatch (i.e. the user is trying to leave the page) or gtm.timer (i.e. 60 seconds have passed since the previous timer event or page load) AND if the batch queue has items in it, the hit will be allowed to fire, thus clearing the batch queue and sending all the items within to GA.\nIf the trigger event was a click, meaning the user clicked a product impression, the GA request will be allowed to complete as well.\nif (shouldFire) { ost = customModel.get(\u0026#39;sendHitTask\u0026#39;); customModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { ost(sendModel); window[batchVariableName] = []; }); } else { Object.keys(customModel.data).forEach(function(key) { customModel.data[key] = null; }); }  If the GA hit is allowed to fire, the first part of the if clause will do just that. After the hit has been sent, the batch queue is cleared of all items.\nIf the GA hit was blocked, then the else clause will clear the model object thus effectively preventing the Google Analytics hit from being dispatched.\n{{JS - Get True View object}} This variable builds the Enhanced Ecommerce object when the tag fires. It\u0026rsquo;s used to build both the impression view object and the impression click object.\nSo, create a Custom JavaScript variable, and put the following code within:\nfunction() { // Configure the following:  var maxBatch = 10, batchVariableName = \u0026#39;_impressions_batch\u0026#39;, productIdAttribute = \u0026#39;data-productid\u0026#39;; // Do not touch anything below this  var targetElement = {{Click Element}}, event = {{Event}}, batch = window[batchVariableName], impressions = google_tag_manager[{{Container ID}}].dataLayer.get(\u0026#39;ecommerce.impressions\u0026#39;), ecomObj = {ecommerce: {}}; if (event === \u0026#39;gtm.click\u0026#39;) { while (!targetElement.getAttribute(productIdAttribute) \u0026amp;\u0026amp; targetElement.tagName !== \u0026#39;BODY\u0026#39;) { targetElement = targetElement.parentElement; } } var latestImpression = impressions.filter(function(impression) { return impression.id === targetElement.getAttribute(productIdAttribute); }).shift(); var impressionsArr = batch.map(function(id) { return impressions.filter(function(impression) { return impression.id === id; }).shift(); }); if (event === \u0026#39;gtm.elementVisibility\u0026#39;){ impressionsArr[maxBatch - 1] = latestImpression; } ecomObj.impressions = impressionsArr; if (event === \u0026#39;gtm.click\u0026#39;) { ecomObj.click = { actionField: { list: latestImpression.list }, products: [latestImpression] }; } return { ecommerce: ecomObj }; }  As you can see, you\u0026rsquo;ll need to configure the exact same settings (maxBatch, batchVariableName, productIdAttribute) in the beginning of this script as you had to in the previous Custom JavaScript variable. It\u0026rsquo;s important that these match exactly to the settings you configured earlier, so double-check to make sure they are the same.\nOther than that, the purpose of the JavaScript is to compile the impression product IDs in the batch queue to the corresponding Enhanced Ecommerce objects. If the event was a click, then the impression that was clicked will be sent as the Product Click target.\nCreate the triggers We\u0026rsquo;ll need a whopping four triggers all attached to the tag.\nAn Element Visibility trigger will handle building the batch queue from product impression views.\nA Custom Event trigger will fire the tag when the user is trying to leave the page.\nA Timer trigger will send the items in the batch queue periodically.\nA Click trigger will fire the tag when the user clicks a product impression.\nThe Element Visibility trigger The Element Visibility trigger will fire whenever a new product impression becomes visible. It will only fire once per element, which ensures that those elements for which an impression hit has already been sent won\u0026rsquo;t get another impression sent while the user is on the page.\nThe Element Visibility trigger will be set to fire when any element with the give product ID attribute becomes visible. It\u0026rsquo;s important that you use the same attribute name you used in the two Custom JavaScript variables created above. If the attribute name was, for example, data-productid, the CSS selector you\u0026rsquo;d need to configure in the Element Visibility trigger should be [data-productid]. Check this if you need a refresher on CSS selectors.\nAnyway, make sure the trigger looks like something like this (with the change in the CSS selector if needed):\n  You can change the percentage of the element that needs to be visible if you wish.\nThe Custom Event trigger The Custom Event trigger is simple. It fires whenever the beforeunload listener (created in the customTask variable) is triggered by the user trying to leave the page.\n  The Timer trigger The Timer trigger will dispatch the items in the queue when it goes off. It\u0026rsquo;s a good backup to have, because otherwise you run the risk of items being left in the queue indefinitely, only to be dispatched when the user triggers another visibility event some time in the future. This might lead to hermit sessions, where the only hits are these impression view hits. Not very useful.\nYou can freely modify the settings of the trigger. Having it fire continuously isn\u0026rsquo;t that big a deal. The Google Analytics tag will always fire, yes, but the customTask will make sure that no hit is actually built unless the batch queue actually has items in it.\nAs you can see, I also make sure that the timer trigger only fires on valid pages by checking if a Data Layer variable for the key ecommerce.impressions is not undefined. This way the trigger will only activate on pages where there\u0026rsquo;s the possibility of viewing impressions. You can change the condition to something else if it makes more sense.\n  The Click trigger The Click trigger has a simple purpose. It fires when a product impression is clicked, thus sending a Product Click payload to Google Analytics (along with any impressions that might have been in the batch queue).\nRemember to change the CSS selector to match the HTML attribute you use to encode the product ID for every product impression element on the page.\nIf you\u0026rsquo;re curious about the [data-productid], [data-productid] * syntax, take a look at this article.\n  Create the tag Finally, the tag itself. Now that you have all the necessary variables and triggers created, it\u0026rsquo;s just a question of putting them all together in the tag.\nFeel free to modify what you send with Event Category, Event Action, and Event Label.\nMake sure you have the transport field set to beacon, since you\u0026rsquo;ll want those beforeunload hits to be sent even if the page has already been unloaded.\n  Tag Type: Universal Analytics\n  Track Type: Event\n  Category: Ecommerce\n  Action: Impressions\n  Label: {{Lookup - Get Impression Label}}\n  Non-Interaction Hit: {{Lookup - Get Impression Interaction}}\n  Enable overriding settings in this tag: Checked\n  Tracking ID: UA-XXXXXX-Y\n  Fields to Set:\n\u0026ndash; Field Name: customTask, Value: {{JS - Function - True view impression handler}}\n\u0026ndash; Field Name: transport, Value: beacon\n  Enable Enhanced Ecommerce Features: True\n\u0026ndash; Read Data from Variable: {{JS - Get True View object}}\n    Finally, add all four triggers created in the previous chapter to this tag.\n  And you\u0026rsquo;re all set! Then it\u0026rsquo;s time to test things out.\nHow to test It\u0026rsquo;s important not to rely on Google Tag Manager\u0026rsquo;s debug mode to test this out. Why? Because even if it says that a Google Analytics tag has fired, it\u0026rsquo;s customTask that governs whether the request was completed or not.\nSo, I recommend you install David Vallejo\u0026rsquo;s excellent GTM/GA Debug extension. It shows you what hits are actually sent to Google Analytics, and it has an excellent Enhanced Ecommerce tab for seeing exactly what impressions are dispatched.\nTest the batch queue first When a page with impression loads, test how your batch queue works.\nYou should see one gtm.elementVisibility event in the Debug mode panel for every product impression you see in the viewport of the page. These events should not dispatch the hit to GA until you reach the same number of visibility events as the maximum queue size.\nSo if you\u0026rsquo;ve configured the maxBatch in the Custom JavaScript variables to 10, then the first nine gtm.elementVisibility events should do nothing, but the tenth one should fire an Impressions hit with ten impressions within. And after that, every tenth visibility event should send another batch, etc.\n  Test with a timer Next, make sure you have some Element Visibility triggers fired after the previous batch has been sent, but not enough to send another batch. In other words, you want the queue to contain some impressions, but not enough to trigger a batch dispatch.\nThen, wait for the Timer trigger to go off. It will fire as soon as the interval is reached. In this guide, I\u0026rsquo;m using a 60 second interval, so within one minute of the page load the trigger should fire. When the trigger fires, it should send the items in the batch queue as an impression hit. The number of impressions should be less than what would be sent if the queue filled up.\nFor example, here the batch queue limit has been reached twice, sending two sets of three impressions to Google Analytics. The third impression hit with just one impression is due to the timer trigger sending the remaining impression.\n  Test a product click Again, scroll until the queue has items but not enough to fill the queue up to its maximum length. Then, click one product impression. What you should see is an Enhanced Ecommerce hit with the Product Click details of the impression that was clicked, and a list of impression views that were in the queue.\n  Test the beforeunload listener Finally, scroll until the queue has items but not enough to fill the queue. Then, click a link away from the current page (not a product impression!). Using the GTM/GA Debug extension, you should see the hits for the previous page including an impression view hit with some impressions (less than the maximum queue length):\n  Final words This is a fairly simple implementation. The \u0026ldquo;catch\u0026rdquo; is that it doesn\u0026rsquo;t use a Custom HTML tag. Everything is handled through a single Universal Analytics tag, utilizing the power of customTask to get the work done.\nIt\u0026rsquo;s not perfect. Without a Custom HTML tag, you\u0026rsquo;ll need to use the default Timer trigger which leaves a lot to be desired. For example, optimally I\u0026rsquo;d want to create a custom timer that would let me reset it whenever the batch queue is cleared and the first new item is added. That way the timer would not fire needlessly on the page. This isn\u0026rsquo;t really doable in the customTask - you would need a Custom HTML tag to do it.\nAnd why not use a Custom HTML tag? For one, this is a fun exercise to do. The other reason is that GTM has a fairly robust thing going on with the Element Visibility trigger, and I want to make use of it without polluting the page with numerous Custom HTML tags; one for every product impression found by the Element Visibility trigger.\nSome might also consider the prerequisites unreasonable. Being forced to have an Enhanced Ecommerce dataLayer object AND the HTML elements decorated with custom data attributes might seem like too much work. It\u0026rsquo;s a lot of work, but not too much work. Tracking impressions reliably by utilizing the Element Visibility trigger requires a strong link between the items on the page and those in the dataLayer. You could build your own custom system to support DOM scraping and custom dataLayer object compositions, but I\u0026rsquo;ll leave that to your capable hands.\nAnyway, I hope you found this guide inspiring. Let me know in the comments what you think!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/trigger-javascript-without-using-custom-html-tag/",
	"title": "#GTMTips: Trigger Javascript Function Without Using A Custom HTML Tag",
	"tags": ["google tag manager", "gtmtips", "javascript", "customtask"],
	"description": "Use customTask and a dummy Universal Analytics tag in Google Tag Manager to trigger JavaScript code without having to use a Custom HTML tag.",
	"content": " Last updated 9 October 2020: customTask updated to a more stable version.\n The Custom HTML tag in Google Tag Manager is splendid. It\u0026rsquo;s your go-to tool when you need to run arbitrary JavaScript on the webpage. Some might even use it to actually add HTML elements to the page, but I\u0026rsquo;m willing to bet running JavaScript is its most common use.\nHowever, there\u0026rsquo;s a downside to Custom HTML tags, which is only made more apparent on single-page apps which do not clear the full page when transitioning from one state to another. Custom HTML tags are injected to the end of \u0026lt;body\u0026gt;, which means you might end up with a lot of clutter in your Document Object Model.\nIn this article, I\u0026rsquo;ll show you a pretty neat trick. We\u0026rsquo;ll use customTask to run a \u0026ldquo;dummy\u0026rdquo; Universal Analytics tag, whose only purpose is to execute your JavaScript code when the tag is triggered. This way, nothing is injected into the DOM, and the only overhead you\u0026rsquo;ll run is the initial effort of creating the Google Analytics tracker.\nTip 94: Trigger custom JavaScript without using a Custom HTML tag   The way it works is fairly simple if you understand how customTask works. This is what a sample function might look like:\nfunction() { return function(model) { // Prevent hit from being sent \tmodel.set(\u0026#39;sendHitTask\u0026#39;, null); // The code you want to execute  var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;/endpoint\u0026#39;, true); xhr.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/json\u0026#39;); xhr.send(JSON.stringify({\u0026#39;key\u0026#39;: \u0026#39;abc12345\u0026#39;})); } }  The first few lines of the closure simply prevent the tag from doing anything. They null every single value stored in the model object, including all the tasks. This way once the customTask finishes, the hit builder will no longer do anything else (such as build and send the hit). That\u0026rsquo;s why I call it a \u0026ldquo;dummy\u0026rdquo; tag.\nAfter // The code you want to execute is where you\u0026rsquo;ll add the JavaScript you want to run. In this example, I\u0026rsquo;m firing a simple HTTP POST request to an endpoint on my server.\nThe tag itself can be any Universal Analytics tag. All it needs is the Tracking ID (can be anything you want since the hit will never be dispatched) and the customTask in its field, like so:\n  As you can see, the tag is extremely simple. Its only purpose is to execute the JavaScript in the customTask variable.\nThe other critical thing is the trigger. Use the trigger to establish when the JavaScript should be executed. In the example above, when the nonIdle event is pushed into dataLayer, the tag will run the code within {{JS - function - Increase counter}}.\nTo summarize, this method seeks to address the following malady:\n  Aside from looking hideous, every single element dynamically inserted into the DOM forces a reflow of the page, which becomes cumulatively more and more expensive the more elements are being added.\nBy avoiding DOM injection, the JavaScript function is run within the bowels of Google Tag Manager\u0026rsquo;s container (where it\u0026rsquo;s compiled into a function) and analytics.js (which builds the hit).\nThis way, you\u0026rsquo;ve essentially created a JavaScript Function Tag, whose only purpose is to trigger the execution of JavaScript code.\ncustomTask strikes again! Do you think you\u0026rsquo;ll have uses for a hack like this? Let me know in the comments.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/product-scoped-custom-dimensions-in-firebase/",
	"title": "#GTMTips: Use Product-scoped Custom Dimensions In GA Firebase",
	"tags": ["google tag manager", "firebase", "custom dimensions"],
	"description": "By hard-coding the dimension/metric index in the item bundles, you can add Product-scoped Custom Dimensions and Metrics in Google Analytics for Firebase data collection. Both iOS and Android SDKs support this method.",
	"content": "Thanks to the intrepid detective work of Jørn Reidel and Ahmed Marof, it looks like one of the big hurdles for doing a full migration from using the legacy Google Analytics and Google Tag Manager SDKs to the latest Tag Manager + Firebase SDK is now a non-issue.\nThe issue is, of course, Product-scoped Custom Dimensions or more specifically the lack of support thereof. Until now, I\u0026rsquo;d been holding against recommending the migration to anyone with Enhanced Ecommerce tracking set up simply because the documentation didn\u0026rsquo;t mention the possibility of sending these custom definitions to Google Analytics.\nBut, Jørn Reidel did what anyone who wants to actually find the truth beyond documentation should have done: he tested different ways to do it, and found the solution. Spoiler alert: it\u0026rsquo;s quite logical.\nTip 93: Send Product-scoped Custom Dimensions with Firebase   Naturally, this applies to the \u0026ldquo;legacy\u0026rdquo; Universal Analytics tags in Firebase GTM only. Google Analytics for Firebase doesn\u0026rsquo;t have a model for Enhanced Ecommerce any more. Also, thanks to the recent news about the upcoming deprecation of Google Analytics for mobile apps, this tip is probably short-lived.\nAnyway, the simple solution is to do what you\u0026rsquo;d do with the dataLayer setup when working with Enhanced Ecommerce for Google Tag Manager on the web: you hard-code the dimension and metric indices into the item bundles themselves.\nThis is what it would look like with Swift (iOS):\nlet product1: [String : Any] = [ AnalyticsParameterItemID: \u0026#34;sku1234\u0026#34;, AnalyticsParameterItemName: \u0026#34;Donut Friday Scented T-Shirt\u0026#34;, AnalyticsParameterItemCategory: \u0026#34;Apparel/Men/Shirts\u0026#34;, AnalyticsParameterItemVariant: \u0026#34;Blue\u0026#34;, AnalyticsParameterItemBrand: \u0026#34;Google\u0026#34;, AnalyticsParameterPrice: 29.99, AnalyticsParameterCurrency: \u0026#34;USD\u0026#34;, AnalyticsParameterQuantity: 1, \u0026#34;dimension4\u0026#34;: \u0026#34;Product Scoped Value 01 - GTM Firebase\u0026#34; ] And the same in Kotlin (Android):\nval product1 = Bundle() product1.putString(FirebaseAnalytics.Param.ITEM_ID, \u0026#34;sku1234\u0026#34;) product1.putString(FirebaseAnalytics.Param.ITEM_NAME, \u0026#34;Donut Friday Scented T-Shirt\u0026#34;) product1.putString(FirebaseAnalytics.Param.ITEM_CATEGORY, \u0026#34;Apparel/Men/Shirts\u0026#34;) product1.putString(FirebaseAnalytics.Param.ITEM_VARIANT, \u0026#34;Blue\u0026#34;) product1.putString(FirebaseAnalytics.Param.ITEM_BRAND, \u0026#34;Google\u0026#34;) product1.putDouble(FirebaseAnalyitcs.Param.PRICE, 29.99) product1.putString(FirebaseAnalytics.Param.CURRENCY, \u0026#34;USD\u0026#34;) product1.putLong(FirebaseAnalytics.Param.QUANTITY, 1) product1.putString(\u0026#34;dimension4\u0026#34;, \u0026#34;Product Scoped Value 01 - GTM Firebase\u0026#34;) If done correctly (i.e. you have a tag firing in GTM that sends the Enhanced Ecommerce data to Google Analytics), you should see your Custom Dimension surface when queried against this product with the Ecommerce action it was sent in.\n  Simple solution to a jarring problem. Thank you Jørn and Ahmed for the detective work!\n"
},
{
	"uri": "https://www.simoahava.com/tags/custom-dimensions/",
	"title": "custom dimensions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/obfuscate-and-duplicate-google-analytics-data/",
	"title": "Obfuscate And Duplicate Google Analytics Data",
	"tags": ["google tag manager", "customtask", "google analytics", "training"],
	"description": "Use this customTask to duplicate and obfuscate all Google Analytics data to which the customTask is added to. This is useful for demos and training situations.",
	"content": "Perhaps you didn\u0026rsquo;t know this, but there\u0026rsquo;s a really handy demo account for Google Analytics you can use to check out how Google Analytics works in a real business context (the data is from the Google Merchandise Store). However, you can access the account with nothing more than read-only access. This is annoying if you wanted to customize the setup.\nWorry not, I have a solution for you! Harnessing the awesome power of customTask, you can create a duplicate of the data collected on any website where you can modify the tracking (e.g. via Google Tag Manager). Even better, the data will be obfuscated using a dictionary of English words (you can edit this list), and hashing each string in the payload predictably against this dictionary.\n  As always, you can find this solution in my customTask Builder tool.\n  Huge thanks to Jaakko Ojalehto, my illustrious 8-bit-sheep developer colleague. He came up with the string replacement algorithm.\nHow to set it up You\u0026rsquo;ll want to fetch the latest version of the code from the customTask Builder tool. See also the instructions for how to deploy the customTask.\nIn Google Tag Manager, the Custom JavaScript variable will end up looking something like this:\nfunction () { // customTask Builder by Simo Ahava  //  // More information about customTask: https://www.simoahava.com/analytics/customtask-the-guide/  //  // Change the default values for the settings below.  // obfuscate: Obfuscates the entire hit payload (using a dictionary of words consistently) and dispatches it to the trackingId you provide.  // https://bit.ly/2RectUl  var obfuscate = { tid: \u0026#39;UA-12345-1\u0026#39;, dict: [\u0026#39;tumble\u0026#39;, \u0026#39;noble\u0026#39;, \u0026#39;flourish\u0026#39;, \u0026#39;abandon\u0026#39;, \u0026#39;liberal\u0026#39;, \u0026#39;team\u0026#39;, \u0026#39;conflict\u0026#39;, \u0026#39;collar\u0026#39;, \u0026#39;tiger\u0026#39;, \u0026#39;stun\u0026#39;, \u0026#39;grace\u0026#39;, \u0026#39;resource\u0026#39;, \u0026#39;phantom\u0026#39;, \u0026#39;imagine\u0026#39;, \u0026#39;information\u0026#39;, \u0026#39;hall\u0026#39;, \u0026#39;sweet\u0026#39;, \u0026#39;agriculture\u0026#39;, \u0026#39;bingo\u0026#39;, \u0026#39;relative\u0026#39;], stringParams: [\u0026#39;uid\u0026#39;,\u0026#39;ua\u0026#39;,\u0026#39;dr\u0026#39;,\u0026#39;cn\u0026#39;,\u0026#39;cs\u0026#39;,\u0026#39;cm\u0026#39;,\u0026#39;ck\u0026#39;,\u0026#39;cc\u0026#39;,\u0026#39;ci\u0026#39;,\u0026#39;gclid\u0026#39;,\u0026#39;dclid\u0026#39;,\u0026#39;dl\u0026#39;,\u0026#39;dh\u0026#39;,\u0026#39;dp\u0026#39;,\u0026#39;dt\u0026#39;,\u0026#39;cd\u0026#39;,\u0026#39;cg[1-5]\u0026#39;,\u0026#39;linkid\u0026#39;,\u0026#39;an\u0026#39;,\u0026#39;aid\u0026#39;,\u0026#39;av\u0026#39;,\u0026#39;aiid\u0026#39;,\u0026#39;ec\u0026#39;,\u0026#39;ea\u0026#39;,\u0026#39;el\u0026#39;,\u0026#39;ti\u0026#39;,\u0026#39;ta\u0026#39;,\u0026#39;in\u0026#39;,\u0026#39;ic\u0026#39;,\u0026#39;iv\u0026#39;,\u0026#39;pr\\\\d{1,3}id\u0026#39;,\u0026#39;pr\\\\d{1,3}nm\u0026#39;,\u0026#39;pr\\\\d{1,3}br\u0026#39;,\u0026#39;pr\\\\d{1,3}ca\u0026#39;,\u0026#39;pr\\\\d{1,3}va\u0026#39;,\u0026#39;pr\\\\d{1,3}cc\u0026#39;,\u0026#39;pr\\\\d{1,3}cd\\\\d{1,3}\u0026#39;,\u0026#39;tcc\u0026#39;,\u0026#39;pal\u0026#39;,\u0026#39;col\u0026#39;,\u0026#39;il\\\\d{1,3}nm\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}id\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}nm\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}br\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}ca\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}va\u0026#39;,\u0026#39;il\\\\d{1,3}pi\\\\d{1,3}cd\\\\d{1,3}\u0026#39;,\u0026#39;promo\\\\d{1,3}id\u0026#39;,\u0026#39;promo\\\\d{1,3}nm\u0026#39;,\u0026#39;promo\\\\d{1,3}cr\u0026#39;,\u0026#39;promo\\\\d{1,3}ps\u0026#39;,\u0026#39;sn\u0026#39;,\u0026#39;sa\u0026#39;,\u0026#39;st\u0026#39;,\u0026#39;utc\u0026#39;,\u0026#39;utv\u0026#39;,\u0026#39;utl\u0026#39;,\u0026#39;exd\u0026#39;,\u0026#39;cd\\\\d{1,3}\u0026#39;,\u0026#39;xid\u0026#39;,\u0026#39;exp\u0026#39;,\u0026#39;_utmz\u0026#39;], priceParams: [\u0026#39;tr\u0026#39;,\u0026#39;ts\u0026#39;,\u0026#39;tt\u0026#39;,\u0026#39;ip\u0026#39;,\u0026#39;pr\\\\d{1,3}pr\u0026#39;,\u0026#39;id\\\\d{1,3}pi\\\\d{1,3}pr\u0026#39;], priceModifier: Math.random(), medium: [\u0026#39;organic\u0026#39;, \u0026#39;referral\u0026#39;, \u0026#39;social\u0026#39;, \u0026#39;cpc\u0026#39;], replaceString: function(t){if(\u0026#39;\u0026#39;===t)return t;\u0026#39;function\u0026#39;==typeof window.btoa\u0026amp;\u0026amp;(t=btoa(t));var n=t.split(\u0026#39;\u0026#39;).map(function(t){return t.charCodeAt(0)}).join(\u0026#39;\u0026#39;)%obfuscate.dict.length;return obfuscate.dict[n]}, init: function(){var c=[];obfuscate.dict.forEach(function(t){obfuscate.dict.forEach(function(o){c.push(t+\u0026#39;-\u0026#39;+o)})}),obfuscate.dict=obfuscate.dict.concat(c)} }; // DO NOT EDIT ANYTHING BELOW THIS LINE  if (typeof obfuscate === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; typeof obfuscate.init === \u0026#39;function\u0026#39;) obfuscate.init(); var readFromStorage = function (key) { if (!window.Storage) { // From: https://stackoverflow.com/a/15724300/2367037  var value = \u0026#39;; \u0026#39; + document.cookie; var parts = value.split(\u0026#39;; \u0026#39; + key + \u0026#39;=\u0026#39;); if (parts.length === 2) { return parts.pop().split(\u0026#39;;\u0026#39;).shift(); } } else { return window.localStorage.getItem(key); } }; var writeToStorage = function (key, value, expireDays) { if (!window.Storage) { var expiresDate = new Date(); expiresDate.setDate(expiresDate.getDate() + expireDays); document.cookie = key + \u0026#39;=\u0026#39; + value + \u0026#39;;expires=\u0026#39; + expiresDate.toUTCString(); } else { window.localStorage.setItem(key, value); } }; var globalSendHitTaskName = \u0026#39;_ga_originalSendHitTask\u0026#39;; return function (customTaskModel) { window[globalSendHitTaskName] = window[globalSendHitTaskName] || customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); customTaskModel.set(\u0026#39;sendHitTask\u0026#39;, function (sendHitTaskModel) { var originalSendHitTaskModel = sendHitTaskModel, originalSendHitTask = window[globalSendHitTaskName], canSendHit = true; try { if (canSendHit) { originalSendHitTask(sendHitTaskModel); } // obfuscate  if (typeof obfuscate === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;tid\u0026#39;) \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;dict\u0026#39;) \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;stringParams\u0026#39;) \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;priceParams\u0026#39;) \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;replaceString\u0026#39;) \u0026amp;\u0026amp; obfuscate.hasOwnProperty(\u0026#39;priceModifier\u0026#39;)) { var _o_hitPayload = sendHitTaskModel.get(\u0026#39;hitPayload\u0026#39;); obfuscate.stringParams.forEach(function(strParam) { var regexParam = new RegExp(\u0026#39;[?\u0026amp;]\u0026#39; + strParam + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;g\u0026#39;); var paramsInHitpayload = _o_hitPayload.match(regexParam) || []; paramsInHitpayload.forEach(function(keyValue) { var parts = keyValue.split(\u0026#39;=\u0026#39;); var urlParts = parts[1].split(\u0026#39;%2F\u0026#39;).map(function(urlPart) { if (/https?:/.test(decodeURIComponent(urlPart))) return urlPart; return urlPart.split(\u0026#39;%20\u0026#39;).map(function(wordPart) { return obfuscate.replaceString(wordPart); }).join(\u0026#39;%20\u0026#39;); }).join(\u0026#39;%2F\u0026#39;); _o_hitPayload = _o_hitPayload.replace(parts.join(\u0026#39;=\u0026#39;), parts[0] + \u0026#39;=\u0026#39; + urlParts); }); }); obfuscate.priceParams.forEach(function(prParam) { var regexParam = new RegExp(\u0026#39;[?\u0026amp;]\u0026#39; + prParam + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;g\u0026#39;); var paramsInHitpayload = _o_hitPayload.match(regexParam) || []; paramsInHitpayload.forEach(function(keyValue) { var parts = keyValue.split(\u0026#39;=\u0026#39;); var price = parseFloat(parts[1]) || 0.00; price = (price * obfuscate.priceModifier).toFixed(2); _o_hitPayload = _o_hitPayload.replace(parts.join(\u0026#39;=\u0026#39;), parts[0] + \u0026#39;=\u0026#39; + price); }); }); _o_hitPayload = _o_hitPayload .replace( \u0026#39;\u0026amp;tid=\u0026#39; + sendHitTaskModel.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\u0026amp;tid=\u0026#39; + obfuscate.tid + \u0026#39;\u0026amp;\u0026#39; ) .replace(/[?\u0026amp;]aip($|\u0026amp;|=[^\u0026amp;]*)/, \u0026#39;\u0026#39;) .replace(/[?\u0026amp;]c[sm]=[^\u0026amp;]*/g, \u0026#39;\u0026#39;) .replace(/[?\u0026amp;]uip=[^\u0026amp;]*/g, \u0026#39;\u0026#39;); if (Math.random() \u0026lt;= 0.10) { _o_hitPayload += \u0026#39;\u0026amp;cs=\u0026#39; + obfuscate.dict[Math.floor(Math.random()*obfuscate.dict.length)] + \u0026#39;\u0026amp;cm=\u0026#39; + obfuscate.medium[Math.floor(Math.random()*obfuscate.medium.length)]; } _o_hitPayload += \u0026#39;\u0026amp;uip=\u0026#39; + (Math.floor(Math.random() * 255) + 1) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0); _o_hitPayload += \u0026#39;\u0026amp;aip=1\u0026#39;; sendHitTaskModel.set(\u0026#39;hitPayload\u0026#39;, _o_hitPayload, true); originalSendHitTask(sendHitTaskModel); } // /obfuscate  } catch(err) { originalSendHitTask(originalSendHitTaskModel); } }); }; }  That\u0026rsquo;s quite a bit of code, because it turns out that obfuscating the data consistently and taking care of all the other possible pitfalls with duplicating Google Analytics data isn\u0026rsquo;t exactly trivial.\nAnyway, to set the thing up, you\u0026rsquo;ll need to edit the configuration object within the var obfuscate = {...} block. Here are the configuration keys and how to use them. Note! All the keys are required for the solution to work. If you remove one of the keys, obfuscation will be aborted.\n   Key Initial value Description     trackingId UA-12345-1 The Tracking ID to which you want the data to be dispatched. Only one tracking ID is supported at this time.   dict ['tumble', 'noble'...] The dictionary of words that will be used. Don\u0026rsquo;t add too many (20 should suffice). When the function is initialized, it will automatically generate compound words from every item in the dictionary.   stringParams ['uid','ua'...] All the Measurement Protocol parameters that will be treated as strings and will be replaced with words in the dictionary. The parameter names are regular expression patterns.   priceParams ['tr','ts'...] All the Measurement Protocol parameters that will be treated as prices and will be modified with the priceModifier value (see below). The parameter names are regular expression patterns.   priceModifier Math.random() The modifier which will be used to modify all prices in the payload. The initial value (Math.random()) basically means that prices will be modified with a random percentage between 0.00 and 1.00.   medium ['organic', 'referral'...] The list of campaign media that will be randomly assigned to 10% of hits (to get some source/medium variance).   replaceString function Internal function, do not modify.   init function Internal function, do not modify.    You\u0026rsquo;ll want to edit trackingId at the very least. the other configurations have completely functional default values, so there\u0026rsquo;s no need to touch them unless you want to. For example, you might want to rewrite the dict to include words that actually have to do with some real industry.\nTo get most out of your data, you\u0026rsquo;ll want to add this customTask to all the hits dispatched to a Google Analytics property from your website. That way you\u0026rsquo;ll get the most comprehensive and realistic data set.\nHow it works The obfuscation itself is fairly complex.\nFirst, when tag is first run, the obfuscator is initialized. This initialization basically takes your dictionary of words, and generates a compound of every word against every other word in the dictionary. Thus the final length of the dictionary is n + n^2 squared, where n is the initial length of the dictionary. For example, if this is your initial dictionary:\n['baby', 'rock', 'sweet']\nThe final dictionary will be:\n['baby', 'rock', 'sweet', 'baby-baby', baby-rock', 'baby-sweet', 'rock-baby', 'rock-rock', 'rock-sweet', 'sweet-baby', 'sweet-rock', 'sweet-sweet']\nThe obfuscation itself is a multi-step process.\n First, all string parameters from the configuration are looped through. If a match is made in the payload, then the value of the string parameter is first turned into a Base64 representation, and then a simple algorithm is used to turn this encoded string into a number, which is then compressed into an index number of the dictionary.  obfuscate.stringParams.forEach(function(strParam) { var regexParam = new RegExp(\u0026#39;[?\u0026amp;]\u0026#39; + strParam + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;g\u0026#39;); var paramsInHitpayload = _o_hitPayload.match(regexParam) || []; paramsInHitpayload.forEach(function(keyValue) { var parts = keyValue.split(\u0026#39;=\u0026#39;); var urlParts = parts[1].split(\u0026#39;%2F\u0026#39;).map(function(urlPart) { if (/https?:/.test(decodeURIComponent(urlPart))) return urlPart; return urlPart.split(\u0026#39;%20\u0026#39;).map(function(wordPart) { return obfuscate.replaceString(wordPart); }).join(\u0026#39;%20\u0026#39;); }).join(\u0026#39;%2F\u0026#39;); _o_hitPayload = _o_hitPayload.replace(parts.join(\u0026#39;=\u0026#39;), parts[0] + \u0026#39;=\u0026#39; + urlParts); }); });  This means that every single string will have a consistent counterpart in the dictionary. Some strings will naturally return the same dictionary word, but that\u0026rsquo;s ok since we\u0026rsquo;re not going for perfect traceability here, and this will also make it even more difficult to reverse-engineer the translated strings back to their original representations.\nIf the string is found to have a / symbol, then each word separated by the slash will be translated separately. This way URLs will be kept intact. In a similar vein, if the string has http: or https:, then the protocol will not be translated, because GA requires valid URLs in certain parameters.\nFinally, if the strings are comprised of words (separated by whitespace), then each word is translated separately.\nNext, the price parameters are matched in a similar way against the hit payload. If a match is made, then the price is modified by the priceModifier from the configuration. Each price using this tracker is modified with the same modifier.  obfuscate.priceParams.forEach(function(prParam) { var regexParam = new RegExp(\u0026#39;[?\u0026amp;]\u0026#39; + prParam + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;g\u0026#39;); var paramsInHitpayload = _o_hitPayload.match(regexParam) || []; paramsInHitpayload.forEach(function(keyValue) { var parts = keyValue.split(\u0026#39;=\u0026#39;); var price = parseFloat(parts[1]) || 0.00; price = (price * obfuscate.priceModifier).toFixed(2); _o_hitPayload = _o_hitPayload.replace(parts.join(\u0026#39;=\u0026#39;), parts[0] + \u0026#39;=\u0026#39; + price); }); });  Then, the tracking ID in the payload is replaced by the one you provide in the configuration object. At the same time, the parameters aip, cs, cm, and uip (for Anonymize IP, Campaign Source, Campaign Medium, and Override IP, respectively) are removed from the payload.  _o_hitPayload = _o_hitPayload .replace( \u0026#39;\u0026amp;tid=\u0026#39; + sendHitTaskModel.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;\u0026amp;\u0026#39;, \u0026#39;\u0026amp;tid=\u0026#39; + obfuscate.tid + \u0026#39;\u0026amp;\u0026#39; ) .replace(/[?\u0026amp;]aip($|\u0026amp;|=[^\u0026amp;]*)/, \u0026#39;\u0026#39;) .replace(/[?\u0026amp;]c[sm]=[^\u0026amp;]*/g, \u0026#39;\u0026#39;) .replace(/[?\u0026amp;]uip=[^\u0026amp;]*/g, \u0026#39;\u0026#39;);  Finally, 10% of all hits are assigned a random campaign source (from the dictionary), with a random medium from the list of medium you provided in the configuration.    Also, a random IP address is generated for the hit. Yes, every hit.\nThen, the IP address is anonymized with the Anonymize IP parameter.\nif (Math.random() \u0026lt;= 0.10) { _o_hitPayload += \u0026#39;\u0026amp;cs=\u0026#39; + obfuscate.dict[Math.floor(Math.random()*obfuscate.dict.length)] + \u0026#39;\u0026amp;cm=\u0026#39; + obfuscate.medium[Math.floor(Math.random()*obfuscate.medium.length)]; } _o_hitPayload += \u0026#39;\u0026amp;uip=\u0026#39; + (Math.floor(Math.random() * 255) + 1) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0) + \u0026#39;.\u0026#39; + (Math.floor(Math.random() * 255) + 0); _o_hitPayload += \u0026#39;\u0026amp;aip=1\u0026#39;;  Modifying the IP addresses like this leads to interesting data in the list of service providers:\n  The last thing that happens is that the hit is dispatched to the Tracking ID you provided.\nsendHitTaskModel.set(\u0026#39;hitPayload\u0026#39;, _o_hitPayload, true); originalSendHitTask(sendHitTaskModel);  Caveats It\u0026rsquo;s not a perfect duplication of the data. Here are some the things the script has trouble with:\n  All campaign information from the original hit is removed. So the assignment of source/medium information will not follow the logic of the original account. To counter this, I generate a random source/medium to 10% of all hits.\n  The prices are modified with the same percentage, not the same value. Thus, if you have a Transaction Revenue of 10.00 and Product Revenue of 8.00, and the modifier is 0.8, the end result will be a Transaction Revenue of 8.00 and Product Revenue of 6.40. This means that someone could deduce what the original price was, if they assumed, for example, that Transaction Revenue is the total sum of all Product Revenus multiplied by their quantities (as it often is).\n  No integer values are modified. So Custom Metrics, Event Values, quantities, and so forth are not touched. I did this because I don\u0026rsquo;t think integers encode information that could be used to identify the original source of the data. Prices are modified because with a specific set of prices a user could guess what the origin of the data was, but not so much with integers. I\u0026rsquo;m happy to modify this in the future if enough people think it\u0026rsquo;s necessary.\n  Final thoughts Whether this solution is useful not, I can guarantee that writing it was a lot of fun! Just obfuscating the data would have been easy. Just mask every string with a random GUID or something. But trying to figure out a dictionary substitution was far more difficult.\nThe algorithm I chose (with Jaakko Ojalehto\u0026rsquo;s help) for the replacement isn\u0026rsquo;t perfect. The distribution isn\u0026rsquo;t even. But I think that\u0026rsquo;s OK. You\u0026rsquo;ll only end up with 420 words by default anyway, so there\u0026rsquo;s going to be a LOT of overlap nevertheless, since even a simple site will produce far more than 420 unique strings in the data.\n  Even if you don\u0026rsquo;t find this dataset useful, I\u0026rsquo;ll guarantee you\u0026rsquo;ll have fun looking at the string combinations produced by the replacement algorithm. In fact, I had to modify the dictionary I initially had, because it resulted in compounds like beat-child sweet-laughter which I think might raise some eyebrows when the data is displayed in a training session.\nLet me know in the comments if this solution needs improvement!\n"
},
{
	"uri": "https://www.simoahava.com/tags/training/",
	"title": "training",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/prevent-google-analytics-duplicate-transactions-with-customtask/",
	"title": "Prevent Duplicate Transactions In Google Analytics With customTask",
	"tags": ["google tag manager", "customtask", "ecommerce", "enhanced ecommerce"],
	"description": "Use customTask to prevent duplicate transactions from being recorded in Google Analytics.",
	"content": "One of the big problems in Google Analytics\u0026rsquo; data model is the immutability of historical data. Once a row of data is written into the data table, it is there practically for good. This is especially annoying in two cases: spam and bogus ecommerce hits. The first is a recognized issue with an open and public data collection protocol, the latter is an annoyance that can explode into full-blown sabotage (you can use the Measurement Protocol to send hundreds of huge transactions to your competitor\u0026rsquo;s GA property, for example).\nIn this article, I\u0026rsquo;ll tackle a different symptom. I\u0026rsquo;ve covered this before, and David Vallejo\u0026rsquo;s written an excellent article about this topic as well.\nThe problem is duplicate transactions. It\u0026rsquo;s very common if your site has a receipt page that can be revisited via the browser cache or history. By revisiting the receipt page, it\u0026rsquo;s often the case that the transaction details are written into dataLayer (or sent with the ga() method) again, resulting in the inflation of your ecommerce data.\nA simpler way to confirm if this is an issue is to create custom report where you inspect Transaction IDs against the Transactions metric. Optimally, you\u0026rsquo;d have one transaction per ID, unless you\u0026rsquo;ve decided to enable transaction updates/upgrades in your data collection.\n  I\u0026rsquo;ve decided to upgrade my solution to utilize customTask, since it lets you do without extra triggers or variable logic. We\u0026rsquo;ll set things up with Google Tag Manager, but there\u0026rsquo;s nothing stopping you from doing this with on-page analytics.js, too.\nHow it works When you add the customTask variable to your tags, it activates any time the tag tries to send a hit to Google Analytics.\nDuring this activation, the method looks for the key \u0026amp;ti in the hit model. This key corresponds to the Transaction ID value.\nNext, it looks into your browser storage for any transaction IDs already sent from the current browser.\nIf the transaction ID in the hit is found in browser storage, this customTask blocks the hit from ever being fired, this preventing the duplicated information from reaching Google Analytics.\nIf the transaction ID in the hit is not found in browser storage, the customTask sends the hit to GA normally, but it also stores the transaction ID in the list of transactions that has already been recorded. Thus, it blocks any future hits with this ID from being sent.\n  NOTE! This auto-blocking feature only works with Enhanced Ecommerce. With Standard Ecommerce, the customTask will only update browser storage but it won\u0026rsquo;t block anything. You\u0026rsquo;ll need to use triggers instead (read on for instructions how to do this).\nThe customTask variable You can use the customTask Builder tool to generate the necessary variable, or you can simply copy-paste the code below into a Custom JavaScript variable, if you wish. The benefit of using the builder tool is that you can combine this customTask with all the other setups I\u0026rsquo;ve added to the tool.\nAnyway, this is what the Custom JavaScript variable should end up looking like:\nfunction() { // customTask Builder by Simo Ahava  //  // More information about customTask: https://www.simoahava.com/analytics/customtask-the-guide/  //  // Change the default values for the settings below.  // transactionDeduper: Configuration object for preventing duplicate transactions from being recorded.  // https://bit.ly/2AvSZ2Y  var transactionDeduper = { keyName: \u0026#39;_transaction_ids\u0026#39;, cookieExpiresDays: 365 }; // DO NOT EDIT ANYTHING BELOW THIS LINE  var readFromStorage = function(key) { if (!window.Storage) { // From: https://stackoverflow.com/a/15724300/2367037  var value = \u0026#39;; \u0026#39; + document.cookie; var parts = value.split(\u0026#39;; \u0026#39; + key + \u0026#39;=\u0026#39;); if (parts.length === 2) return parts.pop().split(\u0026#39;;\u0026#39;).shift(); } else { return window.localStorage.getItem(key); } }; var writeToStorage = function(key, value, expireDays) { if (!window.Storage) { var expiresDate = new Date(); expiresDate.setDate(expiresDate.getDate() + expireDays); document.cookie = key + \u0026#39;=\u0026#39; + value + \u0026#39;;expires=\u0026#39; + expiresDate.toUTCString(); } else { window.localStorage.setItem(key, value); } }; var globalSendHitTaskName = \u0026#39;_ga_originalSendHitTask\u0026#39;; return function(customTaskModel) { window[globalSendHitTaskName] = window[globalSendHitTaskName] || customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); var tempFieldObject, dimensionIndex, count, ga, tracker, decorateTimer, decorateIframe, iframe; customTaskModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendHitTaskModel) { var originalSendHitTaskModel = sendHitTaskModel, originalSendHitTask = window[globalSendHitTaskName], canSendHit = true; var hitPayload, hitPayloadParts, param, val, regexI, trackingId, snowplowVendor, snowplowVersion, snowplowPath, request, originalTrackingId, hitType, nonInteraction, d, transactionId, storedIds; try { // transactionDeduper  if (typeof transactionDeduper === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; transactionDeduper.hasOwnProperty(\u0026#39;keyName\u0026#39;) \u0026amp;\u0026amp; transactionDeduper.hasOwnProperty(\u0026#39;cookieExpiresDays\u0026#39;) \u0026amp;\u0026amp; typeof sendHitTaskModel.get(\u0026#39;\u0026amp;ti\u0026#39;) !== \u0026#39;undefined\u0026#39;) { transactionId = sendHitTaskModel.get(\u0026#39;\u0026amp;ti\u0026#39;); storedIds = JSON.parse(readFromStorage(transactionDeduper.keyName) || \u0026#39;[]\u0026#39;); if (storedIds.indexOf(transactionId) \u0026gt; -1 \u0026amp;\u0026amp; [\u0026#39;transaction\u0026#39;, \u0026#39;item\u0026#39;].indexOf(sendHitTaskModel.get(\u0026#39;hitType\u0026#39;)) === -1) { canSendHit = false; } else if (storedIds.indexOf(transactionId) === -1) { storedIds.push(transactionId); writeToStorage(transactionDeduper.keyName, JSON.stringify(storedIds), transactionDeduper.cookieExpiresDays); } } // /transactionDeduper  if (canSendHit) { originalSendHitTask(sendHitTaskModel); } } catch(e) { originalSendHitTask(originalSendHitTaskModel); } }); }; }  There\u0026rsquo;s a configuration object in the variable transactionDeduper that you can modify, if you wish. The configuration object must have both keyName and cookieExpiresDays.\nSet keyName to what you want the name of the cookie or the localStorage key to be. The default value is _transaction_ids.\nSet cookieExpiresDays to the number of days the cookie should exist. A cookie is only used if the user\u0026rsquo;s browser doesn\u0026rsquo;t support localStorage. If localStorage is used, no expiration is set.\nThe main logic happens near the end of the code block, where I\u0026rsquo;ve boxed the solution with // transactionDeduper and // /transactionDeduper .\nIt really is very simple, and it follows the process I outlined in the previous chapter.\nIf you\u0026rsquo;re using Enhanced Ecommerce, then this customTask will take care of everything for you. In case the transaction ID is found in the list of stored IDs, it will simply block the hit from departing to Google Analytics.\nIf you\u0026rsquo;re using Standard Ecommerce, the customTask will only write the ID into storage, and you\u0026rsquo;ll need to handle the blocking logic yourself.\nIn any case, you need to add this customTask variable to all the Enhanced Ecommerce and/or Transaction tags that have the power to send purchase information to Google Analytics. For instructions how to add the customTask to your tags, see this guide or follow the instructions in the customTask Builder tool.\nTriggers and variables for Standard Ecommerce Due to how Standard Ecommerce is split into transaction and item hits, the blocking logic would be extremely difficult to automate in customTask. This is why you\u0026rsquo;ll need to create an exception trigger for your Transaction tag, which blocks the tag from firing if the transaction ID is found in the list of stored IDs.\nData Layer variable for the Transaction ID Create a Data Layer variable for transactionId, like this:\n  1st Party Cookie variable for the transaction ID list Create a 1st Party Cookie variable for the transaction ID list, using the keyName you configured in the customTask. This is what a default setup would look like:\n  Custom JavaScript variable to check if the ID is within the list Finally, create a Custom JavaScript variable which returns true if the transaction ID is found in the list.\nfunction() { // Change this to match the keyName you added to customTask:  var keyName = \u0026#39;_transaction_ids\u0026#39;; var ids = JSON.parse((!!window.Storage ? window.localStorage.getItem(keyName) : {{Cookie - _transaction_ids}}) || \u0026#39;[]\u0026#39;); return ids.indexOf({{DLV - transactionId}}) \u0026gt; -1; }  Name the variable something like {{JS - transactionId sent}}.\nThe exception trigger The last step is to create a trigger which blocks your transaction tag from firing. It must use the same event as the transaction tag. So, if the transaction tag fires on a Page View trigger, the exception trigger must also be a Page View trigger (read more about exceptions here).\nIn the trigger conditions, check if {{JS - transactionId sent}} equals true. Thus, the exception will block the tag it is attached to if the transaction ID is found in the list of IDs already recorded.\nHere\u0026rsquo;s an example of what the exception looks like with a Custom Event trigger, and how the exception is added to the tag.\n    Final thoughts My quest for improving Google Analytics\u0026rsquo; data quality using customTask continues.\nPreventing hits from being sent if certain conditions arise is nice and elegant to run through customTask, since you don\u0026rsquo;t have to mess with complicated triggers cluttering your GTM interface.\nHowever, every customTask used does increase the opaqueness of your setup, so the trade-off is that you won\u0026rsquo;t know what individual tags do at a glance without drilling into their set fields (and understanding what customTask does in the first place).\nAs always, let me know what you think about this solution in the comments, and let me know also if you have suggestions for improving it! Thank you.\n"
},
{
	"uri": "https://www.simoahava.com/tags/nodejs/",
	"title": "nodejs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/google-cloud/scrape-domain-and-write-results-to-bigquery/",
	"title": "Scrape The URLs Of A Domain And Write The Results To BigQuery",
	"tags": ["google cloud", "bigquery", "nodejs"],
	"description": "Use a Google Compute Engine virtual machine to scrape all the internal (and external) links of a given domain, and write the results to a BigQuery table.",
	"content": "In my intense love affair with the Google Cloud Platform, I\u0026rsquo;ve never felt more inspired to write content and try things out. After starting with a Snowplow Analytics setup guide, and continuing with a Lighthouse audit automation tutorial, I\u0026rsquo;m going to show you yet another cool thing you can do with GCP.\n  In this guide, I\u0026rsquo;ll show you how to use an open-source web crawler running in a Google Compute Engine virtual machine (VM) instance to scrape all the internal and external links of a given domain, and write the results into a BigQuery table. With this setup, you can audit and monitor the links in any website, looking for bad status codes or missing titles, and fix them to improve your site\u0026rsquo;s logical architecture.\nHow it works The idea is fairly simple. You\u0026rsquo;re using a Google Compute Engine VM instance to run the crawler script. The purpose here is that you can scale the instance up as much as you like (and can afford) to get the extra power you might not have with your local machine.\n  The crawler runs through the pages of the domain you specify in the configuration, and writes the results into a BigQuery table.\nThere are only a few moving parts here. Whenever you want to run the crawl again, all you need to do is just start the instance again. You won\u0026rsquo;t be charged for the time the instance is stopped (the script auto-stops the instance once the crawl is done), so you can simply leave the instance in its stopped state until you need to do a recrawl.\nYou could even create a Google Cloud Function that starts the instance with a trigger (an HTTP request or a Pub/Sub message, for example). There are many ways to skin this cat, too!\nThe configuration also has a setting for utilizing a Redis cache by way of GCP Memorystore, for example. The cache is useful if you have a huuuuuuge domain to crawl and you want to be able to pause/resume the crawl, or even utilize more than one VM instance to do the crawl.\nThe cost of running this setup really depends on how big the crawl is and how much power you dedicate to the VM instance.\nOn my own site, the ~7500 links and images being crawled take about 10 minutes on a 16 CPU, 60 GB instance (without Redis) VM instance. This translates to around 50 cents per crawl. I could scale down the instance for a lower cost, and I\u0026rsquo;m sure there are other ways of optimizing it, too.\nPreparations The preparations are almost the same as in my earlier articles, but with some simplifications.\nInstall command line tools Start by installing the following CLI tools:\n  Google Cloud SDK\n  Git\n  To verify you have these up and running, run the following commands in your terminal:\n$ gcloud -v Google Cloud SDK 228.0.0 $ git --version git version 2.19.2 Set up a new Google Cloud Platform project with Billing Follow the steps here, and make sure you write down the Project ID since you\u0026rsquo;ll need it in a number of places. I\u0026rsquo;ll use my example of web-scraper-gcp in this guide.\n  Clone the Github repo and edit the configuration Before getting things up and running in GCP, you\u0026rsquo;ll need to create a configuration file first.\nThe easiest way to access the necessary files is to clone the Github repo for this project.\n  Browse to a local directory where you want to write the contents of the repo to.\n  Run the following command to write the files into a new folder named web-scraper-gcp/:\n  $ git clone https://github.com/sahava/web-scraper-gcp.git   Next, run the command mv config.json.sample config.json while in the web-scraper-gcp/ directory.\nFinally, open the file config.json for editing in your favorite text editor. Here\u0026rsquo;s what the sample file looks like:\n{ \u0026#34;domain\u0026#34;: \u0026#34;www.gtmtools.com\u0026#34;, \u0026#34;startUrl\u0026#34;: \u0026#34;https://www.gtmtools.com/\u0026#34;, \u0026#34;projectId\u0026#34;: \u0026#34;web-scraper-gcp\u0026#34;, \u0026#34;bigQuery\u0026#34;: { \u0026#34;datasetId\u0026#34;: \u0026#34;web_scraper_gcp\u0026#34;, \u0026#34;tableId\u0026#34;: \u0026#34;crawl_results\u0026#34; }, \u0026#34;redis\u0026#34;: { \u0026#34;active\u0026#34;: false, \u0026#34;host\u0026#34;: \u0026#34;10.0.0.3\u0026#34;, \u0026#34;port\u0026#34;: 6379 } } Here\u0026rsquo;s an explanation of what the fields are and what you\u0026rsquo;ll need to do.\n   Field Value Description     \u0026quot;domain\u0026quot; \u0026quot;gtmtools.com\u0026quot; This is used for determining what is an internal and what is an external URL. The check will be a pattern match, so if the crawled URL includes this string, it will be considered an internal URL.   \u0026quot;startUrl\u0026quot; \u0026quot;https://www.gtmtools.com/\u0026quot; A fully qualified URL address that represents the entry point of the crawl.   \u0026quot;projectId\u0026quot; \u0026quot;web-scraper-gcp\u0026quot; The Google Cloud Platform project ID.   \u0026quot;bigQuery.datasetId\u0026quot; \u0026quot;web_scraper_gcp\u0026quot; The ID of the BigQuery dataset the script will attempt to create. You must follow the naming rules.   \u0026quot;bigQuery.tableId\u0026quot; \u0026quot;crawl_results\u0026quot; The ID of the table the script will attempt to create. You must follow the naming rules.   \u0026quot;redis.active\u0026quot; false Set to true if you want to use a Redis cache to persist the crawl queue.   \u0026quot;redis.host\u0026quot; \u0026quot;10.0.0.3\u0026quot; Set to the IP address via which the script can connect to the Redis instance.   \u0026quot;redis.port\u0026quot; 6379 Set to the port number of the Redis instance (usually 6379).    Once you\u0026rsquo;ve edited the configuration, you\u0026rsquo;ll need to upload it to a Google Cloud Storage bucket.\nUpload the configuration to GCS Browse to https://console.cloud.google.com/storage/browser and make sure you have the correct project selected.\n  Next, create a new bucket in a region nearby, and give it an easy-to-remember name.\n  Once done, enter the bucket, choose Upload files, and locate the config.json file from your local computer and upload it into the bucket.\n  Edit the install script The Git repo you downloaded comes with a file named gce-install.sh. This script will be used to fire up the VM instance with the correct settings (and it will initiate the crawl when started). However, you\u0026rsquo;ll need to edit the file so that it knows where to fetch your configuration file from. So, open the gce-install.sh file for editing.\nEdit the following line:\nbucket=\u0026#39;gs://web-scraper-config/config.json\u0026#39; Change the web-scraper-config part to the name of the bucket you just created. So if you named the bucket my-configuration-bucket, you\u0026rsquo;d change the line to this:\nbucket=\u0026#39;gs://my-configuration-bucket/config.json\u0026#39; Make sure the required services have been enabled in Google Cloud Platform Final preparatory step is to make sure you have the required services enabled in Google Cloud Platform.\n  Browse here, and make sure the Compute Engine API has been enabled.\n  Browse here, and make sure the BigQuery API has been enabled.\n  Browse here, and make sure the Google Cloud Storage API has been enabled.\n  Create the GCE VM instance Now you\u0026rsquo;re ready to create the Google Compute Engine instance, firing it up with your install script. Here\u0026rsquo;s what\u0026rsquo;s going to happen when you do so:\n  Once the instance is created, it will run the gce-install.sh script. In fact, it will run this script whenever you start the instance again.\n  The script will install all the required dependencies to run the web crawler. There\u0026rsquo;s quite a few of them because running a headless Chrome browser in a virtual machine isn\u0026rsquo;t the most trivial operation.\n  The penultimate step of the install script is to run the Node app containing the code I\u0026rsquo;ve written to perform the crawl task.\n  The Node app will grab the startUrl and BigQuery information from the configuration file (downloaded from the GCS bucket), and will crawl the domain, writing the results into BigQuery.\n  Once the crawl is complete, the VM instance will shut itself down.\n  To create the instance, you\u0026rsquo;ll need to run this command:\n$ gcloud compute instances create web-scraper-gcp \\  --metadata-from-file=startup-script=./gce-install.sh \\  --scopes=bigquery,cloud-platform \\  --machine-type=n1-standard-16 \\  --zone=europe-north1-a Edit the machine-type and zone if you want to have the instance run on a different CPU/memory profile, and/or if you want to run it in a different zone. You can find a list of the machine types here, and a list of zones here.\nOnce done, you should see something like this:\n  Check to see if it works First, head on over to the instance list, and make sure you see the instance running (you should see a green checkmark next to it):\n  Naturally, the fact that it\u0026rsquo;s running doesn\u0026rsquo;t really tell you much, yet.\nNext, head on over to BigQuery. You should see your project in the navigator, so click it open. Under the project, you should see a dataset and a table.\n  If you see them, the next step is to run a simple query in the query editor. Click the table name in the navigator, and then click the QUERY TABLE link. The Query editor should be pre-filled with a table query, so between the SELECT and FROM keywords, type: count(*). This is what the query should end up looking like:\n  Finally, click the Run button.\nThis will run the query against the BigQuery table. The crawl is still probably running, but thanks to streaming inserts it is constantly adding rows to the table. The query should return a result that shows you how many rows there are in the table currently:\n  If you see a result, it means the whole thing is working! Keep on monitoring the size of the table. Once the crawl finishes, the virtual machine instance will shut down, and you\u0026rsquo;ll be able to see it in its stopped state.\n  Final thoughts First of all, this was an exercise. I\u0026rsquo;m fully aware of awesome crawling tools such as Screaming Frog, which you can use to achieve very much the same thing.\nHowever, this setup has some cool features:\n  You can modify the crawler with additional options, AND you can pass flags to the Puppeteer instance running in the background.\n  Since this crawler uses a headless browser, it works better on dynamically generated sites than a regular HTTP request crawler. It actually generates the JavaScript and crawls the dynamic links, too.\n  Because it writes the data to BigQuery, you can monitor the status codes and link integrity of your website in tools like Google Data Studio.\n  Anyway, I didn\u0026rsquo;t set out to create a tool that replaces some of the stuff already out there. Instead, I wanted to show you how easy it is to run scripts and perform tasks in the Google Cloud.\nLet me know in the comments if you\u0026rsquo;re having trouble with this setup! I\u0026rsquo;m happy to see where the problem might lie.\n"
},
{
	"uri": "https://www.simoahava.com/tags/app/",
	"title": "app",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/how-to-unset-a-user-scoped-custom-dimension-in-google-analytics/",
	"title": "How To Unset A User-scoped Custom Dimension In Google Analytics",
	"tags": ["google tag manager", "google analytics", "app", "customTask"],
	"description": "By sending a valueless Custom Dimension parameter with the hit payload to Google Analytics, you can unset any previous value held by the Custom Dimension.",
	"content": "Scope in Google Analytics\u0026rsquo; Custom Dimensions refers to how the value in the Custom Dimension is extended to all hits in the same scope.\nHit- and product-scoped Custom Dimensions apply to the given hit alone - they are not extended to any other hits in the session or by the same user.\nSession-scoped Custom Dimensions apply the last value sent during the session to all the hits in that session.\nUser-scoped Custom Dimensions apply the last value set during the session to all the hits in the session AND to all future hits by the same client until a new value is sent.\nUntil now, I was under the impression that User-scoped Custom Dimensions could never be unset, because I thought it was not possible to send a \u0026ldquo;null\u0026rdquo; or \u0026ldquo;empty\u0026rdquo; value to a Custom Dimension. The analytics.js library, used by GTM for web and on-page Universal Analytics, prevents null values from being sent with Custom Dimensions (but not empty strings), so this issue never manifested itself in my implementations.\nHowever, sending hits with Measurement Protocol or by using the legacy Google Tag Manager for iOS/Android SDKs can have you run into this issue. Here\u0026rsquo;s what it looks like in reports:\n  I ran into this problem while looking at some erratic client data. For some reason, it looked like a user-scoped Custom Dimension that was set in one session was no longer available at a later session. In the image above, you can see what this looks like.\nThe data is from one single user who had two successive sessions, some four hours apart. The first session, at 10:49, had two screen views, and you could query this data with the user-scoped Custom Dimension, returning the value b.\nThen, in the latter session with just one screen view, you could no longer query this user-scoped Custom Dimension.\nHow to send a parameter without a value If you\u0026rsquo;re using Measurement Protocol, you can easily reproduce this. All you have to do is copy an existing hit from your website, which includes a user-scoped Custom Dimension, and resend it by replacing the Custom Dimension value with nothing.\nLet\u0026rsquo;s say the request originally had these parameters:\n\u0026amp;cd5=page\u0026amp;cd6=logged-in\u0026amp;cd7=this-user-id\u0026amp;cd8=hello\nHere, dimension \u0026amp;cd7 is user-scoped in Google Analytics, so if this were the last hit sent in the session, the user would have the value this-user-id until some other, valid value is sent to the Custom Dimension.\nHowever, if I now copy this hit request and replace the relevant part with this:\n\u0026amp;cd5=page\u0026amp;cd6=logged-in\u0026amp;cd7\u0026amp;cd8=hello\nYou can see how I removed the value from the parameter, but I still include the parameter in the query string (you could also use \u0026amp;cd6=logged-in\u0026amp;cd7=\u0026amp;cd8=hello and it will have the same result). Now, the value is sent to Google Analytics, and it essentially nulls the value from the GA reporting interface, meaning you no longer can query this user\u0026rsquo;s user-scoped Custom Dimension.\nIn BigQuery, this manifests as something like:\n  As you can see, there\u0026rsquo;s a bunch of Custom Dimension indices with empty strings as their values. If any one of these corresponds with a user-scoped or session-scoped Custom Dimension, it would reset any previous value this field had.\nWhere the problem lies Naturally, most people will be using GTM and GA for the web, so this problem will never manifest.\nHowever, Google Tag Manager\u0026rsquo;s legacy SDKs (iOS and Android) have a quirky feature.\nIf you\u0026rsquo;ve defined a Data Layer variable in your Google Analytics tag, and that Data Layer key is not populated with a value when the tag fires, then the SDK will automatically send the parameter without a value, thus nulling any user- or session-scoped Custom Dimensions that receive these empty parameters.\n  Here you can see an example of what a request to GA looks like when using the normal Universal Analytics tag in the legacy iOS SDK. As you can see, there\u0026rsquo;s a bunch of Custom Dimensions (circled in red) which do not have any value. These are dimensions that had no value in TAGDataLayer when the hit was dispatched.\nThis issue is not present in the latest GTM SDK (Firebase), which is a good thing, of course.\nAlso, I didn\u0026rsquo;t test if this problem is present if using the native Google Analytics SDK. I would imagine that it is, because the legacy GTM SDK uses the Google Analytics Services SDK to dispatch the hits to GA.\nWhat this means This has two implications.\nFirst, if you\u0026rsquo;re using the legacy Google Tag Manager SDKs and most likely the Google Analytics Service SDK, too, it\u0026rsquo;s possible that any user-scoped or session-scoped Custom Dimensions will have corrupt data.\nIt would be a fair assumption to make that if the key does not have a value in Data Layer, the key is not included in the hit, or that at the very least, a valueless parameter will NOT override a previously set value, but these assumptions are, based on my findings, wrong.\nThus you\u0026rsquo;ll need to rethink your approach in implementation, perhaps making sure that tags that should send user- and session-scoped Custom Dimensions do not fire unless the Data Layer variables resolve to proper values. This might mean creating new tags in your container.\nSecond, this can also be a good thing. There are times when resetting Custom Dimensions is necessary. For example, if you\u0026rsquo;re using a user-scoped Custom Dimension to store experiment information from A/B tests, you might want to reset these fields for users who have been in an earlier test but are no longer included in a currently running test. You know, just to keep the amount of noise down in your reports.\nTo do this, you could write some code which checks if the user is included in a current experiment and if not, send the dimension without a value.\nYou can do this easily in the web, too. Just set the dimension value to an empty string:\nga(\u0026#39;tracker.set\u0026#39;, \u0026#39;dimension25\u0026#39;, \u0026#39;\u0026#39;); // or  ga(\u0026#39;send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;resetDimension\u0026#39;, \u0026#39;reset\u0026#39;, {dimension25: \u0026#39;\u0026#39;});  If you\u0026rsquo;re using Google Tag Manager for web, it\u0026rsquo;s a bit more difficult since you can\u0026rsquo;t set empty fields. However, it\u0026rsquo;s easy enough to just create a Custom JavaScript variable which returns an empty string:\n  The Custom JavaScript variable would look like this:\nfunction() { return \u0026#39;\u0026#39;; }  Final thoughts I think it\u0026rsquo;s good that we have the option to reset a Custom Dimension value. And yes, I know it\u0026rsquo;s not technically resetting the value, but more about exploiting a feature of the Google Analytics reporting interface which prevents you from querying Custom Dimensions with empty values.\nHowever, I think it\u0026rsquo;s bad that the mobile SDKs (Firebase excluded) are doing this automatically. It\u0026rsquo;s always confusing when Google Tag Manager for apps diverges from how it works on the web, and this is another example of it doing so. Instead of dropping the parameter from the requests if no valid value is found, as it does on the web, the parameter is sent with an empty value.\nI don\u0026rsquo;t expect this to be fixed in the legacy containers, since they are expected to move out of the way of Firebase, and this is certainly yet another reason to upgrade your setup.\nYou should probably look through your data for this phenomenon, especially if you\u0026rsquo;re collecting app data using one of the legacy SDKs. If you have GA360 and access to the BigQuery exports, you can query for Custom Dimension fields that have an empty string as their value.\n  In the Google Analytics user interface, I originally found this problem by creating a segment where I include users that have a valid value for a user-scoped Custom Dimension but exclude sessions that don\u0026rsquo;t have a valid value.\nThis segment would thus include data from users with sessions that DO have a valid value and sessions that DON\u0026rsquo;T have a valid value. After that, it\u0026rsquo;s a question of using the User Explorer report and drilling down to individual user journeys to see if the valueless sessions came after those that had a valid value.\n"
},
{
	"uri": "https://www.simoahava.com/google-cloud/lighthouse-bigquery-google-cloud-platform/",
	"title": "Audit Multiple Sites With Lighthouse And Write Results To BigQuery",
	"tags": ["google cloud", "cloud functions", "bigquery", "nodejs", "lighthouse"],
	"description": "Set up a simple GCP Cloud Function that uses Lighthouse to periodically audit a set of URLs and writes the audit results to BigQuery and Google Cloud Storage.",
	"content": "Google Cloud Platform is very, very cool. It\u0026rsquo;s a fully capable, enterprise-grade, scalable cloud ecosystem which lets even total novices get started with building their first cloud applications. I wrote a long guide for installing Snowplow on the GCP, and you might want to read that if you want to see how you can build your own analytics tool using some nifty open-source modules.\nBut this guide will not be about Snowplow. Rather, it will tackle Google\u0026rsquo;s own open-source performance audit tool: Lighthouse.\nWith this guide, you\u0026rsquo;ll be able to build your own Google Cloud project, using Lighthouse to audit any number of sites you like, and write the results into storage and a BigQuery table!\n  You can try Lighthouse in your Chrome browser right now. Just follow the instructions here to see how you can enable the Audit tool in DevTools.\nOne of the limitations of the DevTools feature is that you can only run it on one site at a time, and only through your web browser. I wanted to build something that lets me audit a whole bunch of sites, and automate it as well. For that reason, I wrote an extremely simple Node.js app, multisite-lighthouse which fulfils these requirements. Simply by editing a configuration file and then running the script, you can run an audit against multiple sites, and the results will be written in a local folder.\nSo that was fun. But then I got a bit more ambitious. Would it be possible to run this app in a serverless cloud environment, and have the results be written in a database or storage somewhere, where they can be queried and fetched at leisure?\n  Well, sure! In this guide, I\u0026rsquo;ll walk you through how to run a Node.js application in a GCP Cloud Function, which writes the results into a BigQuery table, and stores the reports into a Cloud Storage bucket. Oh, and it\u0026rsquo;s practically free.\nHow it works First, check out my mad Powerpoint-driven design skills:\n  Let\u0026rsquo;s step through what goes on here.\n  The Cloud Function is a piece of Node.js code running in the Google Cloud. It\u0026rsquo;s called \u0026ldquo;serverless\u0026rdquo; because the setup is fully self-contained. The environment itself takes care of running the code and the infrastructure necessary to do so.\n  The Cloud Function is triggered by a message pushed into a Pub/Sub topic. Pub/Sub is short for Publish/Subscribe, which means you can publish messages into a topic, which will then alert any subscribers of that topic. In other words, it can be used as a simple real-time messaging queue, where published messages can trigger services within the Google Cloud.\n  The message itself can be published by you manually, by using the Google Cloud SDK in your local terminal, or by other applications which have been authorized to communicate with the topic. A nice use case would be to have your site\u0026rsquo;s build pipeline trigger the audit as soon as the new release of the site has been deployed.\n  The message can also be published by Cloud Scheduler, which is a Google Cloud service that lets you schedule certain actions such as the publishing of a Pub/Sub message. You can set the scheduler to trigger your Cloud Function every night, or every week, or every month, for example.\n  When the Cloud Function is triggered, it fires up an instance of the Google Chrome browser in headless mode, using a piece of software called Puppeteer.\n  The Lighthouse audit tool is run in this browser instance, and it will perform the audit on any URLs you have defined in a special configuration file.\n  Once the audit is complete, the results are written into Google Cloud Storage, which functions as a file archive in the cloud, allowing you to store and fetch items from it at will (and at low cost).\n  The results are also written into a BigQuery table. BigQuery is a scalable data warehouse, where records are stored in table format (as opposed to a more traditional relational database). Due to its flat nature, it\u0026rsquo;s fast, scalable, and extremely responsive. We don\u0026rsquo;t need its full potential in this project, but having the data stored in BigQuery opens up opportunities in e.g. Google Data Studio for visualization of the results.\n  The cost? Next to nothing. With just one or two URLs you\u0026rsquo;re facing practically zero cost. At most, it will be a handful of cents per month thanks to Google Cloud Storage.\nPreparations To be able to follow this guide, you\u0026rsquo;ll need to install some tools and create a Google Cloud Platform project.\nInstall command line tools First, install the following command line tools:\n  Node.js and npm.\n  Google Cloud SDK.\n  Git.\n  To verify you have these up and running, run the following commands in your terminal (example from OS X):\n$ node -v v11.0.0 $ npm -v 6.4.1 $ gcloud -v Google Cloud SDK 228.0.0 $ git --version git version 2.19.2 Naturally, you might see some other version numbers, and that\u0026rsquo;s OK. Just try to work with recent releases.\nSet up a new Google Cloud Platform project with Billing The other thing you\u0026rsquo;ll need to do is create a GCP project, and you\u0026rsquo;ll need to enable billing for this project. The things you build with this guide will most likely cost you absolutely nothing, or, at worst, just a handful of cents a month. But you still need to provide a credit card to enable some of the services we\u0026rsquo;ll need.\nTo set up a new project, you can follow Step 1 and Step 2 in my Snowplow + GCP guide.\nOnce you\u0026rsquo;ve created the project, make sure you copy its Project ID - you\u0026rsquo;ll need it when walking through this guide.\n  Once you\u0026rsquo;ve got the command line tools running and the GCP project created, it\u0026rsquo;s time to get to work!\nSet things up in GCP We\u0026rsquo;ll need to enable some services in your Google Cloud Platform project. Also, we\u0026rsquo;ll need create a BigQuery dataset, a storage bucket in Cloud Storage, and we\u0026rsquo;ll need to configure a Cloud Scheduler job to run the audits periodically.\nEnable the services First, make sure you\u0026rsquo;ve got the correct project selected while in your GCP console dashboard.\n  Then, go to https://console.cloud.google.com/apis/library/cloudfunctions.googleapis.com and click the blue ENABLE button to enable the Cloud Functions API.\n  Once enabled, go to https://console.cloud.google.com/apis/library/cloudscheduler.googleapis.com and click the blue ENABLE button to enable the Cloud Scheduler API.\nThe other services we\u0026rsquo;ll need (BigQuery API, Cloud Storage API, and Pub/Sub API) are already enabled in your project.\nCreate the BigQuery dataset Now, browse to https://console.cloud.google.com/bigquery, and click your project ID in the list of availabe projects.\n  We need to create a dataset which will soon contain a table where the audit results will be loaded to.\nType lighthouse as the dataset ID, like this:\n  You can set the location of the dataset to somewhere in your geographical vicinity, if you wish. Once done, click Create dataset.\nCreate the GCS bucket We\u0026rsquo;ll need to create a bucket in Google Cloud Storage. You can think of it like a storage drive, where the bucket represents a specific root directory to which the audit reports will be written.\nWe\u0026rsquo;ll also use the bucket to handle state in the Cloud Function. This is necessary to prevent unintentional retries from writing your data over and over again into BigQuery and storage.\nBrowse to https://console.cloud.google.com/storage/browser and click Create bucket.\nYou\u0026rsquo;ll need to edit some options.\n  Name: Give the bucket a name that\u0026rsquo;s globally unique. \u0026lt;yourname\u0026gt;-lighthouse-reports should work nicely. You\u0026rsquo;ll need this name when configuring the application, so you might as well make note of it now.\n  Default storage class: Choose an applicable storage class. The storage class determines how much it costs to store and query the data. You\u0026rsquo;ll be doing a little bit of fetching and a little bit of storing, so the Regional option is typically a good one to go with.\n  Location, choosing the location will impact costs and latency, so find one that suits you best. Honestly, the costs will be so low you might as well just take the one which is geographically closest to you.\n    Once happy with the settings, click Create.\nSet up the scheduled job Finally, browse to https://console.cloud.google.com/cloudscheduler and click Create job. We\u0026rsquo;ll use the Cloud Scheduler to fire up our Cloud Function periodically.\nIn the first screen, you\u0026rsquo;ll need to choose a region. You might as well choose one that\u0026rsquo;s geographically close to you. Click Next when ready.\n  Next, after some deliberation, you\u0026rsquo;ll need to give your job a name. Just use something descriptive.\nThen, you need to establish the frequency of the job. Cloud Scheduler uses UNIX\u0026rsquo; cron format, which can be a bit of a mystery, but it does have a nice logic to it. You can read how it works here, but here are some examples you can input into the field.\n  Every day at 04:00 AM (daily): 0 4 * * *\n  Every Monday at 04:00 AM (weekly): 0 4 * * 1\n  First day of every month at 04:00 AM (monthly): 0 4 1 * *\n  Next, choose a Timezone with which the \u0026ldquo;04:00 AM\u0026rdquo; part is calculated.\nThen, choose Pub/Sub as the Target of the job.\nType launch-lighthouse into the Topic field, and all into the Payload field. Don\u0026rsquo;t worry, these settings will become more apparent as we trod along.\nIn the end, the setup should look something like this:\n  Once ready, click Create.\nGood job! That\u0026rsquo;s it for the Google Cloud Platform preparations. Now we\u0026rsquo;ll need to actually build the application itself.\nBuild and configure the application In this part of the guide, you\u0026rsquo;ll install application source from my GitHub repo, update its configuration, and deploy it to GCP\u0026rsquo;s Cloud Function environment.\nClone the repo and install dependencies Open your terminal (or other command line) application, browse to a directory you\u0026rsquo;re comfortable to work with, and type the following command:\ngit clone https://github.com/sahava/multisite-lighthouse-gcp.git\nThis will download and install the source files from my GitHub repo into the directory multisite-lighthouse-gcp/ in the folder where you ran the command.\n  Next, type cd multisite-lighthouse-gcp and press enter. You should now be in the folder with the source files.\nNow, type npm install to install the dependencies of the project:\n  This will take a while, depending on your internet connection. The dependencies include some heavy pieces of software (such as puppeteer, which I use to launch a headless Google Chrome instance for the audits). Once it\u0026rsquo;s done, you should see a note that a bunch of packages were successfully installed.\nModify the configuration file While in the multisite-lighthouse-gcp directory, the next thing you\u0026rsquo;ll need to do is edit the config.json file. So open the file with the text editor of your choice. This is what the default configuration looks like:\n{ \u0026#34;source\u0026#34;: [ { \u0026#34;url\u0026#34;: \u0026#34;https://www.google.com/\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;googlesearch\u0026#34; },{ \u0026#34;url\u0026#34;: \u0026#34;https://www.ebay.com/\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ebay\u0026#34; } ], \u0026#34;projectId\u0026#34;: \u0026#34;multisite-lighthouse-gcp\u0026#34;, \u0026#34;pubsubTopicId\u0026#34;: \u0026#34;launch-lighthouse\u0026#34;, \u0026#34;datasetId\u0026#34;: \u0026#34;lighthouse\u0026#34;, \u0026#34;minTimeBetweenTriggers\u0026#34;: 300000, \u0026#34;gcs\u0026#34;: { \u0026#34;bucketName\u0026#34;: \u0026#34;lighthouse-reports\u0026#34; }, \u0026#34;lighthouseFlags\u0026#34;: { \u0026#34;output\u0026#34;: [\u0026#34;html\u0026#34;, \u0026#34;csv\u0026#34;], \u0026#34;emulatedFormFactor\u0026#34;: \u0026#34;desktop\u0026#34; } } Please see the following list of fields and possible values, and edit the configuration file accordingly.\n   Field Values Required Description     source {url, id} Yes An array of objects, each object representing one URL to audit. Type the full URL into the url field, and give a unique, descriptive id value into the respective field. For each additional URL you want to audit, add another object into the array.   projectId temp-test-simo Yes The Project ID of your Google Cloud Platform project.   pubsubTopicId launch-lighthouse Yes Set this to launch-lighthouse. It\u0026rsquo;s the name of the Pub/Sub topic that will be used to trigger the audits.   datasetId lighthouse Yes The Dataset ID of the dataset you just created in BigQuery.   minTimeBetweenTriggers 300000 Yes The minimum time in milliseconds to wait before a Pub/Sub trigger can start the Cloud Function for any given ID. This is to add some idempotency to the function (more on this below).   gcs {bucketName} Yes The name of the Google Cloud Storage bucket you created earlier.   lighthouseFlags See here. No If you want to store the reports in Google Cloud Storage, set the output field to have an array of the formats you want to write the reports in. Available values are \u0026quot;csv\u0026quot;, \u0026quot;json\u0026quot;, and \u0026quot;html\u0026quot;. For each value in this array, a report will be written for every URL audited.    The only optional field is lighthouseFlags. If you don\u0026rsquo;t set lighthouseFlags.output to an array of available filetypes, no reports will be written to Google Cloud Storage. The other field provided by default is emulatedFormFactor, which specifies that the performance audit should emulate a desktop browser.\nThe minTimeBetweenTriggers flag is used to add a wait time before you can audit any given ID again. The Cloud Functions service doesn\u0026rsquo;t guarantee that a function is run just once per trigger. This means that any code that\u0026rsquo;s run should aim to be idempotent; that is, multiple executions of the code in a short amount of time should not result in additional state changes (e.g. BigQuery loads).\nI ended up using a simple stateful mechanism in Google Cloud Storage. A file named states.json is written into the root of the GCS bucket, and this contains timestamps for when each trigger last activated its respective event. If this activation was less than 300 seconds ago (or whatever you specify in the configuration file), the function will abort.\nOnce done with the configuration, save the changes you made.\nDeploy the function and test You\u0026rsquo;re almost done! Now you\u0026rsquo;ll need to just deploy the function, and then run a simple test.\nThe Cloud Function itself is the launchLighthouse() method you can find if you browse the source code of index.js. This function is what your Pub/Sub trigger will launch when your Cloud Scheduler job goes off, or when you manually instruct the trigger to fire.\nThe Cloud Function will run in Google Cloud\u0026rsquo;s serverless environment, launching instances of headless Google Chrome to perform the audits on the URLs you provide in the configuration file. The results will then be written into a BigQuery table, as well as stored into the GCS bucket if you so wish.\nDeploy the Cloud Function While in the multisite-lighthouse-gcp folder, type the following command:\ngcloud auth login\nThis should open up a browser window where you need to enter your Google ID credentials. Make sure you log in with the same Google ID you use to manage your Google Cloud Platform project with! You\u0026rsquo;ll see that Google Cloud SDK wants to access a bunch of things in your name, so click Allow to let it do just that.\nIt\u0026rsquo;s Google - what evil could they do?\nIf successful, you should see this screen:\n  Now, open your terminal window again, and type the following command in the multisite-lighthouse-gcp folder:\ngcloud config set project \u0026lt;projectId\u0026gt;\nType your GCP Project ID in lieu of \u0026lt;projectID\u0026gt;, and press enter.\n  Now that you\u0026rsquo;ve authenticated against Google Cloud, and now that you\u0026rsquo;ve switched the project to your GCP project, it\u0026rsquo;s time to finally deploy the function with (type it all in one line):\ngcloud functions deploy launchLighthouse --trigger-topic launch-lighthouse --memory 2048 --timeout 540 --runtime=nodejs8\nIf you want to run the function in a specific region, you can add --region=\u0026lt;some GCP region\u0026gt;, e.g. --region=europe-west1 to the command.\nIt will take a minute or two for the function to be deployed to the cloud. Once done, you should see this as a result:\n  The deploy should fail in an error if you have mistakes in the configuration file config.json, so make sure you follow the instructions above for how to configure the function.\nThe initial deployment creates the launch-lighthouse Pub/Sub topic and subscriptions automatically.\nSo now that the Pub/Sub topic has been created, you can actually test the whole thing.\nTest with Pub/Sub The Cloud Scheduler will send the message all to the Pub/Sub topic launch-lighthouse using the schedule you defined earlier. The topic all will, in turn, trigger the Cloud Function you just deployed as many times as it takes to audit all the URLs in the source field of the configuration.\nYou can test this in two ways.\n  You can browse to https://console.cloud.google.com/cloudscheduler and click Run now next to the schedule job.\n  You can use the command line to publish the all message.\n  Since the first is so easy, I\u0026rsquo;ll show you how to do the second.\nOpen the terminal, and make sure you have your Google Cloud Platform set with:\ngcloud config set project \u0026lt;projectId\u0026gt;\nJust remember to replace \u0026lt;projectId\u0026gt; with your actual project ID.\nNow, run:\ngcloud pubsub topics publish launch-lighthouse --message all\n  You should see a messageIds response - that means the message was published successfully.\nCheck logs Now, browse to https://console.cloud.google.com/functions/list, where you should see details about your Cloud Function, hopefully with a green checkmark next to its name.\nClick the action menu at the end of the row, and choose View logs.\n  You\u0026rsquo;ll see a bunch of log results here. Here are the entries you should see (in approximately, but not definitely, this order):\n   Message Description     Function execution started The Cloud Function starts. If you sent the message all, it now sends a new Pub/Sub message for all the IDs in your configuration file, thus starting new Cloud Functions by itself (one for each source entry in the config).   {id}: Sending init PubSub message This is the initialization message for source ID {id}.   {id}: Init PubSub message sent The initialization message was sent successfully.   Function execution took N ms, finished with status: \u0026lsquo;ok\u0026rsquo; The Cloud Function triggered with all has now completed its task.   Function execution started The Cloud Function triggered by the initialization message now starts.   {id}: Received message to start with URL {url} Message to start audit received.   {id}: Starting browser for {url} A headless browser is started for the audit.   {id}: Browser started for {url} The browser startup is successful.   {id}: Starting lighthouse for {url} Beginning the actual audit.   {id}: Lighthouse done for {url} Lighthouse audit complete.   {id}: Browser closed for {url} Browser instance shut down.   {id}: Writing {output} report to bucket {bucketName} If you\u0026rsquo;ve defined the output Lighthouse flag, then a report for each filetype listed in output will now be written into your GCS bucket.   {id}: Writing log to bucket {bucketName} The full log of the audit is written to storage, too.   {id}: BigQuery job with ID {UUID} starting for {url} The BigQuery job starts and signals the end of the Cloud Function.   Function execution took N ms, finished with status \u0026lsquo;ok\u0026rsquo; Function complete.      If you had multiple URLs defined in the source of the configuration file, then you\u0026rsquo;ll see parallel entries for all the URLs. Each URL is treated in its own Cloud Function, so there will be a lot of overlap in the logs.\nIf you see permission errors then they are almost certainly because you\u0026rsquo;ve either forgotten to enable the required services, or because you haven\u0026rsquo;t filled in the configuration file correctly.\nCheck BigQuery Now, browse to https://console.cloud.google.com/bigquery. Here, click open your project in the left navigation, and choose the dataset you created. You should see a new table under it, named reports. Click it.\nIn the main console, you should now see a bunch of column definitions. Above those definitions is the link titled Preview, so go ahead and click it. This will \u0026ldquo;Preview\u0026rdquo; the BigQuery data stored in the data, and thus not cost you a penny.\n  Feel free to scroll around the columns.\nDo note that the BigQuery schema only contains those audits that have a \u0026ldquo;weight\u0026rdquo; in determining the total score. I did this purely out of convenience. I might update the schema at some point to simply contain all the audit fields, but this made more sense to start with.\nCheck Cloud Storage Browse to https://console.cloud.google.com/storage/browser and click the bucket name you created earlier. You might see some .appspot.com buckets there, too. Don\u0026rsquo;t worry - they were created automatically when you deployed your Cloud Function.\nYou should now see new folders within the bucket - each named after an id value in your configuration file\u0026rsquo;s source array. You\u0026rsquo;ll also see the states.json file, which makes sure your function only runs when it\u0026rsquo;s supposed to.\n  Click one of the folders to see the contents.\nIf you defined an output array in the Lighthouse flags of the configuration file, you will now see a file prefixed with report_ for each file type you added to the array. Remember, there were three possible reports Lighthouse can provide you: CSV, HTML, and JSON.\n  You\u0026rsquo;ll also see a JSON file prefixed with log_, which will contain the full audit object for you to peruse.\nFeel free to download the files and explore them. The HTML report is especially interesting (and visually pleasing) to read.\nThings to note Cloud Scheduler Don\u0026rsquo;t forget that you have a Cloud Scheduler running! Periodically, it will push the all message into your Pub/Sub topic, thus auditing each URL in your source list, writing the results into BigQuery, and storing the logs in Cloud Storage.\nManually trigger the audit You can always trigger the audit manually with\ngcloud pubsub topics publish launch-lighthouse --message all\nor, if you want to trigger an audit for a specific ID only, the command is\ngcloud pubsub topics publish launch-lighthouse --message \u0026lt;id\u0026gt;\nwhere \u0026lt;id\u0026gt; is the ID of the URL you set in the source array of the configuration file.\nCloud Functions don\u0026rsquo;t have the best performance Unfortunately, running the headless browser in the Cloud Function is not the most efficient thing in the world. In fact, the Performance report (which is, for many, the most interesting one), will most likely under-report the load times quite severely.\nThere\u0026rsquo;s not much to do about this currently. Cloud Functions simply do not have the CPU power required to perform complex or expensive tasks like this.\nThis has been identified by the developers (see here), and it\u0026rsquo;s possible that in the near future either Cloud Functions, Puppeteer, or Google Chrome will enable the processes to run smoother and faster.\nUpdating the configuration file is a bit annoying Due to how the configuration file has been bundled with the Cloud Function itself, any update to the configuration file requires you to redeploy the Cloud Function itself. This is annoying, since deployment always takes a couple of minutes.\nI\u0026rsquo;m probably going to update the application so that the configuration file is stored in a Google Cloud Storage bucket. That way when you want to update the file, all you need to do is upload it to the bucket using the Google Cloud Console user interface. And that\u0026rsquo;s really fast.\nOther ideas You can follow the GitHub project to see where things are heading. Especially the Issues page should be of interest, since it\u0026rsquo;s essentially a to-do list for me.\nFinal thoughts This was a fun exercise, and I hope you managed to complete it following the guide above. If not, please let me know in the comments where you had trouble.\nI\u0026rsquo;m not too happy with the fact that Chrome is just too slow when run through a Cloud Function. It\u0026rsquo;s possible that the whole thing is quite useless until the browser\u0026rsquo;s performance in the sandbox is improved. I might have to add the option of using an AppEngine or Kubernetes Engine instance instead. This way you could scale the environment as much as you like to improve the accuracy of the performance audit. Naturally, this will incur some costs not present in the current setup.\nOn the other hand, I love the fact that the data is being collected in BigQuery, since I can create a nice dashboard with Data Studio that shows the performance progress of the URLs I\u0026rsquo;ve selected for auditing.\nCloud Functions themselves are simply wonderful. Total game-changers. Running serverless applications with almost no cost at all, and triggering them in different ways (HTTP requests, Pub/Sub triggers) has huge potential for the future. Thanks to some background service magic, it\u0026rsquo;s so refreshing not having to worry about authentication and access control levels - the Cloud Function has all the required access levels for performing a wide variety of tasks.\nCheck the Cloud Function documentation for inspiration: https://cloud.google.com/functions/docs/tutorials/.\nAs always, please let me know in comments if you have questions about this setup, or if you have suggestions for improvement.\n"
},
{
	"uri": "https://www.simoahava.com/tags/lighthouse/",
	"title": "lighthouse",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/use-click-variables-with-element-visibility-trigger/",
	"title": "#GTMTips: Use Click Variables With Element Visibility Trigger",
	"tags": ["google tag manager", "gtmtips", "visibility", "built-in variables"],
	"description": "If you are using the Element Visibility trigger, you can use the built-in Click variables to populate data on the element that activated the trigger.",
	"content": "If you are enjoying the Element Visibility trigger as much as I am, you\u0026rsquo;ll be glad to know of a very simple tip that might make your life easier when using Google Tag Manager.\nThe tip is this: If you\u0026rsquo;ve activated the built-in Click variables, they will be automatically populated with details about the element that caused the Element Visibility trigger to activate!\nTip 92: Use Built-in variables to analyze the visible element   Yes, it\u0026rsquo;s confusing they\u0026rsquo;re still named Click variables, especially since they\u0026rsquo;re duplicated in the Form variables, and even more so since they can be used with the Element Visibility trigger to identify which element became visibility.\nBut that doesn\u0026rsquo;t matter. What matters is that this is extremely useful especially if you\u0026rsquo;ve specified a CSS selector in your Element Visibility trigger, and the selector matches more than one element.\nShort and sweet this time, have a nice week!\n"
},
{
	"uri": "https://www.simoahava.com/tags/built-in-variables/",
	"title": "built-in variables",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/visibility/",
	"title": "visibility",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/gcp/",
	"title": "gcp",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/install-snowplow-on-the-google-cloud-platform/",
	"title": "Install Snowplow On The Google Cloud Platform",
	"tags": ["google tag manager", "snowplow", "google cloud", "gcp"],
	"description": "A walkthrough for deploying the Snowplow Analytics pipeline in the Google Cloud Platform environment.",
	"content": "Last updated 18 Jan 2019: Added details about the free tier limitations, and showed how to avoid the Dataflow jobs auto-scaling out of control.\nI\u0026rsquo;m (still) a huge fan of Snowplow Analytics. Their open-source, modular approach to DIY analytics pipelines has inspired me two write articles about them, and to host a meetup in Helsinki. In my previous Snowplow with Amazon Web Services guide, I walked you through setting up a Snowplow pipeline using Amazon Web Services. This time around, I\u0026rsquo;m looking at the wondrous Google Cloud Platform, for which Snowplow introduced support in an early 2018 release.\n  I won\u0026rsquo;t be offering a comprehensive comparison between GCP and AWS, nor will I walk you through every single possible customization you could do when firing up the instances and building the pipeline. Those are left for you to discover by yourself, or to consult a data engineer who can help you with scale.\nIn fact, the setup I\u0026rsquo;ll walk you through will be suboptimal in many places. It won\u0026rsquo;t be the most robust setup for a large flow of data through the pipeline, but what it will provide you is the comprehensive list of steps you need to make to get things running.\nNote that Snowplow also offers their own Snowplow Insights service for setting up and managing the pipeline so that you can jump straight into data collection and analysis. I really recommend this service especially if you are unsure about how to set up the pipeline in an economical and scalable way. You might accrue a lot of extra costs if you don\u0026rsquo;t know how to scale the required resources to suit what you actually need for efficient analysis.\n  You can also follow the official Snowplow wiki, if you wish. The steps are similar to what I go through in this guide, though I do think this guide will be better suited for you in case you are unfamiliar with how GCP works.\n  My Powerpoint visualization skills just get better and better each passing year!\nWord of warning This probably won\u0026rsquo;t come as a surprise to you, but working with a cloud platform is not free of charge. If you\u0026rsquo;re a first-time user, you should be eligible for the free trial of $300 to be spent during the first 12 months. This is more than enough to build a pipeline and test it out over a number of days, but it won\u0026rsquo;t get you far.\n UPDATE 18 Jan 2019: The free tier only gives you 8 CPUs total across your entire project. That means that you must keep count of all the instances you\u0026rsquo;ll be using for the collector and for the ETL machine. Start with a low number, e.g. 1 or 2 CPUs per instance to see how Snowplow works. Once you have the budget, you can shoot for more ambitious setups.\n   You\u0026rsquo;ll need to keep a close eye on the billing dashboard of your project. It helps you get an idea of how much cost you have factually accumulated to this day, and what the projected cost for the whole month will be.\nWhat you\u0026rsquo;ll need To follow this guide, you will need:\n  A Google account - something to log into Google services with, such as your Google Mail ID.\n  A credit card - you\u0026rsquo;ll need to enable billing in your Google Cloud account to fire up some of the services we need.\n  A custom domain name - this is optional, but it\u0026rsquo;s necessary if you want to set up the tracker as a secure HTTPS endpoint. Without a custom domain name, you\u0026rsquo;re forced to use HTTP only. You can get a cheap domain name from Google Domains. You might be able to set something up for free using Cloud Endpoints, but this guide will not cover this.\n  Good luck! Let me know in the comments if some part of this guide was particularly unclear.\nFirst steps Here we\u0026rsquo;ll setup the GCP project, make sure you have the necessary resources enabled, and we\u0026rsquo;ll also install the Google Cloud SDK so that you can interact with your project via your local terminal, too.\nStep 1: Set up a new project The first thing you\u0026rsquo;ll need to do is set up a new Google Cloud project.\n(1) Browse to https://console.cloud.google.com/cloud-resource-manager/. Remember to login with the Google ID which you\u0026rsquo;ll use to manage this project. Click CREATE PROJECT.\n   If this is the first time you use GCP with this account, you might see an offer for free credit. Take it!\n (2) Give the project a name.\nFor convenience, it might be good to change the Project ID to something more legible and easy to understand.\nIf you have Organizations and Locations (such as folders) set up for your Google Cloud account, choose the appropriate ones from the menu.\nAlso, if you already have a Billing Account set up, choose that as well from the respective drop-down menu.\n  Choose CREATE when you\u0026rsquo;re done.\nAfter clicking the button, GCP will do some loading and spinning for a while, after which you should see your new project dashboard. If you don\u0026rsquo;t, make sure to select it from the project selector menu.\n  Congratulations! You\u0026rsquo;ve created the project.\nStep 2: Enable Billing You\u0026rsquo;ll need to create a billing account so that GCP can invoice you if necessary (sucks, I know).\n(1) Browse to https://console.cloud.google.com/billing.\n(2) Click Add billing account.\n  Here, follow the steps to add your credit card, or, if you\u0026rsquo;re eligible for the free trial, to activate your free trial.\nMake sure you follow the prompts to link your Snowpow project to the Billing account you just created.\nStep 3: Enable the required services Next, you need to enable the services and APIs we\u0026rsquo;ll need to get started. These steps let you set up the collector. When you move to the enrich and BigQuery load steps, you\u0026rsquo;ll need to enable additional services.\n(1) Browse to https://console.cloud.google.com/apis/library.\nYou should see a search bar, so start by searching for Compute Engine API and click the relevant result. We\u0026rsquo;ll need the Compute Engine to fire up our virtual machines on which the Snowplow collector will reside.\n  Upon entering the Compute Engine API page, simply click the blue ENABLE button at the top.\n  Once it\u0026rsquo;s done, you should see something like this where the Enable button used to be:\n  Now, follow these exact same steps for the Cloud Pub/Sub API. Pub/Sub (for Publisher/Subscriber) is a real-time message queue we\u0026rsquo;ll use to process the data fed into the Snowplow pipeline.\n  Once you\u0026rsquo;re done enabling these services, you\u0026rsquo;re ready to move onto the next step!\nStep 4: Install the Google Cloud SDK The next thing you\u0026rsquo;ll want to do is to install the Google Cloud SDK locally in your machine. It\u0026rsquo;s very convenient because it lets you access your GCP project from the command line, and it lets you test some of the services from your local machine rather than having to find the appropriate paths through the often confusing UI.\nTo install the SDK, follow the steps for your platform, starting from here.\nOnce you\u0026rsquo;ve installed the SDK, you should be able to run these commands in your terminal / shell:\n$ gcloud auth login $ gcloud config set project PROJECT_ID The first command logs you in with Google Cloud using the Google Account you choose in the web prompt. The second command points the current gcloud setup to the project ID you\u0026rsquo;ll give (replace PROJECT_ID with the ID you configured when you created the project). The end result should be something like this:\n  Step 5: Setup a service account The final initialization step is to setup a service account. A service account is basically a Google Cloud account which has full access to your GCP services and resources. It\u0026rsquo;s a necessary step if you want to run services on GCP programmatically rather than with your own, personal Google account.\n Note that when you enable Compute Engine API for your project, a Compute Engine default service account is created for you automatically. the steps below really only apply if you wanted to create a different service account for your pipeline.\n (1) Browse to https://console.cloud.google.com/apis/credentials.\n(2) Make sure you have the correct project selected in the project selector menu.\n(3) Click the blue Create credentials selector, and choose Service account key from the menu.\n  Since you\u0026rsquo;ve enabled the Compute Engine API, you should be able to choose the default GCE service account from the drop-down.\n  Keep the JSON option selected, and choose Create.\nThe browser should automatically download the JSON file, so make sure you find it in your local files and store it securely.\n At this point, it would be a good idea to create a local folder for this entire Snowplow experiment. In that local folder, you can store this service account key and any temporary configuration files you\u0026rsquo;ll work on, and other stuff such as executables and binaries you\u0026rsquo;ll eventually upload to the GCP, if necessary.\n Once you\u0026rsquo;ve created the service account you are ready to move on. All the initial steps have now been completed, congratulations!\nSetting up the Pub/Sub topics   Snowplow uses a collector as the endpoint of your tracker requests. This collector resides in a Compute Engine instance (a virtual machine, basically), and it processes the requests you send to the endpoint.\nThese requests are then published into Pub/Sub topics, from where they are then passed on further down the pipeline for enrichment and parsing. Pub/Sub is essentially a real-time messaging pipeline, which collects messages in topics, which are then available for subscriptions to access.\nMake sure you check out Snowplow\u0026rsquo;s official wiki for more information on this part of the pipeline.\nStep 1: Create the Pub/Sub topics The first thing you\u0026rsquo;ll need to do is create topics in the Pub/Sub you\u0026rsquo;ve enabled for your project.\n(1) Browse to https://console.cloud.google.com/cloudpubsub/topicList.\n(2) Make sure you have the correct project selected in the project selector.\n(3) Click the blue Create a topic button.\n  (4) Type good after the project path as in the screenshot below, and click CREATE.\n  (5) Click CREATE TOPIC and type bad as the name of the new topic, and then click CREATE.\n  Snowplow uses these two topics to filter out hits and requests that validate (good stream) and those that have issues and errors (bad stream).\nStep 2: Create a subscription (optional) Next, you can create a subscription with which you can test the stream. This is optional, but might be a good idea if you want to see how the pipeline is working.\n(6) Click the good topic to enter its configuration page.\n(7) Choose CREATE SUBSCRIPTION from the top navigation.\n  (8) Give the subscription a name (test-good).\n(9) Leave the other settings as they are, and click CREATE.\n  Now that you\u0026rsquo;ve created the Pub/Sub topics, it\u0026rsquo;s time to step into one of the more complicated steps of this guide: setting up the collector itself.\nCreate an HTTP endpoint with the Scala Stream Collector   Snowplow uses a collector written in Scala for processing the requests sent from your website. These records are parsed by the Scala Stream Collector and then distributed into the Pub/Sub topics you created in the previous chapter.\nWe\u0026rsquo;ll start with a simple HTTP endpoint using the default IP address assigned to the Compute Engine instance you\u0026rsquo;ll spin up. This is only to test that the whole pipeline works. You\u0026rsquo;ll want to configure your own custom domain name with an HTTPS endpoint for the actual tracker!\nStep 1: Create the config file In the local directory where you\u0026rsquo;re storing all your project files (such as the service account credentials you created earlier), create a new file named application.config, and copy-paste the contents of this sample config within.\nAt this time, the only line you need to edit is the one with googleProjectId = your-project-id. Change your-project-id to your actual Google Cloud project ID.\nKeep this file at hand, because you\u0026rsquo;ll need to make some changes to it soon.\nStep 2: Fire up a GCE instance You are now ready to start a Compute Engine instance.\n NOTE! Because this is just a test - remember to shut down the instance as soon as you\u0026rsquo;re done so that you don\u0026rsquo;t accumulate extra costs.\n (1) Browse to https://console.cloud.google.com/compute/instances and make sure your project is selected in the project selector.\n(2) Click Create.\n  Next, you\u0026rsquo;ll be transported to the configuration screen.\n(3) Give the instance a name.\n(4) Choose a region (somewhere close by, preferably) - you can use the default zone.\n(5) Make sure the service account you\u0026rsquo;ve created (or the default Compute Engine service account) is selected in the relevant list.\n(6) Choose Set access for each API from the scope selector.\n  (7) Scroll down the list of APIs, and choose Enabled for the Cloud Pub/Sub API.\n  (8) Scroll down to Firewall and check Allow HTTP traffic. Next, click the Management, security, disks, networking, sole tenancy link.\n  (9) Select the Networking tab, and type collector into the Network tags field.\n  Once you\u0026rsquo;ve done these changes, click Create to fire up the instance!\nStep 3: Create a firewall rule You\u0026rsquo;ll need to create a firewall rule which accepts incoming connections from your website.\n(1) Browse to https://console.cloud.google.com/networking/firewalls/list and make sure the correct project is selected.\n(2) Click CREATE FIREWALL RULE.\n  (3) In the configuration that opens, give the rule some name, e.g. snowplow-firewall-rule.\n(4) Scroll down to Target tags and type in the tag name you gave in the end of the previous step (collector if you used my example).\n(5) Make sure IP ranges is selected in the Source filter menu, and type 0.0.0.0/0 into the Source IP ranges field.\n(6) Under Protocols and ports, check tcp and type 8080 as the port value.\n  When ready, click Create to finalize the firewall setup.\n  Step 4: Create a storage bucket for your configuration file Next thing you\u0026rsquo;ll need to do is upload the configuration file for the collector into a Cloud Storage bucket. This is because the file needs to be available for your instance to use, and it\u0026rsquo;s extremely convenient to have the file available in Google Cloud, because the service account can simply pull the file directly from the bucket.\n(1) Browse to https://console.cloud.google.com/storage/browser, and, as always, make sure you have the right project selected before clicking Create bucket.\n  (2) Give the bucket a descriptive (and unique) name, such as snowplow-yourname-collector-bucket.\n(3) You can leave the rest of the settings with their default values, then click Create.\n  (4) In the view that opens up, click Upload files, find the application.config file you downloaded earlier, and upload it to the bucket.\n  Good job! You are now ready to connect your GCE instance and fire up the collector!\nStep 5: SSH into the Compute Engine instance Now that you have the virtual machine running in the cloud, and you have the configuration file uploaded to a Cloud Storage bucket, the next step is to connect to the virtual machine, download all the remaining files, and start the collector.\n It\u0026rsquo;s very simple and trivial to do the following steps using the Google Cloud SDK on your local machine, too. If you want to try it out, follow the relevant steps in Snowplow\u0026rsquo;s wiki.\n (1) Browse to https://console.cloud.google.com/compute/instances.\n(2) Click the SSH option next to your instance.\n  (3) Visit https://dl.bintray.com/snowplow/snowplow-generic/ and find the file that starts with snowplow_scala_stream_collector_google_pubsub_ and check what the latest version number is that doesn\u0026rsquo;t have the rcl suffix. Make note of this version number (e.g. 0.14.0), no need to download the file.\n  (4) Next, in the SSH window, run the following commands in order, pressing enter after each command.\n$ sudo apt-get update $ sudo apt-get -y install default-jre $ sudo apt-get -y install unzip $ wget https://dl.bintray.com/snowplow/snowplow-generic/snowplow_scala_stream_collector_google_pubsub_\u0026lt;VERSION\u0026gt;.zip $ gsutil cp gs://\u0026lt;YOUR-BUCKET-NAME\u0026gt;/application.conf . $ unzip snowplow_scala_stream_collector_google_pubsub_\u0026lt;VERSION\u0026gt;.zip $ java -jar snowplow-stream-collector-google-pubsub-\u0026lt;VERSION\u0026gt;.jar --config application.conf Replace all instances of \u0026lt;VERSION\u0026gt; with the latest version of the collector ZIP file which you checked in (3).\nReplace \u0026lt;YOUR-BUCKET-NAME\u0026gt; with the name you gave the cloud storage bucket in the previous step (e.g. snowplow-yourname-collector-bucket).\n  After running the last command, if all has been configured correctly, you should see the following output in the instance shell:\n  You are now ready to send a test request to the endpoint, after which you can check your Pub/Sub subscription if it received the message!\nStep 6: Send a test request and verify it was published into Pub/Sub To send the request, you\u0026rsquo;ll first need to check what the external IP of your Cloud instance is.\n(1) Browse to https://console.cloud.google.com/compute/instances and click the Snowplow instance name.\n(2) Copy the IP address from the External IP field.\n  The next step requires you to send an HTTP POST request with a specific payload to this IP endpoint. There are many ways to do it - you could use the JavaScript console of the web browser, for example.\nMy preference for testing HTTP endpoints quickly is to use curl, which is a command-line tool available in almost any terminal. If you\u0026rsquo;re using an operating system that doesn\u0026rsquo;t come equipped with curl, I recommend downloading and installing it from here.\n(3) Open the terminal on your local machine, and copy-paste the following command, switching \u0026lt;EXTERNAL_IP\u0026gt; with the actual external IP you copied from the GCE instance settings.\n$ curl -d \u0026#34;\u0026amp;e=pv\u0026amp;page=curl-test\u0026amp;url=http%3A%2F%2Fjust-testing.com\u0026amp;aid=snowplow-test\u0026#34; -X POST http://\u0026lt;EXTERNAL_IP\u0026gt;:8080/com.snowplowanalytics.iglu/v1 The payload is very simple - it basically has only four parameters:\n   Parameter Value Description     e pv Event type, Pageview in this case.   page curl-test Page name.   url http://just-testing.com Source URL.   aid snowplow-test Application ID.    The endpoint is suffixed with /com.snowplowanalytics.iglu/v1 to denote the schema you\u0026rsquo;ll be using for processing and validating the incoming data. Since we\u0026rsquo;re working with the most out-of-the-box solution available for testing purposes, you can use this default schema for now.\nLater, we\u0026rsquo;ll use the Google Analytics plugin to simplify things and to send a more complete payload, but for the purposes of testing this will do.\n(4) If everything worked, you should see a status code OK as a response.\n  Now you can test if your record was published in the good Pub/Sub topic. To test this, you need to pull the recent records from the topic by using the subscription you created earlier. The easiest way to do this is by using the Google Cloud SDK that you should now have installed and configured on your local machine.\n(5) The command you\u0026rsquo;ll need to use is:\n$ gcloud pubsub subscriptions pull --auto-ack test-good Where test-good is the name you gave to the subscription when you created it. If it worked, you should see the following output:\n  Don\u0026rsquo;t worry about the fact that it looks all garbled. This is the request stored in binary Thrift format. You should see all sorts of interesting bits and pieces such as your local machine IP address, the User-Agent string (just curl for now), and the data payload itself. If you see all this in the good Pub/Sub topic, it means everything is working!\nYou have now successfully created a collector behind an HTTP endpoint. You could proceed to the enrichment stage, but I urge you to first stop the GCE instance so you don\u0026rsquo;t accumulate extra costs for having it running.\nI also recommend you continue with the next chapter, where you\u0026rsquo;ll learn how to use your own custom domain as an HTTPS-secured endpoint for the collector. Using HTTP is crippling, and you can\u0026rsquo;t really create a production-ready endpoint with a single GCE instance behind the HTTP protocol.\nCreate an HTTPS endpoint with a custom domain name This step replaces the previous chapter, effectively. Instead of using an ephemeral, external IP behind the HTTP protocol, we\u0026rsquo;ll assign a static IP address to our virtual machine instance. On top of that, we\u0026rsquo;ll configure a custom domain name to point to this static IP, and we\u0026rsquo;ll have everything work behind the HTTPS protocol.\nYou\u0026rsquo;ll still want to read the previous chapter, though. We\u0026rsquo;ll be doing a lot of similar things here.\nIMPORTANT! If you want to skip reading the previous chapter, then please note that you must do steps (1) and (4) from the previous chapter before moving on with the HTTPS endpoint.\nThis is, basically, what the final collector product should look like. You\u0026rsquo;re using a load balancing system to automatically scale the instances with incoming traffic, AND you\u0026rsquo;ll be able to avoid pesky cross-protocol errors due to using HTTPS as the sole endpoint. There\u0026rsquo;s the added security, too.\nStep 1: Create an instance template Instead of working with a single GCE instance, we\u0026rsquo;ll use a cluster of instances that will be scaled up and down automatically with traffic. This will, naturally, reflect on the cost structure of your monthly GCP invoices, so keep an eye on the estimated charges in your dashboard.\n(1) If you followed the previous chapter, you can go ahead and delete the GCE instance and the Firewall Rule. You\u0026rsquo;ll start from scratch here.\n(2) You can keep the application.conf file as it is in the storage bucket. The same settings you used in the previous chapter apply here.\n(3) Go to https://console.cloud.google.com/compute/instanceTemplates/list and click Create instance template.\nThe instance template is what each new virtual machine will use as its \u0026ldquo;template\u0026rdquo;, meaning it will inherit the settings from this template as well as the startup script that will fire up the collector itself.\nThe steps for the instance template are almost the same as for a single GCE instance from the previous chapter, but let\u0026rsquo;s go over them anyway.\n(4) Give a descriptive name for the instance.\n(5) Make sure the default Compute Engine service account is selected.\n(6) Choose Set access for each API in the Access Scopes list.\n  (7) Scroll down the list to Cloud Pub/Sub and choose Enable.\n(8) Under Firewall, select Allow HTTP traffic.\n(9) Expand the Management, security, disks, networking, sole tenancy accordion.\n  (10) Scroll down to Startup script, and copy-paste the following code within:\n#! /bin/bash sudo apt-get update sudo apt-get -y install default-jre sudo apt-get -y install unzip archive=snowplow_scala_stream_collector_google_pubsub_\u0026lt;VERSION\u0026gt;.zip wget https://dl.bintray.com/snowplow/snowplow-generic/$archive gsutil cp gs://\u0026lt;YOUR-BUCKET-NAME\u0026gt;/application.conf . unzip $archive java -jar snowplow-stream-collector-google-pubsub-\u0026lt;VERSION\u0026gt;.jar --config application.conf \u0026amp; (11) Edit the two instances of \u0026lt;VERSION\u0026gt; with the latest version number you can find for the snowplow_scala_stream_collector_google_pubsub_ prefix here (don\u0026rsquo;t use the version with the _rcl prefix). At the time of writing, the latest version was 0.14.0.\n(12) Replace \u0026lt;YOUR-BUCKET-NAME\u0026gt; with the name of the Cloud Storage bucket that stores the application.conf file.\n  This startup script is run whenever a new instance is built with this template. It loads all the dependencies and the runs the collector Java file.\n(13) Click open the Networking tab, scroll to Network tags, and add collector as a tag.\n  (14) Click Create when done.\n  Step 2: Create a firewall rule You\u0026rsquo;ll need to create a Firewall rule to allow external connections to your GCE instances.\n(1) Browse to https://console.cloud.google.com/networking/firewalls/list, and click CREATE FIREWALL RULE.\n(2) Give the rule a name.\n(3) Scroll down to Target tags, and type collector in the field.\n(4) In Source IP ranges, type 0.0.0.0/0.\n(5) Check the box next to TCP in the Protocols and ports selection (with Specified protocols and ports selected), and type in 8080.\n  (6) Click Create when ready.\n  Step 2: Create an instance group Next, we\u0026rsquo;ll need to create an instance group, which is used by the load balancer as the backend service.\n(1) Scroll to https://console.cloud.google.com/compute/instanceGroups/list, and click Create instance group.\n(2) Give the group a descriptive name.\n(3) Feel free to set the Location closer to home, if you wish.\n  (4) Select the instance template you created in the previous step from the Instance template selector.\n   UPDATE 18 Jan 2019: You can also set Autoscaling Off. This means that the instance will not generate new machines automatically. You can start off with this, and then move to an autoscaling setup if you find the Collector lagging behind a lot.\n (5) Scroll down to Health check, click the menu, and select Create a health check.\n  (6) Set the following options:\n Name: a descriptive name Protocol: HTTP Port: 8080 Request path: /health Check interval: 10 seconds Unhealthy threshold: 3 consecutive failures  You can leave the rest of the settings to their default values.\n  (7) Click Save and continue once finished with the health check settings.\n(8) Click Create to finish creating the instance group.\n  Once you create the group, you should see it loading for a while, after which you will see that the group has been created and a new instance has already been fired up!\nAt this point, you can quickly test if the instance is working.\n(9) Browse to https://console.cloud.google.com/compute/instances, and copy the IP address from the External IP field next to the instance created from your instance group.\n  (10) Open your terminal software and run the following command:\n$ curl http://\u0026lt;EXTERNAL_IP_HERE\u0026gt;:8080/health Replace \u0026lt;EXTERNAL_IP_HERE\u0026gt; with the IP address you copied.\nYou should see a OK message as the response.\nNext, try:\n$ curl http://\u0026lt;EXTERNAL_IP_HERE\u0026gt;:8080/i You should see a garbled GIF response (something like GIF89a????!?,D;).\nIf you see those two, your instance is working, and you\u0026rsquo;ve successfully created an HTTP endpoint.\nHuh? That\u0026rsquo;s exactly what you did in the previous chapter!\nBut by using Instance groups you are now ready for the biggest step here: creating a load balancer.\nStep 3: Create a load balancer The purpose of the load balancer is to automatically scale your system up and down, depending on things like traffic spikes and CPU usage. It works by establishing a single IP address in the public-facing internet, which then tunnels/proxies the traffic to all the necessary internal IP addresses (your GCE instances, basically), without the outside world knowing this.\nThe other benefit of the load balancer is that we can use it to assign a static IP address to the balancer, and with a static IP we can apply our custom domain name so that a Google-managed SSL certificate can be applied to the endpoint.\nSo, there\u0026rsquo;s a lot of stuff in this step, pay attention!\n(1) Browse to https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list, and click Create load balancer.\n(2) Click Start configuration in the HTTP(S) Load Balancing panel.\n  (3) Give the load balancer a name, click Backend configuration, then Create or select backend services \u0026amp; backend buckets, then choose Backend services -\u0026gt; Create a backend service.\n  (4) Give the backend service a name.\n(5) Leave all the other settings to their default values, but choose the Instance group you created earlier from the respective menu. Set 8080 in the Port numbers field, and select the Health check you\u0026rsquo;ve also created earlier in the Health check menu.\n  (6) Click Create when ready.\n  (7) Click open Host and path rules, and make sure the backend service you created is visible in the rule.\n  (8) Next, click Frontend configuration.\n(9) Give the frontend service a name.\n(10) Choose HTTPS as the Protocol.\n(11) Select the IP Address list, and click create IP address.\n  (12) Choose a name for the IP address and click RESERVE.\n  (13) Make sure the new IP address is selected in the frontend configuration, and copy the IP address to a text editor or something. You\u0026rsquo;ll need it when configuring the DNS of your custom domain name!\n(14) Make sure 443 is set as the Port.\n(15) In the Certificates menu, choose Create a new certificate.\n  (16) Give the new certificate a name.\n(17) Choose Create Google-managed certificate.\n(18) Type the name of the domain you will set to point to your collector in the Domains field.\n(19) When ready, click Create.\n  (20) When finished with the frontend configuration, click Done.\n  (21) Click Review and finalize to see all the changes. You\u0026rsquo;ll probably see that your certificate is still in the PROVISIONING status, because you haven\u0026rsquo;t updated your domain DNS yet.\n  (22) When satisfied, click Create to create the load balancer.\nStep 4: Configure your DNS At this point, you need to go to your DNS settings for the custom domain name you want to point to your load balancing collector.\nYou need to create an A record for the domain name, which points to the static IP address you created for the load balancer frontend above. You can set the TTL to something like 600 seconds to see the change quicker.\nThis is what it would look like in GoDaddy.\n  Once you\u0026rsquo;ve configured the DNS, it will take a while for the new name to resolve. You can check what the status is by running this in your terminal:\n$ host tracker.gtmtools.com Replace tracker.gtmtools.com with the domain name you configured in the DNS. If it works, you should see a response with the IP address you configured.\n  Step 5: Testing everything Once you\u0026rsquo;ve configured the domain name and it resolves to the correct IP, you might still need to wait a while for the SSL certificate to be provisioned for the domain name. This might take anywhere from a couple of minutes to some hours, so be patient.\nYou can check the status of your system by browsing to the load balancer list and clicking the load balancer you created.\nThe frontend should show a green checkmark next to your certificate name to indicate that the certificate has been assigned.\nIn the backend service, you should see one healthy instance created.\n  If all is fine, the next step is to test the endpoint itself. Open the terminal and run the following command:\n$ curl -d \u0026#34;\u0026amp;e=pv\u0026amp;page=https-test\u0026amp;url=https%3A%2F%2Fjust-testing.com\u0026amp;aid=snowplow-test\u0026#34; -X POST https://\u0026lt;CUSTOM_DOMAIN\u0026gt;/com.snowplowanalytics.iglu/v1 Replace \u0026lt;CUSTOM_DOMAIN\u0026gt; with the domain name you\u0026rsquo;ve configured to point to the load balancer.\nIf it worked, you should see an OK response.\n  Next, you can try and retrieve the payload from the Pub/Sub topic with:\n$ gcloud pubsub subscriptions pull --auto-ack test-good You should see something like this as the response:\n  If you see that, then congratulations, you now have an HTTPS endpoint for your collector!\nIf something\u0026rsquo;s wrong, then you\u0026rsquo;ll need to start narrowing down where the problem is. Check your Google Compute Engine pages, make sure there\u0026rsquo;s a healthy GCE instance running. You can even SSH into the instance, browse to the /var/log/ directory, and open the daemon.log file for editing to see if there was a problem with the startup script.\nOther than that, it\u0026rsquo;s very difficult to say what the likely source of the problem is. Personally, I made many mistakes initially with port configurations and setting up the individual components. But if you follow this guide to the letter, you should be fine.\nSometimes the problem is that you just need to wait for the DNS to resolve and the certificate to be assigned to the new domain name. This might take a while.\nNext up, enrichment and loading the data into BigQuery!\n Note! If you\u0026rsquo;re going to take a break now, remember to STOP your GCE instance. You can always restart it when ready to go on. You don\u0026rsquo;t want to accumulate any unwanted costs from having the instance running for no purpose!\n Set up the Google Analytics tracker   Before we go on to enrichment, now is a good time to set up the tracker. For this purpose, we\u0026rsquo;ll be using the Snowplow Google Analytics plugin, because it\u0026rsquo;s an easy way to leverage existing tracking on your site. If you want, feel free to use the regular Snowplow JavaScript tracker.\nTo begin with, head on over to my customTask Builder Tool.\n(1) Click the option labelled Copy Hits to Snowplow Collector Endpoint.\n(2) Click Copy to clipboard.\n  (3) Next, in Google Tag Manager, create a new Custom JavaScript variable, and paste the contents of the clipboard there.\n(4) In the beginning of the code block, remove var _customTask = , so that the first characters of the block are function() {.\n(5) In the end of the block, remove the semicolon that is the very last character of the block.\n(6) Edit the line starting with var snowplowEndpoint = '...'; so that the string contains the URL to your collector endpoint.\n  (7) Save the variable with some name, e.g. JS - Snowplow duplicator.\n(8) Next, open your Page View tag, check Enable overriding settings in this tag, scroll down to More Settings \u0026gt; Fields to set, and add a new field:\nField name: customTask\nValue: {{JS - Snowplow duplicator}}\n  That\u0026rsquo;s it for the tracker. The way it works now is that whenever your Page View tag fires, it will copy the payload to the Snowplow collector endpoint.\nIf you want, you can publish your container now, after which every visitor to your website will start sending those Page Views to your collector. However, I recommend you use Preview mode for now, so that only your Page Views are sent to the collector.\nOnce the pipeline is up and running, you can start collecting more comprehensive data from your visitors.\nPrepare the ETL step   Before moving forward to the extract, transform, and load (ETL) of your collector data, we\u0026rsquo;ll need to do some preparations.\nStep 1: Enable the Dataflow API The enrichment process and the BigQuery loader require a new service to be enabled.\n(1) Browse to the API library.\n(2) Search for dataflow, and click the Dataflow API selector.\n  Cloud Dataflow lets you enrich a data stream with minimal latency. This is exactly what we need. We need the enricher to pull in events for the Pub/Sub topic to which the collector writes them, enrich and shred them to proper format, and write them back into a Pub/Sub topic for BigQuery loading.\n(3) Click the ENABLE button in the API page to enable this service.\nStep 2: Create the necessary Pub/Sub topics and subscriptions By now, you should have two Pub/Sub topics, good and bad for hits processed by your collector.\nWe\u0026rsquo;ll need to create a bunch of additional topics and subscriptions for the remaining steps of the pipeline.\n(1) Browse to https://console.cloud.google.com/cloudpubsub/topicList.\n(2) Click open the good topic, and click Create Subscription. Give the subscription the name good-sub.\n  (3) Next, create the following topics:\n bq-bad-rows bq-failed-inserts bq-types enriched-bad enriched-good  (4) Then, click open bq-types and create a new subscription for it named bq-types-sub.\n(5) Finally, click open enriched-good and create a new subscription for it named enriched-good-sub.\nThis is what the topic list should look like:\n  This is what the subscription list should look like:\n  You can, of course, create additional subscriptions for testing and debugging, but these are what the following steps of the pipeline specifically need.\nStep 3: Create a new storage bucket for temporary files You\u0026rsquo;ll need to create a new storage bucket for temporary files created by the enrichment process. But this is a good place to also store some files required by the enrich and load stages.\n(1) Browse to https://console.cloud.google.com/storage/browser/.\n(2) Click CREATE BUCKET and create a new bucket with the name snowplow-yourname-temp like so:\n  (3) Click into that bucket and click Create folder, and name the new folder temp-files:\n  Step 4: Create the iglu_resolver.json configuration Snowplow uses something called resolvers to automatically identify the parameters of each incoming hit. This is necessary for many reasons, the main being hit validation (to identify valid requests from invalid ones), for enriching and shredding the hits to the proper data format, and for mutating and loading the data into BigQuery tables.\n(1) Download the iglu_resolver.json file from here and open it for editing.\n(2) Change the two vendorPrefixes parameters to also include the Google Analytics namespace (Note! You don\u0026rsquo;t have to do this if you\u0026rsquo;re using the vanilla Snowplow JavaScript tracker).\n  (3) Save the file locally.\n(4) Browse to https://console.cloud.google.com/storage/browser/, click open the temporary file bucket you just created, and upload the modified iglu_resolver.json to the root of that bucket (so not in the temp-files folder).\n  With this resolver configuration, you\u0026rsquo;re instructing the enricher and loader that hits using the Google Analytics namespace might be coming in.\nStep 5: Create a new BigQuery dataset Before moving on, you\u0026rsquo;ll need to create a new dataset in BigQuery that will hold the table where your data will end up.\n(1) Browse to https://console.cloud.google.com/bigquery, and choose your project from the selector to the left. Click CREATE DATASET.\n  (2) Give the dataset an ID, such as snowplow_yourname_dataset, and click Create dataset.\n  NOTE! At this point you might want to just create a new table manually and partition that table on the derived_tstamp column. This way the BigQuery table is automatically partitioned by date of the hit, making it easier to manage and query by date.\nThis guide proceeds without creating a partitioned table just so that you can see how the mutator works.\nStep 6: Create the BigQuery configuration file (1) Open a new file for editing in a plain text editor.\n(2) Copy-paste the following within:\n{ \u0026#34;schema\u0026#34;: \u0026#34;iglu:com.snowplowanalytics.snowplow.storage/bigquery_config/jsonschema/1-0-0\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Snowplow Page View Data\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026lt;Random UUID\u0026gt;\u0026#34;, \u0026#34;projectId\u0026#34;: \u0026#34;\u0026lt;Your GCP project name\u0026gt;\u0026#34;, \u0026#34;datasetId\u0026#34;: \u0026#34;\u0026lt;The BigQuery dataset ID\u0026gt;\u0026#34;, \u0026#34;tableId\u0026#34;: \u0026#34;pageviews\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;enriched-good-sub\u0026#34;, \u0026#34;typesTopic\u0026#34;: \u0026#34;bq-types\u0026#34;, \u0026#34;typesSubscription\u0026#34;: \u0026#34;bq-types-sub\u0026#34;, \u0026#34;badRows\u0026#34;: \u0026#34;bq-bad-rows\u0026#34;, \u0026#34;failedInserts\u0026#34;: \u0026#34;bq-failed-inserts\u0026#34;, \u0026#34;load\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;STREAMING_INSERTS\u0026#34;, \u0026#34;retry\u0026#34;: false }, \u0026#34;purpose\u0026#34;: \u0026#34;ENRICHED_EVENTS\u0026#34; } } (3) Change the value of \u0026quot;id\u0026quot; to a random UUID which you can generate here. A valid value would be e.g. \u0026quot;3a27d47f-aeaf-4034-84b6-b1e82ca711d6\u0026quot;.\n(4) Change the value of \u0026quot;projectId\u0026quot; to your GCP project ID (e.g. \u0026quot;snowplow-production-simoahava\u0026quot;).\n(5) Change the value of \u0026quot;datasetId\u0026quot; to the ID you just gave your BigQuery dataset, e.g. \u0026quot;snowplow_simoahava_dataset\u0026quot;.\n(6) Make sure the Pub/Sub topic and subscription names correspond with those you created earlier in this chapter.\n(7) Save the file locally as bigquery_config.json.\n(8) Upload it to the temporary file bucket, where you already uploaded iglu_resolver.json to:\n  PHEW! That\u0026rsquo;s all the prep work done. Now you\u0026rsquo;ll need to just get the enricher and the BQ loader up and running in a new virtual instance group!\nFinalize the ETL process For ETL (extraction, transformation, and loading of your collector data), we\u0026rsquo;ll create a new instance template for an auto-scaling instance group.\nYou could use the same instance group as your collector, but this is not a sustainable way to run the pipeline, because the data streams from your site to the collector, from your collector to the enricher, and from the enriched stream to the BigQuery loader are vastly asymmetrical.\nIt will lead to a lot of redundancy and overhead if you have all your eggs in one basket.\nOptimally, you\u0026rsquo;d run the enrichment and the BQ loader in their own groups, too, but for the sake of this exercise I\u0026rsquo;ll bunch them together for now.\nStep 1: Create the instance template Since you\u0026rsquo;ve done all the preparations in the previous chapter, this final step of the ETL is actually pretty simple. All you\u0026rsquo;ll need to do is create the instance template and fire an instance group off of it.\nWell, it\u0026rsquo;s simple to you. It took me hours and hours and hours to get the thing working, so you\u0026rsquo;re welcome!\n(1) Browse to https://console.cloud.google.com/compute/instanceTemplates/list, and click CREATE INSTANCE TEMPLATE.\n(2) Give the template a name, e.g. snowplow-etl-template.\n  (3) Make sure the Compute Engine default service account is selected, and choose Set access for each API in the Access scopes list.\n  (4) Change the following API scopes:\n BigQuery: Enabled Cloud Pub/Sub: Enabled Compute Engine: Read Write Storage: Read Write  Leave the other options with their default values.\n  (5) Scroll down and click the Management, security, disks, networking, sole tenancy accordion heading.\n  (6) Under Automation, copy-paste the following code into Startup script:\n#! /bin/bash enrich_version=\u0026#34;0.1.0\u0026#34; bq_version=\u0026#34;0.1.0\u0026#34; bucket_name=\u0026#34;\u0026lt;cloud storage bucket name\u0026gt;\u0026#34; project_id=\u0026#34;\u0026lt;gcp project id\u0026gt;\u0026#34; region=\u0026#34;\u0026lt;region where to run the dataflow instances\u0026gt;\u0026#34; sudo apt-get update sudo apt-get -y install default-jre sudo apt-get -y install unzip wget https://dl.bintray.com/snowplow/snowplow-generic/snowplow_beam_enrich_$enrich_version.zip unzip snowplow_beam_enrich_$enrich_version.zip wget https://dl.bintray.com/snowplow/snowplow-generic/snowplow_bigquery_loader_$bq_version.zip unzip snowplow_bigquery_loader_$bq_version.zip wget https://dl.bintray.com/snowplow/snowplow-generic/snowplow_bigquery_mutator_$bq_version.zip unzip snowplow_bigquery_mutator_$bq_version.zip gsutil cp gs://$bucket_name/iglu_resolver.json . gsutil cp gs://$bucket_name/bigquery_config.json . ./beam-enrich-$enrich_version/bin/beam-enrich --runner=DataFlowRunner --project=$project_id --streaming=true --region=$region --gcpTempLocation=gs://$bucket_name/temp-files --job-name=beam-enrich --raw=projects/$project_id/subscriptions/good-sub --enriched=projects/$project_id/topics/enriched-good --bad=projects/$project_id/topics/enriched-bad --resolver=iglu_resolver.json ./snowplow-bigquery-mutator-$bq_version/bin/snowplow-bigquery-mutator create --config $(cat bigquery_config.json | base64 -w 0) --resolver $(cat iglu_resolver.json | base64 -w 0) ./snowplow-bigquery-mutator-$bq_version/bin/snowplow-bigquery-mutator listen --config $(cat bigquery_config.json | base64 -w 0) --resolver $(cat iglu_resolver.json | base64 -w 0) \u0026amp; ./snowplow-bigquery-loader-$bq_version/bin/snowplow-bigquery-loader --config=$(cat bigquery_config.json | base64 -w 0) --resolver=$(cat iglu_resolver.json | base64 -w 0) --runner=DataFlowRunner --project=$project_id --region=$region --gcpTempLocation=gs://$bucket_name/temp-files  UPDATE 18 Jan 2019.\nIt might be wise to start off by capping the auto-scaling of the Dataflow jobs. Otherwise, since you\u0026rsquo;re working with streaming inserts, you might end up with lots and lots of virtual machines being started to support the hit stream.\nTo prevent the loader from auto-scaling out of control, you can cap the number of available workers at e.g. 3 (so max. 3 CPUs are used by the job), or you can turn off auto-scaling altogether by capping the worker number at 1.\nTo set the maximum of 3 workers to the loader, add this to the end of the snowplow-bigquery-loader command:\n--maxNumWorkers=3\nSetting the maximum to 1 will lead to a single-worker, non-autoscaling setup. To have more than one worker but no autoscaling, you\u0026rsquo;ll also need to add the option --autoscalingAlgorithm=NONE to the loader.\n You\u0026rsquo;ll need to populate the five variables in the beginning of the code block before saving the instance template.\n enrich_version: Get the latest Beam Enrich version number from here. bq_version: Get the latest BigQuery Loader version number from here. bucket_name: Type the name of the storage bucket for temporary files you created earlier. project_id: Type your GCE project ID. region: Choose a region for the Dataflow instances to run in (you need to choose from this list).  Once you\u0026rsquo;ve made the changes to the startup script, click Create.\n  Step 2: Start up a new instance group Now that you have the template created, it\u0026rsquo;s time to create and start an auto-scaling instance group using this template. The idea is that when requests start flooding in, you\u0026rsquo;ll want to fire up new instances for your enrichment and loading steps so that your data collection won\u0026rsquo;t be impacted too severely by latency.\n UPDATE 18 Jan 2019: Instead of creating an auto-scaling instance group, it\u0026rsquo;s more than enough to just use a single machine in the group to handle the Dataflow! This part of the guide was originally based on some misinformation. You can still go ahead with the instance template, but once we reach the section about the instance group, please read the steps carefully.\n (1) While still in the instance template list, click open the template you just created.\n(2) In the top of the page, click CREATE INSTANCE GROUP.\n  (3) Give the new group a descriptive name, such as snowplow-etl-group.\n(4) Choose a region, e.g. europe-west1.\n(5) Make sure the instance template you just created is selected in the Instance template menu.\n   UPDATE 18 Jan 2019: You can set Autoscaling Off. The machine starting the Dataflow jobs does not need to autoscale. The Dataflow jobs are the ones that will require more power depending on the throughput rate, but you can start this machine with one static instance, scaling up manually if necessary.\n (6) You can keep the rest of the options with their default values, but if you like, you can set the health check to what you use for the collector instance, too.\n  When ready, click Create.\nTest everything When you create the instance group, it runs the startup script from beginning to end. These are the steps it takes, in order:\n  Beam Enrich is started as a new Dataflow job, using iglu_resolver.json for configuration. The Dataflow job essentially starts a new GCE instance group for the job.\n  The BigQuery Mutator is run with the create command, and this creates the pageviews table with a simple atomic structure in your BigQuery dataset.\n  The Mutator is next run in its own thread with the listen command. This is basically a Java program which listens for incoming, enriched requests being populated in the enriched-good Pub/Sub topic. Each request is parsed for values and data types, and if the pageviews table in BigQuery doesn\u0026rsquo;t have a corresponding column for some value in the request (validated against a schema resolved by the Iglu Resolver), a new column is created.\n  Finally, the BigQuery Loader starts up as its own Dataflow GCE instance, and this will load your enriched data into the corresponding columns and rows in your BigQuery table.\n  Before starting the debugging, make sure you\u0026rsquo;re sending some hits from your site to the collector. So browse around, visit different pages, and check the network requests to make sure the requests to your collector are completing successfully.\n  Step 1: Check that Beam Enrich is running To check if Beam Enrich is running, go to the Dataflow job list. You should see beam-enrich in the list with the status Running.\n  Step 2: Check that the mutator created the table To check if the mutator\u0026rsquo;s create command worked, go to your BigQuery dataset, and expand it. You should see a table named pageviews under it, and the table should be populated with a number of columns, such as app_id, etl_tstamp, etc. These columns are the \u0026ldquo;default\u0026rdquo; atomic data columns Snowplow uses.\n  Step 3: Check that the mutator creates additional columns where necessary Since you are collecting data with the Google Analytics tracker, you should shortly see a bunch of new columns added to the table description.\n  Step 4: Check that the BigQuery loader Dataflow job started Visit the Dataflow job list again. You should see something like main-root-XXXXX-YYYYY in the list with the status Running.\n  Step 5: Check that data is flowing into your BigQuery table Visit your BigQuery dataset/table again, and select the pageviews table.\nIn the Query editor, type the following query and press Run query:\nSELECT * FROM `your_dataset_name.pageviews` Where your_dataset_name should be replaced with the name of your dataset. You should see a bunch of results returned after the query is complete.\n  Troubleshooting You can visit the VM instances list and SSH directly into the ETL instance.\n  There, you can visit /var/log/, and open the file daemon.log for editing with e.g.\n$ pico /var/log/daemon.log It\u0026rsquo;s a full log of the the instance\u0026rsquo;s processes. You\u0026rsquo;ll need to run through the log and see where the error is.\nThere are many things that might have gone wrong, but the most common (in my own experience) are:\n  Incorrect access scopes in the instance template - double-check here.\n  Typo in iglu_resolver.json or bigquery_config.json, or forgot to add them to the correct cloud storage bucket - double-check resolver here and BigQuery config here.\n  Typo in the startup script - double-check here.\n  Collector not running - double-check here.\n  Impatience - wait 10-15 minutes before trying all the steps above again. It\u0026rsquo;s possible you\u0026rsquo;re just too hasty and the virtual machine instance hasn\u0026rsquo;t started up properly yet.\n  Final thoughts It\u0026rsquo;s a long guide, but I still worry if it\u0026rsquo;s detailed enough. I\u0026rsquo;m a bit ashamed at not being able to tell you exactly how you should scale and group your instances, or how you should optimize your load balancing system. But these are things you need to experiment with by yourself, or with the help of a seasoned data engineer. Remember that you can also utilize the Snowplow Insights service to help you set things up and manage the pipeline!\nI very much prefer the GCP user interface over AWS - there\u0026rsquo;s a flow to things, and related resources are grouped and linked together in a logical way. It makes moving from one service to another much smoother.\nSetting up the pipeline is fun (if you\u0026rsquo;re into that kind of thing), but it does result in a huge data table full of columns and rows of often undecipherable data. One thing I tend to forget is that the real work begins after the pipeline is created. Making sense of the data requires, at the very least, visualization in a tool such as Data Studio, but you might want to look at what Snowplow has to say about data modelling, too.\nI think Snowplow\u0026rsquo;s done an impressive job of making it possible create your own analytics pipeline with a fairly manageable cost. There are things that could have been done far more smoothly, such as utilizing Docker containers offered by Snowplow.\nThere are many moving parts in the pipeline. For me, personally, the race between different services enriching, shredding, mutating, and loading each incoming request is still a bit of a mystery. Snowplow has services (such as the BigQuery forwarder) that handle these problems, but setting it up was beyond the scope of this guide.\nThere\u0026rsquo;s also the whole world of enrichments that I deliberately skipped. It\u0026rsquo;s painful to see the columns for e.g. geographical data showing null, but I\u0026rsquo;ll leave those for another guide.\nOne thing to note is that you should keep a keen eye on projected and actualized costs. Once you\u0026rsquo;ve managed to build the pipeline itself, your next job as a data engineer is to look for optimization opportunities. With a scalable cloud infrastructure, it\u0026rsquo;s important to understand the often strenuous relationship between latency, scale, and cost.\n  I hope this guide has been useful. I also hope it doesn\u0026rsquo;t become outdated too soon.\nHuge thanks to Snowplow for providing me with help and resources throughout writing this guide. I\u0026rsquo;m especially grateful to Yali Sassoon and Joao Luis for their support.\n"
},
{
	"uri": "https://www.simoahava.com/tags/conferences/",
	"title": "conferences",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/on-public-speaking/",
	"title": "On Public Speaking",
	"tags": ["public speaking", "personal experience", "conferences"],
	"description": "Some thoughts on public speaking and conferences.",
	"content": "Speaking in front of an audience can be downright intimidating.\nAs a speaker, you are offering yourself for unsolicited feedback and criticism, you will most likely be ridiculed for the vacuity of your arguments, and you will be berated for your rhetoric or told exactly what you did wrong and why, when all you want is to have a cup of coffee, wind down, and enjoy the rest of the event in a good mood.\n  On the other hand, you have been blessed with the incredibly rare opportunity to share your knowledge on a public stage. People will hang on to every word you say, and you have a real chance of compressing years of experience, knowhow, and practical tips into a 45-minute-long-with-5-minutes-for-questions bundle of altruistic power.\nIn this article, I tap into my inexhaustible vein of hubris and ego, and I\u0026rsquo;ll tell you exactly what I think about public speaking and conferences alike. Maybe there\u0026rsquo;s a tip or two for someone here, but I\u0026rsquo;m pretty sure my method of preparing for talks (and for giving talks) isn\u0026rsquo;t one to emulate in any shape or form.\n10 Things I\u0026rsquo;ve Learned About Public Speaking Because lists are cool, I\u0026rsquo;ll present my ideas in a list (bonus tip: circular reasoning works like a charm on the public stage, too!).\n1. Rehearse as little as necessary This is one of the things I live by.\n I try to rehearse my talks as little as possible.\n I walk through the slides in my silent voice once or twice so that I\u0026rsquo;ll know if I\u0026rsquo;m risking going past the time limit. Note that there\u0026rsquo;s no penalty for going under the allotted time - in fact, many organizers might actually thank you for that since it leaves time for Q\u0026amp;A, and it also makes up for the delays incurred by other, sloppier speakers.\nI don\u0026rsquo;t rehearse because I don\u0026rsquo;t want my talk to appear scripted. The rhetoric devices I employ are mostly conversational, and any kind of scripting kills that.\nThis means that I find myself running down tangents more often than not, and every now and then I go in so deep that I lose track of the main path. But having done this for years, I now have a full toolbelt of methods I can use to segue back to the original point.\n2. Self-deprecation is your best friend  There\u0026rsquo;s nothing as powerful and disarming as laughing at yourself.\n I try ridicule myself at least twice in every talk. I think it\u0026rsquo;s important to show that I, too, am just a human, charmingly flawed, clumsy, and often unintelligible.\n3. Throw shoutouts to the other speakers as much as possible One difficult thing about conferences is trying to identify a common theme. Why am I speaking on the same stage as, say, Rand Fishkin a bit earlier? The conference organizers might have a difficult time in drawing the parallel, so it\u0026rsquo;s up to you as a speaker to help them out.\n I always try to refer to speakers who were on stage before me, and if I know what upcoming speakers will be talking about, I\u0026rsquo;ll mention them, too.\n Just make sure you don\u0026rsquo;t crack too many jokes at their expense (I\u0026rsquo;m often guilty of this) - they might not share your opinions on self-deprecation, especially when you present those opinions in their name.\n4. Be ridiculously thankful for being invited to speak I have nothing but anecdotal evidence, but I have a feeling that many speakers forget to be grateful for the opportunity to speak on the big stage.\nIt\u0026rsquo;s a tremendous amount of ridiculously hard work that organizing a conference takes. As a speaker, you typically only see bits and pieces of this hard work, and most often in the form of recovering from a technical snafu, or getting ushered on stage by the MC.\n When you are invited to speak at a conference, start your stream of never-ending \u0026ldquo;thank yous\u0026rdquo; already in your first response to their invitation. Be grateful that you were considered and accepted to participate as a laborer in the event.\n Thank the MC for the kind introduction when you step on the stage, thank the organizers for having you over, thank the audience for having the patience to listen to you drawl, thank the tech people for how everything worked like a charm (even if it didn\u0026rsquo;t), and thank the cloakroom folks for taking care of your belongings.\n5. Be professional You might think that this conflicts with points (1) and (2) above, but there\u0026rsquo;s a balance between being disarming and professional at the same time.\n I never have stage fright - and this is an honest statement. Why? Because I know the subject matter of my talk through and through.\n If you feel nervous about your talk, just focus on the stuff you know really well. Make that the core of your talk. Make it obvious how well you know what you\u0026rsquo;re talking about. Exuding insecurity is not a good look for anyone.\nI also try to avoid scripted laughs such as memes and written jokes in my slides. Most of the self-deprecation and tangential stuff happens in the beginning and end of my talks, because I want the core to be informative as hell.\n6. Find a balance between inspiration and action Because my talks typically revolve around technical stuff such as \u0026ldquo;things you can do with X\u0026rdquo; or \u0026ldquo;how to turn an organization into Y\u0026rdquo;, I rarely have the kind of inspirational content you might see in keynotes of some of the larger conferences out there.\n That\u0026rsquo;s OK. Inspiration can be derived from hardcore technical content, too. There\u0026rsquo;s nothing as inspirational as seeing the potential of some tool or platform, and waiting for the chance to try it out when going back to the office the following day.\n But there\u0026rsquo;s also room for reaffirming the audience\u0026rsquo;s preconceptions (that\u0026rsquo;s the inspirational part). It\u0026rsquo;s OK to let them know they\u0026rsquo;re on the right path, and to show them where that path might lead if all the pieces lock in together. It\u0026rsquo;s OK to say platitudes like \u0026ldquo;Believe in yourself and the world will be your oyster\u0026rdquo; (actually, never say that, please), as long as you cut it with some self-deprecation right after.\nThough this leads me to\u0026hellip;\n7. Never underestimate your audience If you ever have to think \u0026ldquo;Should I dumb this talk down?\u0026rdquo;, my advice is: DON\u0026rsquo;T.\n First of all, going slightly over the audience\u0026rsquo;s heads is better than playing into their comfort zones. As an audience member, being enlightened about potential knowledge gaps is a good thing.\n However, and this is why not rehearsing is a good thing, leave room for interpreting the audience\u0026rsquo;s reactions. If you see confusion in their faces, take a step back and explain the idea again from another angle, and if you see them give you the \u0026ldquo;Come on dude, I know this stuff\u0026rdquo; look, switch gears and tell them something they should certainly not know about.\nThe fault isn\u0026rsquo;t in the audience\u0026rsquo;s intelligence - it\u0026rsquo;s in your rhetoric. So fix it.\n8. Be patient after the talk After the talk, just as you\u0026rsquo;re heading for the open bar, you will have people coming up to you to talk about your presentation, your life, your family, your ukulele hobby, or your clothes. Don\u0026rsquo;t complain or roll your eyes.\n Remember, you have been employed by the conference as a performer. You are expected to play your part to the very end.\n And hey, why are you complaining about social contact? It\u0026rsquo;s absolutely delightful, humbling, and uplifting to be greeted by so many people who come and thank you for your contributions!\n9. Avoid the speakers\u0026rsquo; lounge Some events have a speakers\u0026rsquo; lounge. It\u0026rsquo;s a great idea, letting the \u0026ldquo;star performers\u0026rdquo; wind down.\nBut far too often I see speakers spending the majority of their time in the lounge, rather than out in the bullpen with the rest of the attendees.\n Remember, again, you are an employee of the event. It\u0026rsquo;s your duty to make the event the best event it can be. And that requires you to mingle, meet with people, and just make your presence felt among the vendor booths, cafeteria lines, and the main lobby.\n It\u0026rsquo;s OK to spend some time in the lounge, especially before and after your talk, but try to avoid it at other times.\n10. If you can\u0026rsquo;t make it to a conference, have a list of recommended speakers at hand There comes situations where you must (gracefully) turn down an invitation.\n When you do so, it would be really great if you have a list of recommendations you can pass to the event organizer as potential speakers in your stead.\n Heck, even if you CAN make the event, it\u0026rsquo;s a good idea to share these recommendations, since they might still be on the lookout for other speakers, too.\n10 Things I\u0026rsquo;ve Learned About Conferences The following list has some overlap with the previous one, but I want to focus especially on what I think makes a conference good, and what I\u0026rsquo;ve learned through my interactions with conference organizers.\n1. Coffee and solid wireless network I would like to say that the key to success is finding the right speakers and a motivated audience.\n But, truth be told, I\u0026rsquo;m most easily persuaded by two things: coffee available at all times, and a Wi-Fi that doesn\u0026rsquo;t suck.\n Those are two most important things you\u0026rsquo;ll need to keep the satisfaction level of the audience at a positive base level.\n2. As few tracks as possible This goes without saying, but multi-track conferences are frustrating.\n You\u0026rsquo;ll always miss some great talk because there was another great talk going on at the same time.\n I totally understand economy of scale, so having more than one track is necessary for any conference that wants to grow, but it\u0026rsquo;s still frustrating.\n3. Be careful with panels I typically dislike panels. Most of them are just speakers sitting next to each other and answering questions.\n Best panels are those where there\u0026rsquo;s a debate or heated discussion going on. So pick speakers who are known to disagree on many topics, and pick these topics for conversation.\n And don\u0026rsquo;t forget audience participation!\n4. Diversity is extremely important Try go for a 50/50 split between male and female speakers. It shouldn\u0026rsquo;t be difficult. If you\u0026rsquo;re having trouble finding female speakers, ask around for lists in Twitter - you\u0026rsquo;ll get plenty of responses!\n At the very least, try to get a 50/50 split in keynote speakers and in panel participants.\n Having an all-male cast in either is just lazy.\nThere are many other manifestations of diversity, but the gender gap is one that\u0026rsquo;s most striking to me (and this is my list).\n5. Speakers might help you sell tickets but it\u0026rsquo;s the audience that makes the conference Don\u0026rsquo;t blow your entire budget on the speakers, and don\u0026rsquo;t spend your entire conference fussing about them and catering to their every need.\n Remember that when the conference starts, the audience is what matters.\n They\u0026rsquo;re the ones who\u0026rsquo;ll spread the word and decide if you\u0026rsquo;re worth a second shot next year.\n6. Implement a code of conduct It\u0026rsquo;s terrible that we live in a world where there\u0026rsquo;s a need for codes of conduct. But, as it turns out, human beings can often be total dicks to each other, intentionally and unintentionally. Having a CoC will let you establish some ground rules for how to behave at your event.\n Create a code of conduct (check out the SearchLove CoC for inspiration), promote it heavily, and make it extremely prominent in all your PR material.\n If you fail to create a safe environment for your conference, you will lose the trust of the attendees, and you might even harm your and your brand\u0026rsquo;s reputation permanently. In addition, you might actually place some of your guests in danger.\n7. Send invitations to speakers in good time This is more of a personal opinion, since I know some conferences simply have a very short build-up time, but try to get invitations out months in advance.\n Many speakers have a full schedule, and might need advance notice of up to six months to be able to make room in the middle of their busy weeks.\n Also, try to scout around if there are other events happening at the same time in the same industry / niche. If possible, avoid too much overlap between other events - it might make it difficult to get speakers AND attendees to join you.\n8. Set rules for vendors Another way of building a \u0026ldquo;safe\u0026rdquo; environment is to prevent vendors from harassing the attendees with their pitches.\n Sponsors are super important, and it\u0026rsquo;s often really interesting to see what different vendors have been up to, but some can be really aggressive in trying to pick up new customers.\n And if vendors are awarded a speaking slot in return for their co-operation, please suggest that their talk be about something more practical than just gushing about how great their tool is and how much all competitors\u0026rsquo; tools suck.\n9. Make sure there\u0026rsquo;s plenty of time for networking For many, conferences are networking events. It\u0026rsquo;s possible some attendees don\u0026rsquo;t actually visit a single talk - they come over to meet vendors, friends, business associates, and to make new contacts.\n Cater to these as much as possible.\n Building networks and bringing people together is one the best ways to get people to come back the following year.\n10. Do something unique There are gazillions of conferences that are so alike it\u0026rsquo;s impossible to tell each one apart.\n So try to think of something that makes your event unique.\n For example, Superweek is on top of a mountain, MeasureCamp is an unconference, Conversion Hotel is on a small island, and Digital Elite Camp is at a spa.\nThere are many ways to make your event memorable, and the more memorable you can make it, the more buzz it will generate.\nSummary These twenty items condense my learnings from the 150+ keynotes and conference presentations I\u0026rsquo;ve done in digital marketing/analytics conferences since 2013.\nI usually think saying this is totally redundant, but these are all my own subjective thoughts. I\u0026rsquo;m not trying to teach you how you should present in conferences, or how you should run an event. I simply wanted to share my own ideas and thoughts about public speaking and attending these events year after year.\nDo you have thoughts about public speaking or running conferences? Please do so in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/tags/personal-experience/",
	"title": "personal experience",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/public-speaking/",
	"title": "public speaking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/apps/",
	"title": "apps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-for-mobile-sunset/",
	"title": "Google Analytics For Mobile Apps Getting Shut Down",
	"tags": ["google analytics", "google tag manager", "mobile", "apps"],
	"description": "Google will be sunsetting and deprecating their Google Analytics for mobile apps data collection, processing, and reporting schemas in 2019 and 2020. Here&#39;s more information about the change.",
	"content": "If you\u0026rsquo;re a user of the free version of Google Analytics, and if you have a free Google Analytics property collecting hits exclusively from the Google Analytics Services SDK (Android or iOS), you might have recently received an email that looks like this (emphasis mine):\n  In a nutshell, Google is now starting the process of deprecating the \u0026ldquo;legacy\u0026rdquo; Google Analytics for Mobile Apps. This covers all data collection SDKs that do not have the word \u0026ldquo;Firebase\u0026rdquo; in them. As such, also the \u0026ldquo;Google Analytics\u0026rdquo; tags in Google Tag Manager for mobile apps will be impacted.\nGoogle is most likely doing this because they want to focus on Firebase Analytics as the new paradigm for Google\u0026rsquo;s mobile app analytics.\nFor reference, here\u0026rsquo;s the email body:\n Dear Google Analytics customer,\n  You are receiving this email because you are in the first wave of customers who have been identified as users of the older style of Google Analytics for mobile apps reporting and the Google Analytics Services SDK. Specifically, we are notifying you regarding your Google Analytics property(ies) (, ID: UA-XXXXXX-YY: ).\n  We want to let you know that in October 2019 we will begin to sunset our Google Analytics for mobile apps reporting and the Google Analytics Services SDK. We are investing our resources in the latest style of app reporting in Google Analytics that works in conjunction with Firebase – Google\u0026rsquo;s integrated app developer platform. As such, the following will take place:\n   In 2019, we will begin to decommission properties that receive data exclusively from the Google Analytics Services SDK. Data collection and processing for such properties will stop on October 31, 2019. Reporting access through our UI and API access will remain available for these properties\u0026rsquo; historical data until January 31, 2020. After our service is fully turned down, these properties will no longer be accessible via our Google Analytics UI or API, and their data will be removed from Google Analytics servers. You will receive further notification when this time nears. At this time, no Analytics 360 properties are impacted by these changes.    We want to give you plenty of time to make the transition. The good news is that the latest solution using the Firebase SDK is even more intuitive and includes free and unlimited event reporting to meet the needs of app-centric businesses. We\u0026rsquo;ve invested heavily to make this solution best-in-class, with new features and capabilities rolling-out continually. Additionally, our offering is closely integrated with other Google products and features to help grow your app business like Crashlytics, AdMob and Remote Config.\n  Getting started with our latest app reporting features is simple and straightforward. Here\u0026rsquo;s how. For additional information on our new Google Analytics app reporting, visit the Help Center.\n  Thank you,\n The Google Analytics Team   What this means As you can see from the email, only free Google Analytics properties are impacted. Furthermore, only those properties that collect data exclusively with the mobile analytics tracking schema will be impacted.\nI\u0026rsquo;m not sure what the actual triggers for identifying these hits will be. Most likely GA will look for specific dimensions being sent with the request to /collect, and use that to determine whether or not the hit was a hit intended to be collected in mobile app views.\nThis has two implications.\n  If you are using Google Analytics 360 properties, then you are off the hook, for now. There has been no indication of a deprecation timeline for GA360.\n  It looks like one way to avoid the impact even if using free Google Analytics is to send the occasional web hit to the property collecting the mobile app data. This means sending hits with the JavaScript SDK, or building a Measurement Protocol request that includes web dimensions.\n  UPDATE! It has been confirmed that the step (2) above will not work if you\u0026rsquo;ve already received the deprecation message. In other words, if your property has already been flagged for deprecation, no amount of trickery will prevent that - it\u0026rsquo;s on its way out.\nNeither of these implications is one to swear by, however. There\u0026rsquo;s no telling if a similar timeline for GA360 will be revealed in the coming months, and sending web hits to the mobile app property might be something that won\u0026rsquo;t work down the line, either.\nSome thoughts First of all, on a personal note, I\u0026rsquo;m very glad about this announcement. I\u0026rsquo;ve considered the \u0026ldquo;mobile version\u0026rdquo; of Google Analytics to be flawed for a long time, as it relies on the paradigm outlined by web data collection too much. It\u0026rsquo;s difficult trying to pigeon hole the fluctuations of mobile app usage into the constricted schema of GA with its screenviews, sessions, and campaign attribution.\nAt the same time, the push to Firebase won\u0026rsquo;t be gentle. It\u0026rsquo;s a very different approach to analytics, as it revolves around the duality of Users and Events. Thus, it harks back to hit stream analytics which, while providing more flexibility to analysis itself, does make simple exploratory analytics more difficult to do than what you might be used to with the Google Analytics UI and reporting API. The Firebase team has been working hard to make the user interface more intuitive, so usability is something they are certainly paying attention to.\nCurrently, Firebase has some strict quotas for custom parameters in data collection. This looks crippling especially for those of us used to the 200 Custom Dimensions provided by Google Analytics 360. I assume that Firebase will need to address these limitations before offering their solution as a migration-ready upgrade from the legacy GA mobile tracking.\nGoing forward For now, I recommend that you start working on a migration plan for your mobile Google Analytics tracking, even if you are using Google Analytics 360. Moving from GA to Firebase isn\u0026rsquo;t as easy as flipping a switch, and you should familiarize with the latter as soon as possible.\nIf you\u0026rsquo;ve been using the latest version of Google Tag Manager for mobile, the migration path (at least with regard to implementation) is a bit smoother, since you\u0026rsquo;ll already be using Firebase Analytics for dispatching the events. But if you\u0026rsquo;re anything like me, you\u0026rsquo;ve prioritized Google Analytics tag setups over the detail of the Firebase events themselves, so you\u0026rsquo;ll most likely need to address the implementation anyway.\nWhile working on the migration plan, it\u0026rsquo;s a good idea to look at other players in the field, too. I\u0026rsquo;d be remiss not to mention Snowplow Analytics, since its approach to the hit stream model might be very tempting to explore while thinking of alternatives to GA.\nWe\u0026rsquo;re in for interesting times, as this announcement will certainly impact lots of users. Let\u0026rsquo;s hope Google keeps us in the loop and tells us more about this transition in the coming months!\n"
},
{
	"uri": "https://www.simoahava.com/tags/data-model/",
	"title": "data model",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/datalayer/",
	"title": "datalayer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/localstorage/",
	"title": "localstorage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/persist-datalayer-across-pages/",
	"title": "Persist Google Tag Manager&#39;s dataLayer Across Pages",
	"tags": ["google tag manager", "datalayer", "localstorage", "data model"],
	"description": "Use this Custom HTML tag script to persist GTM&#39;s dataLayer (and data model) across pages until a given timeout is reached.",
	"content": " UPDATE 4 June 2020: Instead of copying the Custom HTML code from the article, please load it from the GitHub Gist instead.\n Four years ago, I wrote an article on how to persist GTM\u0026rsquo;s dataLayer from page to page. Unfortunately, the solution was a bit clumsy, requiring you to give specific commands for the interactions, which made it really unwieldy in the long run. Google Tag Manager still doesn\u0026rsquo;t offer us a native way to persist the dataLayer array or its internal data model from one page to the other, so I thought it was about time I revisit this idea.\n  This time, there won\u0026rsquo;t be an API to interact with. Instead, the solution will simply store the contents of the dataLayer array AND the internal data model from page to page, until the user hasn\u0026rsquo;t interacted with GTM\u0026rsquo;s dataLayer for a given amount of time.\nSetting it up To set it up, you need to create a Custom HTML tag, in which you\u0026rsquo;ll copy the following code. If you like, you can also copy the code from this gist.\n UPDATE 4 June 2020: Please copy the code from the gist link above rather than from the article below. The gist is kept up-to-date, and has stability fixes that help resolve some issues users have been having with the solution.\n The Custom HTML tag code \u0026lt;script\u0026gt; (function() { // Set the timeout for when the dataLayer history should be purged. The default is 30 minutes.  // The timeout needs to be in milliseconds.  var timeout = 30*60*1000; // Change dataLayerName only if you\u0026#39;ve defined another named for the dataLayer array in your  // GTM container snippet.  var dataLayerName = \u0026#39;dataLayer\u0026#39;; // Don\u0026#39;t change anything below.  // Initial settings  var initialLoad = true, oldPush = window[dataLayerName].push; // Method to copy items from dataLayer from before the GTM container snippet was loaded.  var backfillHistory = function() { var tempHistory = [], i = 0, len = window[dataLayerName].length - 1; for (; i \u0026lt; len; i++) { tempHistory.push(window[dataLayerName][i]); } return tempHistory; }; // Method to check if object is a plain object.  // From https://bit.ly/2A3Fuqe  var isPlainObject = function(value) { if (!value || typeof value !== \u0026#39;object\u0026#39; || // Nulls, dates, etc.  value.nodeType || // DOM nodes.  value === value.window) { // Window objects.  return false; } try { if (value.constructor \u0026amp;\u0026amp; !value.hasOwnProperty(\u0026#39;constructor\u0026#39;) \u0026amp;\u0026amp; !value.constructor.prototype.hasOwnProperty(\u0026#39;isPrototypeOf\u0026#39;)) { return false; } } catch (e) { return false; } var key; for (key in value) {} return key === undefined || value.hasOwnProperty(key); }; // Method to merge the stored data model and the history model together.  // From https://bit.ly/2FrPQWL  var mergeStates = function(storedModel, historyModel) { for (var property in storedModel) { if (storedModel.hasOwnProperty(property)) { var storedProperty = storedModel[property]; if (Array.isArray(storedProperty)) { if (!Array.isArray(historyModel[property])) historyModel[property] = []; mergeStates(storedProperty, historyModel[property]); } else if (isPlainObject(storedProperty)) { if (!isPlainObject(historyModel[property])) historyModel[property] = {}; mergeStates(storedProperty, historyModel[property]); } else { historyModel[property] = storedProperty; } } } }; window[dataLayerName].push = function() { try { // Build the history array from local storage  window._dataLayerHistory = JSON.parse( window.localStorage.getItem(\u0026#39;_dataLayerHistory\u0026#39;) || \u0026#39;{\u0026#34;timeout\u0026#34;: null, \u0026#34;history\u0026#34;: [], \u0026#34;model\u0026#34;: {}}\u0026#39; ); // Initial settings  var timeNow = new Date().getTime(), states = [].slice.call(arguments, 0), results = oldPush.apply(window[dataLayerName], states), oDataLayer = window[dataLayerName], dHistory = window._dataLayerHistory, oDataModel = window.google_tag_manager[{{Container ID}}].dataLayer.get({split: function() { return []; }}); // Method to reset the history array to the current page state only  dHistory.reset = function() { dHistory.timeout = null; dHistory.history = backfillHistory(); dHistory.model = {}; mergeStates(oDataModel, dHistory.model); window.localStorage.setItem(\u0026#39;_dataLayerHistory\u0026#39;, JSON.stringify(dHistory)); }; // From https://bit.ly/2A2ZcCG  dHistory.model.get = function(key) { var target = dHistory.model; var split = key.split(\u0026#39;.\u0026#39;); for (var i = 0; i \u0026lt; split.length; i++) { if (target[split[i]] === undefined) return undefined; target = target[split[i]]; } return target; }; // Add history if this is the initialization event itself  if (initialLoad) { dHistory.history = dHistory.history.concat(backfillHistory()); initialLoad = false; } // If timeout is reached, reset the history array  if (dHistory.hasOwnProperty(\u0026#39;timeout\u0026#39;) \u0026amp;\u0026amp; dHistory.timeout \u0026lt; timeNow) { dHistory.reset(); } // Push latest item from dataLayer into the history array  dHistory.history.push(oDataLayer[oDataLayer.length-1]); // Merge GTM\u0026#39;s data model with the history model  mergeStates(oDataModel, dHistory.model); // Update the timeout  dHistory.timeout = timeNow + timeout; // Write the new history into localStorage  window.localStorage.setItem(\u0026#39;_dataLayerHistory\u0026#39;, JSON.stringify(dHistory)); return results; } catch(e) { console.log(\u0026#39;Problem interacting with dataLayer history: \u0026#39; + e); var states = [].slice.call(arguments, 0), results = oldPush.apply(window[dataLayerName], states); return results; } }; })(); \u0026lt;/script\u0026gt; In the very beginning of the snippet, there are two variables whose values you need to modify.\nvar timeout = 30*60*1000;\nThe line above establishes the timeout for the local storage. This means that once the user hasn\u0026rsquo;t interacted with dataLayer for as long as you set in the timeout, the history will be reset to start from the current page. The default value is 30 minutes, and if you want to modify it make sure you set the timeout in milliseconds, as in the default value.\nvar dataLayerName = 'dataLayer';\nThe line above is the name of the dataLayer array that Google Tag Manager uses, and it defaults to the unmodified container snippet. If you\u0026rsquo;ve changed the dataLayer name in the container snippet, make sure it\u0026rsquo;s updated here, too.\nOther tag settings In addition to copy-pasting the code above, set the Tag Priority value to 9999 or any number that\u0026rsquo;s higher than any other Tag Priority for tags firing on the All Pages trigger.\n  Trigger Set this Custom HTML tag to fire on the All Pages trigger. You want it to be the first tag that fires on the page. Naturally, if you have tags firing on an event that\u0026rsquo;s pushed into dataLayer before the Google Tag Manager container snippet, you need to make sure this tag fires on that trigger instead.\n  How it works Whenever something is pushed into dataLayer, it is also pushed into a new array under window._dataLayerHistory. This is a global object, and you can access it from anywhere on the page, including GTM\u0026rsquo;s Custom HTML tags and Custom JavaScript variables.\nIn addition to being added to this history array, this array is also consistently written into the window.localStorage structure, which persists across pages until the user decides to clear their browser storage.\n  In short, there\u0026rsquo;s a new window._dataLayerHistory object that contains information about all the items pushed into dataLayer across pages, and you can access this object from any JavaScript context on the page.\n  The history array The array itself, representing the history of the window.dataLayer array, can be found at window._dataLayerHistory.history.\nWhen the Custom HTML tag is first loaded on any page, this history array is first back-filled with items from the current window.dataLayer that were pushed before the Custom HTML tag was fired. This is necessary, because the Custom HTML tag creates its own .push() listener only when it fires, at which point the window.dataLayer array will already contain items.\nOne quirky thing you might notice is that if there\u0026rsquo;s a window.dataLayer.push() call taking place in a tag sequence, the object pushed into the history array will not contain the gtm.uniqueEventId key. There\u0026rsquo;s not much I can do about this, unfortunately, but it shouldn\u0026rsquo;t be a big deal.\nThe data model If you\u0026rsquo;re not familiar with GTM\u0026rsquo;s data model, it\u0026rsquo;s essentially a lookup table to which GTM copies and merges the key-value pairs you push into the dataLayer array.\nIt\u0026rsquo;s important to understand this distinction, because GTM uses the internal data model for Data Layer variables.\n  The data model is also persisted from page to page in window._dataLayerHistory.model. This object has a get() method you can use to fetch data model values, just like GTM\u0026rsquo;s own native interface does:\nwindow._dataLayerHistory.model.get('someOldVariableFromAPreviousPage');\nThis might be useful. For example, if your site writes to dataLayer something like {userLoggedIn: true} when the user logs into the site, but it only does this when the user actual logs in, you can fetch this value on later pages by querying the history object:\nwindow._dataLayerHistory.model.get('userLoggedIn');\nThe history model applies the same type of recursive merge that GTM does with its internal data model. This might lead to unexpected outcomes with objects and arrays, so be sure to read up on recursive merge before moving on.\nReset You can also reset the history by executing this command:\nwindow._dataLayerHistory.reset();\nThis nulls the timeout and resets the history array and history model to the states of the current page, thus removing any history from both. It also resets the object stored in browser storage to this, reset state.\nApplications There are many things you could do with a persistent dataLayer and data model. Here are some examples.\nGet number of pages loaded To identify how many pages the user has visited, you could have a Custom JavaScript variable that does this:\nfunction() { return window._dataLayerHistory.history.filter(function(obj) { return obj.event === \u0026#39;gtm.js\u0026#39;; }).length; }  This returns the number of times that the gtm.js event has been pushed into dataLayer, and you can use this as a reasonably good proxy for determining how many pages the user has visited. Note that if you use either gtag.js or Google Optimize, this variable might not return an accurate result.\nCheck the entire dataLayer history for some key or value Here\u0026rsquo;s an extension of this solution I wrote for the window.dataLayer array (i.e. the dataLayer of the current page only). With this, you can search the entire history of the dataLayer array for a given key-value pair. This is what the modified Custom JavaScript variable looks like:\nfunction() { // Modify the searchObject below.  //  // Add each key-value pair you want to look for directly into the searchObject object. Use  // strings for keys.  //  // The variable will look for any key-value pair you specify, and return true if any one of them  // is found. If you use dot notation, the variable will try to find a key with this name first,  // after which it will parse the nested structure looking for a match.  var searchObject = { \u0026#39;user.consentGiven\u0026#39;: \u0026#39;false\u0026#39; }; var dataLayerName = \u0026#39;_dataLayerHistory\u0026#39;; // Don\u0026#39;t edit anything below this line.  var getValueForObjectString = function(obj, key) { return key.split(\u0026#34;.\u0026#34;).reduce(function(o, x) { return (typeof o == \u0026#34;undefined\u0026#34; || o === null) ? o : o[x]; }, obj); }; return window[dataLayerName].history.filter(function(obj) { var found = false; var prop; for (prop in searchObject) { if (obj[prop] == searchObject[prop] || getValueForObjectString(obj, prop) == searchObject[prop]) { found = true; } } return found; }).length \u0026gt; 0; }  Summary You can use this script to persist the dataLayer array as well as Google Tag Manager\u0026rsquo;s internal data model from one page to the next.\nIt\u0026rsquo;s not foolproof. For example, it doesn\u0026rsquo;t understand command arrays or command functions, and it doesn\u0026rsquo;t understand any manual .set() commands you run against GTM\u0026rsquo;s own, internal data model. (Note, if any of the terms in this paragraph were alien to you, please read my article on GTM\u0026rsquo;s internal data model).\nAs always, this was more a tech demo than a turnkey solution. Please let me know in the comments if you have uses for this kind of a solution. Also, if you have improvement suggestions, let me know of those, too!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/search-datalayer-for-key-value-pair/",
	"title": "#GTMTips: Search dataLayer For A Key-Value Pair",
	"tags": ["google tag manager", "gtmtips", "dataLayer", "javascript"],
	"description": "Search the dataLayer array for specific key-value pairs. You can use this to enable or block your triggers if a specific value was pushed into dataLayer earlier.",
	"content": "One of the difficulties of working with Google Tag Manager and the dataLayer structure is that GTM doesn\u0026rsquo;t preserve history of the items collected into its data model. Or, at least, it doesn\u0026rsquo;t preserve it in a manner that would let us access it.\nThis is typically a very niche problem, but it does surface every now and then. For example, say you wanted to query whether an event with some specific value has already been pushed into dataLayer. Well, this is impossible, since any event you use to trigger this check would overwrite this previous event value in the data model (since any given key can only have one value at a time).\nIn other words, we need a mechanism that lets us query for earlier values of specific keys which have been (possibly) overwritten since then. For this, we\u0026rsquo;ll need some Custom JavaScript variable magic.\nTip 91: Looking for key-value pairs   First, create the Custom JavaScript variable. This is the code you\u0026rsquo;ll need to add:\nfunction() { // Modify the searchObject below.  //  // Add each key-value pair you want to look for directly into the searchObject object. Use  // strings for keys.  //  // The variable will look for any key-value pair you specify, and return true if any one of them  // is found. If you use dot notation, the variable will try to find a key with this name first,  // after which it will parse the nested structure looking for a match.  var searchObject = { \u0026#39;user.consentGiven\u0026#39;: \u0026#39;false\u0026#39; }; // Change this if you have renamed the dataLayer array.  var dataLayerName = \u0026#39;dataLayer\u0026#39;; // Don\u0026#39;t edit anything below this line.  var getValueForObjectString = function(obj, key) { return key.split(\u0026#34;.\u0026#34;).reduce(function(o, x) { return (typeof o == \u0026#34;undefined\u0026#34; || o === null) ? o : o[x]; }, obj); }; return window[dataLayerName].filter(function(obj) { var found = false; var prop; for (prop in searchObject) { if (obj[prop] == searchObject[prop] || getValueForObjectString(obj, prop) == searchObject[prop]) { found = true; } } return found; }).length \u0026gt; 0; }  Here, you\u0026rsquo;ll need to modify the searchObject by adding key-value pairs you want to search for in the dataLayer array.\nFor example, if you wanted to find whether the event key has had the value askForConsent at some point, the object would look like this:\nvar searchObject = { \u0026#39;event\u0026#39;: \u0026#39;askForConsent\u0026#39; };  If you wanted to check whether that event value was pushed earlier OR whether the user has already given consent, you could do this:\nvar searchObject = { \u0026#39;event\u0026#39;: \u0026#39;askForConsent\u0026#39;, \u0026#39;user.consentGiven\u0026#39;: true };  Note that if you use dot notation in the key name, the variable will first check whether a key with exactly that name (user.consentGiven) is found, after which it will check whether a nested structure like that ({user: {consentGiven: true}}) is found.\nNext, you\u0026rsquo;ll need to modify the dataLayerName variable value in case you have renamed the dataLayer array to something else than the default.\nAnd then you\u0026rsquo;re all set. The variable will return true in case any of the key-value pairs you specify in the searchObject is found in the dataLayer array.\nCaveats There are two major caveats with this solution.\n1. dataLayer is unreliable Nothing\u0026rsquo;s stopping site code from deleting items from the dataLayer. In fact, if your array size surpasses 300 members, Google Tag Manager will automatically start removing items from the beginning.\nSimilarly, if you\u0026rsquo;re running a single-page app, it\u0026rsquo;s possible the array is cleared between each page transition.\nIn other words, there\u0026rsquo;s no guarantee that this variable will find anything simply because the dataLayer array is not immutable nor is it supposed to be.\n2. Timing matters Another important thing to note is that the dataLayer array is checked when the variable is run. This might lead to some unexpected results, especially if you\u0026rsquo;re comparing the variable to what Preview mode is showing.\nWhen a tag fires with a trigger, Preview mode will show the state of Google Tag Manager\u0026rsquo;s variables at the time the trigger event was pushed into dataLayer. This might be a significant amount of time before the tag and its associated variables are actually executed.\nThus, the state of the web page that the variable tries to access might differ from the state of the web page when the tag was actually triggered. This is mainly a problem if you try to consolidate what you try to look for with searchObject and what you try to access with Data Layer variables.\nFor example, if the object with user.consentGiven is pushed a very short while after the event that triggered the tag (and the search variable), the tag will not be able to access this with a Data Layer variable, but the Custom JavaScript variable\u0026rsquo;s searchObject will find the match in the dataLayer array.\nThis is just something to pay attention to, especially if you\u0026rsquo;re wondering why the variable claims something is found in dataLayer but the tag still can\u0026rsquo;t access that particular value.\nSummary Do you have ideas where this variable might be useful? I\u0026rsquo;ve mainly come across two use cases:\n  On a single-page app, blocking the All Pages trigger from firing a Page View if there already is a \u0026ldquo;virtual pageview\u0026rdquo; event in dataLayer.\n  Checking for state transitions, such as whether a user used to be logged out, but is now logged in (compared to being logged in all the time).\n  What else?\n"
},
{
	"uri": "https://www.simoahava.com/analytics/new-errors-tab-preview-mode/",
	"title": "#GTMTips: The Errors Tab In Preview Mode",
	"tags": ["google tag manager", "gtmtips", "preview mode"],
	"description": "The Errors tab in Google Tag Manager&#39;s Preview mode surfaces exceptions thrown by the built-in tag templates.",
	"content": "While using the Google Tag Manager user interface around Halloween 2018, you might have noticed a new tab in Google Tag Manager\u0026rsquo;s Preview mode. The tab is named Errors and shows you the number of exceptions thrown by GTM\u0026rsquo;s tag templates on the page. In this short #GTMTips post, I\u0026rsquo;ll quickly walk you through what the tab shows.\nTip 90: The Errors Tab In Preview Mode   The tab\u0026rsquo;s name is an apt description of what the tab shows. If a tag template in GTM fails to fire due to an error, this tab would show details about the tag that failed and the error that caused it.\n  Note that it\u0026rsquo;s actually quite rare to have a tag throw exceptions if something goes awry. The Twitter tag, as in the screenshot above, is one of the rare tags that actually throws an error if you try to, for example, add an incorrectly formatted value as its parameter (plainvalue as the value of the content_ids parameter, for example).\nIf an error is thrown, you\u0026rsquo;ll see which tag threw the error and what the error was.\nAs you can see from the screenshot above, the error message isn\u0026rsquo;t always very helpful, but at least you\u0026rsquo;ll know which tag failed, and you can use this information to debug the tag thoroughly.\nTime will tell if this tab will surface other types of issues with a container running on a web page!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/fire-tag-upon-scroll-depth-and-time-spent/",
	"title": "Fire GTM Tag Upon Scroll Depth And Time Spent",
	"tags": ["google tag manager", "triggers", "scroll depth", "timer"],
	"description": "How to fire a tag in Google Tag Manager when the user has scrolled past a certain depth and spent a certain amount of time on the page. This can be done with the built-in Scroll Depth and Timer tirggers.",
	"content": "Over 5 years ago, I wrote an article titled Track Adjusted Bounce Rate In Universal Analytics. It basically explored a number of different methods to tweak the Bounce Rate metric so that it becomes more meaningful in your Google Analytics reports.\nNow, writing that article wasn\u0026rsquo;t necessarily my proudest moment. It\u0026rsquo;s not because the solution was poor, but rather because I was suggesting it makes sense to tweak a metric. The concept of \u0026ldquo;adjusted Bounce Rate\u0026rdquo; sounds like the analyst is fixing a metric to be more beneficial to their cause, rather than fixing the business problem that caused the metric to be poor in the first place.\nAnyway, the combination of time spent and scroll depth reached is great for evaluating content digestion, and in some cases it might even make sense to use this combination to make Bounce Rate more meaningful.\n  With the introduction of the Timer trigger and the Scroll Depth trigger, we now have a more elegant way of implementing the combination of time spent and scroll threshold reached using Google Tag Manager. In this article, I\u0026rsquo;ll show you how to do it.\nThe logic The idea is simple. Knowing that in GTM, multiple triggers attached to a tag follow OR-logic, we can add both a Scroll Depth trigger and a Timer trigger to the tag, but establish a dependency between these two triggers so that the tag doesn\u0026rsquo;t execute until both of the triggers have fired.\nBasically, in the Scroll Depth trigger, we also check whether the Timer trigger has fired, by querying if gtm.timerInterval has been pushed into dataLayer with a value that equals the interval we set for the Timer trigger.\nConversely, in the Timer trigger, we check if gtm.scrollDepthThreshold has been pushed into dataLayer with the threshold we are waiting for the user to scroll past.\nIn other words, the tag will not execute until both of these triggers have fired.\nCreate the variables For this, you\u0026rsquo;ll first need to activate the Scroll Depth Threshold Built-in variable by browsing to Variables in the GTM user interface, and then clicking the red CONFIGURE button under the Built-in Variables heading.\nIn the overlay that opens, check the box next to Scroll Depth Threshold, like so:\n  Next, you\u0026rsquo;ll need to create a Data Layer variable for the key gtm.timerInterval, which returns the interval you are about to configure in a Timer trigger. For this, scroll down to User-defined Variables in the Variables user interface, and create a new variable that looks like this:\n  And that\u0026rsquo;s it for the variables you\u0026rsquo;ll need for this guide.\nCreate the triggers You\u0026rsquo;ll need two triggers for this. First, a Scroll Depth trigger, where you establish the scroll depth threshold you want the user to scroll past for it to count as engagement. Second, a Timer trigger, where you establish the minimum amount of time the user needs to spend on the page for it to count as engagement.\nRemember, it\u0026rsquo;s the combination of these two that will fire your tag.\nFor example, I\u0026rsquo;m going to set the scroll depth requirement to 50 percent scrolled, and I only want to fire the trigger on my article pages. In addition to this, I want the user to spend a minimum of 30 seconds on the page for the combination to count as engagement.\nWith this in mind, here\u0026rsquo;s what the Scroll Depth trigger will look like:\n  Two important things to note here. First, the Vertical Scroll Depths setting has a single value of 50 in the Percentages field. This means the trigger will fire as soon as the user scrolls past 50 percent of any page where this trigger is active on.\nSecond, one of the Fire this trigger when\u0026hellip; conditions is\nDLV - gtm.timerInterval equals 30000\nThis means that even though the Scroll Depth trigger itself fires, any tag it\u0026rsquo;s attached to will not fire unless a timer trigger has already fired, where the timer interval was 30 seconds (that\u0026rsquo;s 30000 milliseconds).\nThe Timer trigger looks like this:\n  As you can see, the Timer has a Limit of 1 (i.e. it will only fire once), and an Interval of 30000 (i.e. it will fire 30 seconds after it was activated).\nThe Fire this trigger when\u0026hellip; condition for the Timer trigger is similar to the logic of the Scroll Depth trigger. When the Timer trigger fires, it will not fire any tags to which it is attached, unless a Scroll Depth trigger has laready fired with a scroll threshold of 50 (the percentage I configured in the Scroll Depth trigger).\nNow, let\u0026rsquo;s add these two triggers to a tag, and perhaps the logic will become clearer.\nFinalizing the setup Create a new Google Analytics Event tag (or whatever tag you want to fire when this engagement is recorded), and add both of the two triggers you created to that tag, like so:\n  When you add both of the triggers to a single tag, the tag could potentially fire when either one of the triggers fires (because multiple triggers employ OR-logic).\nHowever, in both of these two triggers, you\u0026rsquo;ve set the condition that the trigger will not fire the tag until the other trigger has already triggered.\nIn other words, the tag will not fire until both triggers have fired, which means it will fire for the last of the two that fires on the page.\nClear? Good.\nPreview it! Here\u0026rsquo;s what it looks like in Preview mode.\n  The Preview mode output is easy to interpret. The tag has only fired once on the page, even though it had two triggers. When the gtm.scrollDepth event was pushed into dataLayer, the tag did not fire, because the gtm.timer even hadn\u0026rsquo;t happened yet.\nFinally, after 30 seconds, the gtm.timer event is pushed into dataLayer, the trigger finds that the Scroll Depth threshold had already been passed a long time ago, and thus the tag fires.\nSummary I hope this trick proves useful. It\u0026rsquo;s a great way to understand better how GTM\u0026rsquo;s triggers can work in unison.\nI still wouldn\u0026rsquo;t hinge my entire content engagement measurement plan around this (there are far more useful ways to measure engagement), but it can definitely be a great way to make your Scroll depth trigger more meaningful (or, conversely, your Timer trigger more useful).\nWhat do you think of this solution? Do you have suggestions for how to improve it? Sound off in the comments, please!\n"
},
{
	"uri": "https://www.simoahava.com/tags/scroll-depth/",
	"title": "scroll depth",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/timer/",
	"title": "timer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/prevent-clicks-and-form-submits-redirect/",
	"title": "#GTMTips: Prevent Clicks And Form Submits From Redirecting",
	"tags": ["google tag manager", "gtmtips", "preview mode"],
	"description": "Three (plus some bonus) tricks to prevent click and form submit redirections from messing with Google Tag Manager&#39;s preview mode use.",
	"content": "Google Tag Manager offers us some nice built-in triggers so that we can automatically listen for specific user interactions on the website, reacting to them however we wish, though typically it would be to fire a tag. The tricky thing especially with the click triggers and form submission tracking is that the page has a nasty habit of redirecting you to the link or form target page before letting you see the respective data in Google Tag Manager\u0026rsquo;s excellent preview mode.\nThere are many solutions to this issue, and I want to share some of them in this #GTMTips article.\nTip 89: Prevent click and form redirects from messing with Preview mode use   So, let\u0026rsquo;s go over some solutions to this popular problem.\nThe silly ESC key trick I first want to share you the easiest and also the silliest (and many times not at all effective) workaround. Just try to press the ESC key in your keyboard before the page has time to redirect you. This often halts the process, letting you stay on the current page.\nWhy is this silly? Because first of all, it\u0026rsquo;s one of those things you sometimes end up doing 20 times in a row, not getting it right, and then thinking there must be more to life than this.\nAlso, if the redirect is handled in a custom way not related to the actual browser click or submit events, pressing the ESC key won\u0026rsquo;t even help.\n1. CMD/CTRL + Click This is typically the easiest way to handle the redirect problem. It\u0026rsquo;s also very simple.\nInstead of just clicking the link or the submit button, simply hold down the CMD key (OS X) or the CTRL key (Windows) and press the left mouse button as you normally would.\nThis key combination automatically opens a new tab where the redirect is handled. Thus, the old tab is left on the page where the click happened, allowing you to inspect the Preview mode result without interruption.\nThis won\u0026rsquo;t work, again, if the redirect is handled with custom code not related to the actual click or submit browser events. This is quite typical especially with forms, but with most link click events this should work pretty nicely.\n2. Custom beforeunload script This solution should work with all redirects regardless of how they\u0026rsquo;re implemented. The trick is to add an event listener to the beforeunload browser, opening a prompt that asks if you really want to leave the current page. You can then press \u0026ldquo;Cancel\u0026rdquo; (or equivalent) to stay on the current page to see what the browser event wrote into dataLayer. To do this, you need to open the JavaScript console in the browser and copy-paste the following code, pressing enter when done:\nwindow.addEventListener(\u0026#39;beforeunload\u0026#39;, function(e) { e.preventDefault(); e.returnValue = \u0026#39;\u0026#39;; });  Now when submitting the form or clicking a link, a prompt should pop up and you can press Cancel to see the Preview mode output.\n  3. Automatically pause page upon a beforeunload event Using Google Chrome\u0026rsquo;s amazing DevTools, you can actually have the page pause in the debugger when a beforeunload event is dispatched.\nIt doesn\u0026rsquo;t let you interact with Preview mode (because that would require the page to un-pause), but you can use the JavaScript console, exploring the dataLayer object there.\nTo do it, you need to open DevTools, select the Sources tab, scroll down the Event Listener Breakpoints, expand Load, and check the box next to beforeunload.\n  Once you\u0026rsquo;ve done this, when you try to submit a form or click a link, the page will pause with a grey overlay, and you can then interact with the JavaScript console:\n  It\u0026rsquo;s not as nice as the previous tip, because you can\u0026rsquo;t interact with Preview mode.\nBonus tip: use a browser extension Personally, I prefer using a browser extension to keep a record of what\u0026rsquo;s stored in dataLayer. No, it\u0026rsquo;s not the same as using Preview mode, but it does let me access all the information that was pushed into dataLayer.\nMy personal favorite is dataslayer simply out of habit, but there are other really nice extensions out there that give you even more functionality:\n  Data Layer Inspector+\n  GTM Debug\n  WASP.inspector\n    Summary Do you have other tips to add to the list?\nAs I wrote above, my personal preference is using a browser extension, because they usually come packed with other goodies as well. However, GTM\u0026rsquo;s Preview mode is not something to ignore, so if you really want to preserve its usefulness on the current page, then leveraging the beforeunload event might be the way to go (if the click or ESC tricks don\u0026rsquo;t work).\nI hope that some day GTM introduces persistence to the Preview mode data, storing history to browser storage so that you could see the full content of Preview mode visited on previous pages, too!\n"
},
{
	"uri": "https://www.simoahava.com/tags/gtm-tools/",
	"title": "gtm tools",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/workspace-mode-gtm-tools/",
	"title": "Workspace Mode In GTM Tools",
	"tags": ["google tag manager", "gtm tools", "api"],
	"description": "Introducing the latest feature to GTM Tools, Workspace mode. With this, you can mass update items in a Google Tag Manager workspace.",
	"content": "It\u0026rsquo;s time for a big feature update to my GTM Tools, a free tool for managing your Google Tag Manager containers, tags, triggers, variables, and now: workspaces.\n  In this article, I\u0026rsquo;ll quickly go over the main features of Workspace mode. Be sure to check out the updated Release Notes \u0026amp; User Guide.\nIntroduction First of all, you can access Workspace mode through the container selection screen, or via the container page:\n  Workspace mode is a pretty big departure from what GTM Tools has been capable of until now. For long, I had been trying to figure out how to elegantly allow mass-updating individual items within GTM, but finally I found the time to make things work, utilizing workspaces to keep things in check.\nIn Workspace mode, the first release includes the following features:\n  Pause / unpause selected tags.\n  Edit the folder of any given item.\n  Rename any item.\n  Edit the raw JSON of any item.\n  As you can see, the feature list is limited. But I\u0026rsquo;ve built the mode in such a way that adding new features will be easy in the future. I\u0026rsquo;m looking forward to adding things like removing unused variables and adding triggers to tags.\n  Current caveats Here are some things you should know about Workspace mode.\n  It does not display changes made or conflicts existing in the workspace itself. This is something I hope to add soon.\n  Changes are made only in the current workspace you are editing. Perhaps at some point I will add the option of copying items / changes to another workspace, too.\n  If you edit the raw JSON of an item, then any other changes you make will override the changes in the JSON. In other words, if you, for example, pause an item using the Workspace mode \u0026ldquo;Pause / Unpause\u0026rdquo; action, and then you also set the flag paused to false in the JSON, the pausing action will override the change you made directly in the JSON.\n  Those are the main caveats at the moment.\nAs always, editing items is at your own risk. Remember that any changes you make to the workspace can potentially disrupt the work others have been putting into it.\nThankfully, workspaces is a pretty solid feature itself, so if you make silly changes, it\u0026rsquo;s easy to roll back by either abandoning individual changes or just deleting the whole workspace and starting from scratch.\nFeatures Here are the features in Workspace mode. Any assets that you change in Workspace mode will be highlighted with a red line in the beginning of each changed row:\n  The Changes column will also display an icon reflecting each change. Finally, the respective Actions menu selection will be bolded if a change has been made there, and if you change the name of an item, the name field will be bolded, too.\nUpdate a single item Any feature can be applied to a single item by clicking the Action menu at the end of the item row:\n  Mass-update multiple selected items You can also apply any change (except Edit JSON and renaming the item, which must always be done row-by-row individually) by first selecting the items you want to mass-update, and then hovering over the category header to choose the action:\n  Naturally, each selection in the category header only applies to selected items within that category.\n1. Pause / Unpause (tags only) Paused tags are indicated with a grey background. You can change the pause status of a tag by choosing Pause / Unpause from the Action menu, or by selecting the tag and changing its status using the category header.\n  2. Edit folder You can change the folder assignment of a single item via the Action / Edit Folder option. You can change the folder assignment of multiple folders at once by selecting them first (by clicking their respective checkboxes), and then choosing edit folder from the category header.\nWhen you choose to edit a folder assignment, a new overlay will pop up. If you chose Edit Folder through the Action menu, then you\u0026rsquo;ll see the current folder for the item in bold font (or (none) if the item doesn\u0026rsquo;t belong to a folder). By selecting a folder in the list of folders, all the selected items will be added to this folder.\n  3. Rename item You can change the name of any item by simply editing the text in the text field of the Name column. All changed names are shown in bold font.\n  4. Edit JSON You can edit the raw JSON of any item by selecting Edit JSON from the Action menu.\n NOTE! This is completely at your own risk. If the JSON has even a single field in the wrong place, a required field missing, or a misconfigured value, the item will not update. Be sure to familiarize yourself with the resource requirements of the GTM API: tags, triggers, and variables.\n Note also that any changes you make to the JSON will be overwritten by other changes you make via the Workspace mode UI. So if you, for example, change the name of the item in the JSON to \u0026ldquo;New name\u0026rdquo;, but then you type \u0026ldquo;Some other name\u0026rdquo; into the name field, the final value will be \u0026ldquo;Some other name\u0026rdquo;.\n  The Edit JSON makes use of the open-source jsoneditor tool.\nReset changes You can reset any changes made in Workspace mode by choosing Reset changes from the Action menu, or the respective selection in the category header when you\u0026rsquo;ve selected one or more items.\nDo note that Reset changes only applies to changes made in Workspace mode. It won\u0026rsquo;t reset any changes you\u0026rsquo;ve made to the workspace in the Google Tag Manager UI.\nUpdate the workspace Once you\u0026rsquo;re ready to update the workspace with the changes you\u0026rsquo;ve made, click the green Update workspace button, and a new overlay will pop open. There will be a list of all the changes you\u0026rsquo;ve made. Once you\u0026rsquo;re ready, click the green Update button to start the update process.\n  The update will process each item one at a time, and once the update is complete, a green checkmark will appear next to the item. Any items that failed to update (due to e.g. JSON errors) will have a red cross next to them.\nOnce the update is complete, you must reload the page. Otherwise the workspace loaded in Workspace mode will not be synchronised with the latest changes you just updated the workspace with.\nSummary These were the main features of Workspace mode. I will be adding new stuff to it over time, but I won\u0026rsquo;t update this article to reflect the new additions. Instead, I recommend you follow the Release notes \u0026amp; User guide as that will (hopefully) always be up-to-date with the latest features.\nPlease let me know in the comments what you think of this release, and what suggestions you might have for future iterations of Workspace mode!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/enhanced-ecommerce-guide-for-google-tag-manager/",
	"title": "Enhanced Ecommerce Guide For Google Tag Manager",
	"tags": ["google tag manager", "enhanced ecommerce", "guide"],
	"description": "Guide to Google Analytics&#39; Enhanced Ecommerce, and how to implement it using Google Tag Manager.",
	"content": "Enhanced Ecommerce is certainly one of the finest reporting user interface features that Google Analytics has to offer. Enhanced Ecommerce, as the name implies, is a set of dimensions, metrics, and reports, which combine to provide you with a fairly complete view into how users are interacting with your products in your webstore. The main downside of Enhanced Ecommerce is, as with all good things, that it\u0026rsquo;s complicated to implement.\n  Luckily, there\u0026rsquo;s Google Tag Manager. Theoretically, you could implement Enhanced Ecommerce through Google Tag Manager without ever adding a single line of functional logic into the codebase of your web site. You could do everything by simply scraping the Document Object Model (DOM). However, as we have hopefully learned over the years - GTM is a tool with which fastidiousness trumps fast.\nWhenever you\u0026rsquo;re working with dataLayer, for example, you\u0026rsquo;re investing into the stability of your data collection process. Thus, even though I will be talking about DOM scraping further on in the guide, the focus here will be on a setup where the front-end produces HTML documents and/or JavaScript that populates the Ecommerce values into the dataLayer without using DOM scraping or other client-side hacks to do the job.\nThere is a lot of information in this article. Much of it is already covered in Google\u0026rsquo;s excellent Enhanced Ecommerce Developer Guide, but there are plenty of gotchas that I might have covered in earlier articles (see the Further Reading chapter), or which I might not even have written down before.\n NOTE! This is an implementation guide. If you want to know how Enhanced Ecommerce works or how to map your site into relevant interactions, you\u0026rsquo;ll want to look at Google\u0026rsquo;s support center documentation.\n How to read this guide Obviously, it would make sense if you read the guide from top-to-bottom. However, I fully acquiesce that the loquaciousness of my rhetoric might be tedious to some in all its verbose glory. Also, sometimes I write douchy things like that.\nSo, depending on what your role is in the implementation project, there might be only some chapters that are relevant for you. However, don\u0026rsquo;t neglect to consult with all the stakeholders in the project! It doesn\u0026rsquo;t make sense for a developer to write code without understanding what the code is needed for. Similarly, it doesn\u0026rsquo;t make sense for the person operating Google Tag Manager to utilize the dataLayer structures without understanding the limitations of the back-end or front-end systems.\n If you are a developer, tasked with implementing the dataLayer objects, you\u0026rsquo;ll want to read the following chapters (in order):\n  Introduction Populate the Data Layer A word on scraping the DOM Combining different data types in a single EEC hit Product-scoped Custom Dimensions and Metrics Remember: be consistent Data types: Actions and all its sub-sections Data types: Impressions and all its sub-sections Data types: Promotions and all its sub-sections   If you are the person tasked with setting up EEC via the Google Tag Manager User Interface, you\u0026rsquo;ll want to focus on the following chapters (in order):\n  Introduction Differences (and similarities) to Standard Ecommerce Send the data to Google Analytics using the Data Layer Send the data to Google Analytics using a Custom JavaScript variable Page View or Event tag Data types: Actions and all its sub-sections Data types: Impressions and all its sub-sections Data types: Promotions and all its sub-sections Debugging the setup Golden rules of Enhanced Ecommerce  As I said, I still wish you\u0026rsquo;d spend the time reading the entire guide. I don\u0026rsquo;t think there\u0026rsquo;s any irrelevant information to anyone working on the project, and it makes sense to understand the full complexity of the implementation rather than focus only on the things that are directly relevant to your work.\nIntroduction Implementing Enhanced Ecommerce requires an overview of the following components:\n  The various data types that can be sent to GA (Actions, Impression Views, Promotion Views).\n  How the 'ecommerce' object is compiled and pushed to dataLayer (or generated with a Custom JavaScript variable).\n  How this data is sent with a tag to Google Analytics.\n  All three of these components are necessary, and all three require some understanding of how Google Tag Manager works, how tags and triggers interact with dataLayer, and how the Enhanced Ecommerce data is sent to Google Analytics. These will all be covered in this guide.\n  Needless to say, if you want a robust implementation, interaction and co-operation with your site\u0026rsquo;s front-end developers is an absolute must. Typically, the front-end developers utilize page templates and server-side variables (using PHP or React.js, for example) to build the HTML structure of any given page. On Enhanced Ecommerce pages, it\u0026rsquo;s important that they integrate the front-end with whatever ecommerce system you have in use, so that this system produces the necessary values dynamically for the front-end developers to funnel through into the dataLayer.\nFor interactions that take place after the page load, such as cart interactions, the front-end developers might need update the site JavaScript to populate the dataLayer object with the necessary values in the client.\nOr you can just throw caution to the wind and scrape everything off the HTML itself through Google Tag Manager. The purpose of this guide isn\u0026rsquo;t to recommend any specific course of action, but to highlight the different ways in which you can run the Enhanced Ecommerce implementation project.\nDifferences (and similarities) to Standard Ecommerce If you\u0026rsquo;ve been running Standard Ecommerce through Google Tag Manager, there are mainly three major differences between that and Enhanced Ecommerce.\n  Standard Ecommerce only collects transactions (i.e. \u0026ldquo;Purchase\u0026rdquo; hits in EEC) - there are no other components to its funnel (in fact, there is no funnel).\n  Standard Ecommerce uses a different category format.\n  Standard Ecommerce doesn\u0026rsquo;t support Product-scoped Custom Dimensions and Metrics.\n  One thing to pay heed to is that you can collect both Standard Ecommerce and Enhanced Ecommerce hits to an Enhanced Ecommerce -enabled Google Analytics view! This means that if you leave your \u0026ldquo;old\u0026rdquo; tracking in place and run an Enhanced Ecommerce implementation in parallel, you might end up double-counting transactions.\nGoogle Analytics does deduplicate identical transactions sent in a session, but it\u0026rsquo;s enough for some component in the two different tracking methods to differ for GA to collect the data twice.\nProduct Category is collected to a different dimension, depending on which method you are using. So if you populate the category key for a product using either method, the dimension you\u0026rsquo;re looking for in GA is going to be different.\n  Implementation guidelines The following chapters will cover certain aspects of an Enhanced Ecommerce implementation. As I stated in the Introduction, how the values are populated into dataLayer depend on the integration between your site\u0026rsquo;s front-end and the Ecommerce platform you use. Some platforms such as WooCommerce for WordPress introduce this integration using a plugin, others such as Magento provide it out-of-the-box as a feature.\nHowever, there are plenty of custom Ecommerce platforms and setups out there which require tinkering for them to produce the values into variables that your front-end developers can then utilize.\nThe next chapter, for example, simply instructs what the generic dataLayer object needs to look like. How you end up populating the values into the required keys is up to you, but typically it\u0026rsquo;s done in one of the following ways:\n  The dataLayer.push() is written directly into the page template by whatever server-side mechanism you have for producing the templates.\n  The dataLayer.push() is generated dynamically by your web server or the Ecommerce platform using typically a combination of JavaScript and HTTP requests.\n  The key-value pairs are populated by scraping the page HTML using Google Tag Manager.\n  Populate the Data Layer Enhanced Ecommerce requires a specific syntax for the objects pushed to dataLayer or generated with a Custom JavaScript variable. For example, a dataLayer.push() with the minimum required Ecommerce object for a Purchase looks like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.purchase\u0026#39;, ecommerce: { purchase: { actionField: { id: \u0026#39;1\u0026#39; }, products: [{ id: \u0026#39;p\u0026#39; }] } } });  Do note that Google Tag Manager only reacts to dataLayer.push() commands. No other array methods will do anything in GTM, so it\u0026rsquo;s imperative that you do not improvise with this. All the interactions with dataLayer must be done with push().\nWithin the push, all the keys listed under and including ecommerce are required. The event key is something I strongly recommend to add to every single dataLayer.push() object.\nNow, you will need to populate the keys in the object with relevant values. For example, the value for ecommerce.purchase.actionField.id would need to be a unique identifier of the order itself. You could generate one randomly using JavaScript, but typically you\u0026rsquo;ll want to use the same order ID that was included in the actual transaction. That way you can later augment details about this transaction using Data Import, for example.\nSimilarly, you\u0026rsquo;ll need to collect details about the products in the transaction, and send any variant, brand, and category information as metadata, too.\nHow you do this really depends on what the integration to your Ecommerce system is like. It\u0026rsquo;s impossible to give generic instructions, because each Ecommerce platform differs in how it provides the values for you to exploit in other on-site code. But if you already have an Ecommerce system in place, I recommend you read through its documentation and try to find out how you can request or access these values and variables from this system.\nSend the data to Google Analytics using the Data Layer When you select the Use data layer option in the Enhanced Ecommerce settings of a Google Analytics Page View or Event tag (or a Google Analytics Settings variable), you instruct the tag to look into the Data Layer and pull the key-value pairs from the ecommerce object pushed most recently into dataLayer.\n  This sounds pretty straight-forward. All you need to do is make sure the object pushed into dataLayer follows the exact correct syntax, and that it\u0026rsquo;s pushed into dataLayer before (or in the same push as) the tag that accesses the data via Use data layer fires.\nHowever, there are two important things to note here.\n  The dataLayer.push() must be perfectly formed. There\u0026rsquo;s no way to modify it on-the-go if you activate the \u0026ldquo;Use data layer\u0026rdquo; option. So it\u0026rsquo;s absolutely vital that you specify and audit the object that is being pushed into dataLayer for Enhanced Ecommerce data collection.\n  When a tag with \u0026ldquo;Use data layer\u0026rdquo; fires, it really only accesses the most recent ecommerce push. So, if you\u0026rsquo;ve split Product Impressions into multiple pushes and then fire the tag when the last one is done, if the tag has \u0026ldquo;Use data layer\u0026rdquo; checked, only the last impressions object is sent to Google Analytics.\n  Point (2) might sound counter-intuitive, but it actually makes sense. If GTM didn\u0026rsquo;t exhibit this behavior, then all ecommerce pushes done on a single page would tag along with all the subsequent Enhanced Ecommerce payloads. By sending only the most recently pushed object, this behavior can be avoided.\nIt\u0026rsquo;s precisely because of this behavior that I again strongly recommend to always include the event key in every single Enhanced Ecommerce object. Why? Because then you can use Custom Event triggers to fire your Enhanced Ecommerce tags at precisely the correct time, and the most recent object they access will always be the one that triggered the tag. That way you won\u0026rsquo;t lose any data because of pushes that didn\u0026rsquo;t trigger any tags.\nHowever, you might want to take a look at the next section, too.\nSend the data to Google Analytics using a Custom JavaScript variable One of my main issues with the dataLayer object syntax for Enhanced Ecommerce in Google Tag Manager is its rigidity. The \u0026ldquo;Use data layer\u0026rdquo; option requires that you follow a very specific, unwieldy syntax.\nThis means that it might be a huge effort to get the data into the correct format in the first place. But then you also need to have a process in place so that you can update it with minimal delay, and so that you can audit it to make sure no data is lost.\nSo, thankfully, Google Tag Manager also lets you send the data using a Custom JavaScript variable.\n  Using the variable has a very simple logic. All the variable needs to do is return a valid ecommerce object. In other words, the variable needs to return exactly the same thing that the developers would push into dataLayer in a perfect world.\nAnd how is this useful? Well, you could have the Ecommerce data pushed into dataLayer in any format, and then use the Custom JavaScript variable to transform it into a valid ecommerce object. This way, you don\u0026rsquo;t need to confuse matters with a rigid dataLayer specification, but rather the Ecommerce platform can push the data into dataLayer in whatever format suits it best. Then, with some JavaScript magic, you can pull this data through the Custom JavaScript variable into your tag.\nOr, if you want to go the DOM scraping route, you can do all the scraping in the Custom JavaScript variable, and have it return a valid ecommerce object with all the key-value pairs in place.\nYou can also modify a valid ecommerce object in the dataLayer in case you need to add some dynamic updates to it (such as Product-scoped Custom Dimensions). For example, here\u0026rsquo;s me adding a Product-scoped Custom Dimension to all the products in a Purchase object:\nfunction() { // Create a copy of the variable to avoid  // modifying the source object by reference  var ecom = JSON.parse(JSON.stringify({{Data Layer Variable - ecommerce}})); try { ecom.purchase.products.forEach(function(prod) { prod.dimension10 = {{get discount}}(prod.id); }); } catch(e) {} return { ecommerce: ecom }; }  With this simple example, I\u0026rsquo;m updating a perfectly valid ecommerce object (represented by {{Data Layer Variable - ecommerce}}), by adding a new field to every single product.\nAs you can see, I\u0026rsquo;m making a copy of the original variable with the JSON.parse(JSON.stringify(obj)) method. Because JavaScript copies objects by reference, any changes you make to the ecom variable would reflect in the original object stored in dataLayer. This might have adverse effects if the variable is run more than once, especially if you\u0026rsquo;re running calculations on any fields (the calculations would be done over and over again).\nOne additional important thing to note. Data Layer Variables are, by default, Version 2. This means that they will merge all the existing ecommerce pushes into one big ecommerce object. So when you create the variable for ecommerce, you might want to use Version 1. Read this article for more information.\nPage View or Event tag Whether you want to use a Page View or Event tag and how you setup your Custom Event triggers is completely up to you.\nPersonally, I always use Event tags. Why? Because if you use a Page View tag, you lack insight into what Enhanced Ecommerce events have been collected in Google Analytics. This might make debugging a broken implementation difficult. By using events, I can always look at the Top Events report to see all the Enhanced Ecommerce hits sent to GA.\n  You can even add things like order ID as the Event Label of these hits, so that you can really start debugging if the event for the order was sent but no transaction was actually received in the Ecommerce reports.\nIf you do use an Event tag, setting the nonInteraction field is, again, up to you. Personally, I set it to true when the event is sent with the page load, and false in all other instances.\nBut you do exactly what works best for you. Sometimes you might want to avoid events if you\u0026rsquo;re approaching the 500 hits per session limit. Not a good reason, but a reason nonetheless.\nA word on scraping the DOM Scraping the DOM refers to the habit of accessing the dynamic HTML representation of the page (the Document Object Model) directly with JavaScript and populating the Enhanced Ecommerce key-value pairs with values found on the page. On a receipt page, for example, you often see the order ID, the total value of the transaction, all products, shipping costs, and so forth. So you could just as well scrape these from the page rather than going through a complicated specification project with your front-end developers.\nfunction() { var orderId = document.querySelector(\u0026#39;#order span.id\u0026#39;).textContent, revenue = document.querySelector(\u0026#39;#order span.total\u0026#39;).textContent, products = [].map.call(document.querySelectorAll(\u0026#39;#order .product\u0026#39;), function(prod) { return { id: prod.getAttribute(\u0026#39;data-id\u0026#39;), name: prod.innerText }; }); return { ecommerce: { purchase: { actionField: { id: orderId, revenue: revenue }, products: products } } }; }  It\u0026rsquo;s not pretty, and it comes with a lot of baggage and technical debt. For one, you\u0026rsquo;re introducing a huge weak link in your tracking. If anything changes on the page, e.g. due to an A/B-test, it\u0026rsquo;s possible your scraper will break.\nFurthermore, it\u0026rsquo;s fairly common to not include all details about products on the receipt page. Typically, just the name, ID, price, and tax of the line item is enough. This means you\u0026rsquo;ll miss all the other vital metadata such as product category, brand, variant, etc.\nIn other words, scraping might be the easy way to get some data quick, and for that it\u0026rsquo;s a great tool. But used on its own to carry the data quality of your entire Enhanced Ecommerce process? Not so much.\nCombining different data types in a single EEC hit Enhanced Ecommerce in Google Tag Manager allows you to send more than one data type in a single Enhanced Ecommerce hit. This is because all the data types are deconstructed into parameters of the Google Analytics request, and as long as each parameter only has a single value, you can combine them to your heart\u0026rsquo;s content.\nHowever, you can\u0026rsquo;t combine them in any way you like. The rule of thumb is that each hit can only contain one Action, one Impression, and one Promotion object. But it gets a bit more complicated than that, since there are more shared fields between the different data types, invalidating certain combinations. Here are the valid combinations you can send:\n  Impression View with Promotion View and Action\n  Impression View with one of Promotion View or Action\n  Promotion View with one of Impression View or Action\n  See the chapter titled Data types: Actions to understand what Actions comprise. Note that Impression Click and Promotion Click are Actions even if they\u0026rsquo;re listed in a different chapter (it was more logical to group them with impression and promotion views, respectively).\nA typical mistake is to try to send a Product Detail View and a Product Add To Cart in the same push, or a Product Add To Cart and a Product Checkout in the same push. This isn\u0026rsquo;t valid, since these objects occupy many of the same parameter names (e.g. all the product keys), so by trying to populate them twice in the same hit, you end up making a royal mess of things.\nProduct-scoped Custom Dimensions and Metrics With Enhanced Ecommerce, Google Analytics introduced Product-scoped Custom Dimensions and Metrics. These are extra metadata that can be added directly into the objects within a products array.\nThe dataLayer syntax is, again, fairly rigid. They must be named dimensionN and metricN (where N is the index number), and they must be embedded in the product object that is included in the products array. For example, this is a valid way to send a Product-scoped Custom Dimension into index 5:\n{ ecommerce: { purchase: { actionField: { id: \u0026#39;t1\u0026#39; }, products: [{ id: \u0026#39;p1\u0026#39;, dimension5: \u0026#39;Black\u0026#39; // Product-scoped Custom Dimension in index 5  }] } } }  As you can see, dimension5 is in the object within in the products array.\nBecause of this rather rigid way of adding the dimensions and metrics, I suggest taking a look at the Custom JavaScript variable option for mutating the object pushed into dataLayer into a valid Enhanced Ecommerce object.\nThe Product List Product List is a very important attribute in Enhanced Ecommerce tracking. For one, it\u0026rsquo;s really one of the only places in Enhanced Ecommerce where some sort of attribution takes place. But also, it\u0026rsquo;s a great way to look at how different parts of the site contribute to the success of your ecommerce efforts.\nThe Product List is a key that you can add into the actionField object of all Action data types. You can also add it to Impression View objects directly.\nThe attribution works in a very specific way. When you send the list attribute in one of your Enhanced Ecommerce actions or impressions, then after that every single subsequent Enhanced Ecommerce action will be credited to that list as long as the product IDs in those subsequent actions match those of the one where you sent the list.\n  In other words, if you send a Product Detail View for a product with ID 123, and you use the list value \u0026ldquo;discount\u0026rdquo;, then every single subsequent Enhanced Ecommerce action for a product with ID 123 within that session will be attributed to that list.\nAs soon as the Product ID changes or you send some other list value, this funnel will break and a new one will start.\nYou\u0026rsquo;ll also see plenty of (not set) product lists in your reports. This is due to sessions expiring and new ones starting without any Product List information. Naturally, if your funnel doesn\u0026rsquo;t include a Product List for all possible interactions, you\u0026rsquo;ll see (not set) due to this, too.\nDo note that attribution is always session-scoped, so if the session timeout is reached or the session is broken for some other reason, attribution will break, too.\nIf this sounds confusing, please read my article on the topic.\nLocal Currency If visitors to your site can choose to shop with different currencies, you can utilize Google Analytics\u0026rsquo; Local Currency feature to normalize these values once they hit your GA reports. All you need to do is specify the currency code either directly in the dataLayer object, or as a field in the tag which sends the Enhanced Ecommerce data to GA.\nFor example, here\u0026rsquo;s what a dataLayer.push() for a purchase made with Euros would look like:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.purchase\u0026#39;, ecommerce: { currencyCode: \u0026#39;EUR\u0026#39;, purchase: { actionField: { id: \u0026#39;order_id\u0026#39;, revenue: \u0026#39;11.00\u0026#39; }, products: [{ id: \u0026#39;product_id\u0026#39;, price: \u0026#39;11.00\u0026#39;, quantity: 1 }] } } });  Note that if you use a local currency, remember to set keys like revenue and price in that local currency (so no need to do any conversions in the client).\nGA will then convert these local currencies to whatever currency you have configured in the Google Analytics view.\nIf you want to add this currency code as a field in the tag (or Google Analytics Settings variable), you can add it by browsing to Fields to Set, and adding a new field:\nField name: currencyCode\nValue: EUR\nInstead of \u0026ldquo;EUR\u0026rdquo;, you can use a variable that pulls the currency code from e.g. the Data Layer or a browser cookie.\nRemember: be consistent One very important thing about Enhanced Ecommerce is that it\u0026rsquo;s (mostly) hit-scoped. The only exception is the minimal attribution with Product Lists and promotions.\nIn other words, if you send a Product Category of T-Shirt with a product in a Product Detail View, that information will only be available to queries involving Product Detail Views and that particular product. Nothing carries over or persists to the subsequent hits.\nYou\u0026rsquo;ll need to include all metadata about all products and all actions in all Enhanced Ecommerce objects.\n  This is one of the reasons why DOM scraping is not a good idea. It\u0026rsquo;s rare to see the same information on every single page where you have Enhanced Ecommerce actions.\nThis is also one of the reasons why implementing Enhanced Ecommerce might be a headache to developers. It\u0026rsquo;s possible that the different parts of the funnel are governed by different systems. For example, a fairly typical CMS setup could handle Product Detail Views, but then cart additions are done using a SaaS cart service, checkout is done using PayPal, and purchase receipt page is yet another system.\nIn order to persist all product data from one page to another, you might need to introduce client-side persistence using cookies or browser storage, for example.\nData types: Actions Actions are Enhanced Ecommerce events that include some type of user-initiated action. This isn\u0026rsquo;t always a clear definition, and e.g. Product Detail Views could arguably not be user-initiated actions. But that\u0026rsquo;s the definition that Google uses.\nThe following chapters will depict what the dataLayer object for each action should look like, and what a typical tag/trigger combination would be.\nProduct Detail Views Product Detail Views are typically sent when the user views details about a product. This is, again typically, on a dedicated product page, but it could just as well be expanded content in a search results list, or something else.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.list String 'Expanded content' Where the Product Detail View occurred. Useful if you want to see how different \u0026ldquo;lists\u0026rdquo; contribute to funnel success.   products[].id String 'P12345' The SKU of the product. I recommend sending any variant IDs using a Product-scoped Custom Dimension.   products[].name String 'T-Shirt' The name of the product. Any variant name can be sent with the variant key.   products[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   products[].variant String 'Large' What variant of the main product this is.   products[].brand String 'NIKE' The brand name of the product.   products[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   products[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() for a Product Detail View would look like with all the relevant keys populated:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.detail\u0026#39;, ecommerce: { detail: { actionField: { list: \u0026#39;Search Results\u0026#39; }, products: [{ id: \u0026#39;product_id\u0026#39;, name: \u0026#39;MY PRODUCT\u0026#39;, category: \u0026#39;guides/google-tag-manager/enhanced-ecommerce\u0026#39;, variant: \u0026#39;Text\u0026#39;, brand: \u0026#39;SIMO AHAVA\u0026#39;, dimension3: \u0026#39;Ecommerce\u0026#39;, metric5: 12, metric6: 1002 }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.detail\nAdd to Cart The Add to Cart hit is sent whenever a product (or products) are added to cart. The quantity of each product needs to reflect the number of items that were added to the cart in this action, and not the final tally of each item in the cart.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.list String 'Search results' Where the Add to Cart happened. If there are multiple places on the site where products can be added to cart, it\u0026rsquo;s a good idea to differentiate between them using the Product List attribute.   products[].id String 'P12345' The SKU of the product.   products[].name String 'T-Shirt' The name of the product.   products[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   products[].variant String 'Large' What variant of the main product this is.   products[].brand String 'NIKE' The brand name of the product.   products[].quantity Integer 1 The quantity of this product added to cart.   products[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   products[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() for an Add to Cart would look like with all the relevant keys populated:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.add\u0026#39;, ecommerce: { add: { actionField: { list: \u0026#39;Shopping cart\u0026#39; }, products: [{ id: \u0026#39;product_id\u0026#39;, name: \u0026#39;MY PRODUCT\u0026#39;, category: \u0026#39;guides/google-tag-manager/enhanced-ecommerce\u0026#39;, variant: \u0026#39;Text\u0026#39;, brand: \u0026#39;SIMO AHAVA\u0026#39;, quantity: 2, dimension3: \u0026#39;Ecommerce\u0026#39;, metric5: 12, metric6: 1002 },{ id: \u0026#39;another_product_id\u0026#39;, name: \u0026#39;MY ADD-ON\u0026#39;, quantity: 1 }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The variable in Event Label is a Data Layer Variable that points to ecommerce.add.actionField.list. This returns the Product List of the hit, if any.\nThe trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.add\nRemove from Cart Remove from Cart is what you would send when items are removed from the cart. The quantity you send for each item needs to reflect the quantity that was removed in the action, and not the quantity of items remaining in the cart.\nData Layer composition The Data Layer composition for Remove from Cart is identical to that of Add to Cart.\nSimilarly, the dataLayer.push() example would be identical, except the first key under ecommerce is named remove rather than add, and the event name is different:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.remove\u0026#39;, ecommerce: { remove: { actionField: {...}, products: [...] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The variable in Event Label is a Data Layer Variable that points to ecommerce.remove.actionField.list. This returns the Product List of the hit, if any.\nThe trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.remove\nCheckout A Checkout is sent when the user enters the checkout funnel. For every discrete step in the funnel, you send a checkout hit to GA. Steps are typically pages (on a multi-page checkout) or sections of the form (on a single-page checkout).\nThe products array should only be sent with the first step. Sending it with any other step will do nothing.\nYou can also add an option key to the checkout hit, in case you want to send extra details about the checkout step itself (such as selected payment method or shipment method). Alternatively, you can use the Checkout Option action to send the option information after the step has first been sent-\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.step Integer 1 The number of the checkout step. These should be sent in order, where the first step is 1. You can define what these steps mean in the Google Analytics Admin user interface.   actionField.option String 'Visa' You can send the option key if there is extra metadata (e.g. shipping or payment method) you want to send with the step hit itself.   products[].id String 'P12345' The SKU of the product.   products[].name String 'T-Shirt' The name of the product.   products[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   products[].variant String 'Large' What variant of the main product this is.   products[].brand String 'NIKE' The brand name of the product.   products[].quantity Number 3 The quantity of the given product checked out.   products[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   products[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() for the first Checkout step would look like with all the relevant keys populated:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.checkout\u0026#39;, ecommerce: { checkout: { actionField: { step: 1 }, products: [{ id: \u0026#39;product_id\u0026#39;, name: \u0026#39;MY PRODUCT\u0026#39;, category: \u0026#39;guides/google-tag-manager/enhanced-ecommerce\u0026#39;, variant: \u0026#39;Text\u0026#39;, brand: \u0026#39;SIMO AHAVA\u0026#39;, quantity: 2, dimension3: \u0026#39;Ecommerce\u0026#39;, metric5: 12, metric6: 1002 },{ id: \u0026#39;another_product_id\u0026#39;, name: \u0026#39;MY ADD-ON\u0026#39;, quantity: 1 }] } } });  The dataLayer.push() for the second Checkout step where you also send the payment method would look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.checkout\u0026#39;, ecommerce: { checkout: { actionField: { step: 2, option: \u0026#39;MasterCard\u0026#39; } } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The variables {{checkout step}} and {{checkout option}} are Data Layer variables for ecommerce.checkout.actionField.step and ecommerce.checkout.actionField.option, respectively. They return the corresponding values from the checkout hit.\nThe trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.checkout\nCheckout option A Checkout option hit is always sent after the corresponding Checkout Step has already been sent. So you can\u0026rsquo;t send a Checkout option hit for step 2 if you haven\u0026rsquo;t first sent a regular Checkout hit for step 2.\nUse the Checkout option hit to send extra information about a checkout step. For example, if Checkout step 2 is where the user chooses the payment method, you\u0026rsquo;ll want to send Checkout step 2 when the user first lands on the payment method selection page, and then the Checkout option hit after they\u0026rsquo;ve clicked or selected the payment method.\nData Layer composition The dataLayer.push() is very simple. In actionField, you indicate with step to which Checkout step this option should be attached (Integer), and option is the option value itself.\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.checkout_option\u0026#39;, ecommerce: { checkout_option: { actionField: { step: 2, option: \u0026#39;Expedited delivery\u0026#39; } } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.checkout_option\nPurchase The Purchase hit is, arguably, the most important hit in the Enhanced Ecommerce funnel. Without a reliable Purchase hit being collected, all the funnels, attribution models, goal values, conversion calculations, and transaction data become fairly unusable.\nIn other words, make sure most of the resources is invested in getting the Purchase hit right.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.id String order_12345 The unique order ID of the transaction. Should match the actual ID of the order.   actionField.affiliation String 'Simo Shop' Extra details about where the purchase happened.   actionField.revenue String '11.00' Total transaction value. You can include tax and shipping, or you can choose to send the revenue without tax and shipping. The value must not include anything else except number separated by a decimal point. Don\u0026rsquo;t use a comma as the separator, and don\u0026rsquo;t include any currency symbols.   actionField.tax String '1.00' Tax paid. Same formatting instructions as with revenue.   actionField.shipping String '2.00' Cost of shipping. Same formatting instructions as with revenue.   actionField.coupon String 'SUMMER2019' The coupon code that was used for the entire transaction.   products[].id String 'P12345' The SKU of the product.   products[].name String 'T-Shirt' The name of the product.   products[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   products[].variant String 'Large' What variant of the main product this is.   products[].brand String 'NIKE' The brand name of the product.   products[].quantity Number 3 The quantity of the given product purchased.   products[].price String '10.00' The price of one item. Same formatting instructions as with revenue.   products[].coupon String 'SHIRTSOFF' The coupon code used for this particular product.   products[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   products[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() for a Purchase hit would look like with all the relevant keys populated:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.purchase\u0026#39;, ecommerce: { currencyCode: \u0026#39;EUR\u0026#39;, purchase: { actionField: { id: \u0026#39;ORDER12345\u0026#39;, affiliation: \u0026#39;Simo\\\u0026#39;s shop\u0026#39;, revenue: \u0026#39;11.00\u0026#39;, tax: \u0026#39;1.00\u0026#39;, shipping: \u0026#39;2.00\u0026#39;, coupon: \u0026#39;SUMMER2019\u0026#39; }, products: [{ id: \u0026#39;product_id\u0026#39;, name: \u0026#39;MY PRODUCT\u0026#39;, category: \u0026#39;guides/google-tag-manager/enhanced-ecommerce\u0026#39;, variant: \u0026#39;Text\u0026#39;, brand: \u0026#39;SIMO AHAVA\u0026#39;, quantity: 2, price: \u0026#39;3.50\u0026#39;, dimension3: \u0026#39;Ecommerce\u0026#39;, metric5: 12, metric6: 1002 },{ id: \u0026#39;another_product_id\u0026#39;, name: \u0026#39;MY ADD-ON\u0026#39;, price: \u0026#39;1.00\u0026#39;, quantity: 1, coupon: \u0026#39;ADD-ONS OFF\u0026#39; }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The variable {{order id}} is a Data Layer variable for ecommerce.purchase.actionField.id, and it returns the transaction ID. The variable {{revenue}} is a Data Layer variable for ecommerce.purchase.actionField.revenue, and it returns the revenue of the transaction.\nThe trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.purchase\nRefund The Refund hit should be sent if the user requests a refund of the product through the website.\nImportant: this does NOT remove the original transaction or negate it in any way. The Refund hit is a hit of its own, and will be used to populate metrics like \u0026ldquo;Refund Amount\u0026rdquo; and \u0026ldquo;Product Refund Amount\u0026rdquo;. This makes the Refund hit type fairly useless, in my opinion, but it might have its uses if you simply want to know whether refunds are requested via the website.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.id String order_12345 If the user requested a full refund of the order, then you only need to send the transaction ID with the hit.   products[].id String 'P12345' If the user only wanted a refund for some items, you need to also send the id of each along with the quantity refunded.   products[].quantity Number 3 The quantity of the given item refunded.    This is what the dataLayer.push() would look like for a completely refunded transaction:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.refund\u0026#39;, ecommerce: { refund: { actionField: { id: \u0026#39;ORDER12345\u0026#39; } } } });  This is what the dataLayer.push() would look like for a partially refunded transaction:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.refund\u0026#39;, ecommerce: { refund: { actionField: { id: \u0026#39;ORDER12345\u0026#39; }, products: [{ id: \u0026#39;another_product_id\u0026#39;, quantity: 1 }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The variable {{order id}} returns the transaction ID that was refunded. The variable {{total quantity refunded}} is a Custom JavaScript variable which returns the total quantity of all items that were refunded.\nThe trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.refund\nData types: Impressions Impressions are what you send when the user sees a listing of products in a Search Results page, for example. Basically, wherever you show a listing of products without going into enough detail for that to count as a Product Detail View, use impressions.\nImpressions comprise of a \u0026ldquo;view\u0026rdquo;, where the user sees the product in a listing, and a \u0026ldquo;click\u0026rdquo;, where the user clicks a product in the listing (to go to its Product Detail page, for example).\nNote that you might be tempted to send all the products on the listing page as impressions in a single hit. This might get you into trouble with Google Analytics\u0026rsquo; maximum payload size of 8KB (that\u0026rsquo;s typically around 30-50 product impressions). To avoid this, you can do a number of things, such as automatically reduce the payload length, or split the impressions into multiple hits.\nThe best thing you can do, but also the most complicated thing to implement, is to create a true view listener, where impressions are sent in batches depending on what the user actually sees in the viewport. So when the user scrolls to a new set of product impressions, you send only those impressions in a hit once they\u0026rsquo;ve all entered the viewport. This requires a lot of coordination with your developers, but it\u0026rsquo;s well worth the effort.\nImpression Views Impression Views are collected when the user sees a product impression in some listing, such as Search Results, Recently Viewed Products, or Related Products.\nWith Impression Views, it\u0026rsquo;s important to populate the list field with a label that describes the listing where the impression view happened.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     impressions[].id String product_id The SKU of the product.   impressions[].name String 'T-Shirt' The name of the product.   impressions[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   impressions[].variant String 'Large' What variant of the main product this is.   impressions[].brand String 'NIKE' The brand name of the product.   impressions[].list String 'Related products' The list name where the impression was viewed.   impressiosn[].position Integer 3 The position of the item in the list (e.g. was it the first item in the list, the second, the third, etc.).   impressions[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   impressions[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() would look like for a set of product impressions with all the relevant keys included:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.impressionView\u0026#39;, ecommerce: { impressions: [{ id: \u0026#39;product_1\u0026#39;, name: \u0026#39;Great article\u0026#39;, category: \u0026#39;guides/google-tag-manager/javascript\u0026#39;, list: \u0026#39;Related products\u0026#39;, position: 1, dimension3: \u0026#39;500 pages\u0026#39; },{ id: \u0026#39;product_2\u0026#39;, name: \u0026#39;Greater article\u0026#39;, category: \u0026#39;guides/google-tag-manager/java\u0026#39;, list: \u0026#39;Related products\u0026#39;, position: 2, dimension3: \u0026#39;1500 pages\u0026#39; }] } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.impressionView\nImpression Clicks Impression Clicks (or Product Clicks) are technically Actions, but I decided to group them under Impressions since that\u0026rsquo;s where they belong.\nAn Impression Click is sent when an impression that the user has viewed is clicked. It\u0026rsquo;s an important metric to follow in case you are measuring Impression Views, because once you start collecting Impression Clicks, too, you can measure things like click-through-rates per product list, position, and individual impression.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     actionField.list String 'Related products' The list where the user first viewed the product and then clicked it. Make sure the value of the list attribute matches that sent with the Impression View.   products[].id String 'P12345' The SKU of the product.   products[].name String 'T-Shirt' The name of the product.   products[].category String 'clothes/shirts/t-shirts' Product category of the item. Can have maximum five levels of hierarchy.   products[].variant String 'Large' What variant of the main product this is.   products[].brand String 'NIKE' The brand name of the product.   products[].position Integer 1 The position of the impression that was clicked.   products[].dimensionN String 'Blue' A Product-scoped Custom Dimension for index number N.   products[].metricN Integer 3 A Product-scoped Custom Metric for index number N.    This is what the dataLayer.push() for an Impression Click would look like with all the relevant keys populated:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.impressionClick\u0026#39;, ecommerce: { click: { actionField: { list: \u0026#39;Related products\u0026#39; }, products: [{ id: \u0026#39;product_2\u0026#39;, name: \u0026#39;Greater article\u0026#39;, category: \u0026#39;guides/google-tag-manager/java\u0026#39;, position: 2, dimension3: \u0026#39;1500 pages\u0026#39; }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.impressionClick\nData types: Promotions Promotions are similar to impressions in that they, too, comprise a View and a Click. However, unlike any of the other Enhanced Ecommerce data types, promotions do not involve products. Rather, you are measuring the promotions themselves. These would typically be banners, product category highlights or something similar.\nWith promotion tracking, you can measure the success of your site marketing efforts, by seeing if viewing and clicking individual banners and advertisements lead to ecommerce success.\nPromotion Views Promotion Views are sent when the user sees a promotion on the site. As with impressions, you can send more than one Promotion View in a hit, but as with impressions, you need to be careful not to group too many views into one hit, as you might be in danger of breaching the 8KB size limit for the Google Analytics payload.\nData Layer composition Available fields in the ecommerce object:\n   Key Type Example Comment     promotions[].id String 'summer_campaign' Unique identifier for the promotion.   promotions[].name String 'Summer Campaign 2019' The name of the promotion.   promotions[].creative String 'front_page_banner_1' A name for the creative where the promotion is showing.   promotions[].position String 'slot_2' Some way to distinguish the position of the promotion in the creative (e.g. second slide of a carousel).    This is what the dataLayer.push() would look like for a set of Promotion Views with all the relevant keys included:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.promotionView\u0026#39;, ecommerce: { promoView: { promotions: [{ id: \u0026#39;summer_campaign\u0026#39;, name: \u0026#39;Summer Campaign 2019\u0026#39;, creative: \u0026#39;Front Page Carousel\u0026#39;, position: \u0026#39;Slide 1\u0026#39; },{ id: \u0026#39;fall_campaign\u0026#39;, name: \u0026#39;Fall Campaign 2019\u0026#39;, creative: \u0026#39;Front Page Carousel\u0026#39;, position: \u0026#39;Slide 2\u0026#39; }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.promotionView\nPromotion Clicks A Promotion Click is collected when an individual promotion is clicked. It\u0026rsquo;s important to measure these if you are also measuring Promotion Views, because this way you\u0026rsquo;ll get a solid understanding of the click-through rate of individual promotions and how these promotions are involved in generating revenue in your webstore.\nAvailable fields in the ecommerce object:\n   Key Type Example Comment     promotions[].id String 'summer_campaign' Unique identifier for the promotion.   promotions[].name String 'Summer Campaign 2019' The name of the promotion.   promotions[].creative String 'front_page_banner_1' A name for the creative where the promotion was clicked.   promotions[].position String 'slot_2' Some way to distinguish the position of the promotion in the creative (e.g. second slide of a carousel).    This is what the dataLayer.push() would look like for a Promotion Views with all the relevant keys included:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;eec.promotionClick\u0026#39;, ecommerce: { promoClick: { promotions: [{ id: \u0026#39;fall_campaign\u0026#39;, name: \u0026#39;Fall Campaign 2019\u0026#39;, creative: \u0026#39;Front Page Carousel\u0026#39;, position: \u0026#39;Slide 2\u0026#39; }] } } });   Remember, if you want to use a Custom JavaScript variable instead, the variable must return the ecommerce object from the dataLayer.push() above.\n Tag configuration example Typical Event tag:\n  The trigger would be a Custom Event trigger with the following setting:\nEvent name: eec.promotionClick\nDebugging the setup Once you have everything in place, you\u0026rsquo;ll want to debug your setup. The things you are looking for are:\n  Are all the keys configured in the dataLayer.push() (or the Custom JavaScript variable) getting picked up and being sent to Google Analytics?\n  Is the payload length too long?\n  Is there a risk of data duplication with some hits?\n  To debug these, you really only need three tools: GTM\u0026rsquo;s own Preview mode, the Google Analytics Debugger browser extension, and Google Chrome browser\u0026rsquo;s DevTools. Yes, there are plenty of other tools you can use, but these have proven to be more than enough in my own experience.\nPreview mode With Preview mode, you can analyze what fields were sent with your Enhanced Ecommerce -enabled Google Analytics tag. You can also compare these fields with what was originally pushed into dataLayer, or what the Custom JavaScript variable returned.\nOnce you have your ecommerce object either pushed into dataLayer or generated by a Custom JavaScript variable, the next step is to go to Preview mode, then browsing the site and generating the action that sends the ecommerce data to GA.\nHere\u0026rsquo;s what you\u0026rsquo;re looking for in the Preview mode:\n  In the left-hand side, you\u0026rsquo;re looking for the Event name that you pushed into dataLayer with the 'event' key. Remember, you should add one in to every single push?\nClick that event, and you should see your Enhanced Ecommerce tag in the list tags that fired. If you don\u0026rsquo;t, it means there\u0026rsquo;s something wrong with its trigger. So find the tag in the list of tags that did not fire, select it, and scroll down to its triggers. You\u0026rsquo;ll see a red X next to each trigger condition that failed, so double-check them and make sure the conditions pass when they should.\n  Anyway, if the tag DID fire and if it sent an event (or page view) hit to Google Analytics successfully, you\u0026rsquo;ll see Succeeded as its Firing Status. Double-check that the Enhanced Ecommerce settings are correct, too.\n  Google Analytics Debugger and Chrome DevTools Now you know that your tag fired and that it sent the event/pageview to GA successfully. But what about the Enhanced Ecommerce payload? Were all the keys and values you configured included?\nFor this, you have plenty of options. You can use a tool like Google Tag Assistant which will tell you what parameters were included in the hit. Or you can use David Vallejo\u0026rsquo;s excellent GTM Debug Chrome extension, which has a great visualization of the Enhanced Ecommerce data.\nThe key is to see what data was included in the hit and to compare that against what was pushed into dataLayer or what the Custom JavaScript variable returned.\nPersonally, I simply use the Google Analytics Debugger extension. It uses the JavaScript Console of the web browser to output a log of all the interactions with Google Analytics trackers on the page.\nFor example, in my Purchase example above, this is what the GA debugger will tell me:\n  Here, you can see that the hit is missing the tax that I pushed into dataLayer. The key I used was orderTax, as you can see in the screenshot.\nSo now I open this guide again, scroll all the way to the Purchase hit, look at the Data Layer Composition example, and see my mistake. It shouldn\u0026rsquo;t be orderTax, it should be just tax.\nI fix the dataLayer.push(), re-run the test and now I can see that everything works.\n  Here\u0026rsquo;s the most important thing about debugging Enhanced Ecommerce hits:\n Never be satisfied with using Preview mode alone. Preview mode will only tell you if a tag fired, NOT if it sent the correct data to Google Analytics. You must debug the actual payload sent to Google Analytics, too. For this, Chrome\u0026rsquo;s DevTools are a great tool, but there are plenty of extensions out there that outline the data in a clear and concise manner.\n Golden rules of Enhanced Ecommerce Now follow some Enhanced Ecommerce tips that I consider absolutely vital for any implementation. I\u0026rsquo;ve accumulated them over years of practice, and I don\u0026rsquo;t hesitate to call them \u0026ldquo;rules\u0026rdquo; even if I\u0026rsquo;m typically very cautious to give any kind of recommendations to anyone.\nSo read through them, and if you have more suggestions for golden rules, let me know in the comments!\nDo whatever you want This is important. You can use Enhanced Ecommerce however you want. There\u0026rsquo;s no law that forces you to use the data types in the way that Google recommends you to.\nFor example, not all websites have a shopping cart. You directly buy products from product pages. In those cases, don\u0026rsquo;t send the Add to Cart event. Or, alternatively, send a dummy Add to Cart whenever the user moves to the checkout funnel.\nIt\u0026rsquo;s entirely up to you.\nHeck, you can even use Enhanced Ecommerce for something completely unrelated to ecommerce. For example, you can use it to track content engagement on your site.\nThe Purchase action is most important The Purchase action is absolutely the most important part of Enhanced Ecommerce. You can ignore or mess up all the other steps in the funnel, but you will never want to screw up the purchase hit. So many metrics and dimensions rely on the transaction data to be accurate.\nWhen you\u0026rsquo;re calculating the investment for implementing Enhanced Ecommerce, add some extra buffer to getting the purchase right. It\u0026rsquo;s that important.\nOnly one action per hit Remember to only send one action per hit. There are other limitations, too, but this is the most common mistake.\nYou might be tempted to send multiple Checkout steps, for example, but this would violate this rule.\nYou can only include a single action in a dataLayer.push (or in the returned object from a Custom JavaScript variable), so make sure to debug this thoroughly.\nInclude all the necessary details in every hit Consistency is key. If you send product details such as product variant, brand and category in a Product Detail View, you\u0026rsquo;ll want to send these with all the subsequent actions (Add to Cart, Checkout, Purchase), too. Otherwise when analyzing the funnel using any of these dimensions as the primary dimension of analysis, you\u0026rsquo;ll only see data for Product Detail Views and not for any of the other steps.\nThere is no automatic persistence or attribution of product metadata - you must send them with every single hit.\nAlso, the metadata must be in the same format in every step. You can\u0026rsquo;t send brand: 'NIKE' in one step, brand: 'Nike' in another, and brand: 'nike' in another, and expect Google Analytics to treat these as the same value. They will generate three different rows with their own funnels when analyzed using Product Brand as the dimension.\nThere is some attribution Note that there is some attribution in Enhanced Ecommerce; in Product Lists and Promotions, specifically.\nWhen a product is purchased, this purchase will be attributed to the last promotion the user clicked (or viewed if there was no click) in the session.\nSimilarly, when any Ecommerce action is taken, this action is also attributed to the last Product List that was included in an earlier action, as long as the Product ID in these two actions match.\nIn other words, if you send a Product Detail View for product with ID my_shirt, using product_page as the Product List attribute value, then any subsequent action that also involves my_shirt will be attributed to the product_page list. Useful, right?\nJust remember that attribution is session-scoped. If the session timeout is reached, or if the session cuts off for some other reason, any Product List and Promotion attribution information is lost. That\u0026rsquo;s why you might see (not set) in the corresponding reports.\nBe wary of the payload size limit Remember that the maximum payload length of a Google Analytics request is 8192 bytes. If you\u0026rsquo;re collecting Product Impressions, or if the typical cart size in your store is 30+ products, you\u0026rsquo;ll want to see if you\u0026rsquo;re approaching this limit.\nThe Google Analytics Debugger will output an error into the console in case you are trying to send hits that are longer than this limit.\nSee this article for information how to track payload length, and this for a little tool that automatically reduces the payload length.\nFurther reading I\u0026rsquo;ve written a lot about Enhanced Ecommerce. The easiest way to find this content is to simply do a search on my site.\nAs for other content, here is a random selection of goodies found on the web:\n  Google\u0026rsquo;s Enhanced Ecommerce Developer Guide for Universal Analytics\n  Google\u0026rsquo;s Enhanced Ecommerce Developer Guide for Google Tag Manager\n  Google\u0026rsquo;s Enhanced Ecommerce Demo Store\n  Bounteous\u0026rsquo; Enhanced Ecommerce Guide\n  Bounteous\u0026rsquo; Enhanced Ecommerce Variable Pack\n  Google Tag Manager For WordPress Enhanced Ecommerce Guide\n  Summary I hope this guide is useful! It is written as an implementation manual rather than a guide for Enhanced Ecommerce in general.\nGoogle\u0026rsquo;s own documentation is excellent, but there are places where it\u0026rsquo;s a bit misleading. With this guide, I wanted to get all the relevant information into one place.\nNote that I don\u0026rsquo;t mention Google Tag Manager For Firebase at all. There\u0026rsquo;s a reason for that: it\u0026rsquo;s still underdeveloped. There is basic support for Enhanced Ecommerce, but nothing for Product-scoped Custom Dimensions or Metrics, for example. Hopefully these will be introduced soon into the SDKs.\nAs always, any comments are much appreciated! Please let me know if the guide was useful and if I glossed over or neglected to add some important information. Thank you!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/format-value-option-in-google-tag-manager-variables/",
	"title": "#GTMTips: Format Value Option In Google Tag Manager Variables",
	"tags": ["google tag manager", "variable", "format value", "gtmtips"],
	"description": "The new Format Value option in Google Tag Manager variables lets you modify the output of GTM&#39;s variables dynamically, without needing to resort to Custom JavaScript.",
	"content": "A very recent addition to Google Tag Manager is the Format Value option in all of GTM\u0026rsquo;s variables. With Format Value, you can modify the output of the variable with a number of pre-defined transformations.\nThis is extremely handy, because you no longer need to create Custom JavaScript variables whose sole purpose of existence it to change the output of other variables to lowercase, or to change undefined values to fallback strings (e.g. (not set)).\nTip 88: Format Value option in GTM\u0026rsquo;s Variables   When you create or modify any variable in GTM, you now see a new option at the bottom of the configuration screen. The option is titled Format Value. By expanding it, you will see the following options:\n  Change Case to\u0026hellip; - This lets you change the case of the string output of the variable to either lowercase or UPPERCASE.\n  Convert null to\u0026hellip; - With this, you can convert null values to some other string (by typing text directly into the field), or you can have null output fall back to another variable by picking it from the variable selector.\n  Convert undefined to\u0026hellip; - Same as with null, you can now have undefined values fall back to strings or the returned value of other variables.\n  Convert true to\u0026hellip; - Same as above, except now you can convert Boolean true to a string or the returned value of some other variable.\n  Convert false to\u0026hellip; - Same as above, except now you can convert Boolean false to a string or the returned value of some other variable.\n  So how is this useful? Well, take the Case conversion as an example. By forcing the variable output to lowercase, for example, you can normalize all the following strings:\n\u0026quot;I love GTM\u0026quot;, \u0026quot;i LoVe gTM\u0026quot;, \u0026quot;I LOVE GTM\u0026quot;.\nWith Case conversion set to Lowercase, the output of all the three strings above would be \u0026quot;i love gtm\u0026quot;.\nNormalization is absolutely vital in tools such as Google Analytics, which treat each case format of any string as unique and distinct from the others.\nBeing able to convert falsy values such as undefined, null and false to string representations is handy, too, since you can now use something like \u0026quot;N/A\u0026quot; or \u0026quot;(not set)\u0026quot; to represent situations where the variable did not return a proper value. Again, in Google Analytics this is relevant, because GA drops undefined fields from hits.\nThe Format Value option is a very useful addition to Google Tag Manager\u0026rsquo;s arsenal. I only hope the number of available formatting options is increased in the future, perhaps even allowing us to write simple anonymous functions directly in the UI through which each varible output is passed.\n"
},
{
	"uri": "https://www.simoahava.com/tags/format-value/",
	"title": "format value",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/variable/",
	"title": "variable",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/cross-domain/",
	"title": "cross-domain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/tracking-cross-domain-iframes-upgraded-solution/",
	"title": "Tracking Cross-domain Iframes - Upgraded Solution",
	"tags": ["google tag manager", "customtask", "universal analytics", "cross-domain", "iframe"],
	"description": "An upgraded solution (using customTask) to tracking iframes cross-domain when using Google Tag Manager and Google Analytics.",
	"content": " **Last updated 18 September 2020: Due to how most browsers now have third-party cookie protections in place, this solution will be very ineffective going forwards. You should instead take a look at a cookieless solution.\n Some years ago, I wrote a post on how to track cross-domain iframes when using Google Tag Manager and Google Analytics. That solution relied on hitCallback to decorate the iframe, and now that I look back on it, it has its shortcomings.\nFor one, the older solution used hitCallback which, while being reliable in that Google Analytics has definitely loaded with the linker plugin when the method is called, doesn\u0026rsquo;t take into account the possible race condition of the script running before the iframe has been loaded.\nAn additional problem was that the older solution simply grabbed the linker from the first Google Analytics tracker object on the page. But this might not be the one we want to use for the cross-domain tracking.\n  In this solution, we\u0026rsquo;ll use the wonderful customTask to decorate the target iframe. The customTask leverages a setInterval() script which polls the page periodically until the target iframe is found or a timeout is reached.\n  Naturally, this script has been added to my customTask Builder tool.\nThe customTask itself This is what the customTask looks like. To deploy it, make sure you read the instructions in the customTask Builder tool before copy-pasting the required code into your page JavaScript (if using the Universal Analytics on-page snippet) or a Custom JavaScript variable (if using Google Tag Manager).\nfunction() { var iframeDecorator = { selector: \u0026#39;iframe#decorateMe\u0026#39;, attempts: 10, intervalMs: 1000, useAnchor: false }; // DO NOT EDIT ANYTHING BELOW THIS LINE  var globalSendHitTaskName = \u0026#39;_ga_originalSendHitTask\u0026#39;; return function(customTaskModel) { window[globalSendHitTaskName] = window[globalSendHitTaskName] || customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); var tempFieldObject, dimensionIndex, count, ga, tracker, decorateTimer, decorateIframe, iframe; if (typeof iframeDecorator === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; typeof iframeDecorator.selector === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof iframeDecorator.attempts === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof iframeDecorator.intervalMs === \u0026#39;number\u0026#39;) { count = 0; ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; tracker = ga.getByName(customTaskModel.get(\u0026#39;name\u0026#39;)); decorateIframe = function() { iframe = document.querySelector(iframeDecorator.selector); if (iframe !== null \u0026amp;\u0026amp; /[?\u0026amp;]_ga=/.test(iframe.src)) { window.clearInterval(decorateTimer); return; } if (iframe === null) { count += 1; if (count === iframeDecorator.attempts) { window.clearInterval(decorateTimer); } return; } window.clearInterval(decorateTimer); iframe.src = (new window.gaplugins.Linker(tracker)).decorate(iframe.src, iframeDecorator.useAnchor); }; decorateTimer = window.setInterval(decorateIframe, iframeDecorator.intervalMs); } }; }  To begin with, make sure to edit the configuration object iframeDecorator. Set the parameters to their correct values.\n   Parameter Example value Description     selector \u0026quot;#decorateMe\u0026quot; String with the CSS selector that matches the iframe you want to decorate.   attempts 10 Number denoting how many times the page is polled for the iframe until the script stops trying.   intervalMs 1000 Milliseconds between each attempt to find the iframe on the page.   useAnchor false Whether to use a hash (#) instead of a URL query parameter to append the linker parameter to the iframe src.    Once you\u0026rsquo;ve edited these, you\u0026rsquo;ll need to add this customTask to your trackers or tags.\nMake sure to add this customTask to a hit or tag that fires after (or shortly before) the iframe element has been added to the page. The script has a timeout which is attempts * intervalMs milliseconds, so if the script doesn\u0026rsquo;t find the iframe by this timeout, the script will no longer poll the page, and the iframe will remain undecorated.\nFor example, if the iframe is in the page HTML template itself, it\u0026rsquo;s typically enough to add this customTask to the Page View hit that fires as soon as the page starts to load (so All Pages trigger in GTM, or the on-page snippet if using Universal Analytics).\nBut if the iframe is added in a modal upon the click of a button, for example, you\u0026rsquo;ll want to fire a Google Analytics hit with that click, and then make sure to include this customTask in the hit.\nHow it works When the hit to Google Analytics is generated, this customTask repeatedly asks the page: \u0026ldquo;Is there an iframe that matches the given CSS selector?\u0026rdquo;. If this iframe is found before the timeout (attempts * intervalMs), then the iframe path is decorated with the cross-domain linker parameter.\nThis, in turn, means that when the page within the iframe loads, the URL of the page will have the _ga= cross-domain linker parameter, which is then utilized by the first tag that fires in the iframe with the allowLinker flag set to true.\nCaveats Note that this only decorates the iframe src path. This is the only thing you can do from the source site, so if it doesn\u0026rsquo;t work, you\u0026rsquo;re out of luck with this solution. Some iframes might introduce a redirect when the original src is loaded, which often eliminates the query parameter from the URL. If this is the case, you can try setting the useAnchor parameter to true, which might be more resilient to redirects.\n If you\u0026rsquo;re interested in a viable alternative to the linker parameter, take a look at the postMessage API introduced in this great article by Dan Wilkerson from Bounteous.\n Be wary, also, that this customTask uses the cookie configuration of the hit that the customTask was added to. So if this hit has an exceptional cookie configuration, it\u0026rsquo;s possible the linker is built with the wrong Client ID.\nIn other words, make sure you are aware of any modifications to the default cookie settings in the tracker, hit or tag to which the customTask is added.\nThere is no real penalty to adding this customTask directly to the tracker or all your Universal Analytics tags. If the iframe already has a cross-domain parameter in the src value, the script simply stops attempting to decorate the iframe.\nSummary This is an alternative approach to my original solution of using hitCallback to decorate the iframe. In my humble opinion, I think this customTask wipes the floor with my original idea, so I\u0026rsquo;m fully recommending using this instead.\nMy feelings on iframes haven\u0026rsquo;t really changed since writing the original article, so I\u0026rsquo;ll just include the relevant excerpt here for you to sympathize with.\n  If you want to vent, please let me know what you think about iframes in the comments of this article.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/create-and-update-google-analytics-session-timeout-cookie/",
	"title": "Create And Update Google Analytics Session Timeout Cookie",
	"tags": ["google tag manager", "google analytics", "customtask", "session"],
	"description": "With this customTask trick, you can create a cookie which mimics Google Analytics&#39; session timeout. This cookie is refreshed with every valid hit, and can be used to detect active/inactive sessions in the browser.",
	"content": "In Google Analytics, the concept of a session is the key aggregation unit of all the data you work with. It\u0026rsquo;s so central to all the key metrics you use (Conversion Rate, Bounce Rate, Session Duration, Landing Page), and yet there\u0026rsquo;s an underlying complexity that I\u0026rsquo;m pretty certain is unrecognized by many of GA\u0026rsquo;s users. And yet, since this idea of a session is so focal to GA (to the point of being overbearing), it\u0026rsquo;s annoying that the browser isn\u0026rsquo;t privy to any of the sessionization parameters that Google Analytics applies to the hits sent from the browser to its servers.\nWell, to rectify this, I\u0026rsquo;ve written a very simple customTask solution, which basically mimics the session timeout (30 minutes by default) in a browser cookie. You can use this cookie for a number things, such as preventing event hits from being dispatched if the session has timed out, or for turning these event hits into non-interactive hits.\n  Naturally, I have also updated my guide to customTask as well as my customTask Builder tool with this trick, so you can start using it with all the other cool customTask tricks out there!\nThe customTask code Here\u0026rsquo;s what the customTask method looks like:\nvar _customTask = function() { // Update expiresMs to be the number of milliseconds when the cookie should expire.  // Update domain to match the parent domain of your website.  var updateSessionCookie = { expiresMs: 1000*60*30, domain: \u0026#39;mydomain.com\u0026#39; }; // DO NOT EDIT ANYTHING BELOW THIS LINE  var globalSendHitTaskName = \u0026#39;_ga_originalSendHitTask\u0026#39;; return function(customTaskModel) { window[globalSendHitTaskName] = window[globalSendHitTaskName] || customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); customTaskModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendHitTaskModel) { var originalSendHitTaskModel = sendHitTaskModel, originalSendHitTask = window[globalSendHitTaskName]; var hitType, nonInteraction, d; try { originalSendHitTask(sendHitTaskModel); // updateSessionCookie  if (typeof updateSessionCookie === \u0026#39;object\u0026#39; \u0026amp;\u0026amp; updateSessionCookie.hasOwnProperty(\u0026#39;expiresMs\u0026#39;) \u0026amp;\u0026amp; updateSessionCookie.hasOwnProperty(\u0026#39;domain\u0026#39;)) { hitType = sendHitTaskModel.get(\u0026#39;hitType\u0026#39;); nonInteraction = sendHitTaskModel.get(\u0026#39;nonInteraction\u0026#39;); if (nonInteraction !== true \u0026amp;\u0026amp; (hitType === \u0026#39;pageview\u0026#39; || hitType === \u0026#39;event\u0026#39;)) { d = new Date(); d.setTime(d.getTime() + updateSessionCookie.expiresMs); document.cookie = \u0026#39;_session_\u0026#39; + sendHitTaskModel.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;=true; expires=\u0026#39; + d.toUTCString() + \u0026#39;; path=/; domain=\u0026#39; + updateSessionCookie.domain; } } // /updateSessionCookie  } catch(e) { originalSendHitTask(originalSendHitTaskModel); } }); }; };  If you\u0026rsquo;re using Google Tag Manager, copy-paste that into a Custom JavaScript variable, and then change the first line to function() { and remove the very last character of the code block (the final semicolon). Then, add it as a new Field to Set in your Google Analytics tag settings.\nif you\u0026rsquo;re using Google Analytics, you need to run this JavaScript before the Google Analytics snippet, and then when creating the tracker modify it to:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-XXXXX-Y\u0026#39;); ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, _customTask); // \u0026lt;-- Add this ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;);  This customTask should be added to all the tags and hits that you want to have an impact on the session refresh. Basically, it should fire with all your pageviews and events, since those are the two that keep the session alive.\nHow it works Any tag or hit that uses this customTask will now run some extra code when the hit has been sent to Google Analytics. The code checks whether the hit was interactive (since non-interactive hits don\u0026rsquo;t keep a session alive) and whether it was an event or pageview (since those are the two hits that keep a session alive).\nIn both these conditions are true, a cookie named _session_UA-XXXXX-Y is created (or updated if it already exists) with an expiration set to whatever you configured as the value of the expiresMs property in the beginning of the code block.\nIn other words, you will now have a cookie which, if it exists, means there is most likely an active session in Google Analytics. I say \u0026ldquo;most likely\u0026rdquo;, because there are many ways that sessions can be broken off even before the timeout expires.\nThis cookie is a reasonable abstraction of the session timeout schema in Google Analytics. It\u0026rsquo;s not perfect, but it gives you an idea.\nThings you can do with it Well, the easiest thing is to use it to prevent Events from firing if the session cookie doesn\u0026rsquo;t exist. Why? Because it\u0026rsquo;s tiresome having sessions that only have events. Even though there\u0026rsquo;s really no problem with having events fire before pageviews (even if all the documentation tries to tell you otherwise), there\u0026rsquo;s a risk you\u0026rsquo;ll have sessions that only have events.\nThose sessions will be marred with the ugly (not set) Landing Page, because Google Analytics is tyrannical in demanding sessions to always include a pageview (I have no idea why).\nSo, in Google Tag Manager, you could do this by first creating a 1st Party Cookie variable for the session cookie. Remember to change the \u0026ldquo;UA-XXXXXX-Y\u0026rdquo; to whatever the tracking ID is for the property whose session timeout you are monitoring.\n  Then, create a new Custom Event trigger which checks if this cookie has the value true:\n  Now, any tag you add this trigger to as an exception will not fire if the session has timed out.\nThis is pretty brutal - you\u0026rsquo;re not sending event hits to GA because you\u0026rsquo;re afraid they\u0026rsquo;ll create orphaned sessions. It\u0026rsquo;s a valid fear to have with GA\u0026rsquo;s sessionization schema, but it might be overkill.\nSo another option is to turn all event hits into non-interactive hits, because these don\u0026rsquo;t increment the session counts or dimensions in your data set, but the data is still collected.\nTo do this, you need a Custom JavaScript variable that looks like this:\nfunction() { return {{UA-XXXXX-Y session}} !== \u0026#39;true\u0026#39;; }  This JavaScript returns false if the session is active, and true if there is no active session. By adding this to the Non-Interaction field in your Event tags, the hit will be non-interactive if there is no active session.\nYou could even modify the customTask to do this logic for you. Within the customTask method, just after try {, you could do something like this (you\u0026rsquo;ll need the 1st Party Cookie for this):\ntry { // ADD THIS  if ({{UA-XXXXX-Y session}} !== \u0026#39;true\u0026#39; \u0026amp;\u0026amp; sendHitTaskModel.get(\u0026#39;hitType\u0026#39;) === \u0026#39;event\u0026#39; \u0026amp;\u0026amp; sendHitTaskModel.get(\u0026#39;nonInteraction\u0026#39;) !== true) { return; } // UP TO HERE  originalSendHitTask(sendHitTaskModel); ... }  This addition prevents the hit from being fired (and the cookie from being created/updated) if the session is not alive and the hit is an interactive event. For all other hit types, the hit gets sent and the cookie gets created. This way you don\u0026rsquo;t need to mess with triggers - you can have customTask do all the legwork for you.\nSummary This customTask trick can be used to bring some insight into the browser whether or not a session is currently active in Google Analytics.\nIt\u0026rsquo;s not perfect, since sessionization depends on so many things (of which a multitude happens in the bowel\u0026rsquo;s of GA\u0026rsquo;s processing servers), but it can be used as an indication of whether or not there\u0026rsquo;s an active session.\nThis solution is also not necessary, especially if you use BigQuery. The raw data you collect from the site is useful and valid even if there\u0026rsquo;s no pageview initiating a session. So make sure you have a use case for this before taking the plunge.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/introducing-customtask-builder/",
	"title": "#GTMTips: Introducing The customTask Builder",
	"tags": ["google tag manager", "gtmtips", "customtask", "tools"],
	"description": "Introducing the customTask Builder tool, with which you can generate the JavaScript necessary to build a Google Analytics customTask script.",
	"content": "One of the coolest features of Google Analytics and, as a consequence, Google Tag Manager is customTask. It\u0026rsquo;s a method you can use to add and execute code as the hit to Google Analytics is being generated.\nI\u0026rsquo;ve written A LOT about customTask, and much of the feedback I\u0026rsquo;ve received has been around the question of how to combine all these different tricks into one customTask script. The problem is, you see, that a tag or hit can only have one customTask script attached to it, so the code within must combine all the different tricks I\u0026rsquo;ve been writing about over the past months.\nTo help with the pain of programming, I wrote a tool which you can find behind this link. Here\u0026rsquo;s a short introduction to it.\nTip 87: Introducing the customTask Builder Tool   The tool itself is fairly simple. You just select the customTask features you want to utilize in the script, and it then generates the JavaScript block that you\u0026rsquo;ll then need to edit to change the default values to something meaningful.\nAt this point, head on over to the customTask Builder Tool itself, and read the instructions within. Be sure to read my guide on customTask to understand what customTask is and why it may be useful to you.\nAnd keep checking the tool at regular intervals. I\u0026rsquo;ll be updating it as I come up with new ideas for the wonderful feature that customTask is.\n"
},
{
	"uri": "https://www.simoahava.com/tags/tools/",
	"title": "tools",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tools/customtask-builder/",
	"title": "customTask Builder",
	"tags": [],
	"description": "",
	"content": "You can use the customTask Builder tool to create a customTask script. customTask is a method of the Google Analytics library, which lets you run JavaScript code when the hit request to Google Analytics is being built.\n Click here to jump directly to the tool.\n This is very useful for a number of reasons, and I recommend you read customTask - The Guide before doing anything with the tool.\nThe tool automatically builds the necessary JavaScript, avoiding any potential conflicts that the overlapping methods might introduce otherwise.\nWhen you add individual features to the script by selecting items in the table, you\u0026rsquo;ll see how the final script is modified accordingly.\nWhen you are satisfied with the result, select all the text within the white code box. You can also click the Copy to clipboard button to automatically copy the code to your clipboard.\nGeneral deployment instructions Upon adding individual items to the script, you\u0026rsquo;ll notice that the first lines of the script contain default values that you\u0026rsquo;ll need to modify. For example, when adding the Client ID as a Custom Dimension item, you\u0026rsquo;ll see these lines added to the code:\n// clientIdIndex: The Custom Dimension index to where you want to send the visitor\u0026#39;s Client ID. // https://bit.ly/2Ms0ZcC var clientIdIndex = 1;  The value 1 is just a default value - you\u0026rsquo;ll need to change this to the Custom Dimension index you have created in Google Analytics for this specific purpose.\nThus after copy-pasting the code into place, make sure you edit all the default values with the actual values you want to configure the items with. Otherwise you will run into severe data quality issues.\nDeployment with Google Tag Manager If you are using Google Tag Manager, you need to copy-paste the code into a Custom JavaScript variable. If you use the \u0026ldquo;Copy to clipboard (for GTM)\u0026rdquo; button, you don\u0026rsquo;t have to modify the JavaScript as instructed below.\nIf, for some reason, you refuse to use the \u0026ldquo;Copy to clipboard (for GTM)\u0026rdquo; button, after copy-pasting the contents to the Custom JavaScript variable, you\u0026rsquo;ll need to modify the first line by removing the var _customTask =  part like this:\n// OLD, CHANGE THIS: var _customTask = function() { // NEW, TO THIS: function() {  The second change you need to do is remove the semi-colon at the very end of the code block. In other words, remove the very last character of the entire code block, which should be a semi-colon ;.\nOnce you\u0026rsquo;re done modifying the default values, save the variable with the name {{customTask}} or something similar and descriptive.\nThen, in your Google Analytics Settings variable, go to Fields to set, and add a new field:\nField name: customTask\nValue: {{customTask}}\nIf you\u0026rsquo;re not using a Google Analytics Settings variable, you need to click Enable overriding settings in this tag in all the Google Analytics tags where you want to add this customTask as a field, and make the same change directly to the tag itself.\nDeployment with analytics.js With the regular (legacy) Universal Analytics library, you first need to make sure the variable is written into memory, so in the site JavaScript (or page template), make sure the code in the code box below is executed before any Google Analytics commands are run.\nThen, when you want to invoke this customTask in your trackers, find where you are creating the tracker and make the following change:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;); ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, _customTask()); // \u0026lt;-- Add this right after creating the tracker  ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;);  The customTask Builder tool    Click to select which feature(s) to include     Client ID as a Custom Dimension   Hit Type as a Custom Dimension   Payload Length as a Custom Dimension   Remove Custom Definitions from Page Speed Timing Hits   Copy Hits to Multiple Properties   Remove PII from Hits   Copy Hits to Snowplow Collector Endpoint   Update Session Cookie   Decorate Cross-domain Iframes   Prevent Duplicate Transactions   Obfuscate And Copy Hit Payload   Use localStorage To Persist Client ID    Copy to clipboard (for GTM) | Copy to clipboard\n  div#customTaskWrapper { padding: 10px; border: 1px dotted; border-radius: 5px; background: #f7f7ff; } button#copy, button#copyGtm { border: 2px solid #ccc; background: #eee; font-size: 1em; margin-top: 10px; } pre#result { overflow: scroll; white-space: pre-wrap; font-size: 0.8em; padding: 5px; background: #fff; line-height: 1.5em; margin-top: 0px; border: 2px solid #ccc; } div#customTaskWrapper table { border-bottom: 2px solid #909ba2; margin-top: 0; } td[data-customtask-id] { cursor: pointer; } td[data-customtask-selected=\"true\"] { background: #d6ffc1 !important; }   (function() { var customTaskItems = document.querySelectorAll('[data-customtask-id]'); var copyBtn = document.querySelector('#copy'); var copyGtmBtn = document.querySelector('#copyGtm'); var rowsInCode = { cid: [7,8,9,10,113,114,115,116,117,118], ht: [11,12,13,14,119,120,121,122,123,124], pl: [15,16,17,18,219,220,221,222,223,224,225,226,227], rcd: [19,20,21,22,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139], dh: [23,24,25,26,245,246,247,248,249,250,251,252,253,254,255,256,257], pr: [27,28,29,30,31,32,33,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218], sp: [34,35,36,37,232,233,234,235,236,237,238,239,240,241,242,243,244], sc: [38,39,40,41,42,43,44,258,259,260,261,262,263,264,265,266,267,268,269], id: [45,46,47,48,49,50,51,52,53,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165], ti: [54,55,56,57,58,59,60,185,186,187,188,189,190,191,192,193,194,195,196,197], o: [61,62,63,64,65,66,67,68,69,70,71,72,73,82,83,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320], lsc: [74,75,76,77,78,79,80,166,167,168,169,170,171,172,173,174,175,176] }; var result = document.querySelector('#result'); var customTask = [ \"var _customTask = function () {\", \" // customTask Builder by Simo Ahava\", \" //\", \" // More information about customTask: https://www.simoahava.com/analytics/customtask-the-guide/\", \" //\", \" // Change the default values for the settings below.\", \"\", \" // clientIdIndex: The Custom Dimension index to where you want to send the visitor's Client ID.\", \" // https://bit.ly/2Ms0ZcC \", \" var clientIdIndex = 1;\", \"\", \" // hitTypeIndex: The Custom Dimension index to where you want to send the hit type of the request.\", \" // https://bit.ly/2KZqLA9\", \" var hitTypeIndex = 2;\", \"\", \" // payloadLengthIndex: The Custom Dimension index to where you want to send the length of the payload of the request.\", \" // https://bit.ly/2PdhPKM\", \" var payloadLengthIndex = 3;\", \"\", \" // removeCustomDefinitionsFromTimingHits: Set to true if you want to remove Custom Dimensions and Custom Metrics from the page speed timing hit.\", \" // https://bit.ly/2nGzw8T\", \" var removeCustomDefinitionsFromTimingHits = true;\", \"\", \" // duplicateHitToTrackingIds: Array of all Universal Analytics tracking IDs to where you want to duplicate the initial hit.\", \" // https://bit.ly/2MwFGGP\", \" var duplicateHitToTrackingIds = ['UA-12345-1'];\", \"\", \" // piiRegex: Array of {name, regex} objects, where the regular expression matches a pattern you want to replace with [REDACTED name].\", \" // https://bit.ly/2wcJym2\", \" var piiRegex = [{\", \" name: 'EMAIL',\", \" regex: /.{4}@.{4}/g\", \" }];\", \"\", \" // snowplowEndpoint: The Snowplow collector endpoint to which you want to send the GA request hit payload.\", \" // https://bit.ly/2OCBzXc\", \" var snowplowEndpoint = 'https://collector.simoahava.com/';\", \"\", \" // updateSessionCookie: Object which contains both an expiration time (in milliseconds) and domain name.\", \" // https://bit.ly/2wh1xsH\", \" var updateSessionCookie = {\", \" expiresMs: 1000 * 60 * 30,\", \" domain: 'mydomain.com'\", \" };\", \"\", \" // iframeDecorator: Configuration object for decorating any iframe with cross-domain parameters when this customTask is run. NOTE! Increasingly inefficient with today's browsers blocking third-party cookies - please read the article linked to below.\", \" // https://bit.ly/2LUoWFf\", \" var iframeDecorator = {\", \" selector: 'iframe#decorateMe',\", \" attempts: 10,\", \" intervalMs: 1000,\", \" useAnchor: false\", \" };\", \"\", \" // transactionDeduper: Configuration object for preventing duplicate transactions from being recorded.\", \" // https://bit.ly/2AvSZ2Y\", \" var transactionDeduper = {\", \" keyName: '_transaction_ids',\", \" cookieExpiresDays: 365\", \" };\", \"\", \" // obfuscate: Obfuscates the entire hit payload (using a dictionary of words consistently) and dispatches it to the trackingId you provide.\", \" // https://bit.ly/2RectUl\", \" var obfuscate = {\", \" tid: 'UA-12345-1',\", \" dict: ['tumble', 'noble', 'flourish', 'abandon', 'liberal', 'team', 'conflict', 'collar', 'tiger', 'stun', 'grace', 'resource', 'phantom', 'imagine', 'information', 'hall', 'sweet', 'agriculture', 'bingo', 'relative'],\", \" stringParams: ['uid','ua','dr','cn','cs','cm','ck','cc','ci','gclid','dclid','dl','dh','dp','dt','cd','cg[1-5]','linkid','an','aid','av','aiid','ec','ea','el','ti','ta','in','ic','iv','pr\\\\d{1,3}id','pr\\\\d{1,3}nm','pr\\\\d{1,3}br','pr\\\\d{1,3}ca','pr\\\\d{1,3}va','pr\\\\d{1,3}cc','pr\\\\d{1,3}cd\\\\d{1,3}','tcc','pal','col','il\\\\d{1,3}nm','il\\\\d{1,3}pi\\\\d{1,3}id','il\\\\d{1,3}pi\\\\d{1,3}nm','il\\\\d{1,3}pi\\\\d{1,3}br','il\\\\d{1,3}pi\\\\d{1,3}ca','il\\\\d{1,3}pi\\\\d{1,3}va','il\\\\d{1,3}pi\\\\d{1,3}cd\\\\d{1,3}','promo\\\\d{1,3}id','promo\\\\d{1,3}nm','promo\\\\d{1,3}cr','promo\\\\d{1,3}ps','sn','sa','st','utc','utv','utl','exd','cd\\\\d{1,3}','xid','exp','_utmz'],\", \" priceParams: ['tr','ts','tt','ip','pr\\\\d{1,3}pr','id\\\\d{1,3}pi\\\\d{1,3}pr'],\", \" priceModifier: Math.random(),\", \" medium: ['organic', 'referral', 'social', 'cpc'],\", \" replaceString: function(t){if(''===t)return t;'function'==typeof window.btoa\u0026\u0026(t=btoa(t));var n=t.split('').map(function(t){return t.charCodeAt(0)}).join('')%obfuscate.dict.length;return obfuscate.dict[n]},\", \" init: function(){var c=[];obfuscate.dict.forEach(function(t){obfuscate.dict.forEach(function(o){c.push(t+'-'+o)})}),obfuscate.dict=obfuscate.dict.concat(c)}\", \" };\", \"\", \" // localStorageCid: Use localStorage to persist Client ID with Google Analytics tags\", \" // https://bit.ly/2GNElc4\", \" var localStorageCid = {\", \" objectName: 'ga_client_id',\", \" expires: 1000*60*60*24*365*2\", \" };\", \"\", \" // DO NOT EDIT ANYTHING BELOW THIS LINE\", \" if (typeof obfuscate === 'object' \u0026\u0026 typeof obfuscate.init === 'function') obfuscate.init();\", \"\", \" var readFromStorage = function (key) {\", \" if (!window.Storage) {\", \" // From: https://stackoverflow.com/a/15724300/2367037\", \" var value = '; ' + document.cookie;\", \" var parts = value.split('; ' + key + '=');\", \" if (parts.length === 2) {\", \" return parts.pop().split(';').shift();\", \" }\", \" } else {\", \" return window.localStorage.getItem(key);\", \" }\", \" };\", \"\", \" var writeToStorage = function (key, value, expireDays) {\", \" if (!window.Storage) {\", \" var expiresDate = new Date();\", \" expiresDate.setDate(expiresDate.getDate() + expireDays);\", \" document.cookie = key + '=' + value + ';expires=' + expiresDate.toUTCString();\", \" } else {\", \" window.localStorage.setItem(key, value);\", \" }\", \" };\", \"\", \" var globalSendHitTaskName = '_ga_originalSendHitTask';\", \"\", \" return function (customTaskModel) {\", \"\", \" window[globalSendHitTaskName] = window[globalSendHitTaskName] || customTaskModel.get('sendHitTask');\", \"\", \" // clientIdIndex\", \" if (typeof clientIdIndex === 'number') {\", \" customTaskModel.set('dimension' + clientIdIndex, customTaskModel.get('clientId'));\", \" }\", \" // /clientIdIndex\", \"\", \" // hitTypeIndex\", \" if (typeof hitTypeIndex === 'number') {\", \" customTaskModel.set('dimension' + hitTypeIndex, customTaskModel.get('hitType'));\", \" }\", \" // /hitTypeIndex\", \"\", \" // removeCustomDefinitionsFromTimingHits\", \" if (typeof removeCustomDefinitionsFromTimingHits === 'boolean' \u0026\u0026 removeCustomDefinitionsFromTimingHits === true) {\", \" if (customTaskModel.get('hitType') === 'timing') {\", \" var _rcd_tempFieldObject = {};\", \" var _rcd_dimensionIndex = 1;\", \" while (_rcd_dimensionIndex !== 201) {\", \" _rcd_tempFieldObject['dimension' + _rcd_dimensionIndex] = undefined;\", \" _rcd_tempFieldObject['metric' + _rcd_dimensionIndex] = undefined;\", \" _rcd_dimensionIndex++;\", \" }\", \" customTaskModel.set(_rcd_tempFieldObject);\", \" }\", \" }\", \" // /removeCustomDefinitionsFromTimingHits\", \"\", \" // iframeDecorator\", \" if (typeof iframeDecorator === 'object' \u0026\u0026 typeof iframeDecorator.selector === 'string' \u0026\u0026 typeof iframeDecorator.attempts === 'number' \u0026\u0026 typeof iframeDecorator.intervalMs === 'number') {\", \" var _id_decorateTimer;\", \" var _id_count = 0;\", \" var _id_ga = window[window['GoogleAnalyticsObject']];\", \" var _id_tracker = _id_ga.getByName(customTaskModel.get('name'));\", \" var _id_decorateIframe = function () {\", \" var _id_iframe = document.querySelector(iframeDecorator.selector);\", \" if (_id_iframe !== null \u0026\u0026 /[?\u0026]_ga=/.test(_id_iframe.src)) {\", \" window.clearInterval(_id_decorateTimer);\", \" return;\", \" }\", \" if (_id_iframe === null) {\", \" _id_count += 1;\", \" if (_id_count === iframeDecorator.attempts) {\", \" window.clearInterval(_id_decorateTimer);\", \" }\", \" return;\", \" }\", \" window.clearInterval(_id_decorateTimer);\", \" _id_iframe.src = (new window.gaplugins.Linker(_id_tracker)).decorate(_id_iframe.src, iframeDecorator.useAnchor);\", \" };\", \" _id_decorateTimer = window.setInterval(_id_decorateIframe, iframeDecorator.intervalMs);\", \" }\", \" // /iframeDecorator\", \"\", \" // localStorageCid\", \" if (typeof localStorageCid === 'object' \u0026\u0026 typeof localStorageCid.objectName === 'string' \u0026\u0026 typeof localStorageCid.expires === 'number' \u0026\u0026 window.Storage) {\", \" var _lsc_clientId = customTaskModel.get('clientId');\", \" var _lsc_obj = JSON.stringify({\", \" clientId: _lsc_clientId,\", \" expires: new Date().getTime() + localStorageCid.expires\", \" });\", \" window.localStorage.setItem(localStorageCid.objectName, _lsc_obj);\", \" }\", \" // /localStorageCid\", \"\", \" customTaskModel.set('sendHitTask', function (sendHitTaskModel) {\", \"\", \" var originalSendHitTaskModel = sendHitTaskModel,\", \" originalSendHitTask = window[globalSendHitTaskName],\", \" canSendHit = true;\", \"\", \" try {\", \"\", \" // transactionDeduper\", \" if (typeof transactionDeduper === 'object' \u0026\u0026 transactionDeduper.hasOwnProperty('keyName') \u0026\u0026 transactionDeduper.hasOwnProperty('cookieExpiresDays') \u0026\u0026 typeof sendHitTaskModel.get('\u0026ti') !== 'undefined') {\", \" var _td_transactionId = sendHitTaskModel.get('\u0026ti');\", \" var _td_storedIds = JSON.parse(readFromStorage(transactionDeduper.keyName) || '[]');\", \" if (_td_storedIds.indexOf(_td_transactionId)  -1 \u0026\u0026 ['transaction', 'item'].indexOf(sendHitTaskModel.get('hitType')) === -1) {\", \" canSendHit = false;\", \" } else if (_td_storedIds.indexOf(_td_transactionId) === -1) {\", \" _td_storedIds.push(_td_transactionId);\", \" writeToStorage(transactionDeduper.keyName, JSON.stringify(_td_storedIds), transactionDeduper.cookieExpiresDays);\", \" }\", \" }\", \" // /transactionDeduper\", \"\", \" // piiRegex\", \" if (typeof piiRegex !== 'undefined' \u0026\u0026 piiRegex.length) {\", \" var _pr_hitPayloadParts = sendHitTaskModel.get('hitPayload').split('\u0026');\", \" for (var _pr_regexI = 0; _pr_regexI  Summary I hope you find this tool useful. I will try to update it as I come up with new customTask ideas, which seems to be quite often.\nPlease let me know in the comments if you are having trouble using the tool, or if you have new customTask ideas in mind you\u0026rsquo;d want to include in the tool.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/decorate-links-decorate-forms-tags/",
	"title": "#GTMTips: Decorate Links And Decorate Forms Tag Types",
	"tags": ["google tag manager", "cross-domain tracking", "gtmtips"],
	"description": "Use the Decorate Forms and Decorate Links tag types to cross-domain track only specific link clicks and form submits from your site.",
	"content": "Getting cross-domain tracking right in Google Analytics is difficult. Even if you use Google Tag Manager. There are many known issues when cross-domain tracking iframes, for example.\nGoogle Tag Manager implements the cross-domain tracking plugin quite handily via the Universal Analytics tag template, and often the easiest way to track links and form submits is to use the Auto-Link Domains option, as described in this great series of posts on cross-domain tracking by Bounteous.\nHowever, sometimes you want more precision in decorating the URLs. Perhaps you only want to decorate one specific link rather than all links that share the same hostname. Or perhaps you only want to decorate forms when they are submitted by your actual visitors rather than internal users.\nThis is where the quite rare Decorate Links and Decorate Forms Universal Analytics tag types come in handy.\nTip 86: Decorate Links and Decorate Forms tags   The principle is very simple. If you set the Decorate Links tag to fire with a Just Links trigger, then any link click that causes that trigger to go off will be decorated by the tag. Similarly, if you set the Decorate Forms tag to fire with a Form trigger, then any form submit that toggles that trigger will have its destination URL decorated by the tag.\n  For example, if you create a Just Links trigger with the following condition:\nClick URL matches RegEx shop\\.domain\\.com/cart/\nThen any click on a link whose href contains shop.domain.com/cart/ will be decorated with cross-domain parameters. However, unlike the Auto-Link Domains plugin, no other link to shop.domain.com will be decorated.\nThis can be very useful if you want full control over when Client ID information is passed from domain to domain.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/two-simple-data-model-tricks/",
	"title": "#GTMTips: Two Simple Data Model Tricks",
	"tags": ["google tag manager", "gtmtips", "data model"],
	"description": "Two simple tricks to derive powerful functionality from Google Tag Manager&#39;s data model.",
	"content": "One of the more difficult concepts in Google Tag Manager is the data model. In essence, the data model is what Google Tag Manager uses to populate the Data Layer variable. You might be tempted to think that it\u0026rsquo;s the same thing as the dataLayer array, but it\u0026rsquo;s not.\nThe data model is a representation of the keys and values you push into dataLayer. Whenever you push any key into dataLayer, GTM grabs this key and updates the corresponding key in its data model with the new value, or in the case of objects and arrays merges the old and the new value together.\nIn this #GTMTips article, I\u0026rsquo;m completely indebted to Mr. Jethro Nederhof from Snowflake Analytics. He published these two tricks in Measure Slack, and I asked if he\u0026rsquo;s OK that I publish them on my blog. He kindly agreed, which is not suprising, since his generosity also spawned our co-authored article on tracking browsing behavior a few months back.\nTip 85: Two simple data model tricks   Both tips have to do with how the data model processes keys and values pushed into it.\nThe first trick lets you prevent recursive merge from happening (useful on single-page apps where you don\u0026rsquo;t want to persist values pushed in earlier pages, for example).\nThe second trick lets you access what the current state of the data model is. This can be useful for a number of things, but especially if you\u0026rsquo;re debugging your setup it might be useful to see all they keys and values that are currently located in the data model\u0026rsquo;s table.\nTrick 1: Use _clear: true to prevent merging of the keys in the object Let\u0026rsquo;s say you are pushing some product impressions into the dataLayer. The first push looks like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ products: [{ sku: \u0026#39;123\u0026#39;, name: \u0026#39;Thick as a Brick\u0026#39; },{ sku: \u0026#39;234\u0026#39;, name: \u0026#39;Aqualung\u0026#39; }] });  At this point, the key products in the data model would have two objects representing the products that were pushed. If you were to create a Data Layer variable for products, that\u0026rsquo;s what you\u0026rsquo;d also get as the return value of the variable.\nThen let\u0026rsquo;s say the user moves to another section of the site without a page load (so it\u0026rsquo;s a single-page app), and on that page there\u0026rsquo;s just one product impression, so the site runs the following code:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ products: [{ sku: \u0026#39;345\u0026#39;, name: \u0026#39;Close to the Edge\u0026#39; }] });  You\u0026rsquo;d expect the data model to now have just one product in the products array, but due to how recursive merge works, the data model looks like this:\n{ products: [{ sku: \u0026#39;345\u0026#39;, name: \u0026#39;Close to the Edge\u0026#39; },{ sku: \u0026#39;234\u0026#39;, name: \u0026#39;Aqualung\u0026#39; }] }  As you can see, the second product of the first push persists, because the new product was simply merged with the old products array.\nAnd here\u0026rsquo;s the tip. To prevent this recursive merge from happening, you need to push the key _clear with the value true in the same object where you push the keys you don\u0026rsquo;t want to merge with their counterparts. So the second push becomes:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ products: [{ sku: \u0026#39;345\u0026#39;, name: \u0026#39;Close to the Edge\u0026#39; }], _clear: true });  So now the key in the products array of the data model only has one object within.\nTrick 2: Get the object representation of the current state of the data model You can always use window.google_tag_manager['GTM-XXXXX'].dataLayer.get(key) to fetch the current value of the given key from the data model. But if you want to get the full contents of the current data model, you need to run the following command:\nvar dataModel = window.google_tag_manager[\u0026#39;GTM-XXXXX\u0026#39;].dataLayer.get({ split: function() { return []; } }); console.table(dataModel);  If you run the command above in the JavaScript console of your browser, you should see a nice table view of the current data model:\n  What a simple way to see what the current state is.\nSummary I\u0026rsquo;ll be the first to admit: these are very technical, niche tricks. You probably won\u0026rsquo;t need them daily, or even monthly, in your implementations.\nHowever, I firmly believe that understanding how the data model works is one of the keys to unlocking Google Tag Manager\u0026rsquo;s real power.\nAnd, again, a huge thanks to Jethro Nederhof for revealing these simple tricks. He gets full credit for the substance of this article.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/get-position-index-of-visible-element/",
	"title": "#GTMTips: Get Position Index Of Visible Element",
	"tags": ["google tag manager", "gtmtips", "element visibility"],
	"description": "Use a Custom JavaScript variable in Google Tag Manager to get the position of the element that became visible, when using the Element Visibility trigger.",
	"content": "It\u0026rsquo;s time for a very simple #GTMTips article (I know, I always write that these are simple tips, but then they escalate into complex behemoths). Today, we\u0026rsquo;ll cover a nifty trick you can use with the Element Visibility trigger in Google Tag Manager. This tip was inspired by a question from Eugen Potlog in the Google Tag Manager Facebook group.\nThe use case is that you have an Element Visibility trigger firing for a number of elements all sharing the same CSS selector. In the image below, I want the visibility trigger to fire when any \u0026lt;h2\u0026gt; header element becomes visibile on the screen. However, I also want to know which one of all those headers caused the trigger to fire. In other words, I want the position of the header; If it\u0026rsquo;s the 5th header from the top, I want to get the number 5 to use in my tags. If it\u0026rsquo;s the first header, I want it to return 1.\nThis is quite easy to do with a Custom JavaScript variable, and below I\u0026rsquo;ll show you how.\nTip 84: Get the position index of the visible element   Here\u0026rsquo;s what the Custom JavaScript variable should look like:\nfunction() { var list = document.querySelectorAll(\u0026#39;h2\u0026#39;), // Make sure this CSS selector matches the one in the trigger  el = {{Click Element}}; return [].indexOf.call(list, el) + 1; }  This makes use of something you might not have thought of before, namely the fact that the built-in variable {{Click Element}} returns the element that caused the Element Visibility trigger to fire. It\u0026rsquo;s a multi-purpose variable in that sense.\nThe script first creates a list of all the possible elements the Element Visibility trigger can fire for, so it\u0026rsquo;s important that the selector argument of document.querySelectorAll(selector) matches the one you use in the Element Visibility trigger. Once the list is created, indexOf() is used against a native array object (because the list returned by querySelectorAll doesn\u0026rsquo;t have the .indexOf() method), and it checks if the list contains the element that fired the trigger.\nIf the list does contain it, the element\u0026rsquo;s position is returned as a positive integer, so the first element on the page would be 1, the second would be 2 and so on. If the element does not match against the list of elements in the variable list, position 0 is returned. This means you have misconfigured the selector(s).\nThis was a simple tip, undoubtedly, but I hope it\u0026rsquo;s useful to some of you!\n"
},
{
	"uri": "https://www.simoahava.com/tags/element-visibility/",
	"title": "element visibility",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/tag-sequencing-custom-html-tags/",
	"title": "#GTMTips: Tag Sequencing With Custom HTML Tags",
	"tags": ["google tag manager", "gtmtips", "tag sequencing", "custom html"],
	"description": "This guide shows you how to set up tag sequencing with Custom HTML tags when using Google Tag Manager.",
	"content": "Tag sequencing was introdced to Google Tag Manager in late 2015. Its main purpose was to facilitate the sequential firing of tags that have dependencies with each other. Due to the asynchronous nature of third-party libraries like Google Tag Manager, it\u0026rsquo;s difficult to establish an order of completion with tags that compete for their chance to fire.\nTag sequencing changed this, as it allows you to establish setup and cleanup tags - the former firing before the main tag, and the latter after.\nSetting up tag sequencing is relatively easy, at least once you understand how it works. However, Custom HTML tags have some exceptional behavior, as you need to utilize certain specific commands to signal the sequence once the Custom HTML tag has completed its execution. This #GTMTips article aims to guide you with setting up Custom HTML tags in a tag sequence.\nTip 83: Setting up Custom HTML tags in a tag sequence   Lesson 1: Sequence is managed with the success and failure callbacks The two callbacks, onHtmlSuccess() and onHtmlFailure() are what tag sequencing with Custom HTML tags really pivots around. The first is used to denote a place in the code when the code has finished running successfully, and execution can move to the next tag in the sequence. The second is used to signal when a failure happens, and here execution is also passed to the next tag in the sequence unless it has the failure toggle on:\n  Just to recap (but be sure to read my guide for a more thorough treatment), here\u0026rsquo;s how you would set up a regular Custom HTML tag in a tag sequence, where both success and failure criteria are established.\n NOTE! Remember to enable the Container ID and HTML ID built-in variables for this.\n (function() { var gtm = window.google_tag_manager[{{Container ID}}]; // Required \twindow[\u0026#39;something\u0026#39;] = getSomethingElse(); if (typeof window[\u0026#39;something\u0026#39;] !== \u0026#39;undefined\u0026#39;) { gtm.onHtmlSuccess({{HTML ID}}); // Success, move to next tag  } else { gtm.onHtmlFailure({{HTML ID}}); // Failure, move to next tag unless it has failure toggle on  } })();  Lesson 2: The success and failure callbacks aren\u0026rsquo;t always necessary Here\u0026rsquo;s something I think most guides have missed: you don\u0026rsquo;t need onHtmlSuccess() and onHtmlFailure() in your Custom HTML tag! The web browser executes all the code it finds in a Custom HTML tag from top-to-bottom before moving to the next item in the sequence.\nFor example, if you\u0026rsquo;ve setup the Facebook pixel with Custom HTML tags, you don\u0026rsquo;t need onHtmlSuccess() in the base pixel tag.\n!function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function(){n.callMethod? n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n; n.push=n;n.loaded=!0;n.version=\u0026#39;2.0\u0026#39;;n.queue=[];t=b.createElement(e);t.async=!0; t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window, document,\u0026#39;script\u0026#39;,\u0026#39;https://connect.facebook.net/en_US/fbevents.js\u0026#39;); fbq(\u0026#39;init\u0026#39;, {{Facebook Pixel ID}}); // The following line is NOT necessary: // window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}});  Why is the last line not necessary? Because GTM would pass execution to the next tag in any case when it reaches the last line of this tag. The web browser runs code synchronously - there\u0026rsquo;s no way it will stop in the middle of this tag and start working on the next tag.\nSo the lesson is:\nIf you have code that is always run from top-to-bottom before moving to the next tag, you don\u0026rsquo;t need onHtmlSuccess() and onHtmlFailure().\nBut, and there\u0026rsquo;s always a \u0026ldquo;but\u0026rdquo;, you\u0026rsquo;ll want to check the next lesson, too.\nLesson 3: If you have onHtmlFailure(), you\u0026rsquo;ll always want to have onHtmlSuccess(), too If you use onHtmlFailure() to signal that at some point in the code an error is met and sequence should not proceed with the next tag (since you have the failure toggle on), you should also add onHtmlSuccess() somewhere in the code.\nThis is because when you use the failure callback, GTM will not automatically proceed to the next tag anymore when it reaches the last line of the Custom HTML tag, even if you have the failure toggle turned off. By using onHtmlFailure(), you are telling Google Tag Manager to wait for either the onHtmlFailure() or onHtmlSuccess() before deciding whether to move to the next tag.\n(function() { var gtm = window.google_tag_manager[{{Container ID}}]; if (true) { console.log(\u0026#39;Worked!\u0026#39;); } else { gtm.onHtmlFailure({{HTML ID}}); } })();  In the above example, sequence will never proceed to the next tag. You have onHtmlFailure() which is never met (because true is always true), but you don\u0026rsquo;t have onHtmlSuccess() anywhere. So only this first tag is run, and the sequence is never continued.\nThis is one way it would work:\nif (true) { console.log(\u0026#39;Worked\u0026#39;); gtm.onHtmlSuccess({{HTML ID}}); } else { gtm.onHtmlFailure({{HTML ID}}); }  This would also work (though it\u0026rsquo;s not a good pattern), if the next tag in the sequence had the failure toggle off:\nif (true) { console.log(\u0026#39;Worked\u0026#39;); gtm.onHtmlFailure({{HTML ID}}); } else { gtm.onHtmlFailure({{HTML ID}}); }  And since it\u0026rsquo;s synchronous code, you can leave both callbacks out, and the sequence will proceed normally after logging Worked to the console in this case, too:\n(function() { if (true) { console.log(\u0026#39;Worked\u0026#39;); } })();  Lesson 4: The callbacks are at their best in asynchronous operations In my view, the purpose of tag sequencing is to establish order when the previous tag has an asynchronous operation. In other words, you want the browser to wait for the operation to complete before telling the next tag to start firing.\nFor example, here we load the jQuery library asynchronously, waiting for it to have completely loaded before moving to the next tag:\n(function() { var gtm = window.google_tag_manager[{{Container ID}}]; var el = document.createElement(\u0026#39;script\u0026#39;); el.async = true; el.src = \u0026#39;https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js\u0026#39;; el.addEventListener(\u0026#39;load\u0026#39;, function() { gtm.onHtmlSuccess({{HTML ID}}); }); document.head.appendChild(el); })();  As you can see, onHtmlSuccess() is embedded in the callback of the load listener, which is invoked only after the library has downloaded and the browser has executed the code within.\nSummary Working with Custom HTML tags in a tag sequence has some gotchas you should be aware of, but there\u0026rsquo;s really very little game-breaking. The biggest \u0026ldquo;mistake\u0026rdquo; I see people doing is adding the onHtmlSuccess() callback to the end of a synchronously executed block of code - this is not necessary unless you have onHtmlFailure() somewhere in the code, too.\nI hope that this article has clarified how Custom HTML tags and tag sequencing work together.\nLet me know in the comments if you have futher questions!\n"
},
{
	"uri": "https://www.simoahava.com/tags/tag-sequencing/",
	"title": "tag sequencing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/fire-trigger-when-user-about-to-leave-page/",
	"title": "#GTMTips: Fire Trigger When User Is About To Leave The Page",
	"tags": ["google tag manager", "gtmtips", "beforeunload"],
	"description": "Use the &#39;beforeunload&#39; browser event to fire a Google Tag Manager trigger (and tag) when the user is about to leave the page.",
	"content": "One of the great ways to leverage Google Tag Manager in your web analytics tracking is to make use of all the possible custom events that the browser has to offer. One such event is beforeunload. It\u0026rsquo;s dispatched in the browser when the user is about to unload the page. This means, typically, that the user is about to leave the page after clicking a link, or they are about to exit the browser by either closing the tab or the entire window.\nWe can use this event for many things, but in this article I\u0026rsquo;ll show you how to setup the listener, and then use it to send an event to Google Analytics which contains the deepest scroll depth threshold the user crossed on the page. So if they scrolled all the way to 75% of the document, this event would send that threshold to Google Analytics. Why? Because sometimes we simply want to know the farthest the user scrolled to any given page, rather than all the thresholds they crossed on the way there.\nTip 82: Fire a trigger on the beforeunload event   First of all, let\u0026rsquo;s create the event listener for beforeunload, so that the impatient readers can go right ahead and start working on their own solutions around this trigger.\n One important thing to note is that if you use a beforeunload listener, you invalidate the Back-Forward Cache of some browsers (e.g. Firefox). On some websites this might break user experience, so be sure to consult with your developers before implementing this, particularly on pages with forms.\n The tag is a simple Custom HTML tag that fires on the All Pages trigger. Feel free to use a more restrictive trigger, if you want the listener to be active only on specific pages.\nHere are the contents:\n\u0026lt;script\u0026gt; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { window.dataLayer.push({ event: \u0026#39;beforeunload\u0026#39; }); }); \u0026lt;/script\u0026gt; All this tag does is create the listener. The listener has a callback which pushes the beforeunload custom event into dataLayer.\nNow if you go to Preview mode, by clicking a link away from any page, you should just see the beforeunload text appear in the Preview mode event list before you are whisked away. If you see it, it means the listener works.\n  Next thing to do is to create a Custom Event trigger with the following settings:\n  If you add this trigger to a tag, that tag will fire just when the user is about to leave the page.\nSet the transport field in Google Analytics tags Because the trigger fires on the threshold of unloading the page, there is a very real risk that the page is unloaded before the asynchronous request initiated by the tag (firing on the trigger) has time to complete. To let your requests complete in time, you can use the Beacon API - another cool browser feature that\u0026rsquo;s almost essential to web analytics tracking.\nThankfully, if you are using Google Analytics tags, you don\u0026rsquo;t need to build the beacon utility yourself. The Universal Analytics library, analytics.js, has a special field called transport, which you can set to the value beacon in case you want to leverate the Beacon API with your Google Analytics hits.\nTo add this field, either use a Google Analytics Settings variable, or override the settings of your tag. Scroll down to Fields to set, and add a new field:\nField name: transport\nValue: beacon\nWith this setting, the tag now utilizes the Beacon API to dispatch the asynchronous request even if the browser has unloaded the page. The cool thing about this implementation is that if the browser doesn\u0026rsquo;t support this API, the tag automatically falls back to either GET or POST, just like it would normally do.\nOne \u0026ldquo;side effect\u0026rdquo; of using the Beacon API is that the request is automatically turned into a POST request. This means that if you\u0026rsquo;re using the Network tab of your browser\u0026rsquo;s developer tools, the request parameters won\u0026rsquo;t be outlined as nicely as they would with a GET request. For this reason, I strongly recommend you use the Google Analytics Debugger to analyze the requests.\nSend the deepest Scroll Depth threshold To send the deepest (or farthest) the user has scrolled on any given page, you need the following components.\nA Scroll Depth trigger with the thresholds configured\n  Note that you do not need to add this trigger to any tag. Its sole purpose is to push the threshold values into dataLayer.\nA Google Analytics tag which fires on the beforeunload trigger\nThe tag needs to fire on the beforeunload trigger, and it needs to send the value of the {{Scroll Depth Threshold}} Built-in variable to Google Analytics. Don\u0026rsquo;t forget to add the transport field there, too!\n  Again, feel free to add trigger exceptions or to modify the Custom Event trigger to restrict this tag to fire only on relevant pages. It doesn\u0026rsquo;t make sense to collect scrolling data on pages where that information is not relevant.\nSummary Once you\u0026rsquo;ve created the Custom HTML tag for the listener, the Custom Event trigger, the Scroll Depth trigger, and the Google Analytics tag, you\u0026rsquo;re good to go. When the user is about to leave any page, the beforeunload event triggers your Google Analytics tag. This tag grabs the latest value from {{Scroll Depth Threshold}} and sends it with the event to GA.\nSo if the user scrolled all the way down to 70% of the page (if you\u0026rsquo;ve set it up as a vertical threshold in the trigger settings), the value 70 would get sent with the tag.\nThis way you\u0026rsquo;ll preserve your hit quota (remember there\u0026rsquo;s a 500 hits per session limit in Google Analytics), and you\u0026rsquo;ll avoid sending a lot of noisy information about the intermediate thresholds to Google Analytics.\nThe scroll depth trick was just a tangent, though. The beforeunload listener can be used for a million different things, such as form abandonment and content engagement tracking.\nDo you have other cool uses for the beforeunload listener? Let us know in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/tags/beforeunload/",
	"title": "beforeunload",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-content-security-policy/",
	"title": "#GTMTips: Google Tag Manager Content Security Policy",
	"tags": ["google tag manager", "content security policy", "csp"],
	"description": "Configure the Content Security Policy (CSP) so that Google Tag Manager works on your website.",
	"content": "A Content Security Policy (CSP) is something you\u0026rsquo;ll configure your web server with to add an additional layer of protection, especially from harmful scripts loaded from third-party vendors. Once you have a CSP in place, all resources loaded and executed by the web page need to pass the CSP directives. For Google Tag Manager, this is very relevant. If you have a CSP in place, you will need to modify it so that Google Tag Manager functions properly.\nIn this short article, I\u0026rsquo;ll show you what directives you\u0026rsquo;ll need in place for GTM to work. In addition to the instructions here, I want you to read the excellent Bounteous article on the same topic. It isn\u0026rsquo;t comprehensive enough when it comes to GTM (which is why I wrote this article), but it has more information on CSPs in general, and it has instructions on how to get your tags to comply with the CSP as well.\nTip 81: Google Tag Manager Content Security Policy   First, here are a number of symptoms to look for, which might help you detect that you have a CSP issue:\n  No debug panel shown even if you are in Preview mode.\n  Debug panel shows, but it has no styles (see the image above).\n  Google Tag Manager doesn\u0026rsquo;t load, and you see a Content Security Policy error in the JavaScript console.\n    Google Tag Manager requires you to allow a number of things: inline scripts, inline eval() use, and inline styles. Listed below are the modifications you need to make in the Content Security Policy, so that Google Tag Manager works properly both in published containers and in Preview mode.\n Thanks to Wieland Lindenthal for the feedback that helped make the directives below more precise.\n    Directive Comment     script-src 'unsafe-eval' 'unsafe-inline' https://tagmanager.google.com/ https://www.googletagmanager.com/ You need to enable the two listed domains in script-src together with the 'unsafe-inline' and 'unsafe-eval' sources. GTM requires both inline script and eval() to run custom code added by users.   style-src 'unsafe-inline' https://tagmanager.google.com/ https://fonts.googleapis.com/ This directive enables the styles and custom fonts in the GTM debug panel.   img-src 'unsafe-inline' https://ssl.gstatic.com/ This directive loads the Google Tag Manager logo image in debug mode.    The first directive is absolutely necessary, the last two directives are useful but not critical. However, I do recommend including the style-src directive, since it\u0026rsquo;s a pain to work in debug mode without the stylesheet.\nSummary These simple instructions should help you fix your site\u0026rsquo;s CSP so that Google Tag Manager works properly.\nI know you might be wary of adding all these directives, especially since they might introduce issues into your site by being so relaxed. However, Google Tag Manager is a script injector, so it does require elevated privileges to work properly on your site. It\u0026rsquo;s part of the bargain when using a tag management solution.\n"
},
{
	"uri": "https://www.simoahava.com/tags/content-security-policy/",
	"title": "content security policy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/csp/",
	"title": "csp",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/automatically-reduce-google-analytics-payload-length/",
	"title": "Automatically Reduce Google Analytics Payload Length",
	"tags": ["google tag manager", "customtask", "google analytics"],
	"description": "Use customTask to automatically reduce the Google Analytics payload length, so that it stays below the maximum of 8192. This is very useful for Enhanced Ecommerce tracking, where payload sizes might grow to be very large.",
	"content": "Here we are, reunited with customTask. This time, we\u0026rsquo;ll put this wonderful mechanism to work for a very, very good cause. One of the lesser known \u0026ldquo;features\u0026rdquo; of Google Analytics is that when the payload size (the request body that is actually sent to Google Analytics with each request) goes past a certain limit, specifically 8192 bytes, the hit is aborted without warning. This can come as a surprise, because there\u0026rsquo;s no indication anywhere in Google Analytics that you are missing hits because of this.\nThe solution? A nice little customTask script, which recursively whittles away parts of the payload until the length is just below the cap.\n  The idea is that you will be able to specify a list of payload parameters to be removed one by one from the payload until the size is safely below the maximum of 8192. First things that will go are some of the default dimensions that no one really uses, such as \u0026lsquo;Java Enabled\u0026rsquo; and \u0026lsquo;Document Encoding\u0026rsquo;. Next, you can list all the Custom Dimensions you want to get rid of, in order of priority.\nFinally, you\u0026rsquo;ll be able to list the Enhanced Ecommerce impression parameters that you want to remove from each impression in the payload. The script will stop as soon as the length is below 8192, so you\u0026rsquo;ll most likely end up with some impressions having more details than others.\nWhy stop at impressions? Because, from experience, that\u0026rsquo;s where most problems lie. It shouldn\u0026rsquo;t be too difficult to customize the script to also include Promotions and even \u0026ldquo;regular\u0026rdquo; Enhanced Ecommerce product objects.\nOnce you\u0026rsquo;ve implemented this in your Enhanced Ecommerce tags, you should never see the following error in the console (if you have Google Analytics Debugger activated, or if you have Universal Analytics running in debug mode):\n  I recommend implementing this customTask together with my solution for sending the payload length as a custom dimension. This way you\u0026rsquo;ll be able to detect if you\u0026rsquo;re approaching the maximum length of the payload with your hits, and you can then proceed to implement this payload length limiter when necessary.\nFor an alternative solution, take a look at this excellent article by the awesome Dan Wilkerson from the equally awesome Bounteous blog.\nThe customTask variable To build the customTask, you need to create a new Custom JavaScript variable. Name it something like {{customTask - reduce payload length}}.\n Note! If you already have a customTask implemented in your Enhanced Ecommerce tags, you\u0026rsquo;ll need to combine the code below with the existing customTask script you already have. For tips on how to do this, consult this guide.\n Add the following code into the customTask variable body:\nfunction() { return function(customModel) { // Add any other default parameters you want to strip from the payload  // into this array in order of priority.  // For more details, visit: https://bit.ly/2MOhjBD  var defaultParams = [\u0026#39;\u0026amp;je\u0026#39;, \u0026#39;\u0026amp;de\u0026#39;, \u0026#39;\u0026amp;sd\u0026#39;, \u0026#39;\u0026amp;vp\u0026#39;, \u0026#39;\u0026amp;sr\u0026#39;]; // List the (regular) Custom Dimensions you want to strip from the  // payload in order of priority. Leave the array empty if you don\u0026#39;t  // want to remove Custom Dimensions.  // For more details, visit: https://bit.ly/2MM6O1O  var customDims = [\u0026#39;\u0026amp;cd198\u0026#39;, \u0026#39;\u0026amp;cd199\u0026#39;, \u0026#39;\u0026amp;cd200\u0026#39;]; // List the impression object parameters you want to strip from the  // payload in order of priority. Only list the field specifier, so  // instead of \u0026#39;\u0026amp;il1pi2va\u0026#39;, write \u0026#39;va\u0026#39;, and instead of \u0026#39;\u0026amp;il1pi4cd3\u0026#39;, write  // \u0026#39;cd3\u0026#39;. For more details, visit: https://bit.ly/2KehCHF  var impressions = [\u0026#39;va\u0026#39;, \u0026#39;br\u0026#39;, \u0026#39;ca\u0026#39;, \u0026#39;ps\u0026#39;, \u0026#39;nm\u0026#39;]; // Don\u0026#39;t touch the code below.  var maxLength = 8192, globalSendTaskName = \u0026#39;_\u0026#39; + customModel.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;, originalSendHitTask = window[globalSendTaskName] = window[globalSendTaskName] || customModel.get(\u0026#39;sendHitTask\u0026#39;); customModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; // Only log if in analytics debug mode.  var log = function(msg) { if (\u0026#39;dump\u0026#39; in ga) { window.console.log.apply(window.console, [msg]); } }; var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var removeKeys = function(hitPayload, keys) { var key, regex; while(hitPayload.length \u0026gt;= maxLength \u0026amp;\u0026amp; keys.length) { key = keys[0]; log(\u0026#39;--\u0026gt; Removing \u0026#39; + key); regex = new RegExp(key + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;gi\u0026#39;); keys.shift(); hitPayload = hitPayload.replace(regex, \u0026#39;\u0026#39;); log(\u0026#39;--\u0026gt; New length: \u0026#39; + hitPayload.length); } return hitPayload; }; var removeImpressions = function(hitPayload, keys) { var key, regex, oldKey; while(hitPayload.length \u0026gt;= maxLength \u0026amp;\u0026amp; keys.length) { if (key !== keys[0]) { key = keys[0]; log(\u0026#39;--\u0026gt; Removing \u0026amp;ilNpiN\u0026#39; + key + \u0026#39; from impression objects\u0026#39;); } regex = new RegExp(\u0026#39;\u0026amp;il\\\\d+pi\\\\d+\u0026#39; + key + \u0026#39;=[^\u0026amp;]+\u0026#39;, \u0026#39;i\u0026#39;); if (!regex.test(hitPayload)) { keys.shift(); } hitPayload = hitPayload.replace(regex, \u0026#39;\u0026#39;); } log(\u0026#39;--\u0026gt; New length: \u0026#39; + hitPayload.length); return hitPayload; }; // If over payload length, remove default parameters.  if (hitPayload.length \u0026gt;= maxLength) { log(\u0026#39;Payload too long (\u0026#39; + hitPayload.length + \u0026#39;), removing default keys...\u0026#39;); hitPayload = removeKeys(hitPayload, defaultParams); } // If over payload length, remove custom dimensions.  if (hitPayload.length \u0026gt;= maxLength) { log(\u0026#39;Payload still too long (\u0026#39; + hitPayload.length + \u0026#39;), removing Custom Dimensions...\u0026#39;); hitPayload = removeKeys(hitPayload, customDims); } // If over payload length, clean up impression objects.  if (hitPayload.length \u0026gt;= maxLength) { log(\u0026#39;Payload still too long (\u0026#39; + hitPayload.length + \u0026#39;), cleaning up Impressions...\u0026#39;); hitPayload = removeImpressions(hitPayload, impressions); } // Send the modified payload.  sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload, true); originalSendHitTask(sendModel); }); }; }  Make sure you edit the defaultParams, customDims, and impressions arrays to include the keys you want to purge from the dataLayer. Note that you will need to list the keys using their Measurement Protocol parameter names rather than what their analytics.js field names are. So instead of 'encoding', for example, you need to use '\u0026amp;de'.\nThe keys I have added to the defaultParams array are:\n   Key Name Description     \u0026amp;je Java Enabled Whether the browser has enabled Java or not.   \u0026amp;de Document Encoding What the character encoding of the current page is (e.g. UTF-8).   \u0026amp;sd Screen Colors The screen color depth (e.g. 24-bits).   \u0026amp;vp Viewport Size The viewable area (in pixels) of the browser or device.   \u0026amp;sr Screen Resolution The screen resolution (in pixels) of the browser or device.    Naturally, you might want to preserve some of these, e.g. \u0026amp;vp or \u0026amp;sr. Feel free to edit the defaultParams array as you see fit.\nWhen adding items to the impressions array, the syntax is slightly more complex. You should only add the actual parameter specifier, which is the key that comes after \u0026amp;ilNpiN. So if the parameter name for Product Impression Variant is \u0026amp;il2pi1va, you\u0026rsquo;d type just 'va' into the array. Or, if you wanted to purge the Product-Scoped Custom Dimension from index 3 from your impression objects, instead of typing '\u0026amp;il1pi1cd3', you\u0026rsquo;d type 'cd3 into the array.\nNote that for defaultParams and customDims you need to type the full parameter name with the leading \u0026amp;.\nAdd the variable to your tag This variable should be added to the tag which sends the Enhanced Ecommerce payload with impression objects to Google Analytics. To add it to the tag, either edit the Google Analytics Settings variable in the impression tag, or add the field directly to the tag.\nBrowse to More Settings -\u0026gt; Fields to set, and add the following field:\nField name: customTask\nValue: {{customTask - reduce payload length}}\nLike so:\n  And that\u0026rsquo;s it for the setup! Now, if your payload is over 8192, the script will first chop out the parameters specified in the defaultParams array. Then it will gobble up the dimensions in the customDims array. Finally, if the payload is STILL too long, it will proceed to clean up your impressions objects by removing the fields listed in impressions one by one, impression by impression.\nHow to debug If you walked through the code, as I\u0026rsquo;m sure you did, you might have noticed how I use a custom method named log() to send some debug data to the console. The thing is, I don\u0026rsquo;t want to pollute the JavaScript console with debug messages when Google Analytics is not in debug mode, so I do a simple check to see if the debug version of analytics.js is loaded:\nif (\u0026#39;dump\u0026#39; in window[\u0026#39;ga\u0026#39;]) {}  Basically, if the global ga object has the method dump, I\u0026rsquo;m assuming the user is using the debug version of analytics.js.\n NOTE! If someone has figured a more robust way to check for the debug version of analytics.js, please let me know! Thank you.\n So, if you want to see debug messages, you can set the library to debug mode by browsing to More Settings -\u0026gt; Advanced Configuration -\u0026gt; Use Debug Version in your Google Analytics tag or Google Analytics Settings variable, and setting it to true. Or, you can use the Google Analytics Debugger browser extension.\nOnce you have the debug library loaded, you can see debug messages such as these if the payload exceeds the maximum length of 8192:\n  In these debug messages you can see how the script walks through your payload, stripping out parameters where necessary.\nSummary I wish analytics.js had a more elegant way of handling payloads that are too large. Just dropping them is slightly overkill, in my opinion. analytics.js could, for example, compile a custom hit to GA when the payload maximum size is breached, and this would turn into a warning or notification in the Google Analytics UI. This way you\u0026rsquo;d at least have a clue that something is amiss.\nSo the customTask solution presented here is, as is typical for scripts that I write, a bandaid for something that will hopefully be remedied natively by the library itself in the future.\nNaturally, the \u0026ldquo;best\u0026rdquo; way to avoid payloads that are too large is to only send relevant data to Google Analytics. In the case of impressions, for example, it doesn\u0026rsquo;t really make sense to send every single impression on the page in one hit. Rather, you\u0026rsquo;d want to only send impressions that the user has actually viewed. Luckily, I might just have a guide coming up that will show you how to track viewed impressions only.\nLet me know in the comments if you find this useful or if you have suggestions for improving the method!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/allow-block-advertising-features-google-analytics/",
	"title": "Allow And Block Advertising Features In Google Analytics",
	"tags": ["google tag manager", "google analytics", "gdpr"],
	"description": "Use the new allowAdFeatures field in Universal Analytics and Google Tag Manager to either allow or block the DoubleClick request. This request is initiated with the Advertising Features setting.",
	"content": "After writing yet another customTask article on how to respect client-side opt-out using Google Analytics and Google Tag Manager, the analytics.js core library was enhanced with a new field that makes it all a lot easier to do. The field, allowAdFeatures, lets you either allow or block the request to DoubleClick that is initiated when Advertising Features have been enabled.\nIn this very short article, I\u0026rsquo;ll quickly show you what the field does and why it\u0026rsquo;s useful.\nAdvertising Features If you go to Google Analytics\u0026rsquo; Property Settings, you can find the relevant toggles under Tracking Info -\u0026gt; Data Collection.\n  When you enable both these toggles, hits collected to this web property on your site will automatically send the advertising hit to the DoubleClick endpoint, too. In other words, the analytics.js library downloaded by the analytics setup on your site (deployed via Universal Analytics, gtag.js, or Google Tag Manager snippets) will automatically be equipped with the necessary tools to perform this redirect. If you want to verify that it\u0026rsquo;s working, you should see the following in the Google Analytics Debugger output in the console:\n  An alternative way to deploy this feature is to load the displayfeatures plugin if using analytics.js:\n... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;); ga(\u0026#39;require\u0026#39;, \u0026#39;displayfeatures\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;);  The require command does the same thing as the Property Setting toggle. The difference is that now you have the option to control whether or not this plugin is loaded.\nThe third way to deploy this feature is to use the respective tag setting in Google Tag Manager:\n  All these settings do the same thing - once a hit is sent to Google Analytics, a request is sent to the DoubleClick server, too, containing information about the user (such as Client ID) and the web property (Property ID). This request is used to build the Demographics and Interests reports and for building remarketing audiences.\nThe new allowAdFeatures field But what if you want to conditionally block the request from being sent? Under GDPR, this might be a very good idea. If the visitor doesn\u0026rsquo;t explicitly opt-in to you aggregating their visit data into DoubleClick servers, you might want to block the request from being sent.\nIn an earlier article, I recommended using customTask for this. However, due to how displayFeaturesTask can\u0026rsquo;t be used to control the plugin / Google Tag Manager tag setting, it turned out to be an inefficient solution.\nLuckily, Google released the allowAdFeatures field, which you can use to conditionally block the DoubleClick request regardless of how it was set up.\nNote that the field can\u0026rsquo;t be used to establish Advertising Features collection. You still need to 1) enable the respective GA Property settings first, OR 2) load the displayfeatures plugin, OR 3) toggle the respective setting in your Google Tag Manager tags. This field simply either allows the request to go through, or blocks it.\nThe default value of the field is true, so if you don\u0026rsquo;t add the field to your setup, the Advertising Features will work regularly, sending the DoubleClick request if necessary. If you set the field to false, the DoubleClick request will be unconditionally blocked on the current page.\nTo use the field in analytics.js, you can do something like this:\n... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;); if (checkUserConsent() === \u0026#39;NO\u0026#39;) { ga(\u0026#39;set\u0026#39;, \u0026#39;allowAdFeatures\u0026#39;, false); } ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); ...  In the example above, if the method checkUserConsent() returns the value 'NO', then Advertising Features will be blocked for this tracker.\nTo use this in Google Tag Manager, you need to go to More Settings -\u0026gt; Fields to Set, and add a new field with:\nField name: allowAdFeatures\nValue: true/false\nNaturally, it\u0026rsquo;s easiest if you use a variable which checks the user consent, and then make sure the variable returns either true or false, depending on whether you want to allow or block the DoubleClick request, respectively.\n  Summary I hope all that made sense. The point of allowAdFeatures is to give you full client-side control over whether or not the DoubleClick request should happen. Due to the flexibility of allowAdFeatures I personally use the following setup:\n  Toggle the Remarketing and Advertising Features settings on in Google Analytics / Property Settings / Tracking Info / Data Collection.\n  Make sure the Advertising Features selections are disabled in Google Tag Manager tags.\n  Use the allowAdFeatures field to conditionally block the Advertising Features if the visitor has not opted in.\n  This, in my experience, is the solution that\u0026rsquo;s easiest to manage.\n"
},
{
	"uri": "https://www.simoahava.com/tags/gdpr/",
	"title": "gdpr",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/13-useful-custom-dimensions-for-google-analytics/",
	"title": "13 Useful Custom Dimensions For Google Analytics",
	"tags": ["google tag manager", "google analytics", "custom dimensions"],
	"description": "Implementation guide for useful Google Analytics Custom Dimensions, such as Hit Type, Session ID, Payload Length, and many more. The guide is for Google Tag Manager.",
	"content": "One of my favorite (and most popular) articles in my blog is Improve Data Collection With Four Custom Dimensions. In that article, I show how you can improve the quality and granularity of your Google Analytics data set with just four Custom Dimensions. The reason I chose the four dimensions (Hit Timestamp, Session ID, Client ID, and User ID) is because I firmly believe that they should be in Google Analytics\u0026rsquo; standard set of dimensions, but for some inexplicable reason they aren\u0026rsquo;t.\n  Since publishing that article three years ago, I have naturally uncovered more of these \u0026ldquo;need-to-have\u0026rdquo; Custom Dimensions, and I\u0026rsquo;ve written a bunch of guides on how to implement them. In this article, I want to pull all of these together into one reference guide, which you can then use to find the relevant information with ease. All of the implementation guides are for Google Tag Manager, but nothing\u0026rsquo;s stopping you from modifying the JavaScript to work with analytics.js or gtag.js.\n1. Improve granularity The first four Custom Dimensions are those introduced in my earlier article. You can follow the link to see more details about the implementation, but some of the solutions below have been updated, so make sure you follow these guides rather than those in the original article.\nThe purpose of these four dimensions is to add more information to Google Analytics data sets about the user journeys that take place on your site. Being able to segment and chop the data on a hit-by-hit basis (Hit Timestamp), group them into sessions (Session ID), sort by GA user (Client ID), and finally analyze cross-device paths (User ID), makes so much sense from an analytics point-of-view. It\u0026rsquo;s amazing that these dimensions are not readily available in Google Analytics\u0026rsquo; data sets. They are available in the BigQuery export\u0026hellip;\n  1.1. Hit Timestamp Original source: Link\nCustom Dimension scope: Hit\nHit Timestamp outputs the local time (i.e. visitor\u0026rsquo;s browser time) of the hit in standardized format. An example would be 2018-05-29T15:04:51.361+03:00 (year-month-dayThour:minutes:seconds.milliseconds timezone_offset). Since it\u0026rsquo;s hit timestamp, it\u0026rsquo;s important that you add this Custom Dimensions to every single Google Analytics tag that fires on your site. Easiest way to do it is to add it to a Google Analytics Settings variable.\nCustom JavaScript variable: {{Hit timestamp}}\nContents:\nfunction() { // Get local time as ISO string with offset at the end  var now = new Date(); var tzo = -now.getTimezoneOffset(); var dif = tzo \u0026gt;= 0 ? \u0026#39;+\u0026#39; : \u0026#39;-\u0026#39;; var pad = function(num) { var norm = Math.abs(Math.floor(num)); return (norm \u0026lt; 10 ? \u0026#39;0\u0026#39; : \u0026#39;\u0026#39;) + norm; }; return now.getFullYear() + \u0026#39;-\u0026#39; + pad(now.getMonth()+1) + \u0026#39;-\u0026#39; + pad(now.getDate()) + \u0026#39;T\u0026#39; + pad(now.getHours()) + \u0026#39;:\u0026#39; + pad(now.getMinutes()) + \u0026#39;:\u0026#39; + pad(now.getSeconds()) + \u0026#39;.\u0026#39; + pad(now.getMilliseconds()) + dif + pad(tzo / 60) + \u0026#39;:\u0026#39; + pad(tzo % 60); }    1.2. Session ID Original source: Link\nCustom Dimension scope: Session\nSession ID is a random, unique string (GUID) which is scoped to the entire session. This means that all hits of the same session can be queried with the same ID. This, in turn, means that you have a way of aggregating all related hits directly in the data reports, rather than having to infer them using segments, for example.\nYou can add Session ID only to your Page View tag, but there\u0026rsquo;s no harm in including it in every single one of your GA tags. In fact, I actually recommend just adding it to the Google Analytics Settings variable you use for all your tags, because it\u0026rsquo;s possible that not all of your sessions will have a pageview in them (if the session breaks, and a new one starts without reloading a page).\nCustom JavaScript variable: {{Random GUID}}\nContents:\nfunction () { // Public Domain/MIT  var d = new Date().getTime(); if (typeof performance !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; typeof performance.now === \u0026#39;function\u0026#39;){ d += performance.now(); //use high-precision timer if available  } return \u0026#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\u0026#39;.replace(/[xy]/g, function (c) { var r = (d + Math.random() * 16) % 16 | 0; d = Math.floor(d / 16); return (c === \u0026#39;x\u0026#39; ? r : (r \u0026amp; 0x3 | 0x8)).toString(16); }); }    1.3. Client ID Original source: Link\nCustom Dimension scope: User\nClient ID is the anonymous cookie identifier that Google Analytics assigns to every single browser instance of any given web visitor. It\u0026rsquo;s how GA calculates the Users metric in views which don\u0026rsquo;t have the User ID feature enabled (more on this in the next chapter).\nSending the Client ID as a Custom Dimension to Google Analytics is absolutely necessary. It\u0026rsquo;s the only way to get row-by-row reports split by user, and it\u0026rsquo;s the only way to query for data collected from any anonymous user.\nThe Client ID is collected using customTask (guide here), so you do not add it to tags using the Custom Dimensions settings. Instead, you add it to Fields to set -\u0026gt; customTask (see screenshot below). Be sure to read my guide on customTask to understand how this mechanism works.\nCustom JavaScript variable: {{customTask - Client ID}}\nContents:\nfunction() { var clientIdIndex = 3; // Change this number to the actual Custom Dimension index number  return function(model) { model.set(\u0026#39;dimension\u0026#39; + clientIdIndex, model.get(\u0026#39;clientId\u0026#39;)); }; }    1.4. User ID Original source: Link\nCustom Dimension scope: User\nUser ID is an identifier that you have somehow linked to the logged in user. Typically it\u0026rsquo;s a pseudonymized key written into dataLayer using an identifier attached to the user in your CRM or some other server-side user registry.\nBecause there are so many ways to implement User ID, depending on how your backend produces the identifier, I do not have a generic guide on how to generate the value. Thus the instructions below apply only to a very specific use case, where the User ID is written to the dataLayer when the page is loaded. But you could just as well have your web server write the User ID in some other global variable, or even in a browser cookie.\nAlso, because you are collecting data from logged in users, make sure whatever solution you implement is compliant with the privacy legislation and regulations of your region.\nData Layer variable: {{DLV - userId}}\nContents: Data Layer Variable Name - userId\n  2. Good all-around Custom Dimensions The next set of Custom Dimensions are just smart things to add to your data collection. Since they are included in this guide, they aren\u0026rsquo;t queriable by default in GA\u0026rsquo;s reporting interface.\n  2.1. Hit Type Original source: Link\nCustom Dimension scope: Hit\nHit Type tells you explicitly what the type of hit collected in Google Analytics was. So it would be event for Events, pageview for Pageviews, timing for Timing hits, etc. You can use this to build segments (e.g. sessions where the first hit was an event), or to add more granularity to path analysis.\nYou should add the variable to every single Google Analytics tag in your container. The variable needs to be added using customTask. Be sure to read my guide on customTask to understand how this mechanism works.\nCustom JavaScript variable: {{customTask - Hit type}}\nContents:\nfunction() { var hitTypeIndex = 5; // Change this number to the actual Custom Dimension index number  return function(model) { model.set(\u0026#39;dimension\u0026#39; + hitTypeIndex, model.get(\u0026#39;hitType\u0026#39;)); }; }    2.2. Full Referrer Custom Dimension scope: Hit\nFull Referrer is simply the value of document.referrer added as a hit-scoped Custom Dimension to every single Google Analytics tag in your container. This will let you analyze what the previous page of any given hit was. This makes a lot of sense for pageview tracking, and it makes a lot of sense for hits that start sessions, since you can now analyze the referral information for sessions that came via some other source than a referral hit.\nJavaScript variable: {{document.referrer}}\nContents: Global Variable Name - document.referrer\n  2.3. Payload Length Original source: Link\nCustom Dimension scope: Hit\nYou can send the payload length of every single Google Analytics hit by adding this hit-scoped Custom Dimension to every single tag in your container. The idea is that you can monitor if you are approaching the 8192 byte limit, since hits that equal or surpass that limit will not get sent to Google Analytics.\nThe solution is added using customTask. Be sure to read my guide on customTask to understand how this mechanism works.\nCustom JavaScript variable: {{customTask - Hit payload length}}\nContents:\nfunction() { // Change this index to match that of the Custom Dimension you created in GA  var customDimensionIndex = 7; return function(model) { var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; var originalSendHitTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { try { var originalHitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var customDimensionParameter = \u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex; // If hitPayload already has that Custom Dimension, note this in the console and do not overwrite the existing dimension  if (hitPayload.indexOf(customDimensionParameter + \u0026#39;=\u0026#39;) \u0026gt; -1) { console.log(\u0026#39;Google Analytics error: tried to send hit payload length in an already assigned Custom Dimension\u0026#39;); originalSendHitTask(sendModel); } else { // Otherwise add the Custom Dimension to the string  // together with the complete length of the payload  hitPayload += customDimensionParameter + \u0026#39;=\u0026#39;; hitPayload += (hitPayload.length + hitPayload.length.toString().length); sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload, true); originalSendHitTask(sendModel); } } catch(e) { console.error(\u0026#39;Error sending hit payload length to Google Analytics\u0026#39;); sendModel.set(\u0026#39;hitPayload\u0026#39;, originalHitPayload, true); originalSendHitTask(sendModel); } }); }; }    2.4. GTM Container ID Custom Dimension scope: Hit\nThis is another of those no-brainer Custom Dimensions. Sending the Container ID serves multiple purposes: It helps you identify valid hits from spam (particularly spam sent from outside your site), it helps you debug data in a situation where you have more than one container running on the site, and it makes it easy to run parallel installations of Universal Analytics (or Global Site Tag) and Google Tag Manager.\nYou need to activate the Container ID Built-in variable for this. Add the variable to your Google Analytics Settings variable, so that it\u0026rsquo;s added to every single Google Analytics tag in your container.\n  3. Browsing behavior The final set of Custom Dimensions is all about tracking browsing behavior. Follow the link in the previous sentence to read a thorough guide on how and why to track browsing behavior in Google Analytics.\nSuffice to say that by adding these dimensions to your data collection, you can make more sense out of how users navigate to, navigate on, and navigate away from your site. It\u0026rsquo;s finally possible to segment your data based on how your visitors open pages in browser tabs, do they navigate with the browser\u0026rsquo;s Back and Forward buttons, and how many tabs they have open at any given time!\nInstead of adding solution-specific implementation information, please follow the steps in the original guide.\n  3.1. Redirect Count Custom Dimension scope: Hit\nRedirect count is the number of times the initial document request was redirected when the user loaded the current page. There are some inconsistencies here, and I haven\u0026rsquo;t been able to deduce a consistent description of redirectCount behavior. However, if it works as it should, it offers interesting data about how your site responds to requests, especially when combined with organic search traffic, for example.\nData Layer variable: {{DLV - browsingBehavior.redirectCount}}\nContents: Data Layer Variable Name - browsingBehavior.redirectCount\n  3.2. Navigation Type Custom Dimension scope: Hit\nNavigation type means the method of navigating to the current page. This includes values such as NAVIGATE (user wrote the address in the address bar or clicked a link to the current page), RELOAD (user reloaded the page), BACK / FORWARD (user clicked the browser\u0026rsquo;s back or forward buttons) and OTHER (some other way of navigating to the page.\nData Layer variable: {{DLV - browsingBehavior.navigationType}}\nContents: Data Layer Variable Name - browsingBehavior.navigationType\n  3.3. Tab Type Custom Dimension scope: Hit\nTab type indicates whether the page load occurred in a newly open tab, or whether the tab already existed. This is done by storing the current tabId into sessionStorage. If the storage doesn\u0026rsquo;t have the current tab ID, we can deduce that the tab is a new one, since sessionStorage is purged when the tab is closed.\nData Layer variable: {{DLV - browsingBehavior.newTab}}\nContents: Data Layer Variable Name - browsingBehavior.newTab\n  3.4. Tabs Open Custom Dimension scope: Hit\nTabs open returns the number of tabs currently open in the browser. This is based on a count of currently active tab IDs stored in localStorage. This counter isn\u0026rsquo;t 100% reliable, because it relies on a number of moving parts which might not always be in sync. But as an indicator it\u0026rsquo;s very useful, since it tells you how your content is typically being digested.\nData Layer variable: {{DLV - browsingBehavior.tabCount}}\nContents: Data Layer Variable Name - browsingBehavior.tabCount\n  3.5. Tab ID Custom Dimension scope: Hit\nThe final cog in the machine is the ID of the current tab. This is useful in case you want to group hits together by the tab in which they occurred.\nData Layer variable: {{DLV - browsingBehavior.tabId}}\nContents: Data Layer Variable Name - browsingBehavior.tabId\n  4. The customTask variable You might have noticed that a number of the solutions in this guide utilize customTask. I might have recommended you to read my customTask guide, too!\nOne of the things you\u0026rsquo;ll learn in the guide is that you can only have one customTask per tag. That means that if you want to implement all the Custom Dimensions in this article, you will need to combine three different customTask solutions into a single Custom JavaScript variable.\nThis is fairly easy to do, since it\u0026rsquo;s more or less just copy-pasting all the code into one long anonymous function, but I\u0026rsquo;ll show you how to do it nevertheless.\nThis is the Custom JavaScript variable you\u0026rsquo;ll be using:\nfunction() { var clientIdIndex = 3; // Change this number to the index of your Client ID Custom Dimension  var hitTypeIndex = 5; // Change this number to the index of your Hit Type Custom Dimension  var payloadLengthIndex = 7; // Change this number to the index of your Payload Length Custom Dimension  return function(model) { var globalSendTaskName, originalSendHitTask, originalHitPayload, hitPayload, customDimensionParameter; if (typeof clientIdIndex === \u0026#39;number\u0026#39;) { model.set(\u0026#39;dimension\u0026#39; + clientIdIndex, model.get(\u0026#39;clientId\u0026#39;)); } if (typeof hitTypeIndex === \u0026#39;number\u0026#39;) { model.set(\u0026#39;dimension\u0026#39; + hitTypeIndex, model.get(\u0026#39;hitType\u0026#39;)); } if (typeof payloadLengthIndex === \u0026#39;number\u0026#39;) { globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; originalSendHitTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { try { originalHitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); customDimensionParameter = \u0026#39;\u0026amp;cd\u0026#39; + payloadLengthIndex; // If hitPayload already has that Custom Dimension, note this in the console and do not overwrite the existing dimension  if (hitPayload.indexOf(customDimensionParameter + \u0026#39;=\u0026#39;) \u0026gt; -1) { console.log(\u0026#39;Google Analytics error: tried to send hit payload length in an already assigned Custom Dimension\u0026#39;); originalSendHitTask(sendModel); } else { // Otherwise add the Custom Dimension to the string  // together with the complete length of the payload  hitPayload += customDimensionParameter + \u0026#39;=\u0026#39;; hitPayload += (hitPayload.length + hitPayload.length.toString().length); sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload, true); originalSendHitTask(sendModel); } } catch(e) { console.error(\u0026#39;Error sending hit payload length to Google Analytics\u0026#39;); sendModel.set(\u0026#39;hitPayload\u0026#39;, originalHitPayload, true); originalSendHitTask(sendModel); } }); } }; }  If you want to drop any of the three solutions from this customTask, simply remove or comment out the respective var ...Index = N; line. For example, if you don\u0026rsquo;t need to collect the payload length, change the code appropriately:\nfunction() { var clientIdIndex = 3; // Change this number to the index of your Client ID Custom Dimension  var hitTypeIndex = 5; // Change this number to the index of your Hit Type Custom Dimension  // var payloadLengthIndex = 7;  return function(model) { ...  Summary This list of Custom Dimensions is certainly not exhaustive, and I\u0026rsquo;m sure you have other suggestions to what could be considered \u0026ldquo;useful\u0026rdquo;.\nOne thing to keep in mind is the restriction of just 20 available Custom Dimension slots in the free version of Google Analytics. This is very unfortunate, and one of the main gripes I have with GA. So it\u0026rsquo;s very likely you\u0026rsquo;re forced to prioritize just which dimension you\u0026rsquo;ll need to be adding to your data collection. If push comes to shove, I would personally be fine with ignoring the five Browsing Behavior dimensions, and just focus on the 8 dimensions listed first in this article.\nRegardless, I consider these Custom Dimensions to add much-needed granularity to GA data. Many of these dimensions are available by default only in the BigQuery export, which, in turn, is mainly available for Google Analytics 360 customers. So by adding these dimensions to your data set, you are, in a way, actually saving money.\nDo you have some go-to Custom Dimensions you\u0026rsquo;d recommend others add to their data collection right this instant? Let us know in the comments.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/add-hit-type-custom-dimension/",
	"title": "#GTMTips: Add Hit Type As A Custom Dimension",
	"tags": ["google tag manager", "gtmtips", "customtask"],
	"description": "Use customTask in Google Tag Manager to automatically send the hit type of each request as a Custom Dimension to Google Analytics. This introduces an additional level of granularity to your report data.",
	"content": "I\u0026rsquo;ve spent a considerable amount of time talking and writing about how to improve the granularity of your Google Analytics data, especially when using Google Tag Manager. I\u0026rsquo;ve also gone on and on and on (and on) about customTask, which makes adding metadata to the Google Analytics hits dispatched from your website a breeze.\nIn this article, I\u0026rsquo;ll introduce a simple way to add yet another level of detail to your GA hits, using customTask as the method of choice. The piece of data we\u0026rsquo;ll add is the hit type of the hit (e.g. pageview, event, and timing) which, for some ludicrous reason, is not a default dimension available in Google Analytics reports.\nTip 80: Add hit type as a Custom Dimension   Adding hit type as a Custom Dimension is fairly easy, because it\u0026rsquo;s a default field available in the model object that customTask automatically receives as a parameter when executed.\nSo, to start with, create a new Hit-scoped Custom Dimension in your Google Analytics Property Settings. Name it Hit type. Make note of the Index number assigned to the new dimension.\n  Then, in Google Tag Manager, create a new Custom JavaScript variable (or edit your existing customTask variable). Add the following code within:\nfunction() { var hitTypeIndex = 2; return function(model) { model.set(\u0026#39;dimension\u0026#39; + hitTypeIndex, model.get(\u0026#39;hitType\u0026#39;)); }; }  Change the hitTypeIndex variable value from 2 in the example to whatever the index number of your Custom Dimension is.\nFinally, add this variable to all your Google Analytics tags in More Settings / Fields To Set. It\u0026rsquo;s easiest if you just add it to your Google Analytics Settings variable (you are using one, right?). The field name should be customTask, and the value should be the Custom JavaScript variable you just created.\n  Now, every tag that has this field set will send its hit type automatically to Google Analytics as part of the hit payload.\nYou can verify this setup works by using the Google Analytics Debugger browser extension, and checking that the hit type is being sent with the correct Custom Dimension parameter (\u0026amp;cdX, where X is the index number).\n  If you can see that parameter, you\u0026rsquo;ll know everything should be working as intended.\nAnd what can you do with this data, you ask? Well, the most useful (my opinion) application is with segments. You can create Advanced Segments such as the one in the title image of this article to find specific sequences of different hit types, such as sessions where an event preceded a pageview hit.\nOr you can pull the data out using the Reporting API, and use the new Hit type Custom Dimension as an additional level of detail. This is particularly useful when combined with dimensions such as hit timestamp, session ID, and the variety of browsing behavior dimensions introduced in this article.\nLet me know in the comments if you can think of other uses for this data! Always happy to delegate actual analytics work to people wiser than me.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/delay-the-history-change-trigger/",
	"title": "#GTMTips: Delay The History Change Trigger",
	"tags": ["google tag manager", "gtmtips", "history", "spa"],
	"description": "Use a Custom HTML tag to delay the History Change trigger in order to accommodate for single-page frameworks that load the content after the history change event has been pushed.",
	"content": "When working with the analytics of single-page applications (SPA), there are a number of things to pay attention to. For example, you need to make sure that Google Analytics doesn\u0026rsquo;t break your session attribution, and that you are not inadvertently inflating your page speed timing metrics. Actually, there are so many \u0026ldquo;gotchas\u0026rdquo; when it comes to SPA tracking in tools like Google Analytics that you just can\u0026rsquo;t get by with a plug-and-play implementation.\nSee Dan Wilkerson\u0026rsquo;s excellent article, which sums nicely the can of worms involved with tracking an SPA.\nOne of the problems with frameworks that load their content dynamically using JavaScript (e.g. React) is when the history event is dispatched before the content has been loaded or the URL has actually changed.\nThis is problematic, since you might actually want to refer to elements loaded in this dynamic state change in your tags, but because the History Change trigger fires as soon as the event is dispatched, the content might not be there yet when your tags fire.\nSo in this article I\u0026rsquo;ll show a quick and dirty way to delay the trigger, so that your tags don\u0026rsquo;t fire until the page has had time to load the content.\nTip 79: Delay The History Change Trigger   The easy way to do it is to fire a Custom HTML tag with the History Change trigger that is causing problems on your site.\nFirst, make sure you have all the history-related Built-in variables enabled in the container.\nThen, in the Custom HTML tag, add the following code:\n\u0026lt;script\u0026gt; (function() { var timeout = 500; // Milliseconds - change to what you want the timeout to be.  window.setTimeout(function() { window.dataLayer.push({ event: \u0026#39;custom.historyChange\u0026#39;, custom: { historyChangeSource: {{History Source}}, newHistoryState: {{New History State}}, oldHistoryState: {{Old History State}}, newHistoryFragment: {{New History Fragment}}, oldHistoryFragment: {{Old History Fragment}} } }); }, timeout); })(); \u0026lt;/script\u0026gt; As said above, make sure this tag fires on the History Change trigger you want to delay.\nThe Custom HTML tag adds a 500 millisecond delay, after which it pushes a custom event named custom.historyChange into the dataLayer. With this event, five new Data Layer variables are pushed, each with the values from the original History event that triggered the Custom HTML tag in the first place. These new variables are:\n  custom.historyChangeSource - the history event that triggered the delay (e.g. pushState or replaceState).\n  custom.newHistoryState - the state object set in the history event.\n  custom.oldHistoryState - the state object that was overwritten with the new state object.\n  custom.newHistoryFragment - the hash fragment set in the history event (e.g. #contactus).\n  custom.oldHistoryFragment - the hash fragment that was overwritten with the new hash.\n  Now, create a new Custom Event trigger for event name custom.historyChange, and create Data Layer variables for all of the new custom variables listed above (or at least the ones that make sense in your case). Here\u0026rsquo;s what the trigger and a sample variable would look like:\n  Add the Custom Event trigger to whatever tag you want to fire with the history event, and use the new Data Layer variables where necessary.\nYou might be wondering why rewrite the original Built-in variables into custom Data Layer variables. This is just a precaution. If your site dispatches more than one history event before the 500 millisecond delay is over, then the Built-in variables will always refer to the latest history state, and not the one that was delayed.\nProblem with this approach There\u0026rsquo;s one, potentially big issue with this approach. By firing a Custom HTML tag with every single history event, you are clogging up the document object model of the page, because that gets unloaded when the user navigates away from the page, and not when new content is pulled in. For example, after ten history events, this is what the DOM can end up looking like:\n  The problem with rewriting the DOM so many times is that each new element added forces a re-evaluation of the document object model, and the more items in the DOM, the longer this re-evaluation takes. So the more Custom HTML tags that fire without a page reload or refresh, the more the page performance will suffer.\nPotential solution One potential solution is to use only a single Custom HTML tag, where you overwrite the window.history.pushState and window.history.replaceState methods. The overwritten code should include the delayed dataLayer.push(), and you must pass the arguments to the original window.history.pushState using apply(), unless you want to destroy your site navigation. See here for inspiration.\nBasically, you\u0026rsquo;re writing a custom event listener, but instead of listening for user interactions, you\u0026rsquo;ll be listening for pushState and replaceState events.\nI\u0026rsquo;m not going to write the solution here - it\u0026rsquo;s difficult to write a generic history listener that works with your particular use case. But the Stack Overflow article should help you get started. Just remember to test thoroughly if overwriting basic browser interfaces like the Window History API.\nSummary The trick outlined in this article should really be a temporary solution (as all hacks should). You really ought to talk to your developers and ask them to implement the custom event push directly into whatever JavaScript is handling the route / state changes on the site.\nBy delegating the work to your developers, they can ensure that the dataLayer event is dispatched at the exact right time. Using a delay like the 500 milliseconds of this article is sub-optimal for two reasons:\n  It can be too short, meaning the delay goes off, the custom event is pushed, but the transition is still not complete.\n  It can be too long, meaning you are wasting time with the trigger, and tracking the page view of each dynamic transition can lag behind the events dispatched with each new bit of content.\n  Regardless, it\u0026rsquo;s a way to make things work, which I guess is a good justification as any for using hacks.\n"
},
{
	"uri": "https://www.simoahava.com/tags/history/",
	"title": "history",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/spa/",
	"title": "spa",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/navigation/",
	"title": "navigation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-browsing-behavior-in-google-analytics/",
	"title": "Track Browsing Behavior In Google Analytics",
	"tags": ["google tag manager", "custom dimensions", "google analytics", "navigation"],
	"description": "Add Custom Dimensions for browsing behavior (tabbed browsing, how the user navigated to the current page) in Google Analytics. This guide is for Google Tag Manager users mainly, but can be adapted for other implementation methods, too.",
	"content": " Last updated 22 May 2019: Added expiration of currently open tabs.\n In this article, Jethro Nederhof of Snowflake Analytics fame and I will introduce you to some pretty neat web browser APIs. The purpose of these APIs is to find out more about how the user navigated to the current page, and what\u0026rsquo;s going on with their browser tabs.\n  There are so many things you can do with this new information. You can build proper navigational path reports, rather than rely on the fuzzy and often incoherent flow reports in Google Analytics. You can identify how visitors interact with your content using browser tabs - a crucial bit of information if you want to make heads or tails of Google Analytics\u0026rsquo; time on page metrics, for example. You can see how many redirects were involved in the current navigation action.\nThe origins of this article are in a number of places, including an impromptu challenge thrown by Yehoshua Coren in Twitter (with great contributions from Marek Lecián, too):\nNew #GTM challenge for @SimoAhava . Can you track how many browser tabs I have open on your site right now? ;)\n\u0026mdash; Analytics Ninja (@AnalyticsNinja) April 12, 2018  However, the main source of inspiration came from Measure Slack, where a similar discussion around tabbed browser was being had:\n  Jethro\u0026rsquo;s idea got the ball rolling in my mind, so I contacted him immediately and asked if he wanted to co-author this article. As it turns out, he\u0026rsquo;d been mulling around lots of different ideas for navigation behavior tracking, so we quickly sketched the outline of this article, and what you are reading now is the final product.\nI consider the enhancements introduced in this article just as necessary as those I introduced way back when in my article Improve Data Collection With Four Custom Dimensions. Once you\u0026rsquo;re done implementing these scripts, I\u0026rsquo;m sure you\u0026rsquo;ll agree.\nAs said, this article is co-authored by Jethro and me. Since I am the editor, all mistakes and errors are entirely my fault, and any snarky comments should be directed at me and not my generous partner in crime (naturally, all snarky comments will be deleted, but I will read them and promise to be insulted).\nYou can skip directly to the Solutions chapter, but I do recommend reading the Theory first. At the very least, make sure you read the Caveats chapter, because there are a couple of gotchas you need to be aware of if implementing this solution.\nAh, damn it. Just read the whole thing, will you?\n1. The why and how (theory) The web, by default, is stateless. Your browsing behavior is confined to the current page only, and it\u0026rsquo;s difficult to programmatically peer into the past (never mind into the future).\nTools like Google Analytics try to make sense of this stateless mess by collecting information in a chronological order. You send a pageview, then you send another pageview. Google Analytics interprets this as navigational behavior where you first viewed the first page and then the second page. Makes sense, right?\nBut what about if you opened the second page in a tab and never even glanced at it? GA still interprets it as a page being viewed.\nWhat about if you reloaded the current page? GA still interprets it as a navigational step.\nWhat about if you pressed the back button of the browser to return to the previous page? Google Analytics makes no distinction - it doesn\u0026rsquo;t know if you clicked a link, typed the URL in the address bar, or used the back button. It\u0026rsquo;s all the same.\n  The flow reports try to make sense of this mess by aligning everything into a navigational pattern (the \u0026ldquo;flow\u0026rdquo;). But the reports are quite difficult to interpret, as they sometimes show sequences that you are certain shouldn\u0026rsquo;t be possible, lack necessary detail, and they don\u0026rsquo;t really give you a robust way to query the information or to build a proper flow report yourself.\nAnd tabbed browsing is still a problem. Each tab initiates a new \u0026ldquo;session\u0026rdquo; in the browser (not to be confused with a Google Analytics session). Thus when GA tries to explain to you that everything is part of a linear navigational flow, the truth is actually more complex: each tab initiates a new branch of navigation!\n  Add to this the fact that in Google Analytics, \u0026ldquo;Referral\u0026rdquo; is a reserved term for campaign attribution. Thus when you are navigating around the site, the referring page is not sent when navigating in-site. This would be really helpful when deciphering navigational paths, since the Previous Page Path dimension does not necessarily represent the previous page viewed (just the previous page for which a pageview was sent). To make some sense out of the mess, sending the referral string in a Custom Dimension is a good practice.\n  There are other, more granular tracking solutions out there, of course. Snowplow, for example, lets you track hit-level data in any way you like. You can include referral information, if you wish. However, since you\u0026rsquo;re approaching Snowplow with the limitations of SQL queries, as soon as the navigation journey starts looping or has multiple pages, the joins you\u0026rsquo;d need to make become frustratingly complex and, as a result, quite unwieldy over large amounts of data.\nSo, we\u0026rsquo;ve had this problem of analytics tools imposing a linear sequence on a non-linear navigational pattern for a couple of decades, ever since tabbed browsing was introduced around the turn of the millennium.\nWhat we\u0026rsquo;re proposing in this article is to add another layer to the taxonomy of Google Analytics\u0026rsquo; rather rigid schema for browsing behavior.\nFROM: Users \u0026gt; Sessions \u0026gt; Hits\nTO: Users \u0026gt; Sessions \u0026gt; Tabs \u0026gt; Hits\nThis browsing behavior would be encoded directly into Custom Dimensions, so you don\u0026rsquo;t have to infer behavior from GA\u0026rsquo;s pageview model - rather, you can query the navigational information directly!\n1.1. The PerformanceNavigation API The API we\u0026rsquo;re going to use to identify browsing behavior is called PerformanceNavigation. It comes with two read-only properties: performance.navigation.type and performance.navigation.redirectCount.\nThe origin story of this API is firmly rooted in the difficulty of measuring performance of any given web page load. Redirects, network lag, and cached resources (locally, server-side, in CDNs) all contribute to the complexity of what the aggregate performance of any given page load is.\n  To help understand this complexity, web browsers implement the PerformanceNavigation API. The two properties exposed by the API can be found under the global window.performance.navigation interface, and they contain the following information:\n   Parameter Detail Description     redirectCount - The number of 3XX HTTP redirects the browser went through when loading the page.   type - An integer representing how the page was loaded.     0 Normal navigation, typing the URL, clicking a link, opening a bookmark, entering via an app link, etc.     1 Page was refreshed / reloaded while already open in the browser tab.     2 The page was retrieved from the browser history by using the Back or Forward button.     255 Any other way to navigate to the page.    The 255 value is interesting, because it is quite undocumented. In tests, it seems to only emerge with the Firefox browser, when the page goes through a client-side refresh, either using window.location.replace() or \u0026lt;meta name=\u0026quot;refresh\u0026quot;\u0026gt;. Note that this goes against the W3C specification, which clearly states that client-side redirects should be contained within one of the regular navigation types.\nNow, even though we mention how this API was probably conceived to give more information about the performance timing metrics, where it really shines is uncovering the navigational paths the users take through your site.\n1.2. Browser storage Ever since browser tabs were introduced to the delight of site visitors and web browser users, they\u0026rsquo;ve been a source of annoyance for web analysts.\nIn Google Analytics, for example, we simply don\u0026rsquo;t know (by default) what the tab situation is of any given page. We don\u0026rsquo;t know if the page was opened in a new tab, for one, since you can\u0026rsquo;t attach listeners to the right-click context menu, and tracking just middle mouse button clicks doesn\u0026rsquo;t give a comprehensive idea of the scope of the phenomenon.\n  The beauty of web analytics is that when we can\u0026rsquo;t track things directly (whether a user submitted a form, for example), we can track them indirectly (page load of a \u0026ldquo;thank you\u0026rdquo; page). So maybe we can use this same indirect approach with browser tabs, too?\nIf we can assign an identifier to every tab the user opens in the current website, we can identify individual navigation sequences! This can be achieved by checking against a common data store whether the given tab ID exists already, in which case the page load happened in a tab that was already open, and vice versa.\nHowever, thanks again to the statelessness of the web, there is the problem of persistence. We can\u0026rsquo;t simply use a global JavaScript property to store the tab ID, because that gets demolished when the user unloads the page by reloading or navigating to another page.\nCookies and localStorage are \u0026ldquo;too\u0026rdquo; persistent, because they are only reset when they are manually cleared (localStorage) or when they expire (cookies). We only want to store the ID of a tab for as long as that tab exists.\n  Enter sessionStorage! It\u0026rsquo;s a browser storage API that is unique to each tab in your web browser. By storing the tab ID in sessionStorage, we can always check if the tab already has an ID (existing tab), or if we need to generate a new one (new tab).\nCombine this with some localStorage logic, where we keep a running tally of how many tab IDs have been generated, remembering to remove any tab ID once that tab is closed, and we can also get a fairly reliable count of tabs open on your website at any given time.\nSo, now we are getting close to having all the bits and pieces at hand. Once we combine the tabbed browsing metadata with that provided by the PerformanceNavigation API, we can get a nice set of Custom Dimensions that expose a great deal of interesting information about how your visitors navigate your site.\n2. Caveats Oh, there are plenty of caveats. The solutions below rely on a number of fragile components, which can get easily messed up due to how browsers act differently in certain circumstances.\n2.1. Cross-domain browsing If your tracking crosses multiple domains, then each domain will get their own tab ID. That\u0026rsquo;s because sessionStorage and localStorage are confined to the current domain only. And if your current domain sails between HTTP and HTTPS protocols, then those will get their own storages, too. You could fix this by using URL parameters on the domain boundaries to pass the metadata from one domain to the next, but this is something you\u0026rsquo;ll need to figure out by yourself.\n2.2. Firefox and Chrome go rogue An additional point of concern is that Firefox and Chrome do not respect the specification that session cookies and sessionStorage should only exist for as long as the tab is alive. Once the browser is closed, these session stores should be purged.\nOn Chrome and Firefox, however, if you open the browser and you have the Show your windows and tabs from last time (Firefox) or Continue where you left off (Chrome) settings turned on, the session stores are restored, too! So even though the tab did close, by restoring the tab it\u0026rsquo;s as if the tab was never closed.\n  That sucks. Not much we can do about that.\nAlso, and this is interesting, when you do restore tabs or continue where you left off, the browser claims that the navigation type was BACK/FORWARD, meaning it equates restoring tabs with using the back or forward button of the browser.\nThis sucks, too. And there\u0026rsquo;s not much we can do about this, either.\n2.3. Firefox and its client-side refresh dilemma Then there\u0026rsquo;s the problem with Firefox implementing OTHER as the navigation type if a client-side refresh is done. This is unfortunate, but doesn\u0026rsquo;t luckily mess things up too bad, because this behavior is contained within this particular navigation type alone.\nIn addition to all of these, because the whole granularity of distinguishing between BACK and FORWARD needs to be coded using a programmatically maintained path of pages the user has visited, all it takes is for them to manually clear the sessionStorage to break the whole setup (at least, for as long as the tab is open).\n  That\u0026rsquo;s why in the beginning of the solution you have the option of NOT tracking the back and forward button presses with detail. In this case, the string BACK/FORWARD is sent to Google Analytics in case either button is pressed. It\u0026rsquo;s not as detailed but it does still tell you whether the user navigated using these buttons (or, as mentioned above, if the tabs were restored after restarting the browser).\nOur hypothesis is that with more data the averages will converge towards a more reliable dataset. So idiosyncrasies such as tabs being restored or users clearing storage will disappear into the data as more and more page views are accumulated.\n2.4. Browser support There\u0026rsquo;s also the matter of browser support, but unlike some online resources claim, support for PerformanceNavigation is pretty well supported in Chrome and Safari.\n2.5. Single-page apps You could implement this solution for single-page apps, but in that case you should add an additional, custom navigation type called VIRTUAL, or something similar, if the navigation was a single-page transition and not a proper page load.\nDo note that window.performance.navigation.type does not update when the user uses the browser\u0026rsquo;s Back / Forward buttons and the transition is from one single-page app state to another. So you\u0026rsquo;d need to code the logic for Back / Forward use when no page load is recorded.\n3. Solutions OK, let\u0026rsquo;s get to the good stuff! In this chapter, we\u0026rsquo;ll introduce the technical solution in Google Tag Manager (Custom HTML tag) that orchestrates the whole thing. In addition to that, each sub-section will detail how to send the respective piece of information to Google Analytics as a Custom Dimension. Basically, all you\u0026rsquo;ll need to edit is your main Page View tag, since that\u0026rsquo;s the only one that will be sending this data to Google Analytics.\n3.1. The Custom HTML tag First, create a new Custom HTML tag, and add the following code within.\n\u0026lt;script\u0026gt; (function() { // Set to false if you only want to register \u0026#34;BACK/FORWARD\u0026#34;  // if either button was pressed.  var detailedBackForward = true; // Set expiration of tab count in milliseconds. The recommended default is  // 72 hours (259200000 ms). Set to 0 if you don\u0026#39;t want to expire the tab count.  var expireTabs = 259200000; if (!!window.Storage) { var openTabs = JSON.parse(localStorage.getItem(\u0026#39;_tab_ids\u0026#39;)) || [], tabId = sessionStorage.getItem(\u0026#39;_tab_id\u0026#39;), navPath = JSON.parse(sessionStorage.getItem(\u0026#39;_nav_path\u0026#39;)), curPage = document.location.href, newTab = false, origin\t= document.location.origin; var tabCount, redirectCount, navigationType, prevInStack, lastInStack, payload, expiration, newTabId; var clearExpired = function(tabs) { if (expireTabs === 0) { return tabs; } return tabs.filter(function(tab) { try { expiration = parseInt(tab.split(\u0026#39;_\u0026#39;)[1], 10); return expiration \u0026gt; (new Date().getTime()); } catch(e) { return false; } }); }; var updateTabExpiration = function(tabId) { if (expireTabs === 0) { return tabId; } try { newTabId = tabId.split(\u0026#39;_\u0026#39;); expiration = parseInt(newTabId[1], 10); if (expiration \u0026gt; new Date().getTime()) { return tabId; } else { newTabId = newTabId[0] + \u0026#39;_\u0026#39; + (new Date().getTime() + expireTabs); sessionStorage.setItem(\u0026#39;_tab_id\u0026#39;, newTabId); return newTabId; } } catch(e) { return tabId; } }; var getBackForwardNavigation = function() { if (detailedBackForward === false) { return \u0026#39;BACK/FORWARD\u0026#39;; } if (navPath.length \u0026lt; 2) { return \u0026#39;FORWARD\u0026#39;; } prevInStack = navPath[navPath.length-2]; lastInStack = navPath[navPath.length-1]; if (prevInStack === curPage || lastInStack === curPage) { return \u0026#39;BACK\u0026#39;; } else { return \u0026#39;FORWARD\u0026#39;; } }; var removeTabOnUnload = function() { var index; // Get the most recent values from storage  openTabs = JSON.parse(localStorage.getItem(\u0026#39;_tab_ids\u0026#39;)) || []; tabId = sessionStorage.getItem(\u0026#39;_tab_id\u0026#39;); openTabs = clearExpired(openTabs); if (openTabs.length \u0026amp;\u0026amp; tabId !== null) { index = openTabs.indexOf(tabId); if (index \u0026gt; -1) { openTabs.splice(index, 1); } localStorage.setItem(\u0026#39;_tab_ids\u0026#39;, JSON.stringify(openTabs)); } }; var generateTabId = function() { // From https://stackoverflow.com/a/8809472/2367037  var d = new Date().getTime(); if (typeof performance !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; typeof performance.now === \u0026#39;function\u0026#39;){ d += performance.now(); //use high-precision timer if available  } return \u0026#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\u0026#39;.replace(/[xy]/g, function (c) { var r = (d + Math.random() * 16) % 16 | 0; d = Math.floor(d / 16); return (c === \u0026#39;x\u0026#39; ? r : (r \u0026amp; 0x3 | 0x8)).toString(16); }) + (expireTabs \u0026gt; 0 ? \u0026#39;_\u0026#39; + (new Date().getTime() + expireTabs) : \u0026#39;\u0026#39;); }; var validNavigation = function(type, newTab) { // Return false if new tab and any other navigation type than  // NAVIGATE or OTHER. Otherwise return true.  return !(newTab === true \u0026amp;\u0026amp; (type !== 0 \u0026amp;\u0026amp; type !== 255)); }; if (tabId === null) { tabId = generateTabId(); newTab = true; sessionStorage.setItem(\u0026#39;_tab_id\u0026#39;, tabId); } else { tabId = updateTabExpiration(tabId); } openTabs = clearExpired(openTabs); if (openTabs.indexOf(tabId) === -1) { openTabs.push(tabId); localStorage.setItem(\u0026#39;_tab_ids\u0026#39;, JSON.stringify(openTabs)); } tabCount = openTabs.length; if (!!window.PerformanceNavigation) { navPath = navPath || []; redirectCount = window.performance.navigation.redirectCount; // Only track new tabs if type is NAVIGATE or OTHER  if (validNavigation(window.performance.navigation.type, newTab)) { switch (window.performance.navigation.type) { case 0: navigationType = \u0026#39;NAVIGATE\u0026#39;; navPath.push(curPage); break; case 1: navigationType = \u0026#39;RELOAD\u0026#39;; if (navPath.length === 0 || navPath[navPath.length-1] !== curPage) { navPath.push(curPage); } break; case 2: navigationType = getBackForwardNavigation(); if (navigationType === \u0026#39;FORWARD\u0026#39;) { // Only add to navigation if not coming from external domain  if (document.referrer.indexOf(origin) \u0026gt; -1) { navPath.push(curPage); } } else if (navigationType === \u0026#39;BACK\u0026#39;) { // Only clear from navigation if not returning from external domain  if (lastInStack !== curPage) { navPath.pop(); } } else { navPath.push(curPage); } break; default: navigationType = \u0026#39;OTHER\u0026#39;; navPath.push(curPage); } } else { navPath.push(curPage); } sessionStorage.setItem(\u0026#39;_nav_path\u0026#39;, JSON.stringify(navPath)); } window.addEventListener(\u0026#39;beforeunload\u0026#39;, removeTabOnUnload); payload = { tabCount: tabCount, redirectCount: redirectCount, navigationType: navigationType, newTab: newTab === true ? \u0026#39;New\u0026#39; : \u0026#39;Existing\u0026#39;, tabId: tabId.replace(/_.+/, \u0026#39;\u0026#39;) }; // Set the data model keys directly so they can be used in the Page View tag  window.google_tag_manager[{{Container ID}}].dataLayer.set(\u0026#39;browsingBehavior\u0026#39;, payload); // Also push to dataLayer  window.dataLayer.push({ event: \u0026#39;custom.navigation\u0026#39;, browsingBehavior: payload }); } })(); \u0026lt;/script\u0026gt;  Do not add any triggers to this tag. Instead, open your Page View tag, and add this Custom HTML tag into its tag sequence by making the Custom HTML tag fire before the Page View tag.\nAccessing GTM\u0026rsquo;s data model directly with dataLayer.set is necessary if you want to modify Data Layer keys within a tag sequence. However, for the sake of transparency, we\u0026rsquo;ll also push a dataLayer object with a custom event (custom.navigation) and the browsing behavior payload.\n  The code in the Custom HTML tag does a number of things.\n  It generates a tab ID for the page, which is stored in sessionStorage. If a new tab ID is thus generated, the page is marked as being in a new browser tab. If a tab ID was already in storage, then the page is flagged as being in an existing tab.\n  This tab ID is also stored in an array within localStorage, where all tab IDs generated on the site are stored. The length of this array is the count of currently open tabs on the domain.\n  If the user leaves the page, or closes the browser tab, or closes the browser, the tab ID is removed from the array in localStorage (thus keeping the count of open tabs accurate).\n  If the page was loaded by navigating to the URL, navigation type is set to NAVIGATE, and the current page is pushed into an array representing the navigation path, stored in sessionStorage.\n  If the page was reloaded, navigation type is set to RELOAD, and the navigation path is kept as it is.\n  If the Back or Forward button was pressed, the script checks if the current page is the penultimate page in the navigation path, in which case navigation type is set to BACK. In other cases, navigation type is set to FORWARD.\n  Count of tabs, count of redirects, navigation type, tab ID, and whether the tab was new or existing are all added to Google Tag Manager\u0026rsquo;s data model, so that the Page View tag can grab these values with Data Layer variables.\n   UPDATE 22 May 2019: I added some extra code to handle expiration. By default, the expiration is 72 hours, so if the storage isn\u0026rsquo;t interacted with in 72 hours, the current count of open tabs is cleared.\n There are some nuances to the steps listed above, mainly to account for edge cases, such as when navigating BACK or FORWARD from an external domain. Even with these precautions, there are situations where the solution will fail (see the Caveats chapter).\nAnd now that we are adding all this information into Data Layer, it\u0026rsquo;s time to pick up the metadata and send it to Google Analytics!\n3.2. Create the Custom Dimensions in Google Analyitcs First, you\u0026rsquo;ll need to create some Custom Dimensions in Google Analytics.\nCreate the following Custom Dimensions in Google Analytics\u0026rsquo; Property Settings, all in hit scope, and make note of the index numbers assigned to them.\n Redirect count Navigation type Tab type Tabs open Tab ID    3.3. Create the Data Layer variables in Google Tag Manager Next, create the corresponding Data Layer variables in Google Tag Manager. Here\u0026rsquo;s my setup:\n   Variable name Value for Data Layer Variable name field     {{DLV - browsingBehavior.redirectCount}} browsingBehavior.redirectCount   {{DLV - browsingBehavior.navigationType}} browsingBehavior.navigationType   {{DLV - browsingBehavior.newTab}} browsingBehavior.newTab   {{DLV - browsingBehavior.tabCount}} browsingBehavior.tabCount   {{DLV - browsingBehavior.tabId }} browsingBehavior.tabId    Here\u0026rsquo;s an example of what one of the variables would look like:\n  3.4. Add the Data Layer variables to your Page View tag Now, open the Page View tag which you have already edited for the Tag Sequence stuff in the beginning of this chapter. Either in a Google Analytics Settings variable or by directly editing the tag fields, add the Data Layer variables to their respective indices in the Custom Dimensions list. This is what my setup looks like:\n  3.5. Test it! Now, save the tag, go to Preview mode, and enter your site.\nNote! You can\u0026rsquo;t really use Preview mode to test if the Data Layer variables are populating correctly. Because you are updating the data model in a tag sequence, the method used does not expose the dynamic changes to the Preview mode user interface.\nYou\u0026rsquo;ll need to either look at the Network requests directly, or use a tool such as Google Tag Assistant recordings or Google Analytics Debugger to check if the data is being sent correctly.\nIn any case, if everything is working, then with your Page View tag you should see the Custom Dimensions being populated with the relevant information.\n  3.6. Custom Report in Google Analytics A handy way to pull it all together is to create a Custom Report in Google Analytics with these settings, for example:\n  This report will contain interesting information about how users navigated to the different pages on your site.\n  Couple this with things like Session ID and Hit Timestamp, and you can really start digging into how users move from one page to the other on your site.\n3.7. Other data stores If you\u0026rsquo;ve taken the time to duplicate your Google Analytics data to Snowplow, you\u0026rsquo;ll naturally have access to a far more granular, raw hit stream resource for querying against. For example, here\u0026rsquo;s a simple SQL query output with all the relevant dimensions included:\n  Do note that Snowplow doesn\u0026rsquo;t differentiate between different scopes of Custom Dimensions, since scope is a concept applied in Google Analytics processing. So if you are sending an identifier into a session-scoped Custom Dimension (e.g. Session ID), the hit stream to Snowplow will interpret this as hit-scoped data, and thus the identifier is pretty useless.\n4. Summary First of all, I\u0026rsquo;m hugely indebted to Jethro Nederhof for agreeing to draft this solution with me. I really love the community of analytics developers - seems like everyone is giddy with excitement when figuring out new solutions to age-old questions, and the amount of knowledge being shared across blogs, Slack channels, and social media is a testament to the selflessness of these good men and women.\nIn my humble opinion, anyone interested in proper page navigation analysis should try out this solution. Understanding things like browser tab usage and Back / Forward browsing can help you figure out where the information architecture blind spots of your site are or whether you need to fix the navigation options you offer your visitors, for example.\nBut, as always on this particular blog, this is a technical solution first and foremost. Jethro and I wanted to highlight some cool tricks you can do with the web browser, and we fully expect others to refine these methods even further.\nIt would be cool if the APIs were developed even further, such as by automatically distinguishing between Back and Forward of the browser. Right now, they\u0026rsquo;re bunched together which is why we need the workaround of managing the navigation path in sessionStorage, and that\u0026rsquo;s a fragile solution indeed.\nLet us know in the comments what you think of this trick, and whether you have suggestions on how to improve it!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/notify-page-google-tag-manager-loaded/",
	"title": "#GTMTips: Notify The Page That Google Tag Manager Has Loaded",
	"tags": ["google tag manager", "gtmtips", "container"],
	"description": "Modify the Google Tag Manager container snippet to notify the page that the GTM container library has loaded.",
	"content": "Here\u0026rsquo;s a quick tip in response to a query in Twitter by Riccardo Mares. By making a small change to the Google Tag Manager container snippet, you can have the \u0026lt;script\u0026gt; element generated by the snippet notify the page as soon as the Google Tag Manager library has downloaded.\nHi @SimoAhava is possible in javascript (from the page) to check (or hook) when GTM has been loaded? Thanks.\n\u0026mdash; Merlinox 🏃🏻🇮🇹 (@merlinox) April 13, 2018  What you do with this information is up to you. If you are working directly with the google_tag_manager interface, for example, it might make sense to not act until the interface has been established.\nTip 78: Notify the page when the GTM container has loaded   You need to add the following code into the container snippet, immediately after the +i+dl; and immediately before the f.parentNode.insertBefore.\nj.addEventListener(\u0026#39;load\u0026#39;, function() { var _ge = new CustomEvent(\u0026#39;gtm_loaded\u0026#39;, { bubbles: true }); d.dispatchEvent(_ge); });  Thus the modified container snippet would look like this:\n(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;https://www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl; j.addEventListener(\u0026#39;load\u0026#39;, function() { var _ge = new CustomEvent(\u0026#39;gtm_loaded\u0026#39;, { bubbles: true }); d.dispatchEvent(_ge); }); f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-XXXXX\u0026#39;);  Now, when the Google Tag Manager container has been downloaded from Google\u0026rsquo;s servers, the load listener will dispatch a new browser event named gtm_loaded.\nIf you want to build a hook for this event, you can use the addEventListener() method, again:\nif (!!window.google_tag_manager) { // Google Tag Manager has already been loaded  doSomethingWith(window.google_tag_manager); } else { window.addEventListener(\u0026#39;gtm_loaded\u0026#39;, function() { // Google Tag Manager has been loaded  doSomethingWith(window.google_tag_manager); }); }  In this example, the code first checks if GTM has already been loaded. If it hasn\u0026rsquo;t, it creates a new listener on the window object that waits for the gtm_loaded event to bubble up to it. As soon as the Google Tag Manager library has been downloaded and initialized, the event is dispatched and the listener for gtm_loaded will go off.\n"
},
{
	"uri": "https://www.simoahava.com/tags/container/",
	"title": "container",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/sort-custom-dimensions-by-index-number/",
	"title": "#GTMTips: Sort Custom Dimensions By Index Number",
	"tags": ["google tag manager", "gtmtips", "hack"],
	"description": "With a little JavaScript snippet executed in the browser&#39;s JavaScript console, you can reorder the Custom Dimension fields in your Google Analytics tags within GTM by their index numbers, in ascending order.",
	"content": "Here\u0026rsquo;s a hacky #GTMTips tip for you. Have you ever had a Google Tag Manager container, where you\u0026rsquo;ve been updating your Google Analytics tags over the years? And perhaps these tags (and, today, Google Analytics Settings variables) have been updated with an ever-expanding list of Custom Dimensions? And perhaps this list of Custom Dimensions is sorted willy-nilly, because once you have 50+ rows, it just doesn\u0026rsquo;t seem like a fun thing to do to go over each row and update them so that they are sorted by Custom Dimension index?\nNo worries, then! I have a solution for you. It involves a little JavaScript snippet you need to execute in the web browser\u0026rsquo;s JavaScript console, and it will sort the Custom Dimension fields for you!\nTip 77: Sort Custom Dimensions by index number   Here\u0026rsquo;s what you need to do.\n  Browse to your Google Tag Manager container, and open the Google Analytics tag or Google Analytics Settings variable you want to modify.\n  Make sure the tag or variable is in edit mode, meaning you can see each field as a text field that you can edit.\n  Open the JavaScript console of your browser.\n  Copy-paste the following snippet in the JavaScript console, and press enter to run it.\n  Save the tag or variable.\n  Here\u0026rsquo;s the snippet, make sure you copy all of it (triple-clicking it should do the job):\nvar el=document.querySelector(\u0026#39;[diff-field$=\u0026#34;customDimensionSection\u0026#34;]\u0026#39;);var rows=el.querySelectorAll(\u0026#34;.simple-table-row[data-ng-repeat]\u0026#34;);var newRows=[];rows.forEach(function(row){var inputIdx=row.querySelectorAll(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;)[0];var inputVal=row.querySelectorAll(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;)[1];newRows.push({idx:inputIdx.value,val:inputVal.value})});newRows.sort(function(a,b){if(parseInt(a.idx)\u0026gt;parseInt(b.idx)){return 1}if(parseInt(a.idx)\u0026lt;parseInt(b.idx)){return-1}return 0});rows.forEach(function(row,i){var inputIdx=row.querySelectorAll(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;)[0];var inputVal=row.querySelectorAll(\u0026#39;input[type=\u0026#34;text\u0026#34;]\u0026#39;)[1];inputIdx.value=newRows[i].idx;inputVal.value=newRows[i].val;inputIdx.dispatchEvent(new Event(\u0026#34;change\u0026#34;));inputVal.dispatchEvent(new Event(\u0026#34;change\u0026#34;))});  Just to clarify, being in edit mode should look like this:\n  And you can open the JavaScript console of you browser handily with a shortcut key combination:\n Chrome  Win: Ctrl + Shift + J Mac: Cmd + Opt + J   Firefox  Win: Ctrl + Shift + K Mac: Cmd + Opt + K   Edge  Win: Ctrl + Shift + J   Internet Explorer  Win: F12, then click on the \u0026ldquo;Console\u0026rdquo; tab   Safari  Mac: Cmd + Opt + C, you need to enable the \u0026ldquo;Show Develop menu in menu bar\u0026rdquo; setting in the Advanced pane of Safari\u0026rsquo;s preferences    The snippet basically rewrites each field in the list of Custom Dimensions so that the rows are in ascending order, sorted by index number of each Custom Dimension.\nIf the snippet stops working, please let me know in the comments below and I will update it.\nNaturally, I am anxiously waiting for sorting of fields to be a built-in feature of the Google Tag Manager UI. But until then, we can hack our way around this limitation.\n"
},
{
	"uri": "https://www.simoahava.com/tags/hack/",
	"title": "hack",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/send-google-analytics-payload-length-as-custom-dimension/",
	"title": "Send Google Analytics Payload Length As Custom Dimension",
	"tags": ["google tag manager", "google analytics", "customtask"],
	"description": "Send the Google Analytics request payload length as a Custom Dimension of the hit. This way you can monitor if you are approaching the maximum size of the payload.",
	"content": "Maybe you knew this, maybe you didn\u0026rsquo;t, but requests sent from your website (or app) to Google Analytics have a maximum size. Or, more specifically, the payload size (meaning the actual content body of the request) has a maximum.\nThis maximum size of the payload is 8192 bytes. This means, basically, that the entire parameter string sent to Google Analytics servers can be no longer than 8192 characters in length. The thing is, if the payload exceeds this, Google Analytics simply drops the hit. There\u0026rsquo;s no warning, no error, nothing. The hit just doesn\u0026rsquo;t get sent. If you are running the Google Analytics debugger browser extension, you can actually see a warning when the payload size is exceeded:\n  If you see this warning, it means that the hit was aborted due to exceeding the 8192 length of the payload.\n Note that if you are using Measurement Protocol to directly send data to Google Analytics, this size limitation applies to the POST request body. If you are sending the data with a GET request, the maximum size of the entire /collect URL is 8000 bytes.\n Anyway, in this article I\u0026rsquo;ll show you how to send the payload size (or at least a very close approximation thereof) as a Custom Dimension to Google Analytics with each hit. That way you\u0026rsquo;ll be able to check if you are approaching this limitation, and thus you can take precautions to avoid exceeding the maximum size. We\u0026rsquo;ll get things done with customTask (what else?) and Google Tag Manager.\n UPDATE: Check out Angela Grammatas\u0026rsquo; excellent article on the very same topic. She uses a slightly different tactic, appending the data in the buildHitTask, which works just as well.\n IMPORTANT REQUEST Before I get started, I have a request to make. Some time ago, I was contacted by someone (Googler, I think), who shared with me a similar solution. For the life of me, I can\u0026rsquo;t find this communication anywhere, because I don\u0026rsquo;t even remember what medium I was contacted over (I\u0026rsquo;ve gone through my mailbox to no avail).\nThis solution was definitely inspired by this person\u0026rsquo;s idea, so I want to give credit where credit is due. So, if you remember approaching me with a solution you used to work on that did something similar, please be in touch and I will update this article with my thanks for your inspirational example.\nSend the hit payload length using customTask If you don\u0026rsquo;t know what customTask is, please check out my guide on the topic. In a nutshell, customTask lets you modify, among other things, the request sent to Google Analytics before it is sent, adding information dynamically to the payload. This information could be anything from a PII-purged payload to the Client ID.\ncustomTask works with a Custom JavaScript variable. To send the hit payload length as a Custom Dimension, you\u0026rsquo;ll first need to create a new hit-scoped Custom Dimension in Google Analytics admin. Once you\u0026rsquo;ve created it, make note of the index assigned to the new dimension.\n  Then, in Google Tag Manager, create a new Custom JavaScript variable. Name it something descriptive, e.g. customTask - hit payload length. This is what you should add within:\nfunction() { // Change this index to match that of the Custom Dimension you created in GA  var customDimensionIndex = 10; return function(model) { var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; var originalSendHitTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { try { var originalHitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var customDimensionParameter = \u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex; // If hitPayload already has that Custom Dimension, note this in the console and do not overwrite the existing dimension  if (hitPayload.indexOf(customDimensionParameter + \u0026#39;=\u0026#39;) \u0026gt; -1) { console.log(\u0026#39;Google Analytics error: tried to send hit payload length in an already assigned Custom Dimension\u0026#39;); originalSendHitTask(sendModel); } else { // Otherwise add the Custom Dimension to the string  // together with the complete length of the payload  hitPayload += customDimensionParameter + \u0026#39;=\u0026#39;; hitPayload += (hitPayload.length + hitPayload.length.toString().length); sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload, true); originalSendHitTask(sendModel); } } catch(e) { console.error(\u0026#39;Error sending hit payload length to Google Analytics\u0026#39;); sendModel.set(\u0026#39;hitPayload\u0026#39;, originalHitPayload, true); originalSendHitTask(sendModel); } }); }; }  To configure this customTask, you just need to update the customDimensionIndex variable with the index number of your Custom Dimension (in my example, the index is 10).\nThis function interrupts sendHitTask, which is the task used to actually send the request to Google Analytics. The payload sent to Google Analytics is appended with the Custom Dimension you have created, and the value of that Custom Dimension is set to the entire length of the payload. Thus, when a tag that uses this customTask fires, the request to Google Analytics will contain the length of the payload as the value of the Custom Dimension you assigned for it.\nTo add it to your tag(s), either use a Google Analytics Settings variable or override the settings of any tag. Scroll down to More Settings \u0026gt; Fields to set, and add a new field.\nField name: customTask\nValue: {{customTask - hit payload length}}\n  Now any tag that has this field set will add the hit payload length as a Custom Dimension to the request.\nYou can verify this by opening the Network tab of your browser\u0026rsquo;s developer tools. The tag is represented by a request to /collect, and by clicking this request you can see that the Custom Dimension parameter is included with the length of the payload:\n  Other ideas You could actually rewrite the customTask to add some further logic if you are nearing the 8192 byte maximum size. For example, it doesn\u0026rsquo;t really help you to add the payload length as a Custom Dimension, if the payload is never sent due to exceeding the size requirement.\nBy adding some custom code, you could do things like:\n  When payload length is at or over 8192 bytes, drop unnecessary fields from the payload until the length is under the limit.\n  When payload length is at or over 8192 bytes, send a new hit to Google Analytics which contains some key information from the \u0026ldquo;broken\u0026rdquo; payload, just so that you\u0026rsquo;ll get an idea of the scope of the problem.\n  A typical reason for exceeding payload length is if you are sending Enhanced Ecommerce impression data, or just very large shopping carts. It only takes something like 50-60 products for you to be approaching the limit of 8192 bytes. Unless you can decrease the size of the product payload using regular means, you could do something like drop all non-critical fields from the product objects (e.g. brand, category, variant), and just include the id, name, price, and quantity.\nOr you could drop all product data except for id in these cases, and then use Data Import to refresh the additional data.\nSummary Well, it looks like customTask to the rescue again. It\u0026rsquo;s such a powerful feature, and I just love the fact that you can get all meta with your Google Analytics data.\nPeppering the payload with details that are otherwise obfuscated is a great way to add a whole new level of debugging opportunities to your data collection.\nHit payload size limit is nasty in that you won\u0026rsquo;t be alerted if a problem surfaces. The Google Analytics UI won\u0026rsquo;t be able to inform you about this problem because the requests are never sent to GA in the first place. With this Custom Dimension, you can add this information to the payload as a Custom Dimension, and then prepare in advance for the eventuality that you might be hitting the payload maximum size sometime soon.\n"
},
{
	"uri": "https://www.simoahava.com/test-page/",
	"title": "Test page",
	"tags": [],
	"description": "",
	"content": "Hello  #wrapper { height: 100%; width: 100%; display: table; } #top { height: 50%; background-color: #5B9BD5; width: 100%; display: table-row; } #bottom { height: 50%; background-color: #fff; width: 100%; display: table-row; } #greeting { color: #fff; font-family: Lobster,sans-serif; font-size: 2em; vertical-align: bottom; text-align: center; display: table-cell; padding: 10px; } #subscribe { vertical-align: top; text-align: center; display: table-cell; padding-top: 20px; } #email-field { width: 700px; height: 50px; color: #838383; font-size: 3em; padding: 5px; border: 2px dotted #ddd; font-family: sans-serif; } #subscribe-button { background-color: #f08406; border: 1px solid #ad5d15; -webkit-box-shadow: inset 0px 1px 0px rgba(255,255,255,0.4); -moz-box-shadow: inset 0px 1px 0px rgba(255,255,255,0.4); box-shadow: inset 0px 1px 0px rgba(255,255,255,0.4); text-shadow: 0px 1px 1px rgba(0,0,0,0.4); color: #fff; padding: 15px 25px; -webkit-border-radius: 6px; -moz-border-radius: 6px; -ms-border-radius: 6px; -o-border-radius: 6px; border-radius: 6px; font-size: 16px; font-weight: bold; font-family: sans-serif; margin-top: 20px; } #thanks { text-align: center; vertical-align: top; padding-top: 20px; font-family: helvetica; font-size: 3em; color: #5B9BD5; } #nothanks a { vertical-align: middle; padding: 10px; font-family: sans-serif; text-decoration: underline; color: #5B9BD5; }  Subscribe NOW to make millions $$$    No thanks, I don't want to make money    "
},
{
	"uri": "https://www.simoahava.com/analytics/use-wildcard-css-selectors-with-all-elements-triggers/",
	"title": "#GTMTips: Use Wildcard CSS Selectors With All Elements Triggers",
	"tags": ["google tag manager", "gtmtips", "css selectors"],
	"description": "Use the wildcard (*) CSS selector with the All Elements trigger in Google Tag Manager. This improves the accuracy of capturing clicks on the target elements.",
	"content": "When using the All Elements trigger in Google Tag Manager, it\u0026rsquo;s easy to overlook the fact that it captures all clicks on the page. It\u0026rsquo;s also brutally accurate - it captures clicks on the exact element that was below the mouse button when a click happened. This means that when working with the All Elements trigger, you need to be more careful when identifying the correct element you actually want to track clicks on.\nIn this short guide, I\u0026rsquo;ll show you a simple trick to make sure you always capture clicks on a given element, regardless of what the surrounding HTML structure looks like. All this requires is a simple tweak to the CSS selector you use in the trigger.\nTip 76: Use the wildcard CSS selector with All Elements triggers   The wildcard selector literally means any descendant of the preceding selector. So given a selector like div#nav * would match any elements that are nested with a \u0026lt;div id=\u0026quot;nav\u0026quot;\u0026gt; element, but not the \u0026lt;div\u0026gt; element itself.\nLet me show you a useful example.\n\u0026lt;div id=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;logo\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/images/logo.png\u0026#34; /\u0026gt; \u0026lt;span\u0026gt;Back home\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; In this scenario, let\u0026rsquo;s say you want to track clicks on \u0026lt;div id=\u0026quot;logo\u0026quot;\u0026gt;, regardless of whether the click falls on the image element or the span.\nIf you try to do it with an All Elements trigger that has either of the following conditions, it won\u0026rsquo;t work:\n  Click ID equals logo\n  Click Element matches CSS selector div#logo\n  Why not? Because the click will not land on the \u0026lt;div\u0026gt;. Rather, it will land on one of the nested elements, because they are block-level elements that actually fill the wrapping \u0026lt;div\u0026gt; completely. Thus there\u0026rsquo;s no area left in the \u0026lt;div\u0026gt; itself that could be clicked!\nSo, you need to instruct the trigger to track clicks on the \u0026lt;div\u0026gt; itself (this is a good precaution in case there\u0026rsquo;s additional padding that does introduce surface area to the \u0026lt;div\u0026gt;, too) and any of its nested elements. This is where the wildcard selector comes in handy.\nThe selector you\u0026rsquo;ll need to use looks like this:\nClick Element matches CSS selector div#logo, div#logo *\nThis literally means:\n Track clicks that land on a \u0026lt;div id=\u0026quot;logo\u0026quot;\u0026gt; element, or any element nested within this \u0026lt;div\u0026gt;.\n I\u0026rsquo;ll go so far as to say that whenever you use the All Elements trigger, always use this element, element * syntax. That way you\u0026rsquo;ll always be able to track clicks on the appropriate element, regardless of the nested HTML structure.\nThe only edge cases I can think of is if you really want to track clicks on the borders of the wrapping element alone and not anything nested within, but I\u0026rsquo;m having trouble justifying this use case in a real-world scenario. But in case you do want to track clicks specifically on a wrapping element, just drop the wildcard selector and simply use div#logo as the only selector.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/add-facebook-messenger-chat-google-tag-manager/",
	"title": "#GTMTips: Embed Facebook Messenger Chat With Google Tag Manager",
	"tags": ["google tag manager", "gtmtips", "facebook"],
	"description": "A quick guide on how to add the Facebook Messenger Chat plugin to your website using Google Tag Manager.",
	"content": "A while ago, I published a #GTMTips article, where I showed how you can add HTML elements to the page programmatically using Google Tag Manager. This is relevant because GTM\u0026rsquo;s validators prevent you from adding custom parameters to HTML elements that are injected directly via the Custom HTML tag. To circumvent this validation, you need to create the element programmatically, before appending it to the document.\nA while ago, Matteo Gamba asked me a question related to the Facebook Customer Chat Plugin. This plugin lets you add the Messenger chat of your own Facebook application directly to the website. It\u0026rsquo;s pretty cool, but the problem is that you are instructed to embed the plugin using custom HTML attributes, and these will not pass validation.\ncool thanks!\nIt might make sense for you to cover this topic with a dedicated post... basically anyone that will try to use GTM to install the messenger chat will incur into this problem 😉\n\u0026mdash; Matteo Gamba (@sliver86) March 18, 2018  So here I am, responding to the suggestion to write a dedicated guide for the Facebook chat plugin.\nTip 75: Embed the Facebook Messenger chat plugin using Google Tag Manager   First of all head on over to the official Facebook Custom Chat Plugin instructions, and do everything instructed until you reach Step 2 of the setup steps part of the guide.\nAt this point, you are instructed to add a specific \u0026lt;div\u0026gt; to the page. So what you\u0026rsquo;ll need to do is create a new Custom HTML tag in Google Tag Manager, with the following code within:\n\u0026lt;script\u0026gt; (function() { // Modify the variable values below  var page_id = \u0026#39;12345678\u0026#39;; var ref = \u0026#39;\u0026#39;; var theme_color = \u0026#39;#2B913F\u0026#39;; var logged_in_greeting = \u0026#39;Hello, logged in user! Welcome to my chat.\u0026#39;; var logged_out_greeting = \u0026#39;Hello, logged out user! Welcome to my chat.\u0026#39;; // Don\u0026#39;t touch the code below  var el = document.createElement(\u0026#39;div\u0026#39;); el.className = \u0026#39;fb-customerchat\u0026#39;; el.setAttribute(\u0026#39;page_id\u0026#39;, page_id); if (ref.length) { el.setAttribute(\u0026#39;ref\u0026#39;, ref); } el.setAttribute(\u0026#39;theme_color\u0026#39;, theme_color); el.setAttribute(\u0026#39;logged_in_greeting\u0026#39;, logged_in_greeting); el.setAttribute(\u0026#39;logged_out_greeting\u0026#39;, logged_out_greeting); document.body.appendChild(el); })(); \u0026lt;/script\u0026gt; Change the five variable values in lines 4-8 of the code. If you don\u0026rsquo;t have a custom webhook accepting requests from postbacks and referrals, you can leave the string empty.\nCreate a new DOM Ready trigger for this Custom HTML tag. The trigger should be delimited to only firing on pages where you want to see the chat plugin.\n  And that should do it! The code builds the required \u0026lt;div\u0026gt; element programmatically, using the variables you configured in lines 4-8 to populate the parameters.\n"
},
{
	"uri": "https://www.simoahava.com/tags/agile/",
	"title": "agile",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/definition-of-success/",
	"title": "Definition Of Success",
	"tags": ["agile", "analytics", "definition of success"],
	"description": "How do you define success as part of a software development project? How do you measure success? How do you integrate parameters of success into analytics done within a project?",
	"content": "Agile analytics isn\u0026rsquo;t a novel concept in any shape or form. Things like feedback loops and process-oriented development seem to integrate flawlessly into the analytics paradigm, at least on paper. Heck, there\u0026rsquo;s even the Build-Measure-Learn framework for continuous development. It would be difficult to argue that analytics doesn\u0026rsquo;t have a role in something with measure in the name!\nHowever, past three years of working at Reaktor, one of the world\u0026rsquo;s top agile technology houses, have introduced me to a whole new set of problems with integrating an \u0026ldquo;analytics mindset\u0026rdquo; into an agile workflow, or an \u0026ldquo;agile mindset\u0026rdquo; into the analytics process.\nThe crux of the matter is how to negotiate the time-boxed methodology of a sprint with the ever-fluctuating and contextual parameters of change. To put it succintly: it\u0026rsquo;s difficult to measure the impact of development work in a way that would establish a robust feedback loop.\n  One of the main things working against agile analytics is the very thing that fuels sprints in general: the Definition of Done (DoD). In Scrum, one of the most popular agile frameworks, DoD is basically a checklist of things that each task/feature/sprint must pass in order for the sprint to be deemed a success. DoD has a lot of things working for it:\n  It\u0026rsquo;s negotiated by everyone in the team.\n  It\u0026rsquo;s not monolithic.\n  It\u0026rsquo;s adjusted to better respond to an ever-changing business context.\n  It quantifies the success of the agile workflow.\n  In previous posts and talks on the topic, I have recommended adding Analytics as part of the DoD in any development project. By having analytics as a keyword in the DoD, the idea is that when developing features, the ever-important question of \u0026ldquo;Should we measure this?\u0026quot; would always be considered. \u0026ldquo;No\u0026rdquo; is a perfectly valid answer to this question - it doesn\u0026rsquo;t make sense to measure everything. But what\u0026rsquo;s important is that this discussion is had in the first place.\n  Anyone working in analytics can share anecdotes of times when they wanted to check some usage statistics from the available data, only to discover that the necessary tags were never deployed to begin with. By adjusting the DoD, this can potentially be avoided.\nHowever, having a Definition of Done has proven to be inadequate for establishing goals fuelled and validated by measurement. Even though a sprint or a feature branch can be deemed done by the criteria established in the DoD, it still doesn\u0026rsquo;t mean that the feature was a success.\nDoing the right things vs. doing things right It\u0026rsquo;s tempting to think of an equivalence between something being done and something being completed. However, I argue that there\u0026rsquo;s a distinction between the two. The first can be formalized with tools such as the Definition of Done. The second one is harder to pin down, because it requires consideration of what is successful.\n Yeah, I can do inspirational images, too!  Consider the following story in a backlog:\n Implement a single-page checkout flow.\n A feature like this could have a very clear Definition of Done, and it would be very easy to validate if this feature can be shipped:\n  Test coverage has been updated to include the new code.\n  Documentation has been written to cover how the new checkout works.\n  Deployment to production is successful.\n  The new checkout is measured in Google Analytics.\n  The new checkout is deployed to 50% of visitors (A/B test).\n  The new checkout is done when it passes these requirements. The team members working on the new checkout can look at these steps and plan their work accordingly.\nBut what determines if the new checkout flow is a success? It\u0026rsquo;s a technical success once deployed to production, but what actually validates that the new checkout brings added value to the organization?\nA new set of criteria need to be devised for this purpose. These criteria need to take into account the following things:\n  Success is subjective. Different stakeholders might have different ideas for what qualifies as a success.\n  Success is temporal. What is successful today, might not be successful tomorrow, especially in a constantly evolving and changing business landscape.\n  Success is non-binary. It\u0026rsquo;s not always possible to pass a true / false verdict for the success of a developed feature. Sometimes there are varying degrees of success.\n  Definition of Done is a great tool, because it guides us to do things right. However, it lacks the scope to guide us in the prioritization of the tasks at hand. It lacks the ability to determine if we\u0026rsquo;re doing the right things.\n  A Definition of Success for the example in this chapter could be something like:\n  This migration is successful for the team when the checkout flow has been deployed to production.\n  This migration is successful for the client when the A/B test shows statistically significant results that the new checkout increases revenue per user.\n  This migration is successful for the end user when the new flow decreases checkout abandonment.\n  If we take this thought experiment to its conclusion, we\u0026rsquo;ll find that completion stems from the union of a feature being done and a feature being done successfully. It\u0026rsquo;s possible for these two to overlap, such as in the success criterion for the team above, and it\u0026rsquo;s possible that success is impossible to determine for certain features and some particular stakeholders.\nDefinition of Success Definition of Success is a series of questions you ask when prioritizing (grooming) or adding new stories to the backlog. The questions are:\n  If this feature is developed, what determines whether it is a success to the end user?\n  If this feature is developed, what determines whether it is a success to the project owner / client?\n  If this feature is developed, what determines whether it is a success to the team?\n  By asking these questions, you are forced to think about the impact of your project work, not just the outcome. Optimally, impact is something you can measure - something that you can use data and analytics to validate with. But it\u0026rsquo;s perfectly fine to establish success criteria as something more ephemeral.\nLet\u0026rsquo;s take a look at some examples.\n   Feature DoS (end user) DoS (client) DoS (team)     Single-page checkout Drop in Checkout Abandonment Increase in Revenue per User Deployed successfully to production   Migrate from AWS to Google Cloud ??? Reduced costs for pipeline management More familiar tools available for pipeline management   Auto-complete site search Find more relevant search results faster Increase in Revenue from site search Improved performance of the search feature   Marketing dashboard More relevant campaigns More visibility to ROI More visibility to ROI    In the first example, establishing success criteria for something with such potential for impact as a new checkout flow is fairly easy. We can determine hard-and-fast metrics for success, with clearly defined goals which need to be surpassed for the feature to be a success.\nA very technical task, such as migrating a cloud backend from one service partner to another can be more difficult to align with success criteria. How can the end user validate success for the migration? Sometimes, it\u0026rsquo;s perfectly fine to not have an answer for all three vectors. In fact, only when you can\u0026rsquo;t find success criteria for a single vector should you be concerned about the validity of the feature in the first place.\nThe final example shows that more than one of the three vectors can share the same success criteria. In this case, building a marketing dashboard improves the visibility of marketing efforts, and this is beneficial to the client as well as the team that built the dashboard. The success criterion for the end user is again difficult to pin down. In the \u0026ldquo;long run\u0026rdquo;, more visibility to the current marketing efforts should result in better campaigns for the benefit of the end users as well.\nSuccess === Value In the end, we want to build valuable and value-adding things. We want the value of development work to be, at the very least, a sum of its parts. Optimally, we want to surpass expectations and build features that collaborate to produce unexpected value in unexpected ways.\nOne thing that unites the two streams of completion and success is value in and of itself. A feature can be considered successfully done if it produces a net increase in value to the parties that have a stake in the project.\nBut value can be difficult to measure. It can be defined with the same terminology we use when talking about micro and macro conversions in analytics. There is value that can be directly derived from the development of a feature. Typically this would have a currency symbol in front of it. Then there is value that is indirectly inferred, usually after a passage of time.\nTo refer to the examples earlier in this article, we could say that the new single-page checkout flow has direct value, because we can measure its impact on the revenue generated by the site when compared to the old checkout flow (preferably in an A/B test!).\nSimilarly, we can say that creating a new marketing dashboard has indirect value, because we hope that by making the data more transparent we can help build better campaigns that will eventually increase the value of our marketing process.\nRegardless of how you pin it down, you should always be able to describe good success criteria by using value as a focal point.\nFuzzy success Here\u0026rsquo;s one final thought to wrap up this article.\nI want to emphasize that Definition of Success is a communication tool rather than a set of strict validation criteria. It simply doesn\u0026rsquo;t make sense to block development while waiting for some feature to produce enough usable data to determine what its impact was.\n  Validation of whether a feature was successful or not can take a long time. Think of Search Engine Optimization efforts, for example. It might be months before any statistically (and intuitively) significant results emerge. It wouldn\u0026rsquo;t make sense to block the development of a critical new feature while you wait for this data.\nThus, because of this fuzzy, multi-dimensional definition of success, it\u0026rsquo;s so much more difficult to determine whether something was successful than to check if the development work is done.\nWhat to do, then?\nWell, I\u0026rsquo;ll reiterate: Definition of Success is a communication tool. It\u0026rsquo;s an approach that helps you evaluate development tasks based on their value potential. It\u0026rsquo;s more important as a discussion topic than as a directive that guides your development work (like Definition of Done can be).\nOne thing you can do is the exercise described earlier in this article. Set aside time for a session where the whole team takes a good hard look at the backlog of your project. As a team, try to figure out success criteria for each story still waiting for development work. Remember the three questions:\n  If this feature is developed, what determines whether it is a success to the end user?\n  If this feature is developed, what determines whether it is a success to the project owner / client?\n  If this feature is developed, what determines whether it is a success to the team?\n  You should find that this exercise alone can unearth things about the backlog items you wouldn\u0026rsquo;t have thought of before.\nIf you can come up with quantifiable, measurable metrics of success - all the better. Make sure these are being measured when the features are deployed, and make sure the team knows how to access this data.\nI think this exercise is good for team-building, too. You are communicating together, as a unit, what the long-term value of your work is. It\u0026rsquo;s a great opportunity to build motivation, since success criteria often make your overall goals clearer. You want the project to be a success, too, so making sure that all the tasks outlined in the backlog contribute to the success of the entire project makes a lot of sense.\nSummary I wanted to write this article because I think there\u0026rsquo;s just far too little talk about value and success in daily development work. I bet it applies to marketing teams, too, and not just developers. I bet it applies to business designers, to PR, to recruitment, to business owners, to shareholders, to communities, and to the universal laws that govern our existence!\nIt\u0026rsquo;s difficult to talk about success because it\u0026rsquo;s so subjective. Yet here I am, asking you to deploy a tool designed for general use, where the very focus is on this subjective notion of success.\nBut I guess that\u0026rsquo;s my point. It is subjective, but it\u0026rsquo;s also something you absolutely should devote time to. The discussion should also be extended to the entire team. Everyone involved in project work should understand that there is a whole undercurrent of communication, where team members are actively thinking in terms of value and what it might mean to different stakeholders.\nMaybe I\u0026rsquo;ve lived in a bubble, but it\u0026rsquo;s striking how rarely long-term, quantifiable success is tabled for discussion in agile contexts. I totally understand the focus on time-boxed sprints, where success is the net outcome of multiple sprints that are all \u0026ldquo;done\u0026rdquo; and \u0026ldquo;complete\u0026rdquo;. But I think this ignores the shift in many development team dynamics, where designers, analysts, CROs, SEOs, social media managers, business owners, and sales managers are now also part of the mix. Thinking of development work as something that only involves software developers is so last season.\nThus, when these \u0026ldquo;fuzzier\u0026rdquo; roles are added to the mix, the discussion around things like value and success must be extended to cover other things than \u0026ldquo;test coverage at 100%\u0026rdquo;, \u0026ldquo;peer-reviewed\u0026rdquo;, and \u0026ldquo;well-written code\u0026rdquo;.\nHow this can be done systematically and comprehensively is still on the drawing board. But I do hope this concept of a Definition of Success rings true - at the very least as a discussion topic and thought experiment, if nothing else.\nHuge thanks to my Reaktor colleagues, especially Aleksi Lumme, Jaakko Knuutila, and Matias Saarinen, for their collaboration on fleshing out the Definition of Success.\n"
},
{
	"uri": "https://www.simoahava.com/tags/definition-of-success/",
	"title": "definition of success",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/gtm360/",
	"title": "gtm360",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/guide-zones-google-tag-manager-360/",
	"title": "Guide To Zones In Google Tag Manager 360",
	"tags": ["google tag manager", "gtm360", "zones", "guide"],
	"description": "Guide to Zones in Google Tag Manager 360. You can use Zones to link multiple containers to a website, firing tags, triggers, and variables from all linked containers on the site.",
	"content": "Google Tag Manager supports loading multiple containers on the same page. It\u0026rsquo;s useful if you have multiple companies or organizations working on the same site, but for one reason or another (e.g. governance) you want to restrict access to your main container. In these situations, having the other party create their own container and adding it to the site is the best of bad options.\nWell, in Google Tag Manager 360, we now have Zones that make managing multiple partners\u0026rsquo; containers quite a bit easier.\n  Zones have boundaries, which are basically page rules you can use to restrict Zones to only certain pages or groups of pages. In addition to this, Zones let you define type restrictions for tags, triggers, and variables (similar to gtm.whitelist), which gives you even more control over what these linked containers can and cannot run on your site.\nWhy use Zones? Zones lets you link multiple containers to a single site. However, instead of allowing the linked container to fully fire its tags and triggers, you can restrict access to the site on two fronts:\n  Boundaries let you specify page rules. The defined Zone is only active (i.e. the containers are only linked) on pages which pass these rules. For example, you could restrict a marketing agency\u0026rsquo;s container to firing only on the landing pages they have created.\n  Type Restrictions allow only certain tags, triggers, and variables to work on the page. This is useful if you want to prevent linked containers from running Custom HTML tags or Custom JavaScript variables, for example.\n    A linked container will thus be able to fire any of its tags, triggers, and variables that have not been restricted on pages that are included in the Zone Boundary.\nCreate a new Zone To create a new Zone, click the Zones entry in the navigation, and then the red NEW button.\n  Naturally, you can have more than one Zone defined in your container, and your Zones can have more than one container linked to each Zone.\nLink a container to a Zone In the Zone settings, you can link a new container to the Zone by clicking the Zone Configuration field to enter the edit mode. Then, either click the No linked containers box (if there are no containers yet linked), or the blue plus button to open the container link overlay.\n  In the overlay that opens, you can either click the add container icon, which opens a list of containers you have access to in the current account, or you can just type any container ID to the Container ID field. Remember to give the container a nickname that will help you identify it when looking at the Zone settings.\nYou can even click the add container icon to open the list of containers you have access to, and instead of selecting an existing container, you can click the blue plus button in the corner to create a brand new container within this workflow!\n  Once you\u0026rsquo;ve selected the container, click ADD to add it to the current zone.\n  There are precautions in place to prevent infinite loops (such as adding the current container as a linked container) and too complex links (such as linking a container that links a container that links a container that links a container).\nA word on governance If you choose a container to which you have access, you\u0026rsquo;ll see some additional Container Details in the container selector overlay.\n  This overview lets you inspect what the Latest and Published versions look like. The latter is especially relevant, since that is the container that will be linked to on the site itself. You can also check which users have access to the linked container. These are all, in my opinion, necessary features for enhancing transparency of the workflow.\nThe fact that you can inspect and modify the users of the linked container means that you can set everything up for the zone within the workflow, without having to pogo between different GTM admin views.\nAll in all, you might find it strange that you can just add any container ID you want into the Zone. In other words, you can pull any container in existence and have it fire tags, triggers, and variables on your site!\nWell, when you think about it, that\u0026rsquo;s how GTM works. Nothing\u0026rsquo;s stopping you from adding a container snippet from any container in existence to your site, and nothing\u0026rsquo;s stopping you from linking a container into your Zone, either.\nIt\u0026rsquo;s possible this might lead to governance issues, but the Type Restrictions and Boundaries exist to somewhat alleviate the friction that emerges from this open setup.\nNothing trumps communication, though. Even with Zones, I still suggest you work on improving communication structures within your organization and with your partners. You\u0026rsquo;ll also need to establish a contingency plan for when something goes awry in a linked container.\nAdd some Boundaries Boundaries are page rules. Page rules are conditions that a page needs to meet for the rule to pass.\nThis means that these conditions must exist when the Google Tag Manager container loads. In other words, you can\u0026rsquo;t establish a condition like \u0026ldquo;Once the user clicks button X, activate this Zone\u0026rdquo;. Though, see the next chapter for an exception.\n  In the image above, All Pages is a good Zone Boundary, and Page Path contains /campaign/ is a good Zone Boundary. These conditions can be checked when the page loads.\nEvent equals gtm.click is not a good Zone Boundary, because when the page rules are evaluated, {{Event}} will not equal gtm.click.\nCustom Evaluation However, there is an exception to how Boundary conditions are evaluated. If you have a single-page app, or otherwise need to re-evaluate the Boundary conditions upon certain events (such as a Custom Event trigger to signal the page transition), you can click the little dot-menu and select Show custom evaluation. When you click it, you\u0026rsquo;ll see a new view, which you can use to add triggers which, when fired, will cause the Zone to re-evaluate the Boundary conditions.\n  As you can see, by default the trigger used to evaluate the Boundary conditions is All Pages. In other words, the conditions listed in the Boundary settings need to be present when the Google Tag Manager container first loads.\nHowever, by adding other triggers to this list, the Boundary conditions will be re-evaluated when the triggers fire. So if you have a single-page app, for example, the following process could happen:\n  When the container first loads, the page path is /, and thus the Zone is not active.\n  When the user then clicks a link to the campaign page, the single-page app changes the URL to /campaign/ without a page load.\n  Now, since the URL changes without a page load, GTM would not re-evaluate the Zone Boundary by default, since it only does so when the All Pages trigger fires upon initial container load.\nBy adding a trigger to this Custom Evaluation list, you can force GTM to re-evaluate the Boundary conditions, perhaps activating the Zone if the conditions pass.\nFor example, here\u0026rsquo;s me using a History Change trigger to force GTM to re-evaluate the Boundary condition upon a browser history event, which is very common with single-page transitions.\n  If the history event changes the page URL to /campaign/, the Zone will become active.\nType Restrictions By default, linked containers work with full capacity. When a container is linked in an active Zone, the container has full freedom to operate just as if it had been added to the page directly.\nWith Type Restrictions, you can delimit the Zone to only permit certain types of tags, triggers, and variables to function in the Zone. To enable Type Restrictions, switch the toggle on.\n  These are the default Type Restrictions that are in place if you toggle Type Restrictions on:\n  All Google tag types are enabled. Google tag types are tags for Google products such as Google Analytics, Adometry, DoubleClick, and AdWords.\n  All trigger types are enabled.\n  All variable types except Custom JavaScript variable are enabled.\n  All other tag types are disabled.\n  Most importantly, this means that Custom HTML tags and Custom JavaScript variables are disabled by default. Thus, when toggling the Type Restrictions on, be aware that most containers will have limited functionality.\nSimilarly, if you disable trigger and variable types, do note that containers can have very complex chains of trigger and variable evaluation. By disabling a single type, you can potentially disrupt all the functionality in a linked container.\nAgain, I stress the importance of communication. It can be a very good idea to restrict only certain types of tags, triggers, and variables to fire in the Zone, but you need to make sure you know what the situation in linked containers is before arbitrarily blocking their core functionality.\nTo add or remove Type Restrictions, first click the main type selection. This opens an overlay where you can toggle individual tag, trigger, and variable types on or off. You can also click the ALLOW ALL / RESTRICT ALL button to perform the respective selection action.\n  Preview Zones Preview mode has some extra functionality, too. When you preview the container you defined the Zone in, Preview mode will show you whether or not the Zone is currently active. An active Zone means that any linked container will be able to fire its tags, triggers and variables (as long as Type Restrictions allow them to do so).\n  You can click the Zone in Preview mode to see details about it, such as what linked containers it holds and what Type Restrictions are in place.\nZones that are not active will display as Inactive in Preview mode.\n  If there is a Custom Evaluation rule that changes the activity status of the Zone, then it will change from Inactive to Active in Preview mode.\nIf you have a linked container in Preview mode, too, and you have access to the Preview in your browser, then you\u0026rsquo;ll see a drop-down menu in the Preview mode pane. This allows you to switch to Preview mode for the linked container, if you so wish.\n  Note that if you do not have access to the linked container, you will not have any visibility to what happens within, Preview mode or not.\nSummary I think Zones are a very elegant solution to the problem of access control rights and running multiple containers on the site. Until now, it\u0026rsquo;s been tempting to just add multiple containers to the page when working with partners who might have a different approach to container governance than you do.\nZones don\u0026rsquo;t fix governance issues, but they do provide you with more tools to facilitate governance.\nThe main problem with Zones is that you lack visibility to a linked container. This is understandable, since it would open a completely different can of worms if you had visibility to containers that you can arbitrarily add to your site.\nFor linked containers to which you do have access, you can always put them in Preview mode, which lets you see what\u0026rsquo;s going on when the Zone is active.\nYes, it\u0026rsquo;s a bummer this is only for Google Tag Manager 360. On the other hand, this is most certainly an enterprise feature, which means it makes sense to package it under a paid plan, together with SLA and other support, too. And it\u0026rsquo;s not like this is a must-have feature for most. Zones or no Zones, a single-container setup is still the preferred way of working with Google Tag Manager, at least in my opinion.\n"
},
{
	"uri": "https://www.simoahava.com/tags/zones/",
	"title": "zones",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/check-container-id-with-eventcallback/",
	"title": "#GTMTips: Check Container ID With eventCallback",
	"tags": ["google tag manager", "gtmtips", "eventcallback"],
	"description": "When using eventCallback with dataLayer.push(), Google Tag Manager automatically passes the container ID which invoked the callback as an argument. Use this to run code against the correct container.",
	"content": "When you use the dataLayer.push() command on a page with a Google Tag Manager container, you pass information to GTM\u0026rsquo;s internal data model and potentially fire tags (if the push() contained an event key). You can also add the eventCallback key to these pushes. The value of this key should be a function, and this function is then executed once the container finishes processing any tags that might have fired on that dataLayer.push().\nThis is useful if you want to give Google Tag Manager time to complete its operations before proceeding with other actions on the page, for example.\nThere\u0026rsquo;s a wrinkle, though. Because Google Tag Manager supports adding multiple containers to the page, the eventCallback is called once for every single container on the page.\n\u0026ldquo;But I don\u0026rsquo;t use multiple containers on the page\u0026rdquo;, I hear you exclaim. Good for you! However, if you are running an Optimize experiment on the page, or if you are also loading gtag.js, eventCallback will be invoked once per each implementation thereof, too. Why? Because they use Google Tag Manager as the underlying tech.\nIn this tip, we\u0026rsquo;ll learn how to detect which Google Tag Manager container invoked the callback and use that to effectively deduplicate the execution of our code.\nTip 74: eventCallback automatically receives the Container ID as the method argument   The tip is simple (as these things usually tend to be). The Google Tag Manager container ID (e.g. GTM-12345) which invoked the eventCallback method will be automatically passed as a parameter to the function you have set up as the eventCallback.\nIf you have multiple containers on the page, eventCallback will fire once for each container, once they finish firing whatever tags the dataLayer.push() triggered. Each time a container invokes the eventCallback method, its container ID will be the argument.\nAs an example, let\u0026rsquo;s say I have three Google Tag Manager containers running on the site.\n  Google Tag Manager container with ID GTM-12345\n  Google Optimize container with ID GTM-23456\n  gtag.js library collecting to UA-12345-1\n  Next, I\u0026rsquo;ll run the following code in the JavaScript console:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;GTMEvent\u0026#39;, eventCallback: function(containerId) { console.log(\u0026#39;Container ID: \u0026#39; + containerId); } });  The output in the console looks something like this:\nContainer ID: GTM-12345 Container ID: GTM-23456 Container ID: UA-12345-1 As you can see, with each iteration of the callback, the container ID was automatically assigned as an argument to the function. With this information, it\u0026rsquo;s easy to deduplicate your eventCallback method so that it only reacts to the \u0026ldquo;correct\u0026rdquo; container.\nFor example, here I want to execute some code only when GTM-12345 invokes the callback:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;GTMEvent\u0026#39;, eventCallback: function(containerId) { if (containerId === \u0026#39;GTM-12345\u0026#39;) { // Run some code  } // Ignore if not GTM-12345  } });  By simply checking what the container ID passed as an argument was, it\u0026rsquo;s trivial to make sure your eventCallback code isn\u0026rsquo;t executed over and over again with each additional container on the page.\n"
},
{
	"uri": "https://www.simoahava.com/tags/eventcallback/",
	"title": "eventcallback",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/customize-scroll-depth-trigger/",
	"title": "Customize The Scroll Depth Trigger In Google Tag Manager",
	"tags": ["google tag manager", "scroll depth", "triggers", "javascript"],
	"description": "Some tips and tricks on how to customize the scroll depth trigger in Google Tag Manager.",
	"content": "Last updated 9 March 2018 with some new tips.\nThe Scroll Depth trigger in Google Tag Manager has a lot going for it. Tracking how far users scroll down a given page has long since been recognized as an important cog in the engagement tracking machine, and there have been really great solutions for implementing scroll depth tracking for web analytics over the years.\nWith Google Tag Manager\u0026rsquo;s native Scroll Depth trigger, it\u0026rsquo;s tempting to think we now have a be-all end-all solution that covers all the bases. However, as with everything else in analytics, the native scroll depth trigger does require customization to provide you with relevant information about your content and the visitors interacting with it.\n  In this article, I\u0026rsquo;ll explore some tricks that you might find useful when customizing the scroll depth trigger.\n1. Variable percentages depending on page The easiest customization you can make is to track different percentages depending on the page in question. For example, on article pages in my blog I might be interested in tracking in increments of 25% of vertical depth scrolled. But on list pages and summary pages I might only be interested in knowing who scrolls down to the bottom.\nThe Vertical Scroll Depths and Horizontal Scroll Depths fields require a number or a list of numbers (comma-separated) to work. Thus you can easily use a Custom JavaScript variable to return one list of numbers for a certain condition, and a different list of numbers for some other condition.\nFor example, here\u0026rsquo;s a Custom JavaScript variable that does what I described in the first paragraph of this chapter.\nfunction() { return {{Page Path}}.indexOf(\u0026#39;/analytics/\u0026#39;) \u0026gt; -1 ? \u0026#39;25,50,75,100\u0026#39; : \u0026#39;75,100\u0026#39;; }  This variable returns the string 25,50,75,100 if the page URL contains /analytics/, and 75,100 otherwise. Thus, on blog pages, it will measure scroll depth in increments of 25 percent, but on other pages only scrolling up to 75% and 100% in depth.\nThen you can just add this variable to the trigger like so:\n  You don\u0026rsquo;t have to use a Custom JavaScript variable. I\u0026rsquo;m quirky in that I prefer to use it, always. But you could just as well use a RegEx Table variable to achieve the same thing:\n  This is a simple customization, but it does allow you to measure relevant metrics for pages where it makes sense.\n2. Only fire the scroll depth trigger on relevant pages This should be a no-brainer, too. It might make sense to have the scroll depth trigger fire your tags only on pages where you are genuinely interested in scroll depth tracking. For example, I\u0026rsquo;m actually only interested in knowing how far users scroll on my blog pages and nowhere else. Thus I could make this simple modification to the trigger to delimit its firing capabilities:\n  A very simple modification but helps you collect only meaningful data.\n2.1. Prevent the Scroll Depth trigger from activating As you can see, there\u0026rsquo;s no \u0026ldquo;Enable when\u0026hellip;\u0026quot; option on the Scroll Depth trigger, similar to what you might see with a Just Links trigger, for example. Thus you might be tempted to think that there\u0026rsquo;s no way to prevent those pesky gtm.scrollDepth events from populating the dataLayer on pages where you are not interested in tracking scroll depths.\nHowever, there\u0026rsquo;s a very easy hack to make sure the trigger doesn\u0026rsquo;t push anything to dataLayer. Just set the percentage threshold to 101. It\u0026rsquo;s impossible to scroll to 101% length of the page, so by setting the threshold to this you\u0026rsquo;ll deactivate the trigger, effectively.\nThus you could modify #2 above by putting the following Custom JavaScript variable into the Percentages field of the trigger:\nfunction() { return {{Page Path}}.indexOf(\u0026#39;/analytics/\u0026#39;) \u0026gt; -1 ? \u0026#39;25,50,75,100\u0026#39; : \u0026#39;101\u0026#39;; }  This sets the thresholds to \u0026lsquo;25,50,75,100\u0026rsquo; on pages where the URL contains /analytics/, and \u0026lsquo;101\u0026rsquo; elsewhere.\n3. Only fire tags on pages where thresholds are not auto-collected One problem with the Scroll Depth trigger is that it doesn\u0026rsquo;t care if the page is long or short. If you have a very short page, it\u0026rsquo;s possible that all the thresholds are met when the page is loaded, which leads to a bunch of gtm.scrollDepth events pushed into dataLayer, firing your tags even though the user never scrolled!\n  Luckily there\u0026rsquo;s a nifty trick we can use to fight this. We can prevent the Scroll Depth trigger from functioning on pages where the ratio between browser viewport height and page height is too high. The browser viewport is the area of the web browser that is filled by your page. The page itself is the entire document, parts of which are likely to be invisible below the fold of the viewport.\nThe higher the ratio between the height of the visible viewport and the height of the page, the less there is to scroll. For example, if the visible viewport is 400 pixels in height, and the page height is also 400 pixels, there is nothing to scroll. The entire page will be visible in the viewport, and any vertical thresholds you have defined in the Scroll Depth trigger will auto-fire.\nSo, if you want to only fire the trigger when the user actually scrolls to 25%, 50%, 75%, and 100%, you need to make sure the ratio between the visible viewport height and the page height is less than 0.25. This means that less than 25% of the page is visible in the browser viewport.\nTo do this, you need to create a Custom JavaScript variable that looks like this:\nfunction() { // Change this to reflect the percentages or pixels you want to fire the trigger for  var verticalScrollDepths = \u0026#39;25,50,75,100\u0026#39;; // Change this to the MAXIMUM ratio of viewport height / page height you want the trigger to activate for  var maximumRatio = 0.25; // Change this to what thresholds should be tracked if the ratio is more than the maximum  // Leave it at \u0026#39;101\u0026#39; if you want to prevent the trigger from functioning in this case  var fallbackDepths = \u0026#39;101\u0026#39;; var heightOfPage = Math.max(document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight); var heightOfViewport = Math.max(document.documentElement.clientHeight, window.innerHeight); var ratio = heightOfViewport / heightOfPage; return ratio \u0026lt; maximumRatio ? verticalScrollDepths : fallbackDepths; }  First thing you\u0026rsquo;ll need to do is edit the verticalScrollDepths variable to reflect the list of thresholds you want to track if the page passes the ratio check.\nThen, edit the maximumRatio to reflect the maximum ratio (between 0 and 1) of the height of the visible viewport vs. height of the actual page. A value of 0.25 would mean that at most 25% of the page can be visible in the viewport when the page is loaded.\nFinally, edit the fallbackDepths variable to give a \u0026ldquo;default\u0026rdquo; value for the cases when the maximum ratio is surpassed. If you want to prevent the Scroll Depth trigger from working at all when the maximum ratio is surpassed, use the value 101, because a depth of 101% can never be tracked.\nThen just add this to the \u0026ldquo;Vertical Scroll Depths\u0026rdquo; field and you should be all set.\nNow the trigger will only fire in cases where the page is long enough to make sense for your tracking.\n4. Track scroll percentages of a specific content element This is a bit trickier, but still doable with some JavaScript magic.\nBy default, the Scroll Depth trigger calculates vertical scrolling depth from the top of the page to the bottom of the page. But you might only be interested in knowing the scroll depths of a specific content element, such as the article body. For example, on my site, I don\u0026rsquo;t really care if users scroll past the article end, through the Disqus comments, all the way to the bottom. But I do care if they scroll to the bottom of the main article body.\nThe way to make this work is to first calculate the height of the HTML element that contains the content whose scroll depths you want to track. Then, you need to check how far from the top of the page this element is. Finally, you need to tell the Scroll Depth trigger to track pixel thresholds that correspond with the desired depths of the content element, and add the distance to the top of the page to those numbers.\nI think it\u0026rsquo;s easier to illustrate.\n  As you can see, the total height of the element is 1200 pixels. If I want to track scroll depth increments of 33%, it means that the respective marks are at 400, 800, and 1200 pixels. Since the top of the content element is exactly 250 pixels from the top of the page, I need to add this number to the thresholds before I feed it into the Scroll Depth trigger, because it measures everything counting from the top of the page. Thus the final pixel depths would be 650, 1050, and 1450 pixels.\nNow, since we don\u0026rsquo;t want to hard-code these into the trigger, as that would lead to a horrible mess to manage (every single page would need its own trigger), we can use a Custom JavaScript variable to dynamically calculate the pixel depths.\nfunction() { // Change the contents of this array to reflect the percentages of scroll depth you want to track  var verticalPercentages = [25,50,75,100]; // Change this to fetch the element you want to track scrolling in  var targetElement = document.querySelector(\u0026#39;div.post-content .main-content-wrap\u0026#39;); var elementHeight = targetElement.offsetHeight; var totalOffsetTop = 0; while (targetElement) { totalOffsetTop += (targetElement.offsetTop - targetElement.scrollTop + targetElement.clientTop); targetElement = targetElement.offsetParent; } return verticalPercentages.map(function(percentage) { return parseInt(elementHeight * (percentage * 0.01) + totalOffsetTop); }).join(); }  You\u0026rsquo;d then need to add this variable reference to the Pixel field in the Vertical Scroll Depths selection of the trigger:\n  This setup calculates dynamically the correct pixel thresholds based on the percentages you wrote in the verticalPercentages array. This way, 0 percent depth is the top of the content element (specified in targetElement), and 100 percent depth is the bottom of the content element.\nWith this workaround, you can get more meaningful measurements on pages which have lots of different content areas filling up the real estate of the document itself.\n4.1. Return the scroll depth percentage of element scrolling One thing missing from #4 is the option to report on the percentages the user scrolled down the element. Because the trigger is setup using pixel depths rather than percentages, the {{Scroll Depth Threshold}} built-in variable will always return the pixel depth the user scrolled past, rather than the respective percentage of scrolling.\nGetting the percentage is actually a bit of a hack. The problem with the Scroll Depth trigger is that it\u0026rsquo;s initialized fairly early in the page load, often before DOM Ready and any dynamically injected content is added to the page. This means that the thresholds of scrolling might be smaller than what the percentages actually would reflect.\nThis is, unfortunately, unavoidable right now. The Scroll Depth trigger has no options to delay its initialization until such a time as the content is completely loaded.\nWhat this also means is that any variables you use to determine the height of the content and, subsequently, what the respective percentages actually reflect, can change from one trigger event to the next. Thus, in order to get the actual percentages used by the Scroll Depth trigger, we\u0026rsquo;ll need to do something that I\u0026rsquo;ve repeatedly instructed to avoid at all costs: implement a side effect in the Custom JavaScript variable.\nBasically, we\u0026rsquo;ll need to store the scroll thresholds in a global variable when the Custom JavaScript variable is first run. After the initial run, we\u0026rsquo;ll always pull whatever was stored in the global variable instead of generating a new set each time the variable is invoked. This way, any triggers and tags that use the thresholds will always access the same values, even if they might be off a bit from what the content thresholds actually are.\nThe modification looks like this:\nfunction() { if (typeof window._gtm_scroll_set === \u0026#39;undefined\u0026#39;) { // Change the contents of this array to reflect the percentages of scroll depth you want to track  var verticalPercentages = [25,50,75,100]; // Change this to fetch the element you want to track scrolling in  var targetElement = document.querySelector(\u0026#39;div.post-content .main-content-wrap\u0026#39;); var elementHeight = targetElement.offsetHeight; var totalOffsetTop = 0; while (targetElement) { totalOffsetTop += (targetElement.offsetTop - targetElement.scrollTop + targetElement.clientTop); targetElement = targetElement.offsetParent; } window._gtm_scroll_set = { thresholds: verticalPercentages.map(function(percentage) { return parseInt(elementHeight * (percentage * 0.01) + totalOffsetTop); }).join(), percentages: verticalPercentages }; } return window._gtm_scroll_set.thresholds; }  As you can see, we populate a global variable window._gtm_scroll_set with an object that contains two properties:\n  thresholds: the array of scroll thresholds used by the Scroll Depth trigger.\n  percentages: the array of vertical percentages you defined in the same variable.\n  Now that we know what the thresholds are, what the percentages are, and we can trust that the same thresholds are returned whenever this Custom JavaScript variable is invoked, we can create another Custom JavaScript variable whose only job is to return which percentage the user scrolled past when the trigger fired. This is what that variable looks like:\nfunction() { if (typeof window._gtm_scroll_set !== \u0026#39;undefined\u0026#39;) { var percentages = window._gtm_scroll_set.percentages; var thresholds = window._gtm_scroll_set.thresholds.split(\u0026#39;,\u0026#39;).map(function(t) { return parseInt(t); }); var crossedIndex = thresholds.indexOf({{Scroll Depth Threshold}}); return percentages[crossedIndex]; } return {{Scroll Depth Threshold}}; }  This Custom JavaScript variable checks which threshold was returned by {{Scroll Depth Threshold}}, and returns the respective percentage value.\nThe order of things is thus:\n  Use the first Custom JavaScript variable in the Scroll Depth trigger\u0026rsquo;s Pixel Depths field.\n  Use the second Custom JavaScript variable wherever you want to get the percentage value for the scroll depth reached when the trigger fires.\n  Note that even though I am introducing a side effect in the Custom JavaScript variable, it is mitigated by checking if the global variable has already been set. Thus the typical downside of side effects (constantly and unpredictably updating global state) is mitigated.\nIt\u0026rsquo;s not perfect - nothing that impacts global state ever is. There\u0026rsquo;s always the risk that some other JavaScript accesses and modifies window._gtm_scroll_set, which would break your setup.\nSummary These four tips ranged from the really simple to the moderately complex.\nThere are many ways in which you can customize Google Tag Manager\u0026rsquo;s default trigger. With the scroll depth trigger, these customizations are almost necessary, because the trigger itself is lacking in some critical configurations options for now.\nFor example, I would like to see the following features in the trigger:\n  Option to establish when the trigger is initialized (e.g. after DOM Ready, with a Data Layer event, etc.).\n  Option to prevent the trigger from auto-firing if the thresholds have been crossed when the page loads.\n  Option to reset the trigger manually, which is useful especially on single-page apps.\n  Option to define a content element instead of the entire page for determining scroll depth.\n  With the tips in this article, you can do plenty, but especially the feature where you could reset the trigger is sorely missing from the current implementation.\nDo you have tips you want to share for Google Tag Manager\u0026rsquo;s Scroll Depth trigger?\n"
},
{
	"uri": "https://www.simoahava.com/analytics/add-html-elements-page-programmatically/",
	"title": "#GTMTips: Add HTML Elements To The Page Programmatically",
	"tags": ["google tag manager", "gtmtips", "javascript", "custom html tag"],
	"description": "Google Tag Manager&#39;s Custom HTML tags strip out any non-standard parameters from HTML elements you add directly to the tag. By addng the elements programmatically, you can work around this limitation.",
	"content": "One of the annoying quirks of Google Tag Manager is that it strips out any non-standard HTML attributes from elements you add to Custom HTML tags. I\u0026rsquo;m using \u0026ldquo;non-standard\u0026rdquo; as a qualifier here, because I don\u0026rsquo;t have an exhaustive list of attributes that are ignored. But at least data attributes (e.g. data-id) and attributes with custom names (e.g. aria-labelledby) are either stripped out upon injection, or GTM might actually prevent you from even saving the container if the Custom HTML tag has elements with these attributes.\nSo this tip might be very useful to you if you want to annotate your custom HTML elements with custom attributes.\nTip 73: Add elements to page programmatically   Here\u0026rsquo;s the problem. Say you have an element that looks like this:\n\u0026lt;script src=\u0026quot;/myScript.js\u0026quot; data-simo-script=\u0026quot;My Script\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\nIf you add that line to a Custom HTML tag, the actual element that ends up on the page will look something like this:\n  As you can see, the data attribute is stripped out.\nHere\u0026rsquo;s another example. Say you have an element that looks like this:\n\u0026lt;input type=\u0026quot;text\u0026quot; aria-labelledby=\u0026quot;firstname\u0026quot;/\u0026gt;\nThis won\u0026rsquo;t even let you save the container. You\u0026rsquo;ll see an error message like this:\n  This is unfortunate, because there might be good reason to add these custom attributes to your elements.\nSo, here\u0026rsquo;s the solution. Use JavaScript instead! You can rewrite the Custom HTML tag to add the element to the page using JavaScript, and thus you can add any custom attributes to it that you want.\nHere\u0026rsquo;s how it works.\n\u0026lt;script\u0026gt; (function() { // Change \u0026#39;script\u0026#39; to whatever element you want to create, e.g. \u0026#39;img\u0026#39; or \u0026#39;input\u0026#39;.  var el = document.createElement(\u0026#39;script\u0026#39;); // Add any standard attributes you need, e.g. \u0026#39;src\u0026#39;, \u0026#39;width\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;name\u0026#39;.  // The syntax is el.setAttribute(attribute_name, attribute_value).  el.setAttribute(\u0026#39;src\u0026#39;, \u0026#39;/myScript.js\u0026#39;); // Add any non-standard attributes with the same method.  el.setAttribute(\u0026#39;data-simo-script\u0026#39;, \u0026#39;My Script\u0026#39;); // Finally, inject the element to the end of \u0026lt;body\u0026gt;  document.body.appendChild(el); })(); \u0026lt;/script\u0026gt;  Here you create the element using JavaScript\u0026rsquo;s DOM methods. All attributes are added with the setAttribute() method, and finally the element is added to the end of \u0026lt;body\u0026gt;.\nIt\u0026rsquo;s a bit of an overhead compared to adding the element directly to the Custom HTML tag, but the end result is cleaner, since you have full control over what attributes are added to the element, and where in the page it is added to.\nTry it out!\n"
},
{
	"uri": "https://www.simoahava.com/tags/custom-html-tag/",
	"title": "custom html tag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/check-for-undefined-value/",
	"title": "#GTMTips: Check For Undefined Value",
	"tags": ["google tag manager", "gtmtips", "triggers", "variables"],
	"description": "How to check if a variable value is undefined when building Google Tag Manager triggers.",
	"content": "This is one of those #GTMTips posts that I was certain I\u0026rsquo;d already written. So it came as a mild surprise when I realized I\u0026rsquo;d never tackled this aspect of Google Tag Manager.\nIt\u0026rsquo;s a short and sweet tip again. Today we\u0026rsquo;ll learn how to check if a variable is undefined using Google Tag Manager.\nTip 73: Check for undefined variable values   If a variable is undefined, it means that a variable with the given name does not currently have any valid value in memory. In JavaScript, the typical check for undefined is done like this:\nfunction checkIfUndefined(variable) { return typeof variable === \u0026#39;undefined\u0026#39;; }  With Google Tag Manager, you can\u0026rsquo;t run JavaScript evaluations in triggers. Instead, if you wanted to do the check above, you\u0026rsquo;d need to create a Custom JavaScript variable that returns true if the variable whose status you want to check is, indeed, undefined.\nHowever, there\u0026rsquo;s an easier way. Google Tag Manager does some magic for you and lets you check for undefined type using a simple string check. This might sound a bit unorthodox, and it is, but currently it\u0026rsquo;s the fastest way to check for undefined type without having to resort to extra trips through a Custom JavaScript variable.\nThe method itself is really simple. In the trigger where you want to run the check, simply create a new condition which checks if the given variable does not equal undefined, like so:\n  This trigger fires if the value in {{Page Type}} is not undefined.\nNote that this only checks for undefined variables. It doesn\u0026rsquo;t work for other falsy values, such as false, null, 0, or NaN. For these, you would have to run additional checks, and you can even use a regular expression:\n{{Variable}} does not match RegEx (ignore case) ^(undefined|null|0|false|NaN|)$\nAll of these falsy values are resolved to strings in GTM for checking against in triggers. The regular expression above matches all falsy values in JavaScript, and is thus useful as a generic check for potentially invalid values in variables.\nCertain problems with GTM\u0026rsquo;s approach can surface if, for example, you actually have the literal string \u0026quot;undefined\u0026quot; as the value of a variable. It\u0026rsquo;s not an impossible scenario to conjure, since \u0026ldquo;undefined\u0026rdquo; is a word that has functional depth in the English language. There\u0026rsquo;s no way for GTM to distinguish between this valid string value and the undefined value the variable might have, at least not in triggers. So to make the check as robust and unambiguous as possible, you would have to resort to a Custom JavaScript variable after all.\nfunction() { var variableToValidate = {{Some Variable}}; return !!variableToCheckForFalsy; }  The Custom JavaScript variable above returns true if the variable has a valid, non-falsy (i.e. truthy) value, and false otherwise.\nBy the way, I did not make up these ridiculous truthy and falsy terms. They are accepted jargon in programming.\n"
},
{
	"uri": "https://www.simoahava.com/tags/variables/",
	"title": "variables",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/customtask-the-guide/",
	"title": "customTask - The Guide",
	"tags": ["google tag manager", "customtask", "tasks", "google analytics"],
	"description": "Guide to the customTask feature in Universal Analytics. Includes instructions how to use customTask, and how to combine multiple customTask solutions.",
	"content": "Last updated 4 September 2018\nIf you have been reading my blog articles over the past year, you might have noticed a disturbing trend. I\u0026rsquo;ve published 9 articles on customTask since the API was released. It might not sound like much, but I can\u0026rsquo;t think of a single feature in Google Analytics or Google Tag Manager that has so completely convinced me of its usefulness in such a short time.\nThe customTask API is a feature of the Universal Analytics library (used by Google Tag Manager\u0026rsquo;s tags, too). It lets you get and set values from and to the (Measurement Protocol) hit as it\u0026rsquo;s being generated. This is really useful for a couple of reasons, which I\u0026rsquo;ve covered in the previous articles, but I\u0026rsquo;ll go over them briefly in the beginning of this guide, too.\n  Suffice to say that especially for Google Tag Manager, customTask adds a lot of value. With GTM, it\u0026rsquo;s always been quite difficult to access the hit-building process when using tag templates. Luckily, customTask offers a solution to this problem, and at least for this particular developer it\u0026rsquo;s opened a whole new world of Google Analytics customization.\nHowever, in spite of writing all those articles on customTask, I recently realized that I never actually took the time to explain more thoroughly how it works. Also, I haven\u0026rsquo;t yet shared solutions for combining multiple customTask tricks in a single tag. This guide seeks to address these points.\n Be sure to check out my customTask Builder tool as well - it will help you compile the necessary JavaScript for customTask to run in your setup without conflicts!\n How Tasks work When you run the ga('send', 'pageview') command either directly in your site JavaScript or indirectly via Google Tag Manager\u0026rsquo;s Google Analytics tags, you actually request that the Universal Analytics JavaScript library (analytics.js) compile an HTTP request to the Google Analytics servers.\nThe endpoint to which the hits are sent is typically /collect on the Google Analytics collector domain. The process of building and sending hits to the GA servers is also known as Measurement Protocol (MP). In fact, MP is the underlying process used by all GA tracking mechanisms, be they the ga('send'...) JavaScript SDK, Google Tag Manager\u0026rsquo;s GA tags, native SDKs for Android and iOS, and custom-built HTTP requests from point-of-sales systems, for example.\n  When you use the JavaScript SDK, either via ga('send', 'pageview') or the Google Analytics tags in GTM, you are thus initiating a sequence of processes, where the final product is the actual MP request to Google Analytics. This sequence comprises a number of tasks, of which customTask is the first one that is executed.\nHere\u0026rsquo;s a quick recap of what each task does, listed in the order they are applied to the hit.\n   Task Description     customTask No functionality of its own, but can be used to manipulate the model object before it is processed by the other tasks.   previewTask If the request is generated from the pageview of a \u0026ldquo;Top Sites\u0026rdquo; thumbnail in Safari, abort the hit.   checkProtocolTask Aborts the request if the page protocol is not valid (http or https).   validationTask Checks if the fields in the request have valid and expected values. Aborts the request if this is not the case (e.g. trying to send a string as the Event Value).   historyImportTask If there is still legacy tracking running on the site, this task imports information from the old GA library cookies into Universal Analytics. Very useful if the site is migrating to Universal Analytics.   samplerTask If you\u0026rsquo;ve decided to manually sample the users to stay within Google Analytics\u0026rsquo; processing limits, this task aborts the request if the user is sampled out of data collection.   buildHitTask Generates the hitPayload string, which is essentially the list of query parameters and their values passed to the Measurement Protocol request.   sendHitTask Sends the hitPayload in a Measurement Protocol request to the GA servers.   timingTask If you are automatically sampling pages for page speed measurements, this task sends the timing hit to Google Analytics.   displayFeaturesTask If you have enabled display features in GA settings, this task compiles the request to the DoubleClick servers.    Each tasks receives the model object as a parameter. The model object contains all the fields that have been set in the tracker, as well as any enhancements applied in the tasks themselves.\nThe beauty of customTask is that because it runs before any of the other tasks, you can use it to overwrite behavior of these other tasks. For example, if you want to run Google Analytics locally or in a Chrome Extension, you\u0026rsquo;ll need to make sure checkProtocolTask is never run, because it aborts the hit builder if the page protocol is not http or https. The customTask would look like this:\nvar customTask = function(model) { // Prevent checkProtocolTask from running  model.set(\u0026#39;checkProtocolTask\u0026#39;, null); };  As you can see, the task is actually a field in the model object that you can manipulate just as you can manipulate any other field in the model. For example, if you want to grab the tracking ID (UA-XXXXXX-Y) and send it in the Custom Dimension with index 15, you can use the following customTask:\nvar customTask = function(model) { // Get the tracking ID from the model object  var trackingId = model.get(\u0026#39;trackingId\u0026#39;); // Set the tracking ID to Custom Dimension 15  model.set(\u0026#39;dimension15\u0026#39;, trackingId); };  To see what fields are available in the model object, check this developer documentation. Note that it\u0026rsquo;s not complete, since it\u0026rsquo;s missing all the tasks.\nBy running these model interactions with customTask, you have a lot of control over how the task sequence is run. Especially with Google Tag Manager, it would be difficult to run any complicated logic for when and how to manipulate sendHitTask, if you applied it directly to the tag as a field.\nHow to add customTask to your hits To add customTask to your Google Tag Manager tags, you need to create a Custom JavaScript variable which returns the customTask method in the variable body. Practically all my customTask articles show examples of what this Custom JavaScript variable looks like.\nWhen you are ready to add the variable to your tags, you can do it either via a Google Analytics Settings variable (recommended), or by overriding the tag settings directly.\nYou need to scroll down to Fields to set, and add a new field which looks like this:\n  If using the on-page Universal Analytics (analytics.js) snippet, you add the customTask like this, for example:\nvar _customTask = function() { // Set Client ID to Custom Dimension 199  return function(customTaskModel) { customTaskModel.set(\u0026#39;dimension199\u0026#39;, customTaskModel.get(\u0026#39;clientId\u0026#39;)); }; }; ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;); ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, _customTask()); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;);  Do note that you can only set one customTask per tracker / hit / tag. If you want to add more than one \u0026ldquo;trick\u0026rdquo; to the customTask function, you need to write the JavaScript that combines all the different solutions into one coherent whole.\nOr, you can use my customTask Builder tool, which does the dirty work for you.\nThings you can do with customTask As written above, customTask is special for two reasons.\n  It has no special functionality of its own. A task queue can and will run perfectly without customTask defined.\n  It runs before any other task, meaning you have full control over how the queue is processed.\n  With these two things in mind, here\u0026rsquo;s the list of all the customTask tricks I\u0026rsquo;ve written about, with a description of what makes them special.\n1. Set Client ID as a Custom Dimension Link to article: #GTMTips: Use customTask To Access Tracker Values In Google Tag Manager.\nThis is one of the simpler tricks. It doesn\u0026rsquo;t manipulate the task queue itself at all, it simply gets the Client ID from the model object, and then sets it as the value of the specified Custom Dimension.\nIt\u0026rsquo;s an elegant solution to a difficult problem. Without customTask, getting and setting the Client ID in a Google Analytics tag was difficult. If the tag is run for a first-time visitor who hasn\u0026rsquo;t received a Client ID yet, the actual clientId field is not available when the regular tag fields are resolved. A typical hack for this solution was to fetch the Client ID by creating a dummy tracker or by sending the Client ID with an Event tag that fired after the Pageview.\nBut with customTask, there\u0026rsquo;s no need for such unorthodox methods.\n2. Duplicate Google Analytics tags to more than one property Link to article: #GTMTips: Send Google Analytics Tag To Multiple Properties.\nThis is customTask at its purest. The customTask function first stores a reference to sendHitTask in a global variable, after which it overwrites sendHitTask in the current model object with the duplicator code.\nThe duplicator first lets the original hit fire to GA, after which it replaces the Tracking ID in the hitPayload with the Tracking ID of the second property. Then, the global sendHitTask is invoked again, and the hit is sent to the second property with an identical payload.\nNote! The reason the reference to sendHitTask is stored in a global variable is to avoid issues with field multiplication. Let\u0026rsquo;s say you always use this to get the original sendHitTask reference:\nvar originalSendTask = model.get('sendHitTask');\nEach time this customTask code is run, it will get a reference to what is stored in the model object\u0026rsquo;s sendHitTask. Unfortunately, when building standard Ecommerce hits, or when automatically sampling timing hits, the tag is automatically sent more than once. Thus the reference to the original sendHitTask is recursively augmented with each iteration of model.set('sendHitTask');, resulting in code being executed erratically.\nBy having a global variable store the reference to the original sendHitTask, this recursive problem is avoided.\n3. Track offline users in Google Analytics Link to article: Track Users Who Are Offline In Google Analytics.\nThis is all David Vallejo. His solution tackles the problem of lost internet connectivity and hits that are aborted because of this. It uses customTask to check if the user is offline, and if they are, the hits are stored in a batch queue. Once connectivity is restored, the queue is processed, and the hits are finally sent to GA.\n4. Remove PII from GA payloads Link to article: #GTMTips: Remove PII From Google Analytics Hits.\nWith GDPR coming soon, covering all your bases with regard to private data is a good idea. Fixing potential PII leaks before they hit Google Analytics servers is a good method as any to make sure you\u0026rsquo;re complying with Google Analytics\u0026rsquo; Terms of Service as well as the increasingly stricter regulations for sending and collecting personal data.\n  Here, customTask takes the hit payload before it is sent to GA, and purges it of all matches against any given regular expressions. These regular expressions can be built to match things like email addresses and social security numbers.\n5. Fix site speed sampling messing with your Custom Metrics Link to article: Prevent Site Speed Sampling Rate From Skewing Custom Dimensions And Custom Metrics.\nThis is a fairly simple trick, but it fixes a potential issue with Custom Metric data being skewed by the automatically collected page timing hits. The issue is that the timing hit copies all parameters from the Page View tag that is being sampled, and thus any Custom Dimensions and Custom Metrics are sent more than once, too.\nHere, customTask is attached to the Page View tag, where it checks if the hit type is timing (due to the automatically generated timing hit), and if it is, it removes all references to Custom Dimensions and Metrics from the request.\n6. Respect opt-out Link to article: #GTMTips: Respect Opt-Out From Advertising And Analytics.\nA simple trick, again. This time customTask checks if the user has specific opt-out settings in the browser, and if they do, it aborts the requests to Google Analytics and DoubleClick. You\u0026rsquo;ll need to manually create the setting itself, be it a pop-up that stores the opt-out in a browser cookie or something, but once you\u0026rsquo;ve done it, this solution makes it easy to enact the opt-out programmatically.\n7. Send details about Optimize experiments to Google Analytics Link to article: Send Event And Custom Dimension If Google Optimize Experiment Is Running.\nThis trick tackles an issue with how Google Optimize data is being reported in Google Analytics. Basically, the Experiment Name and Experiment ID dimensions are scoped to the user for the duration of the experiment. Thus it\u0026rsquo;s not possible to get a segment of the actual sessions when the user was actively participating in an experiment page.\nThe customTask checks for certain features of the Page View tag it is attached to, and if these features are found, it updates a Custom Dimension in the hit payload with details about the experiment. You can then segment your GA data with these Custom Dimensions to see sessions where the user was actually active on an experiment page.\n8. Auto-link domains with regular expressions Link to article: #GTMTips: Auto Link Domains With Regex.\nWith this trick, customTask brings Google Tag Manager to feature parity with analytics.js. Regular, on-page Google Analytics tracking lets you use regular expressions with the Auto-Link Domains plugin. Google Tag Manager only accepts string values.\nHere, customTask actually applies the Auto-Link plugin to the tracker object itself, passing the array of regular expressions to the plugin. This way, domains can be automatically linked with cross-domain parameters based on a regular expression match, rather than an exhaustive list of strings.\n9. Send Google Analytics payloads to Snowplow Link to article: #GTMTips: Automatically Duplicate Google Analytics Hits To Snowplow.\nI\u0026rsquo;m a huge fan of Snowplow, mainly because it\u0026rsquo;s open-source and tackles many difficult issues having to do with sessionization and tracking schemas. This trick uses customTask to replicate what the \u0026ldquo;official\u0026rdquo; Snowplow Google Analytics plugin does.\n  Google Tag Manager doesn\u0026rsquo;t support adding plugins to tags, so using customTask to replicate plugin features is as good a solution as any.\n10. Send Google Analytics payload length as a Custom Dimension Link to article: Send Google Analytics Payload Length As Custom Dimension.\nThis tip lets you collect the payload length of each Google Analytics request as a Custom Dimension in Google Analytics.\nThe maximum length of a Google Analytics payload is 8192 bytes. It\u0026rsquo;s useful to check if you\u0026rsquo;re approaching this value with some of your hits, because if the payload length exceeds this, the hit is never sent to Google Analytics.\nBe sure to check my tip below on how to automatically reduce the payload length.\n11. Send Hit Type as a Custom Dimension Link to article: #GTMTips: Add Hit Type As A Custom Dimension.\nWith this trick, you can send the hit type (e.g. pageview, event, timing) as a Custom Dimension with every hit to which this customTask is attached.\nVery useful for debugging, since you can now build segments where you directly refer to the hit type, which is not available (surprisingly) as a default dimension in Google Analytics.\n12. Automatically reduce payload length Link to article: Automatically Reduce Google Analytics Payload Length.\nWith this customTask, the payload sent to Google Analytics is automatically parsed and, if necessary, reduced to go under the maximum payload length of 8192 bytes.\nIt\u0026rsquo;s very typical especially with Enhanced Ecommerce to send very long payloads to Google Analytics. Unfortunately, if these payloads exceed the maximum length, the hits are never sent and key data will thus be missing from your data set.\nYou can use this trick to recursively parse the payload and drop unnecessary parameters from it, which only serve to waste valuable space under the length cap.\n13. Create and update a session cookie Link to article: Create And Update Google Analytics Session Timeout Cookie.\nThis trick creates (or updates) a browser cookie which expires after a timeout you have configured (30 minutes by default).\nEach hit that uses this customTask will refresh the cookie very time the hit is fired.\nThus, if this cookie exists in the browser, it\u0026rsquo;s an indication that there might well be a Google Analytics session active for the current user.\nYou can use the cookie to block event hits from being dispatched to Google Analytics if there is no active session, for example.\n14. Tracking cross-domain iframes Link to article: Tracking Cross-Domain Iframes - Upgraded Solution.\nHere we\u0026rsquo;ll update an older solution of mine by using customTask and a window.setInterval() polling script together to great effect.\nThe idea is that when the hit is built, the customTask looks for the given iframe on the page, and if it\u0026rsquo;s found within a configured timeout, the iframe is reloaded with cross-domain tracking parameters.\nThis is a more elegant way to approach the problem of cross-domain iframes, because it skips past the race condition of running the script before the iframe has been loaded on the page, and it also uses the correct tracker object instead of just the first tracker on the page.\n15. Prevent duplicate transactions from being sent Link to article: Prevent Duplicate Transactions In Google Analytics With customTask.\nAgain an update to an old trick.\nIn this solution, customTask stores all sent TransactionIDs in the browser. Once a transaction hit is queued for dispatching to GA, the customTask checks if the hit\u0026rsquo;s Transaction ID is found in this list of IDs that have already been sent. If a match is made, the hit is blocked, thus preventing duplicated data from being sent to GA.\nIf no match is made, the hit is sent normally, and the Transaction ID is written into the list of sent IDs, so that any future attempt to send a transaction with this ID will be blocked.\nContrary to the older solution, this upgraded method will block Enhanced Ecommerce as well. For Standard Ecommerce, you can (and should) still use the customTask, but you\u0026rsquo;ll still need to use triggers and exceptions. This is all detailed behind the link above.\n16. Obfuscate and duplicate Google Analytics data Link to article: Obfuscate And Duplicate Google Analytics Data.\nThis is a rather complex little script which duplicates all the Google Analytics data to which the customTask is added to, and dispatches it to the Tracking ID you provide in the configuration.\nThe catch is that in addition to duplicating it, the data is also obfuscated. All the strings are converted to words in a finite set, and this is done predictably (so any given string is always converted to the same word). All prices are also modified by a randomized percent value.\nThe point is that you can use this to create a demo or training GA data set out of any actual and real data set without revealing where the data comes from.\n17. Use localStorage for Client ID persistence in Google Analytics Link to article: #GTMTips: Use localStorage For Client ID Persistence In Google Analytics.\nHere, I tackle the latest attack on cookie storage by Apple: Intelligent Tracking Prevention 2.1.\nBy using localStorage to save the Client ID, you can circumvent the policy of limiting cookies set with document.cookie to a maximum expiration of 7 days.\nIt\u0026rsquo;s not perfect, and it\u0026rsquo;s definitely more a tech demo rather than a fully functional solution to solve your enterprise web analytics woes, but the functionality should be sound. There are some hiccups with regard to cross-domain tracking, so remember to revisit the article if you find that something is broken - I\u0026rsquo;m going to try to keep it up-to-date if Google decides to update the cross-domain linker parameter format, for example.\nPutting it all together (literally)  NOTE! I\u0026rsquo;ve built a tool which you can use to compile the necessary JavaScript automatically. You\u0026rsquo;ll still want to read through this chapter, though, as it explains why building the script can be a chore.\n If you like customTask as much as I do, chances are you\u0026rsquo;ll want to use more than one trick listed above (and elsewhere) in your tags. There\u0026rsquo;s a catch though. You need to respect the basic features of JavaScript, mainly:\n  You can only set a single field once per tag, so you can only set one customTask field per tag.\n  You can only set a single model parameter once per tag, so you can only set one sendHitTask attribute per tag, for example.\n  So, let\u0026rsquo;s say you want to do both tricks (2) (Send GA hit to more than one property) and (4) (Remove PII from GA hits). You might be tempted to try this:\nfunction() { return function(model) { // Specify the PII regular expressions  var piiRegEx = [{ name: \u0026#39;EMAIL\u0026#39;, regex: /.{4}@.{4}/g }]; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_originalSendTask\u0026#39;; var originalSendTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); // Clear the payload from PII:  model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { ... }); // Duplicate the hit to another tracking ID:  model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { ... }); }; }  Unfortunately, this would not work. You are setting the sendHitTask field twice, meaning the second one is the only one that counts (since it overwrites the first set).\nInstead, you need to apply some JavaScript chops and combine both of these solutions in the same sendHitTask. It\u0026rsquo;s not exactly trivial, and there\u0026rsquo;s no textbook method for doing it. You\u0026rsquo;ll need to figure out the order in which the code should be processed.\nIn this example, we want the PII purge to happen before any hits are sent or duplicated. So the final code would look something like this:\nfunction() { return function(model) { // Specify the PII regular expressions  var piiRegEx = [{ name: \u0026#39;EMAIL\u0026#39;, regex: /.{4}@.{4}/g }]; // Specify the GA tracking ID of the property to which you want to duplicate the hits  var newTrackingId = \u0026#39;UA-12345-2\u0026#39;; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_originalSendTask\u0026#39;; var originalSendTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); var i, hitPayload, parts, val, oldTrackingId; model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { // Clear the payload of PII:  hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;).split(\u0026#39;\u0026amp;\u0026#39;); for (i = 0; i \u0026lt; hitPayload.length; i++) { parts = hitPayload[i].split(\u0026#39;=\u0026#39;); // Double-decode, to account for web server encode + analytics.js encode  val = decodeURIComponent(decodeURIComponent(parts[1])); piiRegEx.forEach(function(pii) { val = val.replace(pii.regex, \u0026#39;[REDACTED \u0026#39; + pii.name + \u0026#39;]\u0026#39;); }); parts[1] = encodeURIComponent(val); hitPayload[i] = parts.join(\u0026#39;=\u0026#39;); } sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload.join(\u0026#39;\u0026amp;\u0026#39;), true); originalSendTask(sendModel); // Rewrite the tracking ID  hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); oldTrackingId = new RegExp(sendModel.get(\u0026#39;trackingId\u0026#39;), \u0026#39;gi\u0026#39;); sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload.replace(oldTrackingId, newTrackingId), true); originalSendTask(sendModel); }); }; }  The code is fairly complex, and complexity increases with each distinct trick you want to run. This is an unfortunate consequence of the limitations of JavaScript and the fact that there is only one customTask to manipulate.\nIn the end, it\u0026rsquo;s really up to your JavaScript skills. You\u0026rsquo;ll need to be aware of how the browser runs the code line-by-line, and thus you\u0026rsquo;ll need to make sure that whatever you want to run first is also executed first by the browser. That\u0026rsquo;s why in the example above I have the PII purge running before the hit duplication. If the PII purge happened AFTER the hit is duplicated, it would be redundant, since it would still allow PII to potentially flow to Google Analytics.\nSummary As I wrote in the beginning, customTask is one of the nicest features to emerge in Google Analytics in a long time. With Google Tag Manager, it\u0026rsquo;s a real powerhouse of customization.\nIt\u0026rsquo;s a good idea to read about the Task queue in general. Understanding how tasks work will give you great insight into how analytics.js compiles and dispatches hits to Google Analytics. Once you have a good grasp on all the different moving parts of the Tasks queue, you should be able to come up with cool customTask solutions of your own, which you can then share in the comments of this article, for example!\n"
},
{
	"uri": "https://www.simoahava.com/tags/tasks/",
	"title": "tasks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/edit-google-analytics-tag-settings/",
	"title": "#GTMTips: Edit Google Analytics Tag Settings",
	"tags": ["google tag manager", "gtmtips", "google analytics", "google analytics settings"],
	"description": "You can find the old interface for editing individual Google Analytics tag settings by checking the &#34;Enable overriding settings in this tag&#34; toggle.",
	"content": "When the Google Analytics Settings variable was introduced in May 2017, it resulted in a significant change in the Google Analytics tag user interface in Google Tag Manager. The default UI for editing a tag was stripped down of all GA-specific settings, and the new Google Analytics Settings drop-down was the replacement.\nUnfortunately, the bulk of Google Tag Manager articles online (including those on this blog) still refer to the old interface in screenshots and instructions. This #GTMTips article is a very quick tip to show you how to reveal tag-specific settings without using a Google Analytics Settings variable.\nTip 72: Edit Google Analytics tag settings   If you want to reveal the More Settings list, which comprises items like Fields to Set, Custom Dimensions and Ecommerce settings (among others), all you need to do is check the box titled:\n Enable overriding settings in this tag.\n This setting means that even if the tag uses a Google Analytics Settings variable as the basis for all its settings, any changes you make in the tag-specific settings can be used to override those set by the variable.\nSimilarly, if you don\u0026rsquo;t want to use a Google Analytics Settings variable, you must check this box to edit the tag-specific fields.\nFor fields that have free text input, such as Fields to Set and Custom Dimensions, any rows you add will override the equivalent settings or dimensions you set in the Google Analytics Settings variable.\nSome fields such as Tracking ID and all the fields that have a drop-down list for value selection (e.g. Enable Enhanced Ecommerce Features) let you either inherit the value from the Google Analytics Settings variable (default option), or you can override the settings variable with some other value.\n  To recap, just check Enable overriding settings in this tag if you want to reveal the hidden More Settings menu in your tags. I would recommend, however, that you take a look at the Google Analytics Settings variable, and take the time to migrate your Google Analytics tags to use it. It will make your life a whole lot easier, especially if managing a boatload of GA tags!\n"
},
{
	"uri": "https://www.simoahava.com/tags/google-analytics-settings/",
	"title": "google analytics settings",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/fix-missing-page-view-broken-triggers/",
	"title": "#GTMTips: Fix Missing Page View Event And Broken Triggers",
	"tags": ["google tag manager", "gtmtips", "triggers", "datalayer"],
	"description": "If you can&#39;t see the Page View event in your Preview mode list, and GTM&#39;s triggers work erratically, you most likely have a broken dataLayer implementation.",
	"content": "Google Tag Manager should be relatively easy to implement. Just paste the container snippet to the \u0026lt;head\u0026gt; of the page and you\u0026rsquo;re good to go! However, at some point you\u0026rsquo;ll want to configure the dataLayer structure, too (read more about dataLayer here). There are two ways to do it: the right way and the wrong way.\nIn this article, we\u0026rsquo;ll see what happens if you do it the wrong way, how to identify the issue, and how to fix it.\nTip 71: Page View event missing, and GTM\u0026rsquo;s triggers don\u0026rsquo;t work?   If you open Preview mode on your site, and look at the list of events in the left-hand side navigation, you should always see the following three GTM default events:\n  Page View - This is the event pushed into dataLayer in Google Tag Manager\u0026rsquo;s container snippet (event name is gtm.js).\n  DOM Ready - This is the event pushed into dataLayer once the page HTML has been rendered by the browser (event name is gtm.dom).\n  Window Loaded - This is the event pushed into dataLayer once the page and all linked resources have completed load, execution, and render (event name is gtm.load).\n  You should always see (2) and (3) - there\u0026rsquo;s very little you can do to mess these up. But it\u0026rsquo;s possible you won\u0026rsquo;t see the Page View event. If you don\u0026rsquo;t it means you\u0026rsquo;ve messed up the dataLayer implementation.\nGoogle Tag Manager establishes the dataLayer construct with its own .push() method in the JavaScript library that represents your container. The gtm.js event is pushed into dataLayer in the container snippet, and it is used to trigger any tags that use the Page View or All Pages triggers.\nThe main reason for not seeing the Page View events is simple. You have overwritten the dataLayer modified in the container snippet with a brand new array. You do it like this:\n\u0026lt;!-- Google Tag Manager container snippet here --\u0026gt; (function...) \u0026lt;!-- Google Tag Manager container snippet ends --\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script\u0026gt; var dataLayer = [{ someVariable: \u0026#39;someValue\u0026#39; }]; \u0026lt;/script\u0026gt; See the problem? By using the syntax var dataLayer = ..., you are resetting the dataLayer variable to a new Array, thus overwriting anything that was in it before (such as Google Tag Manager\u0026rsquo;s own listener). Since you overwrite dataLayer, it no longer works properly with GTM, and a typical symptom is that GTM\u0026rsquo;s triggers don\u0026rsquo;t work anymore, either. So if you have a Click / All Elements trigger on the site and nothing is pushed to dataLayer when you click around, chances are you\u0026rsquo;ve overwritten GTM\u0026rsquo;s dataLayer with your reinitialization.\nHow to fix it? Simple. Always check for the existence of dataLayer before adding items to it, and always add items to dataLayer only with the push() command.\n// WRONG! NEVER USE THIS: var dataLayer = [{ pageType: \u0026#39;home\u0026#39; }]; // CORRECT! ALWAYS USE THIS: window.dataLayer = window.dataLayer || []; window.dataLayer.push({ pageType: \u0026#39;home\u0026#39; });  This is a popular topic on this blog and many others simply because it\u0026rsquo;s so easy to screw it up. It doesn\u0026rsquo;t help that somewhere in GTM\u0026rsquo;s documentation there are still instructions to use the declarative method for setting up dataLayer.\nAny other typical Google Tag Manager mistakes you can think of, which have been around for a long time, and probably will surface for a long time to come?\n"
},
{
	"uri": "https://www.simoahava.com/tags/amazon/",
	"title": "amazon",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/aws/",
	"title": "aws",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/",
	"title": "Snowplow: Full Setup With Google Analytics Tracking",
	"tags": ["google tag manager", "snowplow", "amazon", "aws"],
	"description": "Step-by-step process for setting up a Snowplow analytics pipeline with data pulled in from your Google Analytics trackers.",
	"content": "A recent guide of mine introduced the Google Analytics adapter in Snowplow. The idea was that you can duplicate the Google Analytics requests sent via Google Tag Manager and dispatch them to your Snowplow analytics pipeline, too. The pipeline then takes care of these duplicated requests, using the new adapter to automatically align the hits with their corresponding data tables, ready for data modeling and analysis.\nWhile testing the new adapter, I implemented a Snowplow pipeline from scratch for parsing data from my own website. This was the first time I\u0026rsquo;d done the whole process from end-to-end myself, so I thought it might be prudent to document the process for the benefit of others who might want to take a jab at Snowplow but are intimidated by the moving parts.\n  And make no mistake. There are plenty of moving parts. Snowplow leverages a number of Amazon Web Services components, in addition to a whole host of utilities of its own. It\u0026rsquo;s not like setting up Google Analytics, which is, at its most basic, a fairly rudimentary plug-and-play affair.\n Image source: https://goo.gl/D2f3xi  Take this article with a grain of salt. It definitely does not describe the most efficient or cost-effective way to do things, but it should help you get started with Snowplow, ending up with a data store full of usable data for modeling and analysis. As such, it\u0026rsquo;s not necessarily a guide rather than a description of the steps I took, in good and bad.\n  WARNING: If you DO follow this guide step-by-step (which makes me very happy), please do note that there will be costs involved. For example, my current, very light-weight setup, is costing me a couple of dollars a day to maintain, with most costs incurred by running the collector on a virtual machine in Amazon\u0026rsquo;s cloud. Just keep this in mind when working with AWS. It\u0026rsquo;s unlikely to be totally free, even if you have the free tier for your account.\nTo start with I ended up needing the following things to make the pipeline work:\n  The Linux/Unix command line (handily accessible via the Terminal application of Mac OS X).\n  Git client - not strictly necessary but it makes life easier to clone the Snowplow repo and work with it locally.\n  A new Amazon Web Services account with the introductory free tier (first 12 months).\n  A credit card - even with the free tier the pipeline is not free.\n  A domain name of my own (I used gtmtools.com) whose DNS records I can modify.\n  A Google Analytics tag running through Google Tag Manager.\n  A lot of time.\n    The bullets concerning money and custom domain name might be a turn-off to some.\nYou might be able to set up the pipeline without a domain name by using some combination of Amazon CloudFront and Route 53 with Amazon\u0026rsquo;s own SSL certificates, but I didn\u0026rsquo;t explore this option.\nAnd yes, this whole thing is going to cost money. As I wrote in the beginning, I didn\u0026rsquo;t follow the most cost-effective path. But even if I did, it would still cost a dollar or something per day to keep this up and running. If this is enough of a red flag for you, then take a look at what managed solutions Snowplow is offering. This article is for the engineers out there who want to try building the whole thing from scratch.\nWhy Snowplow? Why do this exercise at all? Why even look towards Snowplow? The transition from the pre-built, top-down world of Google Analytics to the anarchy represented by Snowplow\u0026rsquo;s agnostic approach to data processing can be daunting.\nLet me be frank: Snowplow is not for everyone. Even though the company itself offers managed solutions, making it as turnkey as it can get, it\u0026rsquo;s still you building an analytics pipeline to suit your organization\u0026rsquo;s needs. This involves asking very difficult questions, such as:\n  What is an \u0026ldquo;event\u0026rdquo;?\n  What constitutes a \u0026ldquo;session\u0026rdquo;?\n  Who owns the data?\n  What\u0026rsquo;s the ROI of data analytics?\n  How should conversions be attributed?\n  How should I measure users across domains and devices?\n  If you\u0026rsquo;ve never asked one of these (or similar) questions before, you might not want to look at Snowplow or any other custom-built setup yet. These are questions that inevitably surface at some point when using tools that give you very few configuration options.\nAt this point I think I should add a disclaimer. This article is not Google Analytics versus Snowplow. There\u0026rsquo;s no reason to bring one down to highlight the benefits of the other. Both GA and Snowplow have their place in the world of analytics, and having one is not predicated on the absence of the other.\nThe whole idea behind the Google Analytics plugin, for example, is that you can duplicate tracking to both GA and to Snowplow. You might want to reserve GA tracking for marketing and advertising attribution, as Google\u0026rsquo;s backend integrations are still unmatched by other platforms. You can then run Snowplow to collect this same data so that you\u0026rsquo;ll have access to an unsampled, raw, relational database you can use to enrich and join with your other data sets.\nSnowplow is NOT a Google Analytics killer. They\u0026rsquo;re more like cousins fighting together for the honor of the same family line, but occasionally quarreling over the inheritance of a common, recently deceased relative.\nWhat we are going to build Here\u0026rsquo;s a diagram of what we\u0026rsquo;re hopefully going to build in this article:\n  I wonder why no one\u0026rsquo;s hired me as a designer yet\u0026hellip;\nThe process will cover the following steps.\n  The website will duplicate the payloads sent to Google Analytics, and send them to a collector written with Clojure.\n  The collector runs as a web service on AWS Elastic Beanstalk, to which traffic is routed and secured with SSL from my custom domain name using AWS Route 53.\n  The log data from the collector is stored in AWS S3.\n  A utility is periodically executed on my local machine, which runs an ETL (extract, transform, load) process using AWS EMR to enrich and \u0026ldquo;shred\u0026rdquo; the data in S3.\n  The same utility finally stores the processed data files into relational tables in AWS Redshift.\n  So the process ends with a relational database that has all your collected data populated periodically.\nStep 0: Register on AWS and setup IAM roles What you need for this step  You\u0026rsquo;ll just need a credit card to register on AWS. You\u0026rsquo;ll get the benefits of a free tier, but you\u0026rsquo;ll still need to enable billing.  Register on AMR The very first thing to do is register on Amazon Web Services and setup an IAM (Identity and Access Management) User that will run the whole setup.\nSo browse to https://aws.amazon.com/, and select the option to create a free account.\n  The free account gives you access to the free tier of services, some of which will help keep costs down for this pipeline, too.\nCreate an Identity and Account Management (IAM) user Once you\u0026rsquo;ve created the account, you can do the first important thing in setting up the pipeline: create an IAM User. We\u0026rsquo;ll be following Snowplow\u0026rsquo;s own excellent IAM setup guide for these steps.\nIn the Services menu, select IAM from the long list of items.\n    In the left-hand menu, select Groups.\n  Click the Create New Group in the view that opens.\n  Name the group snowplow-setup.\n  Skip the Attach Policy step for now by clicking the Next Step button.\n  Click Create Group.\n  Now in the left-hand menu, select Policies.\n  Click Create Policy.\n  Select the JSON tab, and replace the contents with the following:\n  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;acm:*\u0026#34;, \u0026#34;autoscaling:*\u0026#34;, \u0026#34;aws-marketplace:ViewSubscriptions\u0026#34;, \u0026#34;aws-marketplace:Subscribe\u0026#34;, \u0026#34;aws-marketplace:Unsubscribe\u0026#34;, \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:*\u0026#34;, \u0026#34;elasticbeanstalk:*\u0026#34;, \u0026#34;elasticloadbalancing:*\u0026#34;, \u0026#34;elasticmapreduce:*\u0026#34;, \u0026#34;es:*\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;rds:*\u0026#34;, \u0026#34;redshift:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;sns:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }   Next, click Review Policy.\n  Name the policy snowplow-setup-policy-infrastructure.\n  Finally, click Create Policy.\n  Now go back to Groups from the left-hand menu, and click the snowplow-setup group name to open its settings.\n    Switch to the Permissions tab and click Attach Policy.\n  From the list that opens, select snowplow-setup-policy-infrastructure and click Attach Policy.\n    Now select Users from the left-hand menu, and click the Add user button.\n  Name the user snowplow-setup.\n  Check the box next to Programmatic access.\n  Click Next: Permissions.\n  With Add user to group selected, check the box next to snowplow-setup, and click the Next: Review button at the bottom of the page.\n  Finally, click Create user.\n  The following screen will show you a success message, and your user with an Access Key ID and Secret Access Key (click Show to see it) available. At this point, copy both of these somewhere safe. You will need them soon, and once you leave this screen you will not be able to see the secret key anymore. You can also download them as a CSV file by clicking the Download .csv button.\nYou have now created a user with which you will set up your entire pipeline. Once everything is set up, you will create a new user with fewer permissions, who will take care of running and managing the pipeline.\nCongratulations, step 0 complete!\nWhat you should have after this step   You should have successfully registered a new account on AWS.\n  You should have a new IAM user named snowplow-setup with all the access privileges distributed by the custom policy you created.\n  Step 1: The Clojure collector What you need for this step  A custom domain name and access to its DNS configuration  Getting started This is going to be one of the more difficult steps to do, since there\u0026rsquo;s no generic guide for some of the things that need to be done with the collector endpoint. If you want, you can look into the CloudFront Collector instead, because that runs directly on top of S3 without needing a web service to collect the hits. However, it doesn\u0026rsquo;t support the Google Analytics hit duplicator, which is why in this article we\u0026rsquo;ll use the Clojure collector.\nThe Clojure collector is basically a web endpoint to which you will log the requests from your site. The endpoint is a scalable web service running on Apache Tomcat, which is hosted on AWS\u0026rsquo; Elastic Beanstalk. The collector has been configured to automatically log the Tomcat access logs directly into AWS S3 storage, meaning whenever your site sends a request to the collector, it logs this request as an access log entry, ready for the ETL process that comes soon after.\nBut let\u0026rsquo;s not get ahead of ourselves.\nSetting up the Clojure collector The first thing you\u0026rsquo;ll need to do is download the Clojure collector WAR file and store it locally in a temporary location (such as your Downloads folder).\nYou can download the binary by following this link. It should lead you to a file that looks something like clojure-collector-1.X.X-standalone.war.\nOnce you\u0026rsquo;ve downloaded it, you can set up the Elastic Beanstalk application.\nIn AWS, open the Services menu again, and select Elastic Beanstalk.\nAt this point, you\u0026rsquo;ll also want to make sure that any AWS services you use in this pipeline are located in the same region. There are differences to what services are supported in each region. I built the pipeline in EU (Ireland), and the Snowplow guide itself uses US West (Oregon). Click the region menu and choose the region you want to use.\n    Next, click the Create New Application link in the top-right corner of the page (just below the region selector).\n  Give the application a descriptive name (e.g. Snowplow Clojure Collector) and description (e.g. I love Kamaka HF-3), and then click Next.\n  In the New Environment view, select Create web server.\n     In the Environment Type view, set the following:   Predefined configuration: Tomcat\nEnvironment type: Single instance\n   Then, click Next.\n  In the Application Version view, select Upload your own, click Choose file, and find the WAR file you downloaded earlier in this chapter. Click Next to upload the file.\n      In the Environment Info view, you\u0026rsquo;ll need to set the Environment name, which is then used to generate the Environment URL (which, in turn, will be the endpoint URL receiving the collector requests).\n  Remember to click Check availability for the URL to make sure someone hasn\u0026rsquo;t grabbed it yet. Click Next once you\u0026rsquo;re done.\n      In Additional Resources, you can leave both options unchecked for now, and click Next.\n  In Configuration Details, select m1.small as the instance type. You can leave all the other options to their default settings. Click Next.\n      No need to add any Environment Tags, so click Next again.\n  In the Permissions view, by clicking Next, AWS assigns default roles to Instance profile and Service role, so that\u0026rsquo;s fine.\n  Finally, you can take a quick look at what you\u0026rsquo;ve done in the Review view, before clicking the Launch button.\n  At this point, you\u0026rsquo;ll see that AWS is firing up your environment, where the Clojure collector WAR file will start running the instant the environment has been created.\n  Once the environment is up and running, you can copy the URL from the top of the view, paste it into the address bar of your browser, and add the path /i to its end, so it ends up something like:\nhttp://simoahava-snowplow-collector.eu-west-1.elasticbeanstalk.com/i\nIf the collector is running correctly, you should see a pixel in the center of the screen. By clicking the right mouse button and choosing Inspect (in the Google Chrome browser), you should now find a cookie named sp in the Applications tab. If you do, it means the collector is working correctly.\n  Congratulations! You\u0026rsquo;ve set up the collector.\nHowever, we\u0026rsquo;re not done here yet.\nEnable logging to S3 Automatically logging the Tomcat access logs to S3 storage is absolutely crucial for this whole pipeline. The batch process looks for these logs when sorting out the data into queriable chunks. Access logs are your typical web server logs, detailing all the HTTP requests made to the endpoint, with request headers and payloads included.\nTo enable logging, you\u0026rsquo;ll need to edit the Elastic Beanstalk application you just created. So, once the endpoint is up and running, you can open it by clicking the application name while in the Elastic Beanstalk service front page.\n    Next, select Configuration from the left-hand menu.\n  Click the cogwheel in the box titled Software Configuration.\n     Under Log Options, check the box next to Enable log file rotation to Amazon S3. If checked, service logs are published to S3.    This is a crucial step, because it will store all the access logs from your endpoint requests to S3, ready for ETL.\nClick Apply to apply the change.\nSet up the load balancer Next, we need to configure the Elastic Beanstalk environment for SSL.\nBefore you do anything else, you’ll need to switch from a single instance to a load-balancing, auto-scaling environment in Elastic Beanstalk. This is necessary for securing the traffic between your domain name and the Clojure collector.\nIn the AWS Services menu, select Elastic Beanstalk, and then click your application name in the view that opens.\n    In the next view, select Configuration in the left-hand menu.\n  Click the cogwheel in the box titled Scaling.\n  From the Environment type menu, select Load balancing, auto scaling, and then click the Apply button, and Save in the next view.\n    You now have set up the load balancer.\nRoute traffic from your custom domain name to the load balancer Next, we\u0026rsquo;ll get started on routing traffic from your custom domain name to this load balancer.\n  Open the Services menu in the AWS console, and select Route 53 from the list.\n  In the view that opens, click Create Hosted Zone.\n  Set the Domain Name to the domain whose DNS records you want to delegate to Amazon. I chose collector.gtmtools.com. Leave Type as Public Hosted Zone, and click Create.\n     In the view that opens, you\u0026rsquo;ll see the settings for your Hosted Zone. Make note of the four NS records AWS has assigned to your domain name. You\u0026rsquo;ll need these in the next step.      Next, you\u0026rsquo;ll need to go wherever it is you manage your DNS records. I use GoDaddy.\n  You need to add the four NS addresses in the AWS Hosted Zone as NS records in the DNS settings of your domain name. This is what the modifications would look like in my GoDaddy control panel:\n    As you can see, there are four NS records with host collector (for collector.gtmtools.com), each pointing to one of the four corresponding NS addresses in the AWS Hosted Zone. I set the TTL to the shortest possible GoDaddy allows, which is 600 seconds. That means that within 10 minutes, the Hosted Zone name servers should respond to collector.gtmtools.com.\nYou can test this with a service such as https://dig.whois.com.au/dig/, or any similar service that lets you check DNS records. Once the DNS settings are updated, you can increase the TTL to something more sensible, such as 1 hour, or even 1 day.\nNow that you\u0026rsquo;ve set up your custom domain name to point to your Route 53 Hosted Zone, there\u0026rsquo;s just one step missing. You\u0026rsquo;ll need to create an Alias record in the Hosted Zone, which points to your load balancer. That way when typing the URL collector.gtmtools.com into the browser address bar, the DNS record first directs it to your Hosted Zone, where a new A record shuffles the traffic to your load balanced Clojure collector endpoint. Phew!\n  So, in the Hosted Zone you\u0026rsquo;ve created, click Create Record Set.\n  In the overlay that opens, leave the Name empty, since you want to apply the name to the root domain of the NS (collector.gtmtools.com in my case). Keep A - IPv4 address as the Type, and select Yes for Alias.\n  When you click the Target field, a drop-down list should appear, and your load balancer should be selectable under the ELB Classic load balancers heading. Select that, and then click Create.\n    Now if you visit http://collector.gtmtools.com/i, you should see the same pixel response as you got when visiting the Clojure collector endpoint directly. So your domain name routing works!\nBut we\u0026rsquo;re STILL not done here.\nSetting up HTTPS for the collector To make sure the collector is secured with HTTPS, you will need to generate a (free) AWS SSL certificate for it, and apply it to the Load Balancer. Luckily this is easy to do now that we\u0026rsquo;re working with Route 53.\n  The first thing to do is generate the SSL certificate. In Services, find and select the AWS Certificate Manager. Click Get started to, well, get started.\n  Type the domain name you want to apply the certificate to in the relevant field. I wrote collector.gtmtools.com.\n  Click Next when ready.\n      In the next step you need to choose a validation method. Since we\u0026rsquo;ve delegated DNS of collector.gtmtools.com to Route 53, I chose DNS Validation without hesitation.\n  Then click Review, and then Confirm and request.\n  Validation is done in the next view. With Route 53, this is really easy. Just click Create record in Route 53, and Create in the overlay that opens. Amazon takes care of validation for you!\n    After clicking Create, you should see a Success message, and you can click the Continue button in the bottom of the screen. It might take up to 30 minutes for the certificate to validate, so go grab a cup of coffee or something! We still have one more step left\u0026hellip;\nSwitch the load balancer to support HTTPS You\u0026rsquo;ll still need to switch your load balancer to listen for secure requests, too.\n  In Services, open Elastic Beanstalk, click your application name in the view that opens, and finally click Configuration in the left-hand menu. You should be able to do this stuff in your sleep by now!\n  Next, scroll down to the box titled Load Balancing and click the cogwheel in it.\n  In the view that opens, set Secure listener port to 443, and select the SSL certificate you just generated from the menu next to SSL Certificate ID. Click Apply when ready.\n    All done! At this point, you might also want to take a look at this Snowplow guide for configuring the collector further (e.g. applying proper scaling settings to the load balancer).\nThe process above might seem convoluted, but there\u0026rsquo;s a certain practical logic to it all. And once you have the whole pipeline up and running, it will be easier to understand how things proceed from the S3 storage onwards.\nSetting up the custom domain name is a bit of chore, though. But if you use Route 53, most of the things are either automated for you or taken care of with the click of a button.\nWhat you should have after this step   The Clojure collector application running in an Elastic Beanstalk environment.\n  Your own custom domain name pointing at the router configured in Amazon Route 53.\n  An SSL secured load balancer, to which Route 53 diverts traffic to your custom domain name.\n  Automatic logging of the collector Tomcat logs to S3. The bucket name is something like elasticbeanstalk-region-id, and you can find it by clicking Services in the AWS navigation and choosing S3. The logs are pushed hourly.\n  Step 2: The tracker You\u0026rsquo;ll need to configure a tracker to collect data to the S3 storage.\nThis is really easy, since you\u0026rsquo;re of course using Google Analytics, and tracking data to it using Google Tag Manager tags.\nNavigate to my recent guide on setting up the duplicator, do what it says, and you\u0026rsquo;ll be set. Remember to change the endpoint variable in the Custom JavaScript variable to the domain name you set up in the previous chapter (https://collector.gtmtools.com/ in my case).\nStep 2.5: Test the tracker and collector Once you\u0026rsquo;ve installed the GA duplicator, you can test to see if the logs are being stored in S3 properly.\nIf the duplicator is doing its job correctly, you can open the Network tab in your browser\u0026rsquo;s developer tools, and look for requests to your collector endpoint. You should see POST requests for each GA call, with the payload of the request in the POST body:\n  If you don\u0026rsquo;t see these requests, it means you\u0026rsquo;ve misconfigured the duplicator somehow, and you should re-read the instructions.\nIf you see the requests but there\u0026rsquo;s an error in the HTTP response, you\u0026rsquo;ll need to check the process outlined in the previous two chapters again.\nAt something like 10 minutes past each hour, the Clojure collector running in Elastic Beanstalk will dump all the Tomcat access logs to S3. You should check that they are being stored, because the whole batch process hinges on the presence of these logs.\nIn S3, there will be a bucket prefixed with elasticbeanstalk-region-id. Within that bucket, browse to folder resources / environments / logs / publish / (some ID) / (some ID). In other words, within the publish folder will be a folder named something like e-ab12cd23ef, and within that will be a folder named something like i-1234567890. Within that final folder will be all your logs in gzip format.\nLook for the ones named _var_log_tomcat8_rotated_localhost_access_log.txt123456789.gz, as these are the logs that the ETL process will use to build the data tables.\nIf you open one of those logs, you should find a bunch of GET and POST entries. Look for POST entries where the endpoint is /com.google.analytics/v1, and the HTTP status code is 200. If you see these, it means that the Clojure collector is almost certainly doing its job. The entry will contain a bunch of interesting information, such as the IP address of the visitor, the User-Agent string of the browser, and a base64 encoded string with the payload. If you decode this string, you should see the full payload of your Google Analytics hit as a query string.\n  Step 3: Configure the ETL process What you need for this step   A collector running and dumping the logs in an S3 bucket.\n  Access Key ID and Secret Access Key for the IAM user you created in Step 0.\n  Getting started This step should be pretty straightforward, at least more so than the previous one.\nThe process does a number of things, and you\u0026rsquo;ll want to check out this page for more info.\nBut, in short, here are the main steps operated by the AWS Elastic MapReduce (EMR) service.\n  The Tomcat logs are cleaned up so that they can be parsed more easily. Irrelevant log entries are discarded.\n  Custom enrichments are applied to the data, if you so wish. Enrichments include things like geolocation from IP addresses, or adding weather information to the data set.\n  The enriched data is then shredded, or split into more atomic data sets, each corresponding with a hit that validates against a given schema. For example, if you are collecting data with the Google Analytics setup outlined in my guide, these hits would be automatically shredded into data sets for Page Views, Events, and other Google Analytics tables, ready for transportation to a relational database.\n  Finally, the data is copied from the shredded sets to a database created in Amazon Redshift.\n  We\u0026rsquo;ll go over these steps in detail next. It\u0026rsquo;s important to understand that each step of the ETL process leaves a trace in S3 buckets you\u0026rsquo;ll build along the way. This means that even if you choose to apply the current process to your raw logs, you can rerun your entire log history, if you\u0026rsquo;ve decided to keep the files, with new enrichments and shredding schemas later on. All the Tomcat logs are archived, too, so you\u0026rsquo;ll always be able to start the entire process from scratch, using all your historical data, if you wish!\nThe way we\u0026rsquo;ll work in this process is run a Java application name EmrEtlRunner from your local machine. This application initiates and runs the ETL process using Amazon\u0026rsquo;s Elastic MapReduce. At some point, you might want to upgrade your setup, and have EmrEtlRunner execute in an AWS EC2 instance (basically a virtual machine in Amazon\u0026rsquo;s cloud). That way you could schedule it to run, say, every 60 minutes, and then forget about it.\nDownload the necessary files The ETL runner is a Unix application you can download from this link. To grab the latest version, look for the file that begins with snowplow_emr_rXX, where XX is the highest number you can find. At the time of writing, the latest binary is snowplow_emr_r97_knossos.zip.\n  Download this ZIP file, and copy the snowplow-emr-etl-runner Unix executable into a new folder on your hard drive. This folder will be your base of operations.\n  At this point, you\u0026rsquo;ll want to also clone the Snowplow Github repo in that folder, because it has all the config file templates and SQL files you\u0026rsquo;ll need later on.\n  So browse to the directory to where you copied the snowplow-emr-etl-runner file, and run the following command:\n  git clone https://github.com/snowplow/snowplow.git If you don\u0026rsquo;t have Git installed, now would be a good time to do it.\n    Now you should have both the snowplow-emr-etl-runner file and the snowplow folder in the same directory.\n  Next, create a new folder named config, and in that, a new folder named targets.\n  Then, perform the following copy operations:\n  Copy snowplow/3-enrich/emr-etl-runner/config/config.yml.sample to config/config.yml.\n  Copy snowplow/3-enrich/config/iglu_resolver.json to config/iglu_resolver.json.\n  Copy snowplow/4-storage/config/targets/redshift.json to config/targets/redshift.json.\n      In the end, you should end up with a folder and file structure like this:\n. |-- snowplow-emr-etl-runner |-- snowplow | |-- -SNOWPLOW GIT REPO HERE- |-- config | |-- iglu_resolver.json | |-- config.yml | |-- targets | |-- redshift.json  Create an EC2 key pair At this point, you\u0026rsquo;ll also need to create a private key pair in Amazon EC2. The ETL process will run on virtual machines in the Amazon cloud, and these machines are powered by Amazon EC2. For the runner to have full privileges to create and manage these machines, you will need to provide it with access control rights, and that\u0026rsquo;s what the key pair is for.\n  In AWS, select Services from the top navigation, and click on EC2. In the left-hand menu, browse down to Key Pairs, and click the link.\n  At this point, make sure you are in the Region where you\u0026rsquo;ll be running all the proceeding jobs. For consistency\u0026rsquo;s sake, it makes sense to stay in the same Region you\u0026rsquo;ve been in all along. Remember, you can choose the Region from the top navigation.\n      Once you\u0026rsquo;ve made sure you\u0026rsquo;re in the correct Region, click Create Key Pair.\n  Give the key pair a name you\u0026rsquo;ll remember. My key pair is named simoahava.\n  Once you\u0026rsquo;re done, you\u0026rsquo;ll see your new key pair in the list, and the browser automatically downloads the file \u0026lt;key pair name\u0026gt;.pem to your computer.\n    Create the S3 buckets At this time, you\u0026rsquo;ll need to create a bunch of buckets (storage locations) in Amazon S3. These will be used by the batch process to manage all your data files through various stages of the ETL process.\nYou will need buckets for the following:\n  :raw:in - you already have this, actually. It\u0026rsquo;s the elasticbeanstalk-region-id created by the Clojure collector running in Elastic Beanstalk.\n  :processing - intermediate bucket for the log files before they are enriched.\n  :archive - you\u0026rsquo;ll need three different archive buckets: :raw (for the raw log files), :enriched (for the enriched files), :shredded (for the shredded files).\n  :enriched - you\u0026rsquo;ll need two buckets for enriched data: :good (for data sets successfully enriched), :bad (for those that failed enrich).\n  :shredded - you\u0026rsquo;ll likewise need two buckets for shredded data: :good (for data sets successfully shredded), :bad (for those that failed shredding).\n  :log - a bucket for logs produced by the ETL process.\n  To create these buckets, head on over to S3 by clicking Services in the AWS top navigation, and choosing S3.\nYou should already have your :raw:in bucket here, it\u0026rsquo;s the one whose name starts with elasticbeanstalk-.\nLet\u0026rsquo;s start with creating a new bucket that will contain all the \u0026ldquo;sub-buckets\u0026rdquo; for ETL.\nClick +Create bucket, and name the bucket something like simoahava-snowplow-data. The bucket name must be unique across all of S3, so you can\u0026rsquo;t just name it snowplow. Click Next a couple of times and then finally Create bucket to create this root bucket.\n  Now click the new bucket name to open the bucket. You should see a screen like this:\n  Click + Create folder, and create the following three folders into this empty bucket:\n  archive\n  enriched\n  shredded\n    Then, in archive, create the following three folders:\n  raw\n  enriched\n  shredded\n  Next, in both enriched and shredded, create the following two folders:\n  good\n  bad\n  Thus, you should end up with a bucket that has the following structure:\n. |-- elasticbeanstalk-region-id |-- simoahava-snowplow-data | |-- archive | | |-- raw | | |-- enriched | | |-- shredded | |-- encriched | | |-- good | | |-- bad | |-- shredded | | |-- good | | |-- bad Finally, create one more bucket in the root of S3 named something like simoahava-snowplow-log. You\u0026rsquo;ll use this for the logs produced by the batch process.\nPrepare for configuring the EmrEtlRunner Now you\u0026rsquo;ll need to configure the EmrEtlRunner. You\u0026rsquo;ll use the config.yml file you copied from the Snowplow repo to the config/ folder. For the config, you\u0026rsquo;ll need the following things:\n  The Access Key ID and Secret Access Key for the snowplow-setup user you created all the way back in Step 0. If you didn\u0026rsquo;t save these, you can generate a new Access Key through AWS IAM.\n  You will need to download and install the AWS Command Line Interface. You can use the official guide to install it with Python/pip, but if you\u0026rsquo;re running Mac OS X, I recommend using Homebrew instead. Once you\u0026rsquo;ve installed Homebrew, you just need to run brew install awscli to install the AWS client.\n  Once you\u0026rsquo;ve installed awscli, you need to run aws configure in your terminal, and do what it instructs you to do. You\u0026rsquo;ll need your Access Key ID and Secret Access Key handy, as well as the region name (e.g. eu-west-1) where you\u0026rsquo;ll be running your EC2 (again, I recommend to use the same region for all parts of this pipeline process).\n$ aws configure AWS Access Key ID: \u0026lt;enter your IAM user Access Key ID here\u0026gt; AWS Secret Access Key: \u0026lt;enter you IAM user Secret Access Key here\u0026gt; Default region name: \u0026lt;enter the region name, e.g. eu-west-1 here\u0026gt; Default output format: \u0026lt;just press enter\u0026gt; This is what it looked like when I ran aws configure.\n  After running aws configure, the next command you\u0026rsquo;ll need to run is aws emr create-default-roles. This will create default roles for the EmrEtlRunner, so that it can perform the necessary tasks in EC2 for you.\nOnce you\u0026rsquo;ve done these steps (remember to still keep your Access Key ID and Secret Access Key close by), you\u0026rsquo;re ready to configure EmrEtlRunner!\nConfigure EmrEtlRunner EmrEtlRunner is the name of the utility you downloaded earlier, with the filename snowplow-emr-etl-runner.\nEmrEtlRunner does a LOT of things. See this diagram to see an overview of the process. At this point, we\u0026rsquo;ll do all the steps except for step 13, rdb_load. That\u0026rsquo;s the step where the enriched and shredded data are copied into a relational database. We\u0026rsquo;ll take care of that in the next step.\nAnyway, EmrEtlRunner is operated by config.yml, which you\u0026rsquo;ve copied into the config/ directory. I\u0026rsquo;ll show you the config I use, and highlight the parts you\u0026rsquo;ll need to change.\naws: access_key_id: AKIAIBAWU2NAYME55123 secret_access_key: iEmruXM7dSbOemQy63FhRjzhSboisP5TcJlj9123 s3: region: eu-west-1 buckets: assets: s3://snowplow-hosted-assets jsonpath_assets: log: s3://simoahava-snowplow-log raw: in: - s3://elasticbeanstalk-eu-west-1-375284143851/resources/environments/logs/publish/e-f4pdn8dtsg processing: s3://simoahava-snowplow-data/processing archive: s3://simoahava-snowplow-data/archive/raw enriched: good: s3://simoahava-snowplow-data/enriched/good bad: s3://simoahava-snowplow-data/enriched/bad errors: archive: s3://simoahava-snowplow-data/archive/enriched shredded: good: s3://simoahava-snowplow-data/shredded/good bad: s3://simoahava-snowplow-data/shredded/bad errors: archive: s3://simoahava-snowplow-data/archive/shredded emr: ami_version: 5.9.0 region: eu-west-1 jobflow_role: EMR_EC2_DefaultRole service_role: EMR_DefaultRole placement: ec2_subnet_id: subnet-d6e91a9e ec2_key_name: simoahava bootstrap: [] software: hbase: lingual: jobflow: job_name: Snowplow ETL master_instance_type: m1.medium core_instance_count: 2 core_instance_type: m1.medium core_instance_ebs: volume_size: 100 volume_type: \u0026#34;gp2\u0026#34; volume_iops: 400 ebs_optimized: false task_instance_count: 0 task_instance_type: m1.medium task_instance_bid: 0.015 bootstrap_failure_tries: 3 configuration: yarn-site: yarn.resourcemanager.am.max-attempts: \u0026#34;1\u0026#34; spark: maximizeResourceAllocation: \u0026#34;true\u0026#34; additional_info: collectors: format: clj-tomcat enrich: versions: spark_enrich: 1.12.0 continue_on_unexpected_error: false output_compression: NONE storage: versions: rdb_loader: 0.14.0 rdb_shredder: 0.13.0 hadoop_elasticsearch: 0.1.0 monitoring: tags: {} logging: level: DEBUG The keys you need to edit are listed next, with a comment on how to edit them. All the keys not listed below you can leave with their default values. I really recommend you read through the configuration documentation for ideas on how to modify the rest of the keys to make your setup more powerful.\n   Key Comment     :aws:access_key_id Copy-paste the Access Key ID of your IAM user here.   :aws:secret_access_key Copy-paste the Secret Access Key of your IAM user here.   :aws:s3:region Set this to the region where your S3 buckets are located in.   :aws:s3:buckets:log Change this to the name of the S3 bucket you created for the ETL logs.   :aws:s3:buckets:raw:in This is the bucket where the Tomcat logs are automatically pushed to. Do not include the last folder in the path, because this might change with an auto-scaling environment. Note! Keep the hyphen in the beginning of the line as in the config file example!   :aws:s3:buckets:raw:processing Set this to the respective processing bucket.   :aws:s3:buckets:raw:archive Set this to the archive bucket for raw data.   :aws:s3:buckets:enriched:good Set this to the enriched/good bucket.   :aws:s3:buckets:enriched:bad Set this to the enriched/bad bucket.   :aws:s3:buckets:enriched:errors Leave this empty.   :aws:s3:buckets:enriched:archive Set this to the archive bucket for enriched data.   :aws:s3:buckets:shredded:good Set this to the shredded/good bucket.   :aws:s3:buckets:shredded:bad Set this to the shredded/bad bucket.   :aws:s3:buckets:shredded:errors Leave this empty.   :aws:s3:buckets:shredded:archive Set this to the archive bucket for shredded data.   :aws:emr:region This should be the region where the EC2 job will run.   :aws:emr:placement Leave this empty.   :aws:emr:ec2_subnet_id The subnet ID of the Virtual Private Cloud the job will run in. You can use the same subnet ID used by the EC2 instance running your collector.   :aws:emr:ec2_key_name The name of the EC2 Key Pair you created earlier.   :collectors:format Set this to clj-tomcat.   :monitoring:snowplow Remove this key and all its children (:method, :app_id, and :collector).    Just two things to point out.\nFirst, when copying the :aws:s3:buckets:raw:in path, do not copy the last folder name. This is the instance ID. With an auto-scaling environment set for the collector, there might be multiple instance folders in this bucket. If you only name one folder, you\u0026rsquo;ll risk missing out on a lot of data.\n  You can get the :aws:emr:ec2_subnet_id by opening the Services menu in AWS and clicking EC2. Click the link titled Running Instances (there should be 1 running instance - your collector). Scroll down the Description tab contents until you find the Subnet ID entry. Copy-paste that into the aws:emr:ec2_subnet_id field.\n  If you\u0026rsquo;ve followed all the steps in this chapter, you should now be set.\nYou can verify everything works by running the following command in the directory where the snowplow-emr-etl-runner executable is, and where the config folder is located.\n./snowplow-emr-etl-runner run -c config/config.yml -r config/iglu_resolver.json\n  You can also follow the process in real-time by opening the Services menu in AWS and clicking EMR. There, you should see the job named Snowplow ETL. By clicking it, you can see all the steps it is running through. If the process ends in an error, you can also debug quite handily via this view, since you can see the exact step where the process failed.\n  Once the ETL has successfully completed, you can check your S3 buckets again. Your Snowplow data buckets should now contain a lot of new stuff. The folder with the interesting data is archive / shredded. This is where the good shredded datasets are copied to, and corresponds to what would have been copied to the relational database had you set this up in this step.\nAnyway, with the ETL process up and running, just one more step remains in this monster of a guide: setting up AWS Redshift as the relational database where you\u0026rsquo;ll be warehousing your analytics data!\nWhat you should have after this step   The snowplow-emr-etl-runner executable configured with your config.yml file.\n  New buckets in S3 to store all the files created by the batch process.\n  The ETL job running without errors all the way to completion, enriching and shredding the raw Tomcat logs into relevant S3 buckets.\n  Step 4: Load the data into Redshift What you need for this step   The ETL process configured and available to run at your whim.\n  Shredded files being stored in the archive / shredded S3 bucket.\n  An SQL query client. I recommend SQL Workbench/J, which is free. That\u0026rsquo;s the one I\u0026rsquo;ll be using in this guide.\n  Getting started In this final step of this tutorial, we\u0026rsquo;ll load the shredded data into Redshift tables. Redshift is a data warehouse service offered by AWS. We\u0026rsquo;ll use it to build a relational database, where each table stores the information shredded from the Tomcat logs in a format easy to query with SQL. By the way, if you\u0026rsquo;re unfamiliar with SQL, look no further than this great Codecademy course to get you started with the query language!\nThe steps you\u0026rsquo;ll take in this chapter are roughly these:\n  Create a new cluster and database in Redshift.\n  Add users and all the necessary tables to the database.\n  Configure the EmrEtlRunner to automatically load the shredded data into Redshift tables.\n  Once you\u0026rsquo;re done, each time you run EmrEtlRunner, the tables will be populated with the parsed tracker data. You can then run SQL queries against this data, and use it to proceed with the two remaining steps (not covered in this guide) of the Snowplow pipeline: data modeling and analysis.\nCreate the cluster In AWS, select Services from the top navigation and choose the Amazon Redshift service.\nAgain, double-check that you are in the correct region (the same one where you\u0026rsquo;ve been working on all along, or, at the very least, the one where your S3 logs are). Then click the Launch cluster button.\n  Give the cluster an identifier. I named my cluster simoahava. Give a name to the database, too. The name I chose was snowplow.\nKeep the database port at its default value (5439).\nAdd a username and password to your master user. This is the user you\u0026rsquo;ll initially log in with, and it\u0026rsquo;s the one you\u0026rsquo;ll create the rest of the users and all the necessary tables with. Remember to write these down somewhere - you\u0026rsquo;ll need them in just a bit.\nClick Continue when ready.\n  In the next view, leave the two options at their default values. Node type should be dc2.large, and Cluster type should be Single Node (with 1 as the number of compute nodes used). Click Continue when ready.\n  In the Additional Configuration view, you can leave most of the options at their default values. For the VPC security group, select the default group. The settings should thus be something like these:\nCluster parameter group: default-redshift-1.0\nEncrypt database: None\nChoose a VPC: Default VPC (\u0026hellip;)\nCluster subnet group: default\nPublicly accessible: Yes\nChoose a public IP address: No\nEnhanced VPC Routing: No\nAvailability zone: No Preference\nVPC security groups: default (sg-\u0026hellip;)\nCreate CloudWatch Alarm: No\nAvailable roles: No selection\nOnce done, click Continue.\nYou can double-check your settings, and then just click Launch cluster.\nThe cluster will take some minutes to launch. You can check the status of the cluster in the Redshift dashboard.\n  Once the cluster has been launched, you are ready to log in and configure it!\nConfigure the cluster and connect to it The first thing you\u0026rsquo;ll need to do is make sure the cluster accepts incoming connections from your local machine.\nSo after clicking Services in the AWS top navigation and choosing Amazon Redshift, go to Clusters and then click the cluster name in the dashboard.\nUnder Cluster Properties, click the link to the VPC security group (should be named something like default (sg-1234abcd)).\n  You should be transported to the EC2 dashboard, and Security Groups should be active in the navigation menu.\nIn the bottom of the screen, the settings for the security group you clicked should be visible.\nSelect the Inbound tab, and make sure it shows a TCP connection with Port Range 5439 and 0.0.0.0/0 as the Source. This means that all incoming TCP connections are permitted (you can change this to a more stricter policy later on).\nIf the values are different, you can Edit the Inbound rule to match these.\n  Now it\u0026rsquo;s time to connect to the cluster. Go back to Amazon Redshift, and open your cluster settings as before. Copy the link to the cluster from the top of the settings list.\n  Next, open the SQL query tool of your choice. I\u0026rsquo;m using SQL Workbench/J. Select File / Connect Window, and create a new connection with the following settings changed from defaults:\nDriver: Amazon Redshift (com.amazon.redshift.jdbc.Driver)\nURL: jdbc:redshift://cluster_url:cluster_port/database_name\nUsername: master_username\nPassword: master_password\nAutocommit: Check\nIn URL, copy-paste the Redshift URL with port after the colon and database name after the slash.\nAs Username and Password, add the master username and master password you chose when creating the cluster.\nMake sure Autocommit is checked. These are settings I have:\n  Once done, you can click OK, and the tool will connect to your cluster and database.\nOnce connected, you can feed the command SELECT current_database(); and click the Execute button to check if everything works. This is what you should see:\n  If the query returns the name of the database, you\u0026rsquo;re good to go!\nCreate the database tables First, we\u0026rsquo;ll need to create the tables that will store the Google Analytics tracker data within them. The tables are loaded as .sql files, and these files contain DDL (data-definition language) constructions that build all the necessary schemas and tables.\nFor this, you\u0026rsquo;ll need access to the schema .sql files, which you\u0026rsquo;ll find in the following locations within the snowplow Git repo:\n  snowplow/4-storage/redshift-storage/sql/atomic-def.sql\n  snowplow/4-storage/redshift-storage/sql/manifest-def.sql\n  Load atomic-def.sql first, and run the file in your SQL query tool while connected to your Redshift database. You should see a message that the schema atomic and table atomic.events were created successfully.\n  Next, run manifest-def.sql while connected to the database. You should see a message that the table atomic.manifest was created successfully.\nNow you need to load all the DDLs for the Google Analytics schemas. If you don\u0026rsquo;t create these tables, then the ETL process will run into an error, where the utility tries to copy shredded events into non-existent tables.\nYou can find all the required .sql files in the following three directories:\n  https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics\n  https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.enhanced-ecommerce\n  https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.measurement-protocol\n  You need to load all the .sql files in these three directories and run them while connected to your database. This will create a whole bunch of tables you\u0026rsquo;ll need if you want to collect Google Analytics tracker data.\nIt might be easiest to clone the iglu-central repo, and then load the .sql files into your query tool from the local directories.\nOnce you\u0026rsquo;re done, you should be able to run the following SQL query and see a list of all the tables you just created as a result (should be 40 in total):\nSELECT * FROM pg_tables WHERE schemaname='atomic';\n  Create the database users Next thing we\u0026rsquo;ll do is create three users:\n  storageloader, who will be in charge of the ETL process.\n  power_user, who will have admin privileges, so you no longer have to log in with the master credentials.\n  read_only, who can query data and create his/her own tables.\n  Make sure you\u0026rsquo;re still connected to the database, and copy-paste the following SQL queries into the query window. For each $password, change it to a proper password, and make sure you write these user + password combinations down somewhere.\nCREATE USER storageloader PASSWORD \u0026#39;$password\u0026#39;; GRANT USAGE ON SCHEMA atomic TO storageloader; GRANT INSERT ON ALL TABLES IN SCHEMA atomic TO storageloader; CREATE USER read_only PASSWORD \u0026#39;$password\u0026#39;; GRANT USAGE ON SCHEMA atomic TO read_only; GRANT SELECT ON ALL TABLES IN SCHEMA atomic TO read_only; CREATE SCHEMA scratchpad; GRANT ALL ON SCHEMA scratchpad TO read_only; CREATE USER power_user PASSWORD \u0026#39;$password\u0026#39;; GRANT ALL ON DATABASE snowplow TO power_user; GRANT ALL ON SCHEMA atomic TO power_user; GRANT ALL ON ALL TABLES IN SCHEMA atomic TO power_user; Again, remember to change the three $password values to proper SQL user passwords.\nIf all goes well, you should see 12 \u0026ldquo;COMMAND executed successfully\u0026rdquo; statements.\nFinally, you need to grant ownership of all tables in schema atomic to storageloader, because this user will need to run some commands (specifically, vacuum and analyze) that only table owners can run.\nSo, first run the following query in the database.\nSELECT \u0026#39;ALTER TABLE atomic.\u0026#39; || tablename ||\u0026#39; OWNER TO storageloader;\u0026#39; FROM pg_tables WHERE schemaname=\u0026#39;atomic\u0026#39; AND NOT tableowner=\u0026#39;storageloader\u0026#39;; In the query results, you should see a bunch of ALTER TABLE atomic.* OWNER TO storageloader; queries. Copy all of these, and paste them into the statement field as new queries. Then run the statements.\n  Now, if you run SELECT * FROM pg_tables WHERE schemaname='atomic' AND tableowner='storageloader';, you should see all the tables in the atomic schema as a result.\nYou have successfully created the users and the tables in the database. All that\u0026rsquo;s left is to configure the EmrEtlRunner to execute the final step of the ETL process, where the storageloader user copies all the data from the shredded files into the corresponding Redshift tables.\nCreate new IAM role for database loader The EmrEtlRunner will copy the files to Redshift using a utility called RDB Loader (Relational Database Loader). For this tool to work with sufficient privileges, you\u0026rsquo;ll need to create a new IAM Role, which grants the Redshift cluster read-only access to your S3 buckets.\n  So, in AWS, click Services and select IAM.\n  Select Roles from the left-hand navigation. Click the Create role button.\n  In the Select type of trusted entity view, keep the default AWS Service selected, and choose Redshift from the list of services. In the Select your use case list, choose Redshift - Customizable, and then click Next: Permissions.\n     In the next view, find the policy named AmazonS3ReadOnlyAccess, and check the box next to it. Click Next: Review.      Name the role something useful, such as RedshiftS3Access and click Create Role when ready.\n  You should be back in the list of roles. Click the newly created RedshiftS3Access role to see its configuration. Copy the value in the Role ARN field to the clipboard. You\u0026rsquo;ll need it very soon.\n      Finally, select Services from AWS top navigation and choose the Amazon Redshift service. Click Clusters in the left-hand navigation to see the list of running clusters.\n  Check the box next to your Snowplow cluster, and click Manage IAM Roles.\n     In the Available roles list, choose the role you just created, and then click Apply changes to apply the role to your cluster.    The Cluster Status should change to modifying. Once it\u0026rsquo;s done, the status will change to available, and you can check if the role you assigned is labeled as in-sync by clicking Manage IAM Roles again.\nEdit the Redshift target configuration If you copied all necessary files back in Step 3, your project config/ directory should include a targets/ folder with the file redshift.json in it. If you don\u0026rsquo;t have it, go back to Step 3 and make sure you copy the redshift.json template to the correct folder.\nOnce you\u0026rsquo;ve found the template, open it for editing, and make sure it looks something like this:\n{\u0026#34;schema\u0026#34;: \u0026#34;iglu:com.snowplowanalytics.snowplow.storage/redshift_config/jsonschema/2-1-0\u0026#34;,\u0026#34;data\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;AWS Redshift enriched events storage\u0026#34;,\u0026#34;host\u0026#34;: \u0026#34;simoahava.coyhone1deuh.eu-west-1.redshift.amazonaws.com\u0026#34;,\u0026#34;database\u0026#34;: \u0026#34;snowplow\u0026#34;,\u0026#34;port\u0026#34;: 5439,\u0026#34;sslMode\u0026#34;: \u0026#34;DISABLE\u0026#34;,\u0026#34;username\u0026#34;: \u0026#34;storageloader\u0026#34;,\u0026#34;password\u0026#34;: \u0026#34;...\u0026#34;,\u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::375284143851:role/RedshiftS3Access\u0026#34;,\u0026#34;schema\u0026#34;: \u0026#34;atomic\u0026#34;,\u0026#34;maxError\u0026#34;: 1,\u0026#34;compRows\u0026#34;: 20000,\u0026#34;sshTunnel\u0026#34;: null,\u0026#34;purpose\u0026#34;: \u0026#34;ENRICHED_EVENTS\u0026#34;}} Here are the fields you need to edit:\n host: The URL of your Redshift cluster database: The database name username: storageloader password: storageloader password roleArn: The Role ARN of the IAM role you created in the previous step  All the other options you can leave with their default values.\nRe-run EmrEtlRunner through the whole process Now that you\u0026rsquo;ve configured everything, you\u0026rsquo;re ready to run the EmrEtlRunner with all the steps in the ETL process included. This means enrichment of the log data, shredding of the log data to atomic datasets, and loading these datasets into your Redshift tables.\nThe command you\u0026rsquo;ll need to run in the root of your project folder (where the snowplow-emr-etl-runner executable is) is this:\n./snowplow-emr-etl-runner run -c config/config.yml -r config/iglu_resolver.json -t config/targets This command will process all the data in the :raw:in bucket (the one with all your Tomcat logs), and proceed to extract and transform them, and finally load them into your Redshift tables. The process will take a while, so go grab a coffee. Remember that you can check the status of the job by browsing to EMR via the AWS Services navigation.\nOnce complete, you should see something like this in the command line:\n  Test it Now you should be able to login to the database using the new read_only user. If you run the following query, it should return a list of timestamps and events for each Client ID visiting your site.\nSELECT u.root_tstamp, u.client_id, h.type FROM atomic.com_google_analytics_measurement_protocol_user_1 AS u JOIN atomic.com_google_analytics_measurement_protocol_hit_1 AS h ON u.root_id = h.root_id ORDER BY root_tstamp ASC   Considering how much time you have probably put into making everything work (if following this guide diligently), I really hope it all works correctly.\nWrapping it all up By following this guide, you should be able to set up an end-to-end Snowplow batch pipeline.\n  Google Tag Manager duplicates the payloads sent to Google Analytics, and sends these to your Amazon endpoint, using a custom domain name whose DNS records you have delegated to AWS.\n  The endpoint is a collector which logs all the HTTP requests to Tomcat logs, and stores them in an S3 bucket.\n  An ETL process is then run, enriching the stored data, and shredding it to atomic datasets, again stored in S3.\n  Finally, the ETL runner copies these datasets into tables you\u0026rsquo;ve set up in a new relational database running on an AWS Redshift cluster.\n  There are SO many moving parts here, that it\u0026rsquo;s possible you\u0026rsquo;ll get something wrong at some point. Just try to patiently walk through the steps in this guide to see if you\u0026rsquo;ve missed anything.\nFeel free to ask questions in the comments, and maybe I or my readers will be able to help you along.\nYou can also join the discussions in Snowplow\u0026rsquo;s Discourse site - I\u0026rsquo;m certain the folks there are more than happy to help you if you run into trouble setting up the pipeline.\nDo note also that the setup outlined in this guide is very rudimentary. There are many ways you can, and should, optimize the process, such as:\n  Add SSL support to your Redshift cluster.\n  Scale the instances (collector and the ETL process) correctly to account for peaks in traffic and dataset size.\n  Move the EmrEtlRunner to AWS, too. There\u0026rsquo;s no need to run it on your local machine.\n  Schedule the EmrEtlRunner to run (at least) once a day, so that your database is refreshed with new data periodically.\n  Good luck!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/",
	"title": "#GTMTips: Automatically Duplicate Google Analytics Hits To Snowplow",
	"tags": ["google tag manager", "gtmtips", "snowplow", "customTask"],
	"description": "A guide for implementing the Google Analytics JavaScript plugin when using Google Tag Manager.",
	"content": "I\u0026rsquo;m back with another customTask tip, but this time I\u0026rsquo;m exploring some new territory. Snowplow just introduced their latest version update, which included (among other things) an adapter for processing Google Analytics payloads. Never heard of Snowplow? It\u0026rsquo;s a collection of open-source libraries designed to let you build your own analytics pipeline, all the way from data collection, through ETL (extract, transform, load), using custom enrichments and JSON schemas, and finally into your own data warehouse, where you can then analyze the data using whatever tools you find preferable. Everything is designed to run over Amazon Web Services, so you don\u0026rsquo;t need to invest in local server hardware or hosting services.\n Snowplow pipeline - from https://goo.gl/X9Jfeo  In essence, it\u0026rsquo;s a full-service, do-it-yourself analytics solution. Snowplow has deservedly gained a lot of momentum over the recent years, as more and more companies have matured to the point where they want full control of their data. And I don\u0026rsquo;t just mean data ownership, but also things like controlling the aggregation schemas that have proven to be rather rigid in Google Analytics, and being in full charge when and how the data is sampled and normalized.\nAnyway, at some point I\u0026rsquo;ll author a proper article about Snowplow - one that it deserves. This time I\u0026rsquo;m just going to show you how to setup the Google Analytics duplicator / tracker, so that you can start collecting hits in your Snowplow pipeline by simply leveraging the payload generated and collected by Google Analytics.\nTip 70: Duplicate Google Analytics payload to Snowplow   If you read the release announcement, you might have noticed that the release is essentially a Google Analytics plugin, which is easy to add if you\u0026rsquo;re using the analytics.js tracking snippet.\nUnfortunately, with Google Tag Manager there is no reliable way to load a plugin in your Google Analytics tags. That means you\u0026rsquo;re left with clumsy workarounds, such as\n  A Custom HTML tag which you use to load analytics.js and create a tracker with the plugin.\n  Some customTask hack where you load the plugin mid-hit.\n  The first one is unwieldy because you would then need to have all your tags use the same tracker name if you wanted them all to duplicate the payloads to Google Analytics.\nThe second simply doesn\u0026rsquo;t work. Even if you do manage to load the plugin in the tracker, Google Analytics would not stop to wait for the plugin to be registered, but would simply send the hit before the plugin has had time to attach and modify the tracker object itself.\nSo in this tip, we\u0026rsquo;re just going to skip the plugin altogether, and replicate its functionality using customTask.\nTo make it all work, create a new Custom JavaScript variable, name it something like {{customTask - Snowplow duplicator}}, and add the following code within:\nfunction() { // Add your snowplow collector endpoint here  var endpoint = \u0026#39;https://collector.simoahava.com/\u0026#39;; return function(model) { var vendor = \u0026#39;com.google.analytics\u0026#39;; var version = \u0026#39;v1\u0026#39;; var path = ((endpoint.substr(-1) !== \u0026#39;/\u0026#39;) ? endpoint + \u0026#39;/\u0026#39; : endpoint) + vendor + \u0026#39;/\u0026#39; + version; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; var originalSendHitTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { var payload = sendModel.get(\u0026#39;hitPayload\u0026#39;); originalSendHitTask(sendModel); var request = new XMLHttpRequest(); request.open(\u0026#39;POST\u0026#39;, path, true); request.setRequestHeader(\u0026#39;Content-type\u0026#39;, \u0026#39;text/plain; charset=UTF-8\u0026#39;); request.send(payload); }); }; }  Then you need to edit every single Google Analytics tag whose data you also want to send to Snowplow.\nAt this point, if you haven\u0026rsquo;t done so yet, it\u0026rsquo;s a good idea to make use of the Google Analytics Settings variable. Instead of having to modify every single tag, you only need to make the necessary change (see below) in the GAS variable, after which you can add that GAS variable to all your Google Analytics tags. Useful!\n  Anyway, the change you need to make is under More Settings / Fields to set of your Google Analytics tags or the Google Analytics Settings variable. If you\u0026rsquo;re editing tags directly, you\u0026rsquo;ll need to check the \u0026ldquo;Enable overriding settings in this tag\u0026rdquo; option to see the More Settings fields. Here\u0026rsquo;s the field you need to add.\nField name: customTask\nValue: {{customTask - Snowplow duplicator}}\nRemember - the change needs to be done in all the Google Analytics tags whose data you want to fork to Snowplow.\nNote! At the time of writing, only the Clojure Collector in Snowplow supports the Google Analytics adapter. Hopefully they\u0026rsquo;ll release support for the Scala Stream Collector soon, as it will give you access to that sweet, juicy Google Analytics real-time data! Make sure you follow the Snowplow discussion forum - it\u0026rsquo;s a good place as any to get information on the roadmap.\nThis is a pretty sweet addition to Snowplow, because it lets you operate with parameters and values that are familiar to you, if you\u0026rsquo;ve used Google Analytics before. It also lets you leverage existing Google Analytics tracking, so you don\u0026rsquo;t need to rewrite the tracking setup on your site just to migrate to Snowplow.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/simple-custom-event-listeners-gtm/",
	"title": "#GTMTips: Simple Custom Event Listeners With Google Tag Manager",
	"tags": ["google tag manager", "javascript", "events", "gtmtips"],
	"description": "How to create simple custom event listeners with Google Tag Manager. You can track any user interaction on the site with this solution.",
	"content": "First of all, check out this article for an overview of how custom event listeners work in Google Tag Manager. The reason I\u0026rsquo;m writing this #GTMTips article is that I want to upgrade the solution slightly, and I want to bring it back into the spotlight. Why? Because it\u0026rsquo;s still one of the most effective ways to customize your Google Tag Manager implementation.\nA custom event listener is a handler you write with JavaScript. It lets you handle any JavaScript DOM events, such as click, form submit, mouse hover, drag, touch, error, page load and unload, and so many more. It also lets you leverage the useCapture parameter which will prove very helpful if you have other JavaScript on the site interfering with GTM\u0026rsquo;s default event triggers.\nTip 69: Create custom event listeners with ease   The solution comprises the following items: a Custom HTML tag firing with the page load, and a Custom JavaScript variable providing the callback. You\u0026rsquo;ll also need a bunch of Data Layer variables to fetch the values pushed into dataLayer by the callback.\nThe idea is that when the page loads, you attach your custom listener to whatever element you want to track for the given event. Then, when this interaction is recorded by the browser, the code pushes an object into dataLayer which you can then use to populate your tags.\nThe Custom HTML tag This is what the Custom HTML tag would look like.\n\u0026lt;script\u0026gt; (function() { // Use events from https://developer.mozilla.org/en-US/docs/Web/Events  var eventName = \u0026#39;dragstart\u0026#39;; // Attach listener directly to element or document if element not found  var el = document.querySelector(\u0026#39;img#download\u0026#39;) || document; // Leave useCapture to true if you want to avoid propagation issues.  var useCapture = true; el.addEventListener(eventName, {{JS - Generic Event callback}}, useCapture); })(); \u0026lt;/script\u0026gt; Fire this on a Page View trigger if attaching directly to the document node, or a DOM Ready trigger if attaching directly to an element and the element is in the HTML source, or a Window Loaded trigger if attaching directly to an element that is added dynamically to the page during the page load.\nMake sure you change the eventName value to reflect which event you want to track. If you want to track clicks, change it to click. If you want to track users hovering over the element, change it to mouseover, and so on.\nYou can choose to add the listener directly to an element by using the appropriate CSS selector as the parameter of document.querySelector(). Alternatively, you can add the listener directly on the document node.\nFinally, you can set useCapture to false if you want to use the bubble phase instead of the capture phase with your event handler. Because you are simply tracking interactions and not actually creating any side effects, I really recommend leaving this as true.\nThis is significant especially if you have other JavaScript on the site messing with event propagation. A typical symptom of this is that your Form and Just Links triggers refuse to work. So by using the capture phase, you are evading propagation-stopping JavaScript, and might just be able to track these events when GTM\u0026rsquo;s default triggers are unable to do so.\nThe last line actually adds the listener, providing a Custom JavaScript variable as the callback.\nThe Custom JavaScript variable Here\u0026rsquo;s what {{JS - Generic Event callback}} should look like:\nfunction() { return function(event) { window.dataLayer.push({ event: \u0026#39;custom.event.\u0026#39; + event.type, \u0026#39;custom.gtm.element\u0026#39;: event.target, \u0026#39;custom.gtm.elementClasses\u0026#39;: event.target.className || \u0026#39;\u0026#39;, \u0026#39;custom.gtm.elementId\u0026#39;: event.target.id || \u0026#39;\u0026#39;, \u0026#39;custom.gtm.elementTarget\u0026#39;: event.target.target || \u0026#39;\u0026#39;, \u0026#39;custom.gtm.elementUrl\u0026#39;: event.target.href || event.target.action || \u0026#39;\u0026#39;, \u0026#39;custom.gtm.originalEvent\u0026#39;: event }); }; }  This callback pushes a bunch of information about the event into dataLayer, namespacing everything with the custom.gtm. prefix. The event name itself will be custom.event.\u0026lt;event name\u0026gt;, e.g. custom.event.click for a click event or custom.event.dragstart when tracking the dragging action.\nThe variables pushed into dataLayer mirror those used by GTM\u0026rsquo;s default triggers, with the exception of custom.gtm.originalEvent which will contain a reference to the original event that invoked the callback. This is significant if you need information from this event object, such as which mouse button was clicked when a click is registered. This is (currently) missing from GTM\u0026rsquo;s default trigger functionality.\nData Layer variables You need to create a Data Layer variable for each of the keys pushed into dataLayer. To mimic Google Tag Manager\u0026rsquo;s naming schema for Built-in variables, you could use something like these:\n   Variable name Data Layer Variable Name     {{Custom Event Element}} custom.gtm.element   {{Custom Event Classes}} custom.gtm.elementClasses   {{Custom Event ID}} custom.gtm.elementId   {{Custom Event Target}} custom.gtm.elementTarget   {{Custom Event URL}} custom.gtm.elementUrl   {{Custom Event Original Event}} custom.gtm.originalEvent    The Custom Event trigger To fire your tags when a custom event is registered, you\u0026rsquo;ll need a Custom Event trigger set to the event name pushed into dataLayer in the Custom JavaScript variable callback. So, to follow the example of the dragstart event (registered when the user starts dragging the given element in the browser), the trigger would look like this:\n  Working example Let\u0026rsquo;s tackle a problem that you might well have on your site. You want to track a form element with id contactUs, but no matter what you do, GTM\u0026rsquo;s own Form trigger refuses to fire. You\u0026rsquo;ve looked around, read my articles, and come to the conclusion that the problem is other JavaScript on the site stopping the propagation of the form submit event. Your friendly local developer tells you that due to the nature of the plugin you use, it\u0026rsquo;s impossible to change this behavior.\nCustom event listeners to the rescue! You can trust the useCapture flag to track the form submission even though propagation has been stopped. Here\u0026rsquo;s what the Custom HTML tag would look like:\n\u0026lt;script\u0026gt; (function() { // Use events from https://developer.mozilla.org/en-US/docs/Web/Events  var eventName = \u0026#39;submit\u0026#39;; // Attach listener directly to element or document if element not found  var el = document.querySelector(\u0026#39;form#contactUs\u0026#39;) || document; // Leave useCapture to true if you want to avoid propagation issues.  var useCapture = true; el.addEventListener(eventName, {{JS - Generic Event callback}}, useCapture); })(); \u0026lt;/script\u0026gt; Add a DOM Ready trigger to this tag, set to fire on pages which have this particular form in the HTML source.\nNow, whenever a form submission is detected, your Custom HTML tag will go off, pushing the following object into dataLayer:\n{ \u0026#39;event\u0026#39;: \u0026#39;custom.event.submit\u0026#39;, \u0026#39;custom.gtm.element\u0026#39;: form#contactUs, \u0026#39;custom.gtm.elementClasses\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;custom.gtm.elementId\u0026#39;: \u0026#39;contactUs\u0026#39;, \u0026#39;custom.gtm.elementTarget\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;custom.gtm.elementUrl\u0026#39;: \u0026#39;https://www.domain.com/my-form-handler/\u0026#39;, \u0026#39;custom.gtm.originalEvent\u0026#39;: submitEvent }  And you can pick these up with the Data Layer variables you created earlier.\nYou\u0026rsquo;ll just need a Custom Event trigger, where the Event name field has the value custom.event.submit.\nOnce you attach that trigger to your tag, you can use the Data Layer variables to populate all the relevant fields.\nSummary Whenever I talk about GTM to someone, and that\u0026rsquo;s very often, I always end up talking about the many ways we can customize Google Tag Manager to work even more efficiently on our websites. Custom Event listeners are still, after all these years, my favorite way of customizing a GTM setup.\nThey give you so much power in tracking user interactions on the site.\nAs always, I hope GTM continues to release new default triggers for us. I\u0026rsquo;ve long dreamed of a \u0026ldquo;blank\u0026rdquo; event trigger, where you simply have to add the DOM event name, and it would also have a checkbox for whether you want to use capture mode or not. It would make this custom solution redundant, but that\u0026rsquo;s only a good thing in my book.\nHave you created any creative custom event listeners in your GTM setups?\n"
},
{
	"uri": "https://www.simoahava.com/tags/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/search-api/",
	"title": "search api",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/static-site/",
	"title": "static site",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/web-development/static-site-search-google-app-engine-search-api/",
	"title": "Static Site Search With Hugo + App Engine + Search API + Python",
	"tags": ["app engine", "search api", "hugo", "static site", "google cloud"],
	"description": "How to make static site search work using Google App Engine&#39;s Search API and a static site generated with Hugo.",
	"content": "As the year changed to 2018, I decided to abandon WordPress, which I had been using for over 12 years as my content management system of choice. I had many reasons to do so, but the biggest motivators were the opportunity to try something new and to abandon the bloat and clutter of WordPress for a more simple, more elegant order of things. Spurred on by another adopter, Mark Edmondson, I decided to give Hugo a go (pun intended).\n  The migration wasn\u0026rsquo;t easy, as I had to convert WordPress\u0026rsquo; relational database into a static list of markdown files. Along the way, I had to configure two themes (regular and AMP), optimize all my images, JavaScript, and stylesheets, and go through every single one of my 200+ articles, looking for stylistic issues and broken links (oh boy, were there many of those!).\nHugo is written in the Go language, and it\u0026rsquo;s fairly easy to use if you\u0026rsquo;re familiar with markdown and the command line of your operating system. The trick about a static site is that all the content is stored in static files in your file server. There is no relational database to fall back on, which means that a static site can be both blazing fast and a chore to maintain.\nOne of the biggest headaches for me was how to set up site search. Without a database or a web server generating dynamic HTML documents, finding a suitable way to index the content in the browser and respond quickly and efficiently to search queries seemed like an insurmountable task.\nI tried a number of things at first, including:\n  Algolia, which I had to give up beacuse I had too much content for their free tier.\n  lunr.js running on a NodeJS virtual machine in Google\u0026rsquo;s cloud, which I had to give up because I got a bill of 400$ for instance upkeep for the month of December alone.\n  Custom-built solution that digested JSON generated by Hugo and parsed it for searching with jQuery directly in the browser, which I had to give up since downloading an indexed JSON file of around 5 megabytes on every page is not conducive to a good user experience.\n  After the failed experiment with lunr.js, I still wanted to give Google\u0026rsquo;s App Engine another chance. I\u0026rsquo;ve been in love with App Engine ever since publishing the first version of my GTM Tools on it. Well, as it turns out, App Engine has a really useful and flexible Search API for Python, which seems to be tailormade to work with the JSON generated by Hugo in a static site!\n  The setup My setup looks like this:\n  The Hugo config file is configured to output an index.json into the public directory, with all the content of my site ready for indexing.\n  A script which deploys this JSON file into the App Engine project.\n  An App Engine project which uses the Python Search API client to build an index of this JSON.\n  The App Engine project also provides an HTTP endpoint to which my site makes all the search queries. Each request is processed as a search query, and the result is returned in the HTTP response.\n  Finally, I have a bunch of JavaScript running the search form and the search results page on my site, sending the request to the App Engine endpoint as well as formatting the search results page with the response.\n  The beauty of using the Search API is that I\u0026rsquo;m well below the quota limits for the free version, and thus I don\u0026rsquo;t have to pay a dime to make it all work!\n  1. The config file modification The change to Hugo\u0026rsquo;s config file is easy to make, because Hugo has built in support for generating the JSON in a format that most search libraries digest. In the configuration file, you need to find the output configuration, and add \u0026quot;JSON\u0026quot; as one of the outputs for the home content type. So it looks something like this:\n[output] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34; ] This configuration change generates an index.json file into the root of your public folder whenever the Hugo project is built.\nHere\u0026rsquo;s an example of a what a blog post might look like in this file:\n[ { \u0026#34;uri\u0026#34;: \u0026#34;https://www.simoahava.com/upcoming-talks/\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Upcoming Talks\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;description\u0026#34;: \u0026#34;My upcoming conference talks and events\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;17 March 2018: MeasureCamp London 20 March 2018: SMX München 19 April 2018: Advanced GTM Workshop (Hamburg) 24 May 2018: NXT Nordic (Oslo) 20 September 2018: Advanced GTM Workshop (Hamburg) 14-16 November 2018: SMXL Milan I enjoy presenting at conferences and meetups, and I have a track record of hundreds of talks since 2013, comprising keynotes, conference presentations, workshops, seminars, and public trainings. Audience sizes have varied between 3 and 2,000.\\nMy favorite topics revolve around web analytics development and analytics customization, but I\\u0026rsquo;m more than happy to talk about integrating analytics into organizations, knowledge transfer, improving technical skills, digital marketing, and content creation.\\nSome of my conference slides can be found at SlideShare.\\nFor a sample, here\\u0026rsquo;s a talk I gave at Reaktor Breakpoint in 2015.\\n You can contact me at simo (at) simoahava.com for enquiring about my availability for your event.\\n\u0026#34; } ] 2. The deploy script The deploy script is a piece of Bash code which builds the Hugo site, copies the index.json into my search project folder, and then deploys the entire search project into App Engine. Here\u0026rsquo;s what it looks like:\ncd ~/Documents/Projects/www-simoahava-com/ rm -rf public hugo cp public/index.json ../www-simoahava-com-search/ rm -rf public cd ~/Documents/Projects/www-simoahava-com-search/ gcloud app deploy curl https://search-www-simoahava-com.appspot.com/update The hugo command builds the site and generates the public folder. From the public folder, the index.json is then copied to my search project folder, which is subsequently deployed into App Engine using the command gcloud app deploy. Finally, a curl command to my custom endpoint makes certain that my Python script updates the search index with the latest version of index.json.\n3. The Python code running in App Engine In App Engine, I\u0026rsquo;ve simply created a new project with a name that\u0026rsquo;s easy enough to remember as the endpoint. I haven\u0026rsquo;t added any billing to the account, because I\u0026rsquo;ve set a challenge for myself to build a free search API for my site.\nSee this documentation for a quick start guide on how to get started with Python and App Engine. Focus especially on how to setup the App Engine project (you don\u0026rsquo;t need to enable billing), and how to install and configure the gcloud command line tools for your project.\nThe Python code looks like this.\n#!/usr/bin/python from urlparse import urlparse from urlparse import parse_qs import json import re import webapp2 from webapp2_extras import jinja2 from google.appengine.api import search # Index name for your search documents _INDEX_NAME = \u0026#39;search-www-simoahava-com\u0026#39; def create_document(title, uri, description, tags, content): \u0026#34;\u0026#34;\u0026#34;Create a search document with ID generated from the post title\u0026#34;\u0026#34;\u0026#34; doc_id = re.sub(\u0026#39;[\\s+]\u0026#39;, \u0026#39;\u0026#39;, title) document = search.Document( doc_id=doc_id, fields=[ search.TextField(name=\u0026#39;title\u0026#39;, value=title), search.TextField(name=\u0026#39;uri\u0026#39;, value=uri), search.TextField(name=\u0026#39;description\u0026#39;, value=description), search.TextField(name=\u0026#39;tags\u0026#39;, value=json.dumps(tags)), search.TextField(name=\u0026#39;content\u0026#39;, value=content) ] ) return document def add_document_to_index(document): index = search.Index(_INDEX_NAME) index.put(document) class BaseHandler(webapp2.RequestHandler): \u0026#34;\u0026#34;\u0026#34;The other handlers inherit from this class. Provides some helper methods for rendering a template.\u0026#34;\u0026#34;\u0026#34; @webapp2.cached_property def jinja2(self): return jinja2.get_jinja2(app=self.app) class ProcessQuery(BaseHandler): \u0026#34;\u0026#34;\u0026#34;Handles search requests for comments.\u0026#34;\u0026#34;\u0026#34; def get(self): \u0026#34;\u0026#34;\u0026#34;Handles a get request with a query.\u0026#34;\u0026#34;\u0026#34; uri = urlparse(self.request.uri) query = \u0026#39;\u0026#39; if uri.query: query = parse_qs(uri.query) query = query[\u0026#39;q\u0026#39;][0] index = search.Index(_INDEX_NAME) compiled_query = search.Query( query_string=json.dumps(query), options=search.QueryOptions( sort_options=search.SortOptions(match_scorer=search.MatchScorer()), limit=1000, returned_fields=[\u0026#39;title\u0026#39;, \u0026#39;uri\u0026#39;, \u0026#39;description\u0026#39;] ) ) results = index.search(compiled_query) json_results = { \u0026#39;results\u0026#39;: [], \u0026#39;query\u0026#39;: json.dumps(query) } for document in results.results: search_result = {} for field in document.fields: search_result[field.name] = field.value json_results[\u0026#39;results\u0026#39;].append(search_result) self.response.headers.add(\u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;https://www.simoahava.com\u0026#39;) self.response.write(json.dumps(json_results)) class UpdateIndex(BaseHandler): \u0026#34;\u0026#34;\u0026#34;Updates the index using index.json\u0026#34;\u0026#34;\u0026#34; def get(self): with open(\u0026#39;index.json\u0026#39;) as json_file: data = json.load(json_file) for post in data: title = post.get(\u0026#39;title\u0026#39;, \u0026#39;\u0026#39;) uri = post.get(\u0026#39;uri\u0026#39;, \u0026#39;\u0026#39;) description = post.get(\u0026#39;description\u0026#39;, \u0026#39;\u0026#39;) tags = post.get(\u0026#39;tags\u0026#39;, []) content = post.get(\u0026#39;content\u0026#39;, \u0026#39;\u0026#39;) doc = create_document(title, uri, description, tags, content) add_document_to_index(doc) application = webapp2.WSGIApplication( [(\u0026#39;/\u0026#39;, ProcessQuery), (\u0026#39;/update\u0026#39;, UpdateIndex)], debug=True) At the very end, I\u0026rsquo;m binding requests to the / endpoint to ProcessQuery, and requests to /update to UpdateIndex. In other words, these are the two endpoints I am serving.\nUpdateIndex loads the index.json file, and for every single content piece within (blog posts, pages, etc.), it grabs the title, uri, description, tags, and content parameters from the content JSON, and creates documents for each instance. Then, each document is added to the index.\nThis is how the Search API can be used to translate any JSON file into a valid search index, that you can then build queries against.\nQueries are made by polling the /?q=\u0026lt;keyword\u0026gt; endpoint, where keyword matches a valid query against the Search API query engine. Each query is processed by ProcessQuery, which takes the query term, polls the search index with that term, and then compiles a result of all the documents that the search index returns for that query (in ranked order). This result is then pushed into a JSON response back to the client.\nThe search API gives you plenty of room to optimize the index and for compiling complex queries. I\u0026rsquo;ve opted for a fairly mundane approach, which might lead to some odd ranking outliers, such as documents that should clearly be at the top of a list of relevant results ending up in the end, but I\u0026rsquo;m still quite happy with how the robustness of the API.\n4. The JavaScript Finally, I need some client-side code to produce the search results page. Since Hugo doesn\u0026rsquo;t have a web server, I can\u0026rsquo;t do the search server-side - it must be done in the client. This is one of the cases where a static site loses some of its shine when compared to its counterparts that come equipped with a web server and server-side processing capabilities. A Hugo site is built and deployed all at once, so there\u0026rsquo;s no dynamic generation of HTML pages after building - everything has to happen in the client.\nAnyway, the search form on my site is very simple. It just looks like this:\n\u0026lt;form id=\u0026#34;search\u0026#34; action=\u0026#34;/search/\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;q\u0026#34; type=\u0026#34;text\u0026#34; class=\u0026#34;form-control input--xlarge\u0026#34; placeholder=\u0026#34;Search blog...\u0026#34; autocomplete=\u0026#34;off\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; When the form is submitted, it does a GET request to the /search/ page on my site, adding whatever was typed into the field as the q query parameter, so the URL becomes something like\nhttps://www.simoahava.com/search/?q=google+tag+manager\nOn the /search/ page, I have a loading spinner which waits until the request to the search endpoint is completed. The search call is done with JavaScript, and it looks like this:\n(function($) { var printSearchResults = function(results) { // Update the page DOM with the search results... \t}; var endpoint = \u0026#39;https://search-www-simoahava-com.appspot.com\u0026#39;; var getQuery = function() { if (window.location.search.length === 0 || !/(\\?|\u0026amp;)q=/.test(window.location.search)) { return undefined; } var parts = window.location.search.substring(1).split(\u0026#39;\u0026amp;\u0026#39;); var query = parts.map(function(part) { var temp = part.split(\u0026#39;=\u0026#39;); return temp[0] === \u0026#39;q\u0026#39; ? temp[1] : false; }); return query[0] || undefined; }; $(document).ready(function() { var query = getQuery(); if (typeof query === \u0026#39;undefined\u0026#39;) { printSearchResults(); return; } else { $.get(endpoint + \u0026#39;?q=\u0026#39; + query, function(data) { printSearchResults(JSON.parse(data)); }); } }); })(window.jQuery)  To keep things simple, I\u0026rsquo;ve only included the relevant pieces of code that can be used elsewhere, too. In short, when the /search/ page is loaded, whatever is included as the value of the q query parameter is immediately sent to the search API endpoint. The response is then processed and built into a search results page.\nSo, if the page URL is https://www.simoahava.com/search/?q=google+tag.manager, this piece of JavaScript turns that into a GET request to https://search-www-simoahava-com.appspot.com/?q=google+tag+manager. You can visit that URL to see what the response looks like.\nThis response is the processed, and the search results page is built.\nSummary This is how I\u0026rsquo;ve chosen to build site search using the flexibility of Hugo together with the powerful Search API offered by Google\u0026rsquo;s App Engine.\nBased on my limited amount of research, it\u0026rsquo;s as good a solution as any, and it seems quite fast without compromising the power of the search query engine. However, as more content builds up, it\u0026rsquo;s conceivable that the query engine either gets slower or I start hitting my free tier quotas, at which point I\u0026rsquo;ll need to rethink my approach.\nThe weak link at the moment is that everything is done client-side. That means that contrary to the philosphy of static sites, a lot of processing takes place in the browser. But I\u0026rsquo;m not sure how this could be avoided, since a static site doesn\u0026rsquo;t offer you the capabilities of a server-side processor.\nAt this time, it\u0026rsquo;s a trade-off I\u0026rsquo;m willing to make, but I am anxious to hear feedback if the search is clumsy or not working properly for you.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/new-tools-released/",
	"title": "New Tools Released",
	"tags": ["google tag manager", "gtm tools", "google sheets", "api"],
	"description": "Introducing GTM Tools by Simo Ahava, a Google Sheets plugin for Google Tag Manager management. Also releasing Slack integration to the original GTM Tools (www.gtmtools.com)",
	"content": "Over the past few weeks, I\u0026rsquo;ve been coding like crazy. The three biggest outcomes of this frenzy have been this new blog design (switched finally away from WordPress and took the plunge back into the world static sites using Hugo), a new Google Sheets add-on for managing Google Tag Manager containers and assets, and a Slack integration in GTM Tools. In this article, I\u0026rsquo;ll quickly introduce the last two, as I\u0026rsquo;m writing a separate article about the site redesign.\nSlack integration for www.gtmtools.com   GTM Tools is a free set of web tools for managing your Google Tag Manager containers, tags, triggers, and variables. There\u0026rsquo;s a bunch of things you can do with the toolset, and I urge you to follow the link in the beginning of this paragraph to learn more, or simply go to https://www.gtmtools.com/ to start using the tool.\nAnyway, a recent addition, inspired by Jeffrey Gomez\u0026rsquo; idea in #measure Slack, was a Slack integration with your GTM container. What this means in practice is that once the integration is enabled, a bot named GTM Tools will enter the channel of your choice. Every 15 minutes, it will check if there\u0026rsquo;s a new, published version of the container you made the integration in, and if one is found, it will inform of this to the channel.\n  To enable the integration, you need to browse to a container page in GTM Tools, and then click the Click here to enable integration link. A modal dialog opens, which instructs you to add the GTM Tools service account email address as a Read user to the GTM container in question. You must do this for the integration to work - GTM Tools must be allowed to query the published version if you want it to inform you when a published version is created.\nOnce you\u0026rsquo;ve added the user, you will need to click the now activated Add to Slack button, which will take you to Slack\u0026rsquo;s own portal. There you\u0026rsquo;ll need to select the workspace and channel to which the GTM Tools bot will be added.\nAnd then you\u0026rsquo;re good to go! You can test the integration by publishing the container, and then waiting for the next 15 minute interval to pass. The bot checks the container every 15 minutes on the hour, so :00, :15, :30, :45 and so on.\nAt some point I hope to develop the bot further, perhaps even adding conversational capabilities to it.\nNew Google Sheets add-on One of the things I\u0026rsquo;ve been using the Google Tag Manager API for since it was introduced is documentation. Until recently, I\u0026rsquo;d been using a simple command-line Python script to output a CSV file of all the relevant fields.\nHowever, GTM has a nifty integration with Apps Script, which means you can access the Google Tag Manager API using Google Sheets\u0026rsquo; script editor, for example. And what better way to create and manage documentation than spreadsheets? Well, truthfully, I\u0026rsquo;m sure there are many better ways to do it, but Google Sheets has proven to be just fine for most of my documentation needs.\nAnyway, I have written and published a Google Sheets add-on which lets you automatically generate documentation from any container you have access to. In addition to creating the documentation, you can also mass-update the \u0026ldquo;Notes\u0026rdquo; field in all your tags, triggers, and variables. So now there\u0026rsquo;s no excuses left NOT to document carefully what each tag, trigger, and variable does.\n  You can read all about the add-on from its dedicated tools page in this blog.\nFeedback? I would really like to get feedback on all my tools, so please drop me a comment in this article or in the relevant tools page.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/opt-out-of-google-analytics-tracking/",
	"title": "#GTMTips: Opt-out Of Google Analytics Tracking",
	"tags": ["googletagmanager", "gtmtips", "opt-out", "google analytics", "tag sequencing", "customtask"],
	"description": "You can have your site visitors easily opt-out of Google Analytics tracking by setting a specific global variable before the first Google Analytics tag fires on the page.",
	"content": "A while ago I posted a #GTMTips post where I detailed the steps you can take to opt-out of all Google Analytics tracking and the DoubleClick redirects that often follow. It was a fun exercise, but because it relies on preventing requests on a tag-by-tag basis (using the ubiquituous customTask), it can be a chore to handle in large containers.\nIn this article, we\u0026rsquo;ll continue with the theme of opting out from Google Analytics tracking by leveraging a solution provided by the tool itself. To make it work, we\u0026rsquo;ll use a Custom HTML tag together with some tag sequencing.\nTip 68: Opt-out of Google Analytics tracking with a global variable   The trick is to set a global variable before the first GA tag fires on the page. This is crucial, because the global variable needs to be created before the tracker object is created. So, unfortunately, customTask will not work this time (I know, HUGE disappointment!).\nThe variable itself is very simple to create. It needs to look like this:\nwindow['ga-disable-UA-XXXXXX-Y'] = true\nHere, UA-XXXXXX-Y is the tracking ID you want to block for all subsequent GA requests sent on the page. You can create multiple global variables like this, one for each tracking ID you want to block.\nSo, if I wanted to block GA tracking to UA-12345-1 when the user has a specific cookie in their browser, I could use something like this in a Custom HTML tag:\n\u0026lt;script\u0026gt; if ({{Cookie - _ga_opt_out}} === \u0026#39;true\u0026#39;) { window[\u0026#39;ga-disable-{{GA ID}}\u0026#39;] = true; } \u0026lt;/script\u0026gt; Here, {{Cookie - _ga_opt_out}} is an (imaginary) first party cookie, which stores the value true if the user has opted out of tracking on my (imaginary) site. {{GA ID}} is a constant variable that returns the Google Analytics tracking ID the user is opting-out from.\nOr, if I want to use the doNotTrack feature that most browsers let you set in their settings:\n\u0026lt;script\u0026gt; if (navigator.doNotTrack \u0026amp;\u0026amp; navigator.doNotTrack === 1) { window[\u0026#39;ga-disable-{{GA ID}}\u0026#39;] = true; } \u0026lt;/script\u0026gt; Note that you should not add any triggers to this Custom HTML tag, as it will only fire when in a tag sequence.\nFinally, you need to find the first Universal Analytics tag that fires on the page with that tracking ID. Typically this would be a Page View tag which fires on the All Pages trigger.\nThen, scroll down to its Advanced Settings, and make sure the Tag Sequencing setting is setup as follows:\n  This setting ensures that before the Page View tag fires, your opt-out script has time to complete, and set the browser to opt-out of Google Analytics tracking to the tracking ID established in the global variable.\nThat\u0026rsquo;s all there is to it, really. The obvious downside is that if you are tracking to more than one GA property, you\u0026rsquo;ll need to exclude all of them. You\u0026rsquo;ll also need to modify the conditional statement in the script to match whatever opt-out scheme your website offers.\n"
},
{
	"uri": "https://www.simoahava.com/tags/googletagmanager/",
	"title": "googletagmanager",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/opt-out/",
	"title": "opt-out",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/upcoming-talks/",
	"title": "Upcoming Talks",
	"tags": [],
	"description": "Simo Ahava&#39;s upcoming conference schedule and event participation.",
	"content": "  March 4, 2021: GAUC Emerce, virtual March 6, 2021: MeasureCamp Nordics, virtual May 11, 2021: Marketing Analytics Summit, virtual    I enjoy presenting at conferences and meetups, and I have a track record of hundreds of talks since 2013, comprising keynotes, conference presentations, workshops, seminars, and public trainings. Audience sizes have varied between 3 and 2,000.\nMy favorite topics revolve around web analytics development and analytics customization, but I\u0026rsquo;m more than happy to talk about integrating analytics into organizations, knowledge transfer, improving technical skills, digital marketing, and content creation.\nSome of my conference slides can be found at SlideShare.\nFor a sample, here\u0026rsquo;s a talk I gave at Reaktor Breakpoint in 2015.\n  You can contact me at simo (at) simoahava.com for enquiring about my availability for your event.\n"
},
{
	"uri": "https://www.simoahava.com/tools/gtm-tools-by-simo-ahava/",
	"title": "GTM Tools Add-on For Google Sheets",
	"tags": [],
	"description": "",
	"content": "The GTM Tools by Simo Ahava is a Google Sheets add-on. It lets you manage and update your Google Tag Manager containers, tags, triggers, and variables.\n  In this first iteration, the tools feature a Documentation builder, which lets you automatically generate documentation for the latest version of your Google Tag Manager container, tags, triggers, and variables. In addition to this, the toolset features a Push changes to Notes functionality, which lets you automatically mass update the Notes field across your tags, triggers, and variables.\n  How to get it In Google Sheets, open the Add-ons menu, and click Get add-ons.\n  You should see the Chrome web store window open. Enter \u0026ldquo;GTM Tools by Simo Ahava\u0026rdquo; into the search field and press Enter. You should see the add-on appear in the search results.\n  Click the + FREE button. You will be asked to sign in with your Google ID, and then approve the add-on access to your data. The add-on requires read and write access to your Google Tag Manager data, and read and write access to your Google Sheets account.\nYou are then ready to use the tool!\nBuild documentation You should see the GTM Tools by Simo Ahava menu item in the Add-ons menu. You might need to reload the page if it isn\u0026rsquo;t there, or if it seems to be missing all its menu items.\nWhen you click Build documentation, the add-on will prompt you for a Google Tag Manager account and container. The list will include all containers your current logged in Google user ID has access to. Once you\u0026rsquo;ve selected a container, click Build to start the process.\n  The builder creates four new sheets: one for your container version, and one each for your tags, triggers, and variables.\n  As you can see, each sheet is prefixed with the container ID.\nEach sheet will contain useful information about your container, based on the Latest version of your container. If no version has been created yet, the documentation builder will not work.\n Never rename these default sheets created by the tool! They are necessary for the add-on to work.\n Mark changes to Notes The next thing you can do is manually edit the Notes field of the tags, triggers, and variables sheets. You can edit or create notes entries for each item, if you wish.\nOnce you\u0026rsquo;ve made all the changes to the Notes fields (changes to other fields are ignored, and you should NEVER, EVER, modify the JSON column), choose Mark changes to Notes in the add-on menu. A new dialog opens.\n  When you click Mark changes, the tool will go through all the tag, trigger, and variable documentation sheets, and highlight each field that diverges from the latest version. This is to give you a visual clue of which fields have changed values within.\n  The dialog will also tell you how many fields were highlighted as a result.\nPush changes to Notes Once you\u0026rsquo;ve made changes to the notes fields, you can push these changes to a GTM container. The container is automatically selected based on the sheet you have active in Google Sheets. So if you have active one of the documentation sheets prefixed for GTM-XXXXX-X, then that will be used as the target container for the push. If you have selected one of the sheets prefixed with GTM-YYYYY-Y, then that will be used, etc.\nSee the screenshot at the beginning of this article for an example of what the modal looks like when you have a valid sheet selected.\nOnce you click PUSH CHANGES, the tool will update all the tags, triggers, and variables in the sheets for this container, modifying the notes field of each to match the changes you made. The changes will be done in the workspace you chose using the selector in the modal.\n  When the push is complete, the modal will inform you how many changes were made. You can also click a link in the modal to have the tools create a new container version of the workspace you just updated.\n   This is at your own risk. If the workspace doesn\u0026rsquo;t validate, it will result in an error. Note that all changes in the workspace will be included in the new version.\n After you\u0026rsquo;ve pushed the changes, you should really create a new version as soon as possible. Otherwise the documentation will always interpret the changes to the notes to be new changes, because they are always compared against the latest container version.\nWhen a new version has been created, you should always re-run Build documentation.\nWhen rebuilding the documentation, the add-on will always show the following prompt if a sheet has already been created by the tool. You will have the option of overwriting the sheet or skipping it in the documentation build process. Unless you\u0026rsquo;ve made some manual changes to the sheets generated by the documentation (I\u0026rsquo;d recommend against this), it\u0026rsquo;s a good idea to always allow the add-on to overwrite the sheets\u0026rsquo; contents with the data from the latest version of the container.\n  Words of caution For the add-on to work properly, follow the following guidelines:\n  Never edit the sheet names - it\u0026rsquo;s imperative that the sheets use the default names created by the add-on\n  Never delete or modify the named ranges created by the add-on\n  Never edit values in the JSON column\n  Changing other column values than Notes will not do anything. At some point I might add the option to mass update the other fields, too.\nPrivacy Policy What information do you collect? The GTM Tools by Simo Ahava add-on collects no information from its users. It is an API tool used for managing and updating Google Tag Manager containers, tags, triggers, and variables.\nThe only thing the add-on logs is the generic Google Cloud Console API usage statistics, which tells the owner how much the enabled APIs are being used, but this data cannot be used to identify users or individual use patterns.\nHow do you use the information? No user or usage information is used. The only thing the owner monitors is API usage, so that it can be determined if quotas need to be increased to ensure the tool works smoothly.\nWhat information do you share? No information is shared with third parties, with other users, with analytics tools, with marketing partners, or any other party.\nTerms of Service Terms of Service (\u0026ldquo;Terms\u0026rdquo;) Last updated: October 30, 2018\nPlease read these Terms of Service (\u0026ldquo;Terms\u0026rdquo;, \u0026ldquo;Terms of Service\u0026rdquo;) carefully before using the Custom Dimension Manager For Google Analytics extension (the \u0026ldquo;Service\u0026rdquo;) operated by Simo Ahava (\u0026ldquo;us\u0026rdquo;, \u0026ldquo;we\u0026rdquo;, or \u0026ldquo;our\u0026rdquo;).\nYour access to and use of the Service is conditioned on your acceptance of and compliance with these Terms. These Terms apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service you agree to be bound by these Terms. If you disagree with any part of the terms then you may not access the Service.\nLinks To Other Web Sites Our Service may contain links to third-party web sites or services that are not owned or controlled by us.\nWe have no control over, and assume no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that we shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services.\nWe strongly advise you to read the terms and conditions and privacy policies of any third-party web sites or services that you visit.\nGoverning Law These Terms shall be governed and construed in accordance with the laws of Finland, without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect. These Terms constitute the entire agreement between us regarding our Service, and supersede and replace any prior agreements we might have between us regarding the Service.\nChanges We reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.\nBy continuing to access or use our Service after those revisions become effective, you agree to be bound by the revised terms. If you do not agree to the new terms, please stop using the Service.\nContact Us If you have any questions about these Terms, please contact us.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/auto-link-domains-with-regex/",
	"title": "#GTMTips: Auto Link Domains With Regex",
	"tags": ["googletagmanager", "gtmtips", "customtask", "auto-link domains", "cross-domain tracking"],
	"description": "Use Google Tag Manager to add load listeners to your asynchronously loading script elements. The load listener will inform GTM once the script has completely loaded, helping you avoid race conditions.",
	"content": " Update 5 March 2019 due to GTM not supporting negative lookbehinds any more.\n Google Tag Manager makes it fairly easy to do cross-domain tracking. Basically, you list the hostnames you want to automatically decorate with linker parameters in the Auto-Link Domains field of your Page View tag, and that takes care of decorating the URLs with the necessary parameter. It\u0026rsquo;s dead easy, even if there are a bunch of traps you need to watch out for (see my post on troubleshooting cross-domain tracking issues).\nHowever, for some reason, GTM doesn\u0026rsquo;t support regular expressions in the Auto-Link Domains field. The autoLink method supports both strings and regular expression literals, but GTM only uses the first. Hopefully, at some point this will be fixed and this article will become obsolete. The reason we want regular expressions is to be able to do things like negative lookbehinds, where we specify that a subset of domains of any given hostname be excluded from decoration. In the image below, you can see an example of this. The regular expression specifies that all other domains except www.simoahava.com should be decorated with cross-domain parameters.\nTo make this work in GTM of today, we\u0026rsquo;ll use the amazing customTask feature (see here for all my customTask solutions).\nTip 67: Use customTask to auto-link domains based on regular expressions   The trick is to use a Custom JavaScript variable as the value of the customTask field. This is what the variable looks like:\nfunction() { return function(model) { var domainList = [ /^(?!dev)[^.]*\\.?hostname\\.com$/, \u0026#39;somedomain.com\u0026#39;, \u0026#39;www.domain.com\u0026#39;, /^example\\.com$/ ]; var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; var name = model.get(\u0026#39;name\u0026#39;); ga(name + \u0026#39;.require\u0026#39;, \u0026#39;linker\u0026#39;); ga(name + \u0026#39;.linker:autoLink\u0026#39;, domainList); }; }  In the domainList array, you need to add all the domains you want to automatically link separating each value with a comma. You can specify both strings (wrap them in quotes) and regular expression literals. When you use strings, the auto-linker uses an open-ended pattern match. Thus if you have 'example.com' as one member of the array, GTM will decorate example.com and all its subdomains.\nRegular expressions give you the power of zero-length assertions such as negative and positive lookbehinds. This makes it easy to match, for example, all subdomains of example.com except for test.example.com.\n/^(?!test)[^.]*\\.?example\\.com$/\nThis regular expression reads out as: match any string that doesn\u0026rsquo;t start with test, but contains example.com or any subdomain other than test.example.com. The clumsy positive lookahead is necessary because GTM doesn\u0026rsquo;t currently (as of March 2019) support negative lookbehinds.\nThe rest of the variable basically invokes the Universal Analytics tracker object used in the current hit, loads the linker plugin, and then decorates the domain list with cross-domain parameters.\nAll you then have to do is add this variable to the value of the customTask field in your Page View tag\u0026rsquo;s Fields to set.\n  This simple fix will make auto-linking domains for cross-domain tracking purposes much more flexible with Google Tag Manager. But, as I mention in the beginning of this article, I really wish that the Auto-Link Domains field in the tag settings will soon support regular expressions, just like the linker plugin regularly does!\n"
},
{
	"uri": "https://www.simoahava.com/tags/auto-link-domains/",
	"title": "auto-link domains",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/add-clientid-to-custom-dimension-gtag-js/",
	"title": "Add Client ID To Custom Dimension With gtag.js",
	"tags": ["gtag", "Universal Analytics", "client id", "custom dimension"],
	"description": "Quick guide on how to add the Universal Analytics Client ID as a custom dimension when using the new global tag (gtag.js).",
	"content": "When Google released gtag.js, the new, global tracking library designed to (eventually) replace analytics.js, many Universal Analytics practitioners and users were confused (see e.g. Jeff\u0026rsquo;s great overview here). It seemed like gtag.js wasn\u0026rsquo;t really solving any immediate problem, since analytics.js had done a bang-up job with Universal Analytics tracking for all these years. However, gtag\u0026rsquo;s modus operandi is the ability to leverage the same semantic information (distributed across dataLayer!) across a number of Google products, starting with GA and AdWords.\nBut migrating to gtag.js isn\u0026rsquo;t just a find-and-replace operation - there are many things to consider due to the fact that they are completely different tracking libraries, and feature parity is yet to be reached.\n  One of the things I was really concerned about was how to add my favorite custom dimension to the hits: the Client ID stored in the _ga cookie. With gtag.js, this is actually ridiculously easy, and you don\u0026rsquo;t need to leverage customTask or the ga.getAll() tracker method (which still does exist when using gtag.js!).\nHuge thanks to Yamata Ryoda for pointing this method out. You can read the original tip (in Japanese) here.\nHow to add Client ID to a custom dimension with gtag.js So after the long preamble here\u0026rsquo;s the tip in all its glory:\ngtag(\u0026#39;config\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, { \u0026#39;custom_map\u0026#39;: { \u0026#39;dimensionX\u0026#39;: \u0026#39;clientId\u0026#39; } });  Just replace UA-12345-1 with your Universal Analytics tracking ID, and the X in dimensionX with the custom dimension index, and gtag.js takes care of the rest.\nSuch a simple way to do it. My main gripe right now is that this isn\u0026rsquo;t officially documented, and we don\u0026rsquo;t know what other \u0026ldquo;special\u0026rdquo; values gtag.js hides under its hood. And what if I wanted to send the string \u0026ldquo;clientId\u0026rdquo; to GA as the value of that custom dimension? Hmm. Might not be that common.\nSummary So there is a really easy way to add the Client ID to a custom dimension when using gtag.js. It\u0026rsquo;s so easy, in fact, that you should do it right now. Sending the Client ID to Google Analytics is almost necessary in order to see reports distributed row-by-row, where each row is a distinct Google Analytics user. You only get this otherwise in the User Explorer reports.\n"
},
{
	"uri": "https://www.simoahava.com/tags/custom-dimension/",
	"title": "custom dimension",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/gtag/",
	"title": "gtag",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/add-load-listener-script-elements/",
	"title": "#GTMTips: Add A Load Listener To Script Elements",
	"tags": ["googletagmanager", "gtmtips", "javascript", "event listener", "race condition"],
	"description": "Use Google Tag Manager to add load listeners to your asynchronously loading script elements. The load listener will inform GTM once the script has completely loaded, helping you avoid race conditions.",
	"content": "One of the challenges in working with Google Tag Manager (or any JavaScript-based platform for that matter) is what to do with race conditions. A race condition emerges when you have two resources competing for execution in the browser, and there is a degree of unpredictability to which \u0026ldquo;wins\u0026rdquo; the race.\nA prime example is working with jQuery. It\u0026rsquo;s one of the most popular JavaScript libraries out there, and websites utilize it for a multitude of things, many useful for Google Tag Manager, too. For example, jQuery trivializes asynchronous HTTP requests and DOM traversal, both of which can cause headaches to GTM users.\nHowever, since jQuery is often, and should often be, downloaded asynchronously, there\u0026rsquo;s the risk that jQuery hasn\u0026rsquo;t loaded yet when GTM starts executing your tags. Thus we need some mechanism to let Google Tag Manager know when an asynchronously downloaded or requested resource has become available.\nTip 66: Add a load listener to script elements   There are two ways you can go about this. The first one is to use a Custom HTML tag, and then fire a dataLayer.push() once the resource has completely loaded. The second way is to use tag sequencing, and tell GTM that the setup tag (where you load the script) has completed by using the internal onHtmlSuccess() method. But I\u0026rsquo;m getting ahead of myself.\ndataLayer.push() in the load listener callback The first method is to create a Custom HTML tag that fires as early as possible. So you\u0026rsquo;d want to add the All Pages trigger to it, so that it fires as soon as the GTM container has loaded.\nAt this point it\u0026rsquo;s important that you do not load the script by simply adding the \u0026lt;script src=\u0026quot;url_to_jquery\u0026quot; async=\u0026quot;true\u0026quot;\u0026gt; into the Custom HTML tag. Instead, we\u0026rsquo;ll introduce some extra control by using JavaScript to create the element, add the listener to it, and inject it manually to the page. You could use the onload attribute directly in the element, but the risk here is that you overwrite any existing onload attribute. By using JavaScript DOM manipulation methods, you won\u0026rsquo;t mess with any pre-existing listeners.\n\u0026lt;script\u0026gt; (function() { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://code.jquery.com/jquery-3.2.1.js\u0026#39;; el.async = \u0026#39;true\u0026#39;; el.addEventListener(\u0026#39;load\u0026#39;, function() { window.dataLayer.push({ event: \u0026#39;jQueryLoaded\u0026#39; }); }); document.head.appendChild(el); })(); \u0026lt;/script\u0026gt; When this Custom HTML tag is run by GTM, the browser creates an asynchronous request to download the jquery-3.2.1.js from the CDN (Content Distribution Network). Once this asynchronous process is over, a dataLayer.push({event: 'jQueryLoaded'}) fires, and you can then build a Custom Event trigger for this event name to fire any tags that are dependent on jQuery having loaded.\nUsing Tag Sequencing If you only have a single tag that needs the asynchronously downloaded resource, a fairly elegant way to do this would be with tag sequencing. With tag sequencing, you would create a Custom HTML tag for this code, and add it as the Setup tag in the sequence. The logic is that the asynchronous request is executed in the Setup tag, and once it\u0026rsquo;s complete, the main tag can fire with confidence that the resource it is dependent on has completely loaded.\nThis is what the Custom HTML tag for the Setup tag would look like:\n\u0026lt;script\u0026gt; (function() { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://code.jquery.com/jquery-3.2.1.js\u0026#39;; el.async = \u0026#39;true\u0026#39;; el.addEventListener(\u0026#39;load\u0026#39;, function() { window.google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}}); }); document.head.appendChild(el); })(); \u0026lt;/script\u0026gt; Note that for this to work, you must enable the Built-in variables Container ID and HTML ID.\nThe mysterious window.google_tag_manager[].onHtmlSuccess() method is an internal function you need to use in tag sequencing if you want GTM to wait for some code (e.g. asynchronous callbacks) to execute before moving from the setup tag to the main tag. If you didn\u0026rsquo;t have this piece of code here, GTM would simply jump straight to the main tag after executing the last line of the setup tag, and in that case it\u0026rsquo;s very possible that jQuery hasn\u0026rsquo;t completely downloaded yet.\nSummary These two methods can be used to get the best of both worlds: asynchronous requests WITH predictability. Race conditions can be brutal and difficult to identify. It\u0026rsquo;s only once you start logging JavaScript errors that you might notice an increase in error messages like jQuery is not defined. This is a signal that you might be trying to use a resource before it has completely loaded. Using a load listener is a handy way to combat this.\n"
},
{
	"uri": "https://www.simoahava.com/tags/event-listener/",
	"title": "event listener",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/race-condition/",
	"title": "race condition",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/search/",
	"title": "Search results",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/seo/",
	"title": "SEO",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/google-tag-manager/",
	"title": "Google Tag Manager",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/gtm-tips/",
	"title": "GTM Tips",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/",
	"title": "Google Tag Manager and Google Analytics",
	"tags": [],
	"description": "Simo Ahava is a Google Developer Expert for Google Analytics and Google Tag Manager. This is a blog all about web analytics development.",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/conference-bio/",
	"title": "Conference Biography",
	"tags": [],
	"description": "",
	"content": "   Simo Ahava is a recognized expert on customizing web analytics and tag management solutions to improve the entire \u0026ldquo;life cycle\u0026rdquo; of data collection, processing, and reporting. His main areas of expertise lie with Google Analytics and Google Tag Manager, and Google has appointed him as a Google Developer Expert in these fields. He is especially interested in communication structures within organizations, as he firmly believes that communication breakdowns are the underlying cause to practically all data-related problems.\nSimo is particularly invested in demystifying analytics development work, and his main focus is on increasing awareness, skills, and critical thinking around data and development.\nSimo is a partner and co-founder at 8-bit-sheep. He also writes a popular blog on all things Google Analytics and Google Tag Manager development at www.simoahava.com. An experienced speaker and prolific blogger, Simo can be seen and heard in conferences, product forums, support communities, and developer meet-ups alike.\nTwitter: @SimoAhava\nGoogle+: +SimoAhava\nLinkedIn: http://fi.linkedin.com/in/simoahava\n"
},
{
	"uri": "https://www.simoahava.com/analytics/send-event-custom-dimension-google-optimize-experiment-running/",
	"title": "Send Event And Custom Dimension If Google Optimize Experiment Is Running",
	"tags": ["customtask", "google analytics", "google optimize", "Google Tag Manager", "universal analytics"],
	"description": "Use customTask to detect if Google Optimize is running an experiment, and send this data to Google Analytics for segmenting.",
	"content": "I really like Google Optimize. It has a fairly intuitive UI, setting up experiments is easy, and there\u0026rsquo;s integrations for both Google Tag Manager and Google Analytics built into the system. It\u0026rsquo;s still a JavaScript-based, client-side A/B-testing tool, so problems with flicker and asynchronous loading are ever-present (though this is somewhat mitigated by the page-hiding snippet).\nOne issue with the Google Analytics integration is the difficulty of creating segments for sessions where the users were actively participating in the experiment. In fact, there\u0026rsquo;s no specific signal in Google Analytics that tells you the NOW the user is \u0026ldquo;experiencing\u0026rdquo; an experiment. You\u0026rsquo;ll find the Experiment Name and Experiment ID dimensions, yes, but they have one drawback:\nThe Experiment Name and Experiment ID dimensions are user-scoped for the duration of the experiment!\nSo if the user takes part in an experiment, all their hits and sessions from that moment on until the experiment is over will be annotated with the Experiment Name and Experiment ID values.\n  But I want to know exactly and only the sessions where my users were seeing the experiment content. I want to use this information to create segments where I can view other conversion goals or funnels than those configured into Optimize. This is increasingly important if you want to use Optimize for some quick personalization proofs-of-concept, since then it\u0026rsquo;s vital to know how the user reacted during the session when they saw the change in the content.\nAnyway, to make this whole thing work with the rarified analytics.js snippet and Google Tag Manager, we will use a feature I have seldom written about: customTask. OK, I\u0026rsquo;ve written about it plenty.\nWith customTask, we can automatically add a session-scoped Custom Dimension to all hits that took part in an experiment, and in some edge cases we\u0026rsquo;ll also send an event to GA to make sure the data is carried over.\nHow it works When you land on a page where an Optimize experiment is running (i.e. the page is (one of) the target(s) of the experiment), your Google Analytics hits will contain the \u0026amp;exp parameter with a value consisting of all the experiment IDs and variant IDs the user is participating in:\n  This key is what GA then uses to attribute the user to these particular experiments. So once that hit reaches GA, my Client ID will be associated with those two experiment IDs until the experiment is over.\nNow, what I actually want to happen is for a Custom Dimension to be added to that Page View hit if it has the \u0026amp;exp key. For this, I need to use customTask so that it can sniff the requests to Google Analytics, and in case they contain this key, dynamically add the Custom Dimension parameter with the experiment string. That way I\u0026rsquo;ll have the experiment data sent to Google Analytics nicely in the Custom Dimension of my choice!\nIf you\u0026rsquo;re using Google Tag Manager to deploy Google Optimize, the same code will apply, but instead of leveraging a Page View hit, the Optimize tag will use a special hitType === 'data' hit, which takes your experiment data to GA.\n  Unfortunately for us, this data hit type is not exposed in Google Analytics, so we can\u0026rsquo;t use that to segment our visitors, nor does piggy-backing it with a Custom Dimension do anything. So, we need to configure Google Tag Manager to actually send an extra event when a data hit is encountered that also has the \u0026amp;exp parameter with it.\nImplement via analytics.js If you\u0026rsquo;re using the analytics.js snippet, here\u0026rsquo;s what you need to do:\n\u0026lt;script\u0026gt; // Analytics.js snippet  (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;https://www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-XXXXXX-Y\u0026#39;,\u0026#39;auto\u0026#39;) // NEW: Add customTask field to tracker  ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, function(model) { // Change this to the Custom Dimension index to which you want to send the experiment data!  var customDimensionIndex = \u0026#39;6\u0026#39;; // Make sure the new hit is only generated once (thanks Vibhor Jain!)  var hasNewHitBeenGenerated = false; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; var originalSendTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); if (sendModel.get(\u0026#39;exp\u0026#39;)) { if (sendModel.get(\u0026#39;hitType\u0026#39;) === \u0026#39;data\u0026#39; \u0026amp;\u0026amp; !hasNewHitBeenGenerated) { var tracker = sendModel.get(\u0026#39;name\u0026#39;); originalSendTask(sendModel); ga(tracker + \u0026#39;.send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;Optimize\u0026#39;, sendModel.get(\u0026#39;exp\u0026#39;), {nonInteraction: true}); hasNewHitBeenGenerated = true; return; } if (hitPayload.indexOf(\u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex + \u0026#39;=\u0026#39;) === -1) { sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload + \u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex + \u0026#39;=\u0026#39; + sendModel.get(\u0026#39;exp\u0026#39;), true); } } originalSendTask(sendModel); }); }); // NEW BLOCK ENDS  ga(\u0026#39;require\u0026#39;, \u0026#39;GTM-XXXXXX\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; The change to the original Optimize-modified analytics.js snippet is the entire block starting with // NEW:... and ending with // NEW BLOCK ENDS.\nThis code listens to all GA hits sent with the default tracker, and if they have the \u0026amp;exp parameter, then the Custom Dimension is dynamically added to the hits, along with the experiment ID string the user is associated with. Note that you must change the value of var customDimensionIndex to reflect the Custom Dimension index you\u0026rsquo;ve created in Google Analytics. I prefer to use a Session-scoped Custom Dimension, but you could use hit-scoped, too, for increased granularity.\n  That\u0026rsquo;s all you need to do with analytics.js. On pages not included in the experiment, no Custom Dimension or any extra hit is sent. Business will be as usual.\nImplement via Google Tag Manager In Google Tag Manager, you need to create a new Custom JavaScript variable, and give it a name like {{JS - customTask - Optimize experiment}}. The variable should look like this:\nfunction() { return function(model) { // Change this to the Custom Dimension index to which you want to send the experiment data!  var customDimensionIndex = \u0026#39;6\u0026#39;; // Make sure the new hit is only generated once (thanks Vibhor Jain!)  var hasNewHitBeenGenerated = false; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; var originalSendTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); if (sendModel.get(\u0026#39;exp\u0026#39;)) { if (sendModel.get(\u0026#39;hitType\u0026#39;) === \u0026#39;data\u0026#39; \u0026amp;\u0026amp; !hasNewHitBeenGenerated) { var tracker = sendModel.get(\u0026#39;name\u0026#39;); originalSendTask(sendModel); ga(tracker + \u0026#39;.send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;Optimize\u0026#39;, sendModel.get(\u0026#39;exp\u0026#39;), {nonInteraction: true}); hasNewHitBeenGenerated = true; return; } if (hitPayload.indexOf(\u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex + \u0026#39;=\u0026#39;) === -1) { sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload + \u0026#39;\u0026amp;cd\u0026#39; + customDimensionIndex + \u0026#39;=\u0026#39; + sendModel.get(\u0026#39;exp\u0026#39;), true); } } originalSendTask(sendModel); }); }; }  It\u0026rsquo;s pretty much exactly the same code you would use in the analytics.js.\nThen, go to your Google Optimize tag, and scroll down to Fields to set. Add a new field with:\nField name: customTask\nValue: {{JS - customTask - Optimize experiment}}\nSo that it looks like this:\n  And that should be it. Now when you browser the experiment pages, you should see a hit with type 'data' being sent, and immediately after that an event hit which looks like this:\n  Summary I really wish that the Optimize / Google Analytics integration would give us a hit or dimension we could use to segment sessions that actively participated in an experiment. Right now you either need to replicate the test conditions in a segment (and if you sample only a part of your visitors even this won\u0026rsquo;t cut it), or use a solution like the one described in this article.\nEven if this were superfluous and unnecessary, I\u0026rsquo;m giddy with excitement to show yet another cool use case for customTask. I\u0026rsquo;m fairly certain that if we hadn\u0026rsquo;t already given our baby boy a gorgeous name, the world would have come to know him as customTask Ahava. For now, it will have to do as his nickname. That\u0026rsquo;s how much I love customTask!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/respect-opt-out-from-advertising-and-analytics/",
	"title": "#GTMTips: Respect Opt-Out From Advertising And Analytics",
	"tags": ["customtask", "googletagmanager", "gtmtips", "universal analytics"],
	"description": "Use customTask to block Google Analytics and the DoubleClick redirect in case the user has opted out from tracking and/or advertising.",
	"content": "With GDPR looming around the corner, it\u0026rsquo;s time to explore the options you have at your disposal for respecting the new, stricter regulations for tracking users and for collecting data about their visits to your website.\n UPDATE 20 June 2018: Google has released the allowAdFeatures field which renders the solution below redundant (at least for the displayFeaturesTask part of it). Please refer to this article for more details on how to conditionally block the advertising hit to DoubleClick.\n In this article, we\u0026rsquo;ll explore the wonderful customTask (again), to see how you can programmatically prevent the request to Google Analytics or the redirect to DoubleClick from ever taking place, in case certain conditions are met. These conditions could be, for example, a cookie which signifies that the user does not want to be tracked, or some other flag in the browser that you can listen to. It\u0026rsquo;s up to you to determine how you want to persist information about the user\u0026rsquo;s tracking preferences, so in this article I\u0026rsquo;ll just show you how to actually implement the blocker, using a browser cookie as an example of where the tracking preferences are stored.\nTip 65: Block the GA request and the DoubleClick redirect   In this hypothetical scenario, we have two 1st party cookies indicating if the user wants to be excluded from Google Analytics tracking and/or the advertising data redirect to DoubleClick:\n  Next, we\u0026rsquo;ll create the respective 1st Party Cookie variables in Google Tag Manager:\n  The variables are named (1) {{Cookie - _ga_opt_out}} and (2) {{Cookie - _dcl_opt_out}}.\nNext, you\u0026rsquo;ll need a Custom JavaScript variable that looks like this:\nfunction() { return function(model) { if ({{Cookie - _ga_opt_out}} === \u0026#39;true\u0026#39;) { model.set(\u0026#39;sendHitTask\u0026#39;, null); } if ({{Cookie - _dcl_opt_out}} === \u0026#39;true\u0026#39;) { model.set(\u0026#39;displayFeaturesTask\u0026#39;, null); } }; }  Finally, you need to add this Custom JavaScript variable to all your Universal Analytics tags by going to Fields to set, and adding a new field:\n  Note that the easiest way to do this is to use a Google Analytics Settings variable, and set the field there. Then add the GAS variable to all your Universal Analytics tags. If you want to set this per-tag, remember to check Enable overriding settings for this tag to find the Fields to set option.\nThe way this works now is that if the user has the _ga_opt_out cookie and its value is 'true', the request to Google Analytics will be blocked. And if the user has the _dcl_opt_out cookie and its value is 'true', the redirect to DoubleClick will be blocked, too.\nThis tip was, again, meant to first and foremost show you the amazing power of the customTask feature. It\u0026rsquo;s a versatile tool with which you can really manipulate what your website is doing in terms of Universal Analytics tracking.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/scroll-depth-trigger-google-tag-manager/",
	"title": "The Scroll Depth Trigger In Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "scroll depth", "trigger"],
	"description": "Introduction and guide to the scroll depth trigger in Google Tag Manager.",
	"content": "Scroll depth tracking in web analytics is one of those things you simply must do, especially if you have a content-heavy site. Tracking scroll depth not only gives you an indication of how much users are digesting your content, but it also lets you turn meaningless metrics such as Bounce Rate into something far more useful.\nIf you\u0026rsquo;ve already been tracking scroll depth in Google Tag Manager, you\u0026rsquo;ve probably been using either Rob Flaherty\u0026rsquo;s brilliant Scroll Depth jQuery plugin, or Bounteous\u0026rsquo; equally ingenious Scroll Tracking recipe. I\u0026rsquo;m sure you\u0026rsquo;ll be very pleased to know that Google Tag Manager just released a native Scroll Depth trigger, with which setting up scroll depth tracking will be a doozy!\n  The new trigger comes with all the base features you\u0026rsquo;d expect in a scroll depth tracking plugin. There\u0026rsquo;s no option to track scrolling to specific HTML elements, but luckily the recently released Element Visibility trigger takes care of this.\nThe trigger configuration You can find the trigger in the Google Tag Manager user interface, by navigating to Triggers, clicking the NEW button, and selecting the Scroll Depth trigger template from the list.\n  When you create the trigger, you\u0026rsquo;ll see the following configurable options:\n Vertical Scroll Depths - set the trigger up for tracking vertical scroll  1. **Percentages** - track percentages of vertical scroll 2. **Pixels** - track vertical pixel depths  Horizontal Scroll Depths - set the trigger up for tracking horizontal scroll  1. **Percentages** - track percentages of horizontal scroll 2. **Pixels** - track horizontal pixel depths  All pages / Some pages - enable the trigger either on all pages or only on some pages  The settings should be quite self-explanatory. For example, to track vertical scroll (i.e. scrolling from top to bottom) so that an event is triggered with 25% of page scrolled, 50% of page scrolled, 75% of page scrolled, and reaching the end, you\u0026rsquo;d set the trigger up as in the very first image of this article.\nYou might want to delimit the trigger to collect data only on content pages, which is where the Some pages option will come useful:\n  That\u0026rsquo;s it for the configuration.\nHow to use the trigger It\u0026rsquo;s a good idea to use the Preview mode for this. Set the trigger up with e.g. 25% thresholds (as in the image at the beginning of this article), visit a page on your site, and scroll some. You\u0026rsquo;ll see the following Data Layer object pushed when you reach a scroll threshold:\n  Here are the relevant Data Layer variables that are created:\n  event: 'gtm.scrollDepth' - this is the name of the event that is automatically pushed into dataLayer. This event, in turn, activates the Scroll Depth trigger.\n  gtm.scrollThreshold: 25 - this is the value of the threshold that was crossed. For example, when I scrolled to 25% of the page, I see the value 25 here. If I\u0026rsquo;d set 25 pixels as the threshold, I\u0026rsquo;d still see 25 here.\n  gtm.scrollUnits: 'percent' - this will show either 'percent' or 'pixels', depending on which unit you chose for the trigger.\n  gtm.scrollDirection: 'vertical' - this will show either 'vertical' or 'horizontal', depending on which type of scrolling action caused the threshold to be passed (and what you configured the trigger to listen to).\n  Note that you don\u0026rsquo;t need to create Data Layer Variables for gtm.scrollThreshold, gtm.scrollUnits, or gtm.scrollDirection. They have been added as new Built-in Variables, named Scroll Depth Threshold, Scroll Depth Units, and Scroll Depth Direction, respectively.\n  You\u0026rsquo;ll find the new Built-in Variables by navigating to Variables in the Google Tag Manager user interface, and clicking CONFIGURE under the \u0026ldquo;Built-In Variables\u0026rdquo; heading.\nPutting it together With this information, it will be easy to setup a Google Analytics Event tag to collect your scroll tracking data. For example, the following tag fires for each 25% of page scrolled.\n  You can even configure the \u0026ldquo;Non-Interaction\u0026rdquo; field to be true for the 25% mark, and false for the rest. This will make sure that your site\u0026rsquo;s Bounce Rate isn\u0026rsquo;t affected by minimal scroll. The Custom JavaScript variable you\u0026rsquo;d use in the Non-Interaction field would look like this:\nfunction() { return {{Scroll Depth Threshold}} === 25; }  This returns true if the user crossed the 25% mark, and false otherwise.\nCaveat You need to be mindful of one thing when working with a scroll depth tracking plugin. If you load the page so that you are on or have crossed any one of the defined thresholds, the gtm.scrollDepth trigger will automatically fire for all the thresholds you have crossed.\nSo, if you are at the very bottom of a page and you reload the page, GTM will fire a trigger for each of the thresholds 25%, 50%, 75%, and 100%, without the user explicitly scrolling.\nThis might have an impact on your data collection, so you just need to be mindful of this. This is also one reason why it\u0026rsquo;s a good idea to have the first threshold as non-interactive, because if the page is very short, it\u0026rsquo;s possible the trigger will fire even if the user didn\u0026rsquo;t scroll one bit.\nSummary The scroll depth trigger now offered by Google Tag Manager natively works very nicely, and checks most of the boxes you\u0026rsquo;d expect in such a plugin.\nIf you want to track scrolling to specific elements, be sure to check out the Element Visibility trigger!\nOne thing that is questionable is how well this jives with a single-page app. When you transition from one page to the other, any scroll tracking trigger that was active on the previous page would still be active, and thus the depth tracker would not reset to accommodate scrolling on the new, dynamically loaded content. It remains to be seen if the plugin will support somehow \u0026ldquo;resetting\u0026rdquo; it for single-page transitions.\nWhat do you think of this release? Does this make the alternatives out there obsolete, or do you think this new trigger is desperately in need of some additional features?\n"
},
{
	"uri": "https://www.simoahava.com/tags/trigger/",
	"title": "trigger",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/dom/",
	"title": "dom",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/element-visibility-trigger-google-tag-manager/",
	"title": "The Element Visibility Trigger In Google Tag Manager",
	"tags": ["dom", "Google Tag Manager", "visibility"],
	"description": "Introduction and guide to the element visibility trigger in Google Tag Manager.",
	"content": "Holy visibility, Batman! Visibility is a seriously undervalued aspect of web analytics tracking. Too often, we fall into the trap of thinking that \u0026ldquo;Page Views\u0026rdquo; actually have something to do with \u0026ldquo;viewing\u0026rdquo; a page. Or that tracking scrolling to 25%, 50%, or 75% of vastly different pages makes sense on the aggregate level. So you will be very pleased to know that the Google Tag Manager team (who have been on FIRE recently), have just published the Element Visibility trigger. And oh BOY what a trigger it is!\n  It\u0026rsquo;s a very impressive trigger. Not only does it let you do some proper scroll tracking (track to elements, not percentages), but it also includes a feature that lets you track changes in the DOM. This has typically been quite difficult to track purely client-side, so having the feature natively in Google Tag Manager is very cool. On top of that, you can actually measure true view impressions (impressions for items that are visible to the user) AND combine that with the actual time the element was viewed for.\nIn short, the Element Visibility trigger fires whenever an element, or elements, you specify appear in the browser\u0026rsquo;s viewport. The viewport is the visible area of the browser window, meaning if an element is visible there, it is viewable by the user.\n  With this in mind, let\u0026rsquo;s jump straight to the features.\nSelection method You have two options here - element ID and CSS Selector. The former uses document.getElementById to match the first element in the page with the given ID. The latter uses CSS Selectors to match the element or a group of elements on the page.\n  At this point I really recommend you read up on CSS selectors (see also my article on the topic). They really make Google Tag Manager more than just the sum of its parts.\n  ID - the trigger waits for an element with a specific id attribute to appear in the viewport.\n  CSS Selector - the trigger waits for elements that match the CSS selector string to enter the viewport.\n  The obvious difference between the two is that you can add multiple element selectors into the CSS selector string. For example, if I want to track when the header, the article title, the article body, and the article footer enter the viewport, I could use something like this as the selector:\n#header, h2.title, div.content, div.footer\nThe trigger could then fire when each one of these enters the viewport (see below).\nThe ID selector, on the other hand, performs better if you have only one element to track.\nWhen to fire this trigger This is where you\u0026rsquo;ll govern what this trigger actually is. Is it a check to see if a certain element is in the viewport when the page is loaded, or is it an advanced scroll trigger?\n    Once per page - this trigger will only fire once on the current page. The moment is when the first element that matches the ID or the selector string enters the viewport. Thus if you\u0026rsquo;ve specified multiple selectors or there are multiple elements with the same ID, this trigger setting will make the trigger fire only once - when the first matched element enters the viewport.\n  Once per element - this trigger will fire just once if an element with a specific ID appears in the viewport (even if multiple elements share the same ID, in which case it will fire just for the first one). However, when using CSS selectors, this setting will fire once for each element matched by the selector(s). In other words, this would be the setting to use if you wanted to create a trigger that fires when different parts of the page enter the viewport!\n  Every time an element appears on-screen - this trigger fires whenever any matched element appears in the viewport, and will do so each time the element reappears.\n  Of these, I wager the first two will be most used. Tracking if an element is visible is very useful for e.g. true view impression tracking, and tracking when multiple different elements appear can be used to create an advanced scroll trigger. There might be use cases for the last option too, though.\nAdvanced - Minimum Percent Visible Here you can specify a value in percentages, which is how much of the element needs to be in the viewport for the trigger to fire. So if you set the value to \u0026ldquo;50\u0026rdquo;, at least 50% of the matched element needs to be visible for the trigger to fire.\n  This is a great way to make sure that enough of the ad or content piece is visible for you to interpret the data as meaningful.\nAdvanced - On-Screen Duration In this field, you can specify the total time in milliseconds the element must be visible in the viewport for the trigger to fire. Note that this is cumulative, so let\u0026rsquo;s say you have the following setting:\n  This would fire the trigger only after the matched element(s) have been visible in the viewport for a total of 5 seconds. In other words, the user could first scroll to the element, view it for one second, then scroll until the element disappears, then scroll back to the element, view it for four additional seconds, at which point the trigger fires.\n  Note that because GTM has to manage a timer for each element that you want to monitor, tracking a single element with the Element ID selection method performs better than a bunch of elements defined with CSS selectors.\nAdvanced - Observe DOM changes This setting lets you track the visibility of elements that might not exist in the DOM when the page first loads! In other words, if you have dynamically inserted elements, you can check this box to track when they become visible, too.\nThe most obvious use case is if you have a form which is resilient to GTM\u0026rsquo;s Form Trigger due to complications in the on-site JavaScript, you can use this trigger setting to wait for an HTML element with the thank you message to pop up. So something like this:\n  Now the trigger will fire when an element with the ID #form-thank-you becomes visible, even if it\u0026rsquo;s dynamically inserted into the Document Object Model!\nSimilar to tracking On-Screen Duration, the DOM Changes setting performs better when you track a single element with the Element ID selection method.\nThe event object contents When the visibility trigger fires, an event with the name gtm.elementVisibility is pushed into Data Layer. The object has a number of variables you can refer to using Built-in Variables (or Data Layer Variables if you want to configure them manually).\n  gtm.element - the element that became visible. You can capture this with the Click / Form Element Built-in Variable.\n  gtm.elementClasses - the class name string of the element that became visible. You can capture this with the Click / Form Classes Built-in Variable.\n  gtm.elementId - the ID value of the element that became visible. You can capture this with the Click / Form ID Built-in Variable.\n  gtm.elementTarget - the target attribute value of the element that became visible. You can capture this with the Click / Form Target Built-in Variable.\n  gtm.elementUrl - the href or action attribute value of the element that became visible. You can capture this with the Click / Form URL Built-in Variable.\n  gtm.visibleRatio - a decimal number between 0 and 100, telling you how much of the element was visible in the viewport when the trigger fired in percentages. You can capture this with the Percent Visibile Built-in Variable.\n  gtm.visibleTime - the total cumulative time the element has been in the viewport when the trigger fires. The number is in milliseconds. You will only see a value larger than 0 if you\u0026rsquo;ve set the \u0026ldquo;On-Screen Duration\u0026rdquo; in your trigger (because without a minimum on-screen duration, the trigger will always fire immediately when the element appears). You can capture this with the On-Screen Duration Built-in Variable.\n  gtm.visibleFirstTime - the time in milliseconds when the element first became visible in the viewport. This time is calculated as the delta from the moment the browser started rendering the Google Tag Manager container snippet to the moment the trigger fired.\n  gtm.visibleLastTime - the time in milliseconds when the element most recently became visible in the viewport. The calculation is similar to gtm.visibleFirstTime, except this time each time the trigger fires for the given element, the value is updated.\n  You can use these variables (preferable as Built-in Variables) to create even more advanced configurations in your other tags, triggers, and variables.\nBuilt-in Variables There are some new built-in variables introduced, too:\n  In addition to these, you have the regular Click / Form Built-in Variables at your disposal, since the element that fired the Element Visibility trigger is used to populate these Built-in Variables.\nSummary The past month or two have been crazy in Google Tag Manager land. Features that we\u0026rsquo;ve been dying to see have been released with impressive speed and attention to detail.\nOut of the bunch, I think this new Element Visibility is my favorite one. It has the potential to turn your entire web analytics tracking around, since you can actually focus on tracking interactions that matter rather than interactions that simply take place. Visibility is such a huge aspect of things we do on a web page. If an element is not visible, it can\u0026rsquo;t really be engaged with by a user.\n"
},
{
	"uri": "https://www.simoahava.com/tags/celebration/",
	"title": "celebration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/happy-5th-birthday-google-tag-manager/",
	"title": "Happy 5th Birthday Google Tag Manager!",
	"tags": ["celebration", "Google Tag Manager"],
	"description": "Post celebrating the 5th birthday of Google Tag Manager.",
	"content": "5 years ago, on 1st October 2012, this lovely video popped up in Google\u0026rsquo;s Analytics Blog:\nIt was accompanied by a blog post, which contained a brief look into many of Google Tag Manager\u0026rsquo;s key features, some of which are still relevant today.\nGoogle Tag Manager is a free tool that consolidates your website tags with a single snippet of code and lets you manage everything from a web interface. You can add and update your own tags, with just a few clicks, whenever you want, without bugging the IT folks or rewriting site code. It gives marketers greater flexibility, and lets webmasters focus on other important tasks. At this time, those of us who started using the tool immediately, or who had been beta-testing it, saw quickly what the key selling point of GTM was. It was a way to impact change on the website as quickly as possible, without having to wait for a release to be verified and pushed live.\nThe ability to consolidate tags in a single container was great, though the initial inventory of tags was a far cry from what we have access to today. A typical source of friction with digital marketing has been to deploy these tags and code snippets on the site, and then manage and keep them up-to-date when the site itself changes. With Google Tag Manager, this task became almost trivial, because now the library itself made sure that the code used in the tags was always up-to-date.\nI wrote my first article about Google Tag Manager almost a year later, when Google Tag Manager looked like this:\n  By the way, my blog looked like this back then:\n  As time passed, more and more features were integrated into the tool. The pace of the development team was impressive, and there was an openness in the community that I had never witnessed before. It seemed like GTM\u0026rsquo;s developers were invested in us, the users, and always listened to our feedback with genuine interest.\nSome key features (in my opinion) that were introduced were:\n  February 2013, Google Tag Assistant was released, with capabilities to analyze GTM implementations, too.\n  August 2013, GTM SDKs for Android and iOS saw daylight.\n  October 2013, Auto-Event Tracking was introduced (and everything changed for the better).\n  April 2014, Universal Analytics was released out of beta.\n  May 2014, 2-step verification was introduced as an additional security layer.\n  July 2014, the Preview \u0026amp; Debug feature was released.\n  October 2014, New version of the UI and the GTM API were released.\n  February 2015, Matches CSS Selector operator was introduced.\n  August 2015, Tag Sequencing was launched.\n  May 2016, the Firebase Google Tag Manager container for mobile was launched.\n  August 2016, Workspaces introduced.\n  September 2016, the instructions for implementing the GTM container snippet were (finally) rewritten. Now the right place for the JavaScript container snippet is in the \u0026lt;head\u0026gt; of the page.\n  October 2016, the AMP container was published.\n  March 2017, version 2 of the Google Tag Manager API was released.\n  September 2017, the YouTube Video trigger was released.\n  That\u0026rsquo;s definitely not an exhaustive list - just a timeline of events that I remember being of special importance.\nIn mid-October 2014, we even had the first \u0026ldquo;Google Tag Manager Summit\u0026rdquo;, when a bunch of dedicated GTM nerds congregated in Copenhagen, Denmark. Many active GTM users were present (people like Julien Coquet, Phil Pierce, Tahir Fayyaz, Doug Hall, Christian Pluzek, and Kristoffer Ewald to name a few), and the session was run by Brian Kuhn, the lead developer of Google Tag Manager, and Lukas Bergstrom, the then Product Manager for Google Tag Manager.\nWe had a great first GTM \u0026quot;summit\u0026quot; at Copenhagen with @briankuhn and @lukasb. A very productive day! #googletagmanager pic.twitter.com/N3aibq40Ow\n\u0026mdash; Simo Ahava (@SimoAhava) October 21, 2014  Another \u0026ldquo;GTM Summit\u0026rdquo; never took place, though time was always reserved in the Google Analytics Partner Summits for sessions dedicated to Google Tag Manager. I hope we can still get our own little Google Tag Manager conference or summit - there\u0026rsquo;s still lots to discuss!\nNow and going forward Today, Google Tag Manager is in an interesting place. It has an impressive adoption rate, with almost 30% of the sites in the Quantcast top 10K websites (per traffic) embedding the GTM snippet on their pages.\nIn the organization, GTM is also interestingly placed. It\u0026rsquo;s not just a tool for the marketing department, nor is it just a tool for business analysts, nor is it just a tool for developers. It\u0026rsquo;s a tool that has the possibility to unite all these stakeholders in a modern organization, thanks to how it offers something for everyone.\nTake a look at the slideshow above, especially from slide 24 onwards. I share some of my thoughts for how Google Tag Manager is so perfectly situated in a modern, digital organization.\nAbsolution doesn\u0026rsquo;t come for free, though. Making the most of Google Tag Manager is an investment, and it requires you to step out of your comfort zone. If you treat it as a way to circumvent your IT process, you\u0026rsquo;ll be sorely missing out on what the tool can do at its best.\nBut what\u0026rsquo;s next? What can Google Tag Manager do to keep the ball rolling?\nIntelligence It\u0026rsquo;s difficult to imagine Google Tag Manager NOT jumping on the bandwagon of intelligent apps. There are so many things in GTM that could benefit from rule-based automation.\nJust imagine dynamically launched tags, based on a user\u0026rsquo;s or a user cohort\u0026rsquo;s behavior rather than a fixed set of triggers you need to define a priori.\nOr tags that are created, deployed, and published at just the right time for just the people.\nOr benchmarking your container content, with something like \u0026ldquo;45% of companies in your market segment use HotJar for extra insight about visitors and their sessions. Get started?\u0026rdquo;.\nBecause Google Tag Manager is a tool designed to alleviate friction and reduce trivial, manual labor in tag management, it would make sense that it took this step towards actually being an insight engine in addition to a tag container.\nCollaboration One of the criticisms against Google Tag Manager has been that it\u0026rsquo;s not enterprise-worthy. There\u0026rsquo;s a lack of multi-user support (access control levels), of delimiting access per tag / folder / workspace to specific groups, of selective publishing, and so forth. These are all things I\u0026rsquo;m certain we\u0026rsquo;ll see in the future, in some shape or form. I would be surprised if the GTM team did not think about the enterprise\u0026rsquo;s needs first in today\u0026rsquo;s competitive market, especially since the release of Google Tag Manager 360.\n  With Workspaces, we took a big leap towards collaborative tag management, and with the approval workflow, it was easier to get thins done together without compromising quality of work. Still, I expect we\u0026rsquo;ll see more features like these in the future. Being able to handle projects with multiple organizations working on the same container is something that GTM simply must have solid support for.\nFull stack With Google Tag Manager for Mobile Apps, we\u0026rsquo;ve already seen GTM in use on the application level. And if you\u0026rsquo;ve used GTM for mobile, you might have been disappointed at its reach, especially if you\u0026rsquo;re familiar with GTM for the web.\nThe problem with a \u0026ldquo;full stack GTM\u0026rdquo; is that the kind of stuff that is borderline OK in the web browser (injecting and executing ad hoc JavaScript) might not work as fluently server-side. So you can\u0026rsquo;t just create a \u0026ldquo;Custom HTML Tag\u0026rdquo; in your iOS container using a native SDK, because iOS applications would not allow that type of unverified code to be released in them after going live.\nBut I\u0026rsquo;m also thinking about server-side stuff, like being able to use a dataLayer queue server-side, too, which then has the capability of executing HTTP requests and communicating with the full technology stack. This way we could run things like monitoring services, which collect data on what tags have fired and raise alerts when anomalies are detected. We could introduce state into Google Tag Manager, by having the web server persist information stored in dataLayer from page to page.\nHey, a guy can dream!\nGoogle integrations It\u0026rsquo;s a scary thought, but in five years\u0026rsquo; time it\u0026rsquo;s possible that GTM will be so tightly bound to the Google ecosystem that its usability for anything else diminishes. I\u0026rsquo;m certain that Google has an incentive (and pressure) to improve the DoubleClick integration in Google Tag Manager, and to make Firebase work more fluently through the Google Tag Manager container. Perhaps AMP will still be a thing in five years, and the AMP / web containers have merged so that it\u0026rsquo;s impossible to distinguish one from the other.\n  And perhaps Google Tag Manager will finally introduce something akin to state, where audience data is pulled from Google Analytics, so that we can use audience rules to fire our tags. It\u0026rsquo;s already implemented in Optimize, so I\u0026rsquo;m thinking: why not!\nSummary It\u0026rsquo;s been an amazing five years. For some like me, these five years have been life-changing. For the entire web analytics industry, I fail to see anything but positive things come out of Google Tag Manager\u0026rsquo;s popularity. It\u0026rsquo;s brought developers back into the mix! We finally have a tool which encourages developer involvement, and really shines when imported into an agile organization, ready to tackle digital challenges with a talented, hybrid team.\nIf there\u0026rsquo;s one thing I wish, it\u0026rsquo;s that Google Tag Manager would remain accessible. With that, I don\u0026rsquo;t mean that I want the UI to be any more coddling than it already it is. No, I mean that the developers, engineers, product managers, evangelists, and other Google folk invested in the tool would stay active in the community, bringing the tool development closer to its most dedicated users. I don\u0026rsquo;t want GTM to become another license tool, where only those who have the money for GTM 360 are privy to the cool stuff.\nSo once again, happy birthday Google Tag Manager! Congratulations to the whole GTM team at Google for taking such good care of a tool loved by so many.\nHere\u0026rsquo;s to another 5, extremely successful years for GTM!\n"
},
{
	"uri": "https://www.simoahava.com/tags/data-quality/",
	"title": "data quality",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/page-timings/",
	"title": "page timings",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/prevent-site-speed-sampling-rate-skewing-custom-dimensions-custom-metrics/",
	"title": "Prevent Site Speed Sampling Rate From Skewing Custom Dimensions And Custom Metrics",
	"tags": ["customtask", "data quality", "page timings", "universal analytics"],
	"description": "Use customTask to prevent your automatically collected site speed metrics from skewing your Google Analytics custom dimensions and metrics.",
	"content": "Universal Analytics can collect Page Timing data from users that load your pages. This data is populated in to the Behavior -\u0026gt; Site Speed -\u0026gt; Page Timings report, and it\u0026rsquo;s a very useful feature for optimizing your website.\n  However, there\u0026rsquo;s a murky underside to this generous feature. The way Page Timings collection works is that when Pageview hits are sent from the site, a sample of these (1% by default) are automatically followed by a timing hit which includes page performance data grabbed from the Navigation Timing API.\n  So far so good. This is how it\u0026rsquo;s supposed to work. If you\u0026rsquo;ve set siteSpeedSampleRate to 100, the first pageview request of every page will be automatically followed by this timing hit.\nHowever, the timing hit copies its information from the pageview hit. Basically, all the Custom Dimensions and Custom Metrics are copied from the pageview. Luckily, things like Enhanced Ecommerce metadata are not copied into the timing hit.\n  As you can see, the timing hit below the pageview has the same Custom Dimension and the same Custom Metric as the Pageview hit. This is annoying, especially if you use hit-scoped Custom Dimensions or pretty much any type of Custom Metrics. This is data inflation that you can\u0026rsquo;t control.\nSo we need a way to reset Custom Dimensions and Custom Metrics for the timing hit.\nI am grateful to Clément Simon for helping me come up with the following solution.\nSolution: customTask If you\u0026rsquo;ve been reading my recent articles, you might have seen this coming. I consider customTask to be one of the most versatile features recently added to analytics.js.\nWe\u0026rsquo;ll use customTask to check if the hit type is a timing hit, and if it is, we\u0026rsquo;ll make sure no Custom Dimensions and Custom Metrics are sent with the hit.\nanalytics.js This is what the situation is with analytics.js before you make the change:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {siteSpeedSampleRate: 100}); ga(\u0026#39;set\u0026#39;, \u0026#39;dimension1\u0026#39;, \u0026#39;My Value\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;, {metric1: 20);  This creates a tracker, sets a Custom Dimension, and then sends a pageview request with a Custom Metric. Since you\u0026rsquo;re sampling Page Timings at 100%, the pageview hit will be instantly followed by a timing hit that contains these two custom definitions, too.\nTo fix it, this is what you\u0026rsquo;ll do:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {siteSpeedSampleRate: 100}); ga(\u0026#39;set\u0026#39;, \u0026#39;dimension1\u0026#39;, \u0026#39;My Value\u0026#39;); ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, function(model) { var tempFieldObject = {}; var i = 1; if(model.get(\u0026#39;hitType\u0026#39;) === \u0026#39;timing\u0026#39;) { while (i !== 201) { tempFieldObject[\u0026#39;dimension\u0026#39; + i] = undefined; tempFieldObject[\u0026#39;metric\u0026#39; + i] = undefined; i++; } model.set(tempFieldObject); } }); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;, {metric1: 20});  This clears all the possible Custom Dimensions and Custom Metrics from the timing hit, and thus data inflation is avoided.\nGoogle Tag Manager To fix this in Google Tag Manager, you\u0026rsquo;ll need a Custom JavaScript Variable, and you\u0026rsquo;ll need to add a new field in your Page View tag.\nThe Custom JavaScript Variable has the following code:\nfunction() { return function(model) { var tempFieldObject = {}; var i = 1; if(model.get(\u0026#39;hitType\u0026#39;) === \u0026#39;timing\u0026#39;) { while (i !== 201) { tempFieldObject[\u0026#39;dimension\u0026#39; + i] = undefined; tempFieldObject[\u0026#39;metric\u0026#39; + i] = undefined; i++; } model.set(tempFieldObject); } }; }  And then you add it to your tag like this:\n  Whichever method you use, the customTask implementation will purge all Custom Dimensions and Custom Metrics from the automatically generated timing hit.\nSummary This was just a quick tip to fix a potential data integrity issue on your site. Granted, it\u0026rsquo;s rare for you to run into problems with the Custom Dimension duplication (unless you use it against the ga:hits metric), but on an aggregate level the Custom Metrics inflation can be troublesome.\nThis isn\u0026rsquo;t the only thing where the Site Speed Sample Rate can wreak havoc on your data. With Google Tag Manager and \u0026ldquo;virtual\u0026rdquo; pageviews, you might be inflating your page timing data by a great deal, and I\u0026rsquo;ve written about how to solve this issue here.\nNote also that these are client-side fixes to something that, in my opinion, should be prevented by analytics.js to start with. I don\u0026rsquo;t understand why the Page Timing hit just copies everything from the pageview. I\u0026rsquo;d want it to be configurable, at the very least.\n"
},
{
	"uri": "https://www.simoahava.com/tags/regular-expression/",
	"title": "regular expression",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/the-regex-table-variable-in-google-tag-manager/",
	"title": "The RegEx Table Variable In Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "regular expression", "variable"],
	"description": "Introduction and guide to Google Tag Manager&#39;s RegEx Table variable type.",
	"content": "Ever since the Lookup Table variable was introduced in Google Tag Manager, users have been craving for more. The Lookup Table does exactly what it promises: lookups. These are exact match operations, which are extremely inexpensive to perform, because they can only have a binary result: either the match exists in the data store being queried or it doesn\u0026rsquo;t. This performance stays constant even if the data store being queried increases in size. However, exact match has one significant problem: it\u0026rsquo;s exact match. Thus even though the Lookup Table variable is extremely useful, it\u0026rsquo;s missing the flexibility of, I don\u0026rsquo;t know, say, regular expressions. You will be pleased to hear, then, that Google Tag Manager has released a new variable type: the RegEx Table!\n  First of all, if you are unfamiliar with regular expressions, here are some great resources:\n  Interactive tutorials by RegexOne\n  Extremely comprehensive guide from regular-expressions.info\n  Bounteous\u0026rsquo; Regular Expressions e-book for Google Analytics\n  Google Analytics Regular Expressions Cheat Sheet by Jay Taylor\n  Needless to say, RegEx is a powerful pattern-matching syntax to learn, and can help you enormously in keeping your Google Tag Manager container lean and mean.\nThe RegEx Table variable You\u0026rsquo;ll find the RegEx Table variable in the list of variable types you can create as a User-defined variable. Once you choose this variable type, you\u0026rsquo;ll see the following configuration:\n  There are many similarities with the Lookup Table variable, for good reason, but there\u0026rsquo;s also a bunch of settings that turn this new variable into a formidable force in its own right.\n1. Input variable The Input variable shares its functionality with the Lookup Table. The input variable is what you\u0026rsquo;ll be making your pattern checks against. For example, if you want to use the RegEx table to look for patterns in the current page path, you\u0026rsquo;d choose the {{Page Path}} variable as the input.\n  The input variable is evaluated row-by-row, from top-to-bottom, against each pattern. When a pattern matches, the respective output is returned and processing of the table stops.\n2. RegEx Table Next, you have the table itself. In the table, you add rows, where each row represents a pattern you want to match in the input, and an output returned by the variable in case the pattern matches.\nThe pattern is always interpreted as a regular expression. All the following patterns are valid examples:\n  simoahava.com - will match \u0026ldquo;simoahava \u0026lt;+ any character +\u0026gt; com\u0026rdquo;\n  simoahava\\.com - will match \u0026ldquo;simoahava.com\u0026rdquo;.\n  ^simoahava\\.com$ - will match exactly \u0026ldquo;simoahava.com\u0026rdquo; (won\u0026rsquo;t allow leading or trailing characters).\n  (simoahava)\\.com - will match \u0026ldquo;simoahava.com\u0026rdquo; and create a group (see below) of \u0026ldquo;simoahava\u0026rdquo;.\n  A pattern like [simoahava\\.com is not a valid regular expression, because \u0026ldquo;[\u0026rdquo; is a reserved character, and it is being incorrectly used in this pattern. Google Tag Manager will not warn you of errors in the regular expression, but you\u0026rsquo;ll know something is wrong if the Preview mode output for the variable is boolean false. Conversely, if no match is made or there is no output for a matched pattern, the variable will return undefined.\nThe output is what the variable returns when a row is matched against the input. The return type is a string, unless you add another variable into the output. This is a great way to chain RegEx Table variables, just as you could chain Lookup Table variables.\nFor example, here\u0026rsquo;s a simple chain of a RegEx Table and a Lookup Table:\n  And here\u0026rsquo;s how to unravel the process:\n  If the page hostname matches the pattern beta\\.simoahava\\.com, then return \u0026ldquo;UA-12345-1\u0026rdquo;.\n  If the page hostname doesn\u0026rsquo;t match either beta\\.simoahava\\.com or \\.simoahava\\.com, also return \u0026ldquo;UA-12345-1\u0026rdquo; (Default Value of the RegEx table).\n  If the page hostname matches \\.simoahava\\.com and the user is in Debug Mode, return \u0026ldquo;UA-12345-2\u0026rdquo;.\n  If the page hostname matches \\.simoahava\\.com and the user is not in Debug Mode, return \u0026ldquo;UA-12345-3\u0026rdquo;.\n  As you can see, the RegEx Table returns the first match that is made. Thus even though beta\\.simoahava\\.com and \\.simoahava\\.com overlap for any hostname that contains the string \u0026ldquo;beta.simoahava.com\u0026rdquo;, the RegEx table returns \u0026ldquo;UA-12345-1\u0026rdquo;, because that is the first match that the variable makes.\n3. Set Default Value As with Lookup Tables, you can set a Default Value that is always returned in case no match is made. Just like pattern outputs, this can be another Google Tag Manager variable.\n4. Ignore Case If you check Ignore Case, patterns are matched regardless of case. So a pattern with WwW\\.SiMOAHava\\.com will match against the domain of my site, as long as Ignore Case is checked.\nIgnore Case is checked by default.\n5. Full Matches Only If you check Full Matches Only, then all patterns must match the entire input. This is the equivalent of wrapping each individual pattern with ^...$.\nFor example, if you have Full Matches Only checked, and you have a pattern of www\\.simoahava\\.com, then the input variable must return exactly www.simoahava.com, without any other characters. If you\u0026rsquo;d have the setting unchecked, then www\\.simoahava\\.com would also match any of the following:\n  greatest.website.ever.is.www.simoahava.com\n  aawwwwww.simoahava.com\n  visit.www.simoahava.com.please\n  And so forth.\nFull Matches Only is checked by default.\n6. Enable Capture Groups and Replace Functionality This is interesting! In addition to matching the input against a pattern and returning a corresponding output, you can actually use parts of the matched pattern within the returned output. This is achieved with capturing groups and the dollar symbol syntax.\nA group (capturing and non-capturing) in RegEx is a pattern that you define with parentheses. Most groups can then be captured using the dollar symbol syntax when using the String.replace() method or, consequently, the Enable Capture Groups and Replace Functionality feature of GTM\u0026rsquo;s RegEx table. Here are the options for the dollar symbol syntax:\n  $$ inserts a \u0026lsquo;$\u0026rsquo;.\n  $\u0026amp; inserts the matched pattern.\n  $`  inserts whatever precedes the matched pattern in the string.\n  $' inserts whatever follows the matched pattern in the string.\n  $n inserts the _n_th capturing group.\n  These all have their uses, but the last one, $n should prove to be the most useful. You can use it to normalize patterns across a range of values. For example, let\u0026rsquo;s say you have a variable which stores the user\u0026rsquo;s phone number in the following formats:\n  358101001000\n  0101001000\n  010-1001000\n  010 100 1000\n  +358101001000\n  You want to normalize all of these to the last format (+358101001000) whenever the phone number variable is used. This is how you\u0026rsquo;d configure the RegEx table:\n  The first pattern looks for strings that start with \u0026lsquo;358\u0026rsquo; followed by any numbers. This pattern is simply replaced with the plus symbol followed by the pattern itself.\nThe second pattern looks for a string of numbers preceded by a \u0026lsquo;0\u0026rsquo;. The output is \u0026lsquo;+358\u0026rsquo; and the string of numbers, omitting the leading \u0026lsquo;0\u0026rsquo;.\nThe third pattern looks for a string of numbers preceded by a \u0026lsquo;0\u0026rsquo;, then a hyphen, and then a string of numbers again. The output is \u0026lsquo;+358\u0026rsquo; and then the two strings of numbers, omitting the leading \u0026lsquo;0\u0026rsquo; and the hyphen.\nThe fourth pattern looks for a string of numbers preceded by a \u0026lsquo;0\u0026rsquo;, then a space, then another string of numbers, a space, and finally one more group of numbers. The output is \u0026lsquo;+358\u0026rsquo; and the three groups of numbers, omitting the leading \u0026lsquo;0\u0026rsquo; and the spaces.\nThe final pattern checks if the phone number is already well-formed, returning the pattern itself if this is the case.\nUsing the RegEx Table like this, we can create simple string transformations which help normalize and clean up data across a variety of formats. As you can see, Full Matches Only is checked in this example. That means we don\u0026rsquo;t have to worry about anything that happens outside the matched pattern, since only full matches to the pattern are transformed.\nIf you leave Full Matches Only unchecked, then Enable Capture Groups and Replace Functionality will replace all matches of the pattern found within the Input Variable with what you have in the Output. For example, if you have a RegEx Table variable that looks like this:\n  Then whenever the string \u0026ldquo;analytics\u0026rdquo; is found within a page path, it will be replaced with \u0026ldquo;google-analytics\u0026rdquo;.\nHere is an example:\n/analytics/track-users-who-are-offline-in-google-analytics/ becomes /google-analytics/track-users-who-are-offline-in-google-google-analytics/.\nNote that the example above only works if Full Matches Only is unchecked. Otherwise the variable would only replace page paths which are exactly analytics, and page paths like that do not exist.\nThe little help bubble actually recommends to avoid combining this pattern replacement with unchecked Full Matches Only. This is because there\u0026rsquo;s no validation of the input variable, and you might end up replacing things that you didn\u0026rsquo;t mean to!\nEnable Capture Groups and Replace Functionality is checked by default.\nSummary That\u0026rsquo;s the RegEx Table in all its simple glory! I know it will make some operations so much simpler. You no longer need to use clumsy Custom JavaScript variables to perform your pattern matches, since the RegEx Table has that built into its modus operandi.\nThe option to replace any matches with custom strings (in which you can incorporate parts of the match using groups) is pretty powerful, too.\nAll in all, this is a very welcome addition to Google Tag Manager\u0026rsquo;s variable offering. It remains to be seen if the Lookup Table still has a place in the table after this, because with the RegEx table you can do exact match lookups, too. The difference is perhaps in the syntax (with Lookup Tables you don\u0026rsquo;t need to use regular expressions) and performance (lookups will always perform much faster than regular expression matches), though the latter might be very insignificant in the context of a web page.\n"
},
{
	"uri": "https://www.simoahava.com/tags/developer/",
	"title": "developer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/digital-marketing/",
	"title": "Digital marketing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/marketer/",
	"title": "marketer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/non-technical/",
	"title": "non-technical",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/digital-marketing/the-myth-of-the-non-technical-marketer/",
	"title": "The Myth Of The Non-Technical Marketer",
	"tags": ["developer", "marketer", "non-technical"],
	"description": "There is no such thing as a non-technical (digital) marketer. Anyone working in digital is already technical. The question is what to do about this.",
	"content": "There\u0026rsquo;s a fabled, mythical beast that prowls the jungles of digital marketing. They have no issues with running and analyzing crawler data, offering suggestions for server-side redirects, building remarketing audiences, implementing tag management solutions, speaking of Data Layers, copy-pasting code from Stack Overflow, configuring bid managers, and speaking at conferences presenting on all the aforementioned activities. However, for some reason, they still claim that they are \u0026ldquo;non-technical\u0026rdquo;, or \u0026ldquo;just marketers\u0026rdquo;.\nThis archetype is only enforced by conference presenters who actually apologize before showing a slide that might have some JavaScript (heaven forbid!), or by bloggers who do the same if they need to show some code or even a screenshot of an HTML template in their articles! What\u0026rsquo;s up with that? If it\u0026rsquo;s necessary to include code to get the point across, then why not just present it without treating it as some bogeyman that needs to be exorcized through ridicule?\n  Why focus on dumbing things down for this imaginary, unskilled audience, rather than tactfully steering them towards a path where they can enhance their technological aptitude?\nI just don\u0026rsquo;t get it. The whole polarization of non-technical vs. technical is silly and artificial, and nothing irks me as much as this constant undervaluing of the human capacity to learn new things. Code allergy should be a thing of the past by now. Why not instead embrace the fact that our industry is rife with opportunities to not only understand more about the technology stack we work with, but also to combine this technical know-how with our marketing skills for some true hybrid insight?\nAnyway, my point is that if you\u0026rsquo;re working in digital marketing, you are already \u0026ldquo;technical\u0026rdquo;. I\u0026rsquo;m really sorry about this, but that\u0026rsquo;s just the way it is. Non-technical (digital) marketer is an oxymoron.\nYou already possess a set of skills that requires training, education, and a technology-oriented mind to handle. You are firmly situated in a continuum of learning, which doesn\u0026rsquo;t subscribe to a binary world of \u0026ldquo;technical\u0026rdquo; vs. \u0026ldquo;non-technical\u0026rdquo;, but rather comprises a vast number of skills that each conspire to make you better at what you do for a living.\nIn fact, it\u0026rsquo;s impossible to categorize the complexity of human experience, yet for some reason we find ourselves doing so in our daily routines, and (sub)consciously subscribe to prototypes that don\u0026rsquo;t really exist. This is why job titles suck - especially those that imply proficiency in some skills (and, via inference, lack thereof in others).\nWhat can be done? If you work in digital and you\u0026rsquo;ve always considered yourself \u0026ldquo;non-technical\u0026rdquo;, the first thing to do is accept the fact that you\u0026rsquo;re not. Indeed, you are a \u0026ldquo;techie\u0026rdquo;, a \u0026ldquo;nerd\u0026rdquo;, and a \u0026ldquo;geek\u0026rdquo; (choose which one works best). The main question is what skills you currently possess, and what learning track you\u0026rsquo;re on.\nTo help you with this discovery, I recommend looking at people in your industry to see how they\u0026rsquo;ve evolved over the years. There are so many folks out there who possess the tech skills required to successfully navigate the digital ocean, and you can rest assured they weren\u0026rsquo;t born with this talent. They\u0026rsquo;ve had to study, learn, work, toil, starve, and sweat to get to where they are now.\nTo get you started, take a look at what Aleyda Solis, Annie Cushing, Mike King, Mike Arnesen, Carmen Mardiros, Mark Edmondson, David Vallejo, Dan Wilkerson, Linda Lawton, and Annemarie Klaassen are doing and sharing. They are just a handful of skilled people out of a multitude, who, in addition to being really good at what they do, are extremely generous with knowledge transfer and inspiring others to improve.\nAlso, take a look at the MeasureCamp (un)conference. It\u0026rsquo;s mainly around digital analytics, but the concept is brilliant in that it minimizes ego and sales pitches (two things that kill any good conference presentation), and instead focuses solely on sharing and learning together.\nOther than that, dig deep inside you and find out just what part of your current skillset is lacking. Why do you call yourself \u0026ldquo;non-technical\u0026rdquo;? Is it because you don\u0026rsquo;t know how to code? Head on over to Codecademy to rectify that. Is it because you don\u0026rsquo;t understand enough about the browser stack to be confident in your technical SEO skills? Take a look at The Technical SEO Renaissance by the inimitable Mike King to see where technical SEO is today. Or perhaps you\u0026rsquo;re interested in mobile optimization, web analytics, and conversion optimization?\nThere\u0026rsquo;s something for each gap in your skill set.\nSo, all you need to do is roll up your sleeves and get to work. Once you accept the fact that you\u0026rsquo;re not \u0026ldquo;non-technical\u0026rdquo;, it\u0026rsquo;s just a question of opening the flood gates and letting the deluge of knowledge carry you away.\nSo should we stop reinforcing these stupid, arbitrary silos? I\u0026rsquo;ll wrap this up with a rant, reworded from something I wrote a couple of years ago in Google+.\n Marketer, analyst, developer - these labels are attached to people often thanks to their job titles, but also in order to reinforce stereotypes that help tools like tag management solutions (TMS) sell better. Marketer is seen as the antithesis of the developer, and the analyst shuttles between these two roles depending on if they\u0026rsquo;re supporting growth in either marketing channels or within the organization. Throw in the mix growth hackers and data scientists while you\u0026rsquo;re at it. Nothing wrong with labels, but once they\u0026rsquo;re used as excuses to belittle the multitude of things that can be done in digital, that\u0026rsquo;s when it really bothers me. The non-technical (read: lazy) marketer has been the primus motor for TMS development. Almost as much as the uncooperative developer, sitting grumpily in his or her dungeon, sipping Jolt cola and laughing at the \u0026ldquo;stupid marketer\u0026rsquo;s\u0026rdquo; requests. The analyst is someone who\u0026rsquo;s hired for insight, but they\u0026rsquo;re reduced to either solving problems between the two aforementioned parties, or to tweaking lazily installed Google Analytics implementations. Seriously, all you need to do is look into an organization that\u0026rsquo;s doing it right to see that these labels are ridiculous. The true, modern, digital employee is a hybrid. They are forced to transcend silos because they\u0026rsquo;ve understood that a holistic, contextual view is what drives growth, instead of a singular focus on verticals like marketing, SEO, PPC, social media, plug-and-play analytics, or some legacy-burdened, development-driven framework.\n So, please. Can we stop being apologetic for the non-technical marketer, the emotionally detached developer, and the data-driven analyst? Perhaps that way people can become more ambitious and strive towards a more multi-disciplined approach - things that today\u0026rsquo;s digital landscape desperately calls for.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/the-youtube-video-trigger-in-google-tag-manager/",
	"title": "The YouTube Video Trigger In Google Tag Manager",
	"tags": ["Google Tag Manager", "trigger", "youtube"],
	"description": "Introduction and guide to the YouTube Video trigger in Google Tag Manager.",
	"content": " Last updated 20 April 2020: Clarified how lazy-loaded videos can be tracked with this trigger.\n Let\u0026rsquo;s cut straight to the chase. Google Tag Manager has just released the YouTube Video trigger, which gives you native support for YouTube video tracking. And it\u0026rsquo;s great! Even though we\u0026rsquo;ve been more than satisfied with the excellent tracking scripts provided by e.g. Cardinal Path and Bounteous (with a small modification from yours truly), this is a no-brainer for native support in Google Tag Manager.\n  The YouTube Video trigger checks pretty much all the boxes I\u0026rsquo;d expect in a video tracking trigger. It has built-in events for things like Start, Progress (e.g. 25%, 50%, 75%), and Complete. You can also use it to decorate the embedded YouTube URLs with the required enablejsapi=1 query parameter, if they don\u0026rsquo;t already have it.\nThe YouTube Video trigger supports tracking lazy-loaded or dynamically inserted videos, too, which will be a relief to sites that defer loading videos until they are actually interacted with by the user.\nCreate and configure the trigger To create the trigger, scroll to Triggers in the Google Tag Manager user interface, and create a new trigger. You\u0026rsquo;ll find the YouTube Video option in the sidebar that flies out when you click to choose a trigger type.\n  Once you\u0026rsquo;ve selected the YouTube Video trigger type, it\u0026rsquo;s time to configure it. Here are the options.\n  Capture - Start - collects a start when the user starts watching the video.\n  Capture - Complete - collects a complete when the user reaches the end of the video.\n  Capture - Pause, Seeking, and Buffering - collects a pause when the user pauses the video or jumps forward or back, and buffering when the video starts buffering due to lack of bandwith.\n  Capture - Progress - collects a progress the moment the user passes either a percentage or time threshold (e.g. 25%, 50%, 75% or 10 second mark, 30 second mark, one minute mark).\n  Advanced - Add JavaScript API support to all videos - if your YouTube embeds lack the necessary enablejsapi=1 parameter, you can check this to automatically add it to all your videos. NOTE! This reloads the iframe, so users might see the video flicker when they first load the page. This option, when checked, also adds the required https://www.youtube.com/iframe_api library to the page.\n  Here\u0026rsquo;s what the dataLayer payload looks like whenever a YouTube event is triggered.\n  event: 'gtm.video' - the event name pushed into dataLayer for all YouTube video events. This is what fires your YouTube trigger.\n  gtm.videoProvider: 'youtube' - specifies the platform whose videos you are tracking. You can take this as a hint that support for other platforms is planned!\n  gtm.videoStatus: 'start' - specifies the status of the video that caused the event to trigger. These different status values are only triggered if you\u0026rsquo;ve enabled them in the trigger settings. Possible values are 'start', 'complete', 'pause', 'buffering', and 'progress'.\n  gtm.videoUrl: 'https://www.youtube.com/watch?v=...' - the original URL of the embedded video.\n  gtm.videoTitle: 'Best of Simo Ahava' - the title of the embedded video.\n  gtm.videoDuration: 197 - the total length of the video in seconds.\n  gtm.videoCurrentTime: 30 - the time mark where the user is at when the video event happened.\n  gtm.videoElapsedTime: 10 - the time elapsed since the last time the video was paused or buffering.\n  gtm.videoPercent: 15 - the percentage mark where the user is at when the video event happened.\n  gtm.videoVisible: true - either true or false, depending on whether or not the video was visible in the browser viewport when the video event happened.\n  One thing that might make you breathe easier is that there are new Built-in Variables for all these items in the dataLayer. You can find them by clicking the red CONFIGURE button when browsing to Variables / Built-In Variables in the Google Tag Manager user interface.\n  Quick word about progress tracking Do note that tracking progress is relative to the entire video length and not the actual time or percentage you\u0026rsquo;ve been watching the video.\nSo if you configure the trigger to fire at 25%, 50%, and 75%, it will fire those events when the user reaches the respective marks in the video timeline even if they haven\u0026rsquo;t watched continuously from the beginning. Thus, if you start playing a video and jump straight to the 25% mark, the event will fire even though you only just started watching.\nThe same applies to the time thresholds.\nYou can leverage the Video Elapsed Time variable to see how long the user has been continuously watching the video since the last pause. Using the time and percentage thresholds only tells you if the user reached a specific milestone in the video, not necessarily if they actually watched all the way to that point. It\u0026rsquo;s a small but potentially significant difference.\nPutting it all together Since there are so many combinations of events you can collect with the YouTube Video trigger, I\u0026rsquo;ll show a fairly generic way of measuring start, pause, percentage progress and complete events with just one Universal Analytics Event tag. The tag looks like this:\n  The trigger that fires this tag looks like this:\n  And the Custom JavaScript Variable named {{JS - Get video action}} looks like this:\nfunction() { var status = {{Video Status}}; switch (status) { case \u0026#39;start\u0026#39;: return \u0026#39;Start playing\u0026#39;; case \u0026#39;pause\u0026#39;: return \u0026#39;Pause\u0026#39;; case \u0026#39;buffering\u0026#39;: return \u0026#39;Buffering\u0026#39;; case \u0026#39;progress\u0026#39;: return \u0026#39;Reached \u0026#39; + {{Video Percent}} + \u0026#39;%\u0026#39;; case \u0026#39;complete\u0026#39;: return \u0026#39;Reached the end\u0026#39;; } }  This translates the default parameter values of the video object in dataLayer to a more readable format. Thus we can use the same Event tag for all video events.\nTrack lazy-loaded / dynamically inserted videos If your videos load during the initial page load, then everything is smooth sailing for you. By checking the Add JavaScript API support to all videos option, GTM will take care of initializing the videos so that they can be tracked by the trigger. This option also downloads the required API library for you (see below).\nHowever, more and more sites are performance-wary these days, and they defer loading any embedded content until the user has intimated that they want to watch the video. You can read about the technical implications here.\nLuckily, the YouTube Video trigger does support tracking videos that are loaded and embedded after the initial page load. The only thing you need to do is make sure that when the page first loads, the YouTube iframe API is loaded as well:\n\u0026lt;script src=\u0026quot;https://www.youtube.com/iframe_api\u0026quot;\u0026gt;\nSo if your site does lazy-load or otherwise dynamically load videos (e.g. if it\u0026rsquo;s a single-page app), make sure that the library above is loaded before GTM\u0026rsquo;s video trigger is initialized. A \u0026ldquo;Page View\u0026rdquo; trigger usually does the job.\nAgain, if there is a YouTube embed video present during the initial page load, you don\u0026rsquo;t have to worry about this. The Add JavaScript API support to all videos takes care of loading this library for you.\nThe YouTube trigger does not work If for some reason the trigger isn\u0026rsquo;t tracking your YouTube embeds, you can use the following method to investigate the issue.\n  Check that the site has loaded the https://www.youtube.com/iframe_api library.\n  Check that the YouTube iframes have the enablejsapi=1 parameter in the src attribute.\n  Check that the YouTube iframes have a parameter that looks like data-gtm-yt-inspected-XXXXXX.\n  If all three check out, it typically means that something on the site reloads the iframe after GTM has inspected it. When Google Tag Manager initializes the video trigger, it initializes the listener on the embed. Any reload of the embed removes this listener, and because the iframe has the data-gtm-yt-inspected-XXXXXX attribute, GTM doesn\u0026rsquo;t re-evaluate the listener.\nTalk to the devs and make sure the iframes are not \u0026ldquo;recycled\u0026rdquo; (same iframe element used for multiple videos), and that once the iframe has been added to the site, it\u0026rsquo;s not reloaded for any reason.\nSummary I\u0026rsquo;m quite happy with the YouTube Video trigger. It does pretty much all I\u0026rsquo;d expect from the first iteration of the feature. I hope we\u0026rsquo;ll see support for other video services and players in the future.\nThere\u0026rsquo;s not a lot I miss. Mainly, I\u0026rsquo;d like for there to be a gtm.videoTotalElapsedTime, which would measure how much in total I\u0026rsquo;ve been watching any given video. The gtm.videoElapsedTime only tells me the elapsed time since the last pause/buffer/seek, but not what the total time watched has been.\nWhat do you think of the YouTube Video trigger? Is it just perfect for your tracking needs, or do you have features in mind you\u0026rsquo;d want to see in a future release?\n"
},
{
	"uri": "https://www.simoahava.com/tags/youtube/",
	"title": "youtube",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/remove-pii-google-analytics-hits/",
	"title": "#GTMTips: Remove PII From Google Analytics Hits",
	"tags": ["customtask", "gtmtips", "JavaScript", "pii"],
	"description": "Use customTask to automatically strip all personally identifiable information (PII) from requests sent to Google Analytics from your website.",
	"content": "Sending personally identifiable information (PII) to Google Analytics is one of the things you should really avoid doing. For one, it\u0026rsquo;s against the terms of service of the platform, but also you will most likely be in violation of national, federal, or EU legislation drafted to protect the privacy of individuals online.\nIn this #GTMTips post, I\u0026rsquo;ll show you a way to make sure that any tags you configure this solution with will not contain strings that might be construed as PII. The tip is for Google Tag Manager, but with very little modifications it will work with Universal Analytics, too.\n(UPDATE 8 September 2017: Check out Brian Clifton\u0026rsquo;s great extension of this solution: Remove PII from Google Analytics)\nTip 64: Remove PII from hits to Google Analytics   The solution hinges around customTask, which has fast become my favorite new feature in the analytics.js library. See the following articles to understand why I think so:\n  #GTMTips: Use customTask To Access Tracker Values In Google Tag Manager\n  #GTMTips: Send Google Analytics Tag To Multiple Properties\n  Track Users Who Are Offline In Google Analytics\n  Anyway, to make the whole thing run, create the following Custom JavaScript variable:\nfunction() { return function(model) { // Add the PII patterns into this array as objects  var piiRegex = [{ name: \u0026#39;EMAIL\u0026#39;, regex: /.{4}@.{4}/g },{ name: \u0026#39;HETU\u0026#39;, regex: /\\d{6}[A+-]\\d{3}[0-9A-FHJ-NPR-Y]/gi }]; var globalSendTaskName = \u0026#39;_\u0026#39; + model.get(\u0026#39;trackingId\u0026#39;) + \u0026#39;_sendHitTask\u0026#39;; // Fetch reference to the original sendHitTask  var originalSendTask = window[globalSendTaskName] = window[globalSendTaskName] || model.get(\u0026#39;sendHitTask\u0026#39;); var i, hitPayload, parts, val; // Overwrite sendHitTask with PII purger  model.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;).split(\u0026#39;\u0026amp;\u0026#39;); for (i = 0; i \u0026lt; hitPayload.length; i++) { parts = hitPayload[i].split(\u0026#39;=\u0026#39;); // Double-decode, to account for web server encode + analytics.js encode  try { val = decodeURIComponent(decodeURIComponent(parts[1])); } catch(e) { val = decodeURIComponent(parts[1]); } piiRegex.forEach(function(pii) { val = val.replace(pii.regex, \u0026#39;[REDACTED \u0026#39; + pii.name + \u0026#39;]\u0026#39;); }); parts[1] = encodeURIComponent(val); hitPayload[i] = parts.join(\u0026#39;=\u0026#39;); } sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload.join(\u0026#39;\u0026amp;\u0026#39;), true); originalSendTask(sendModel); }); }; }  Once you add this variable to your Universal Analytics tags as the customTask field, any hits sent by these tags will be parsed by this variable, which replaces the instances of PII with the string [REDACTED pii_type].\n  At the beginning of the code snippet, you\u0026rsquo;ll see the configuration object piiRegex. It\u0026rsquo;s an array of object literals, where each object has two properties: name and regex. The first is what will be used in the replace string after \u0026ldquo;REDACTED\u0026rdquo;. So if name is \u0026ldquo;EMAIL\u0026rdquo;, you\u0026rsquo;ll see \u0026ldquo;[REDACTED EMAIL]\u0026rdquo; in your Google Analytics reports wherever PII data was removed.\nThe second parameter, regex, is where you\u0026rsquo;ll add the regular expression literal for whatever PII pattern you want to redact. In the example above, I have two patterns:\n  /.{4}@.{4}/g - this matches all @ symbols plus the four preceding and four following characters. So if ANY part of the payload (URL, Custom Dimension, Event Label, etc.) has the @ symbol, then the string will be obfuscated. Thus, simo.s.ahava@gmail.com becomes simo.s.a[REDACTED EMAIL]l.com.\n  /\\d{6}[A+-]\\d{3}[0-9A-FHJ-NPR-Y]/gi - this is a reasonably good abstraction of the Finnish personal identity code. It\u0026rsquo;s not perfect, because the personal identity code is actually a calculation, so you can\u0026rsquo;t use simple pattern matches to only find valid codes. This regular expression will probably result in many false positives, especially if your GA hits include UUIDs or any type of alphanumeric hashes. But it\u0026rsquo;s still better than collecting this sensitive data.\n  You can add your own regular expression patterns as new objects of the array.\nWhen you add this variable into the customTask field of a Universal Analytics tag, the code will run through the entire payload, looking for matches to the regular expressions you provide in the configuration array. If any matches are made, they are redacted.\nDo you have other, useful regular expressions for finding and weeding out personally identifiable information?\n"
},
{
	"uri": "https://www.simoahava.com/tags/pii/",
	"title": "pii",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-users-who-are-offline-in-google-analytics/",
	"title": "Track Users Who Are Offline In Google Analytics",
	"tags": ["customtask", "google analytics", "Google Tag Manager", "JavaScript", "guest post"],
	"description": "Use this solution to automatically collect data from users who are offline, and send the data to Google Analytics once they are back online.",
	"content": "The steady increase in mobile use over the last years has introduced some new challenges for web analytics. It\u0026rsquo;s not just about mismatches in the tracking model (the concept of sessions is even more absurd for apps than it is for desktop browsing), but about something more fundamental, more basic. Think about it: if a visitor visits the website using a mobile device, there\u0026rsquo;s a significant chance of them losing internet connectivity and going unintentionally offline.\nActually, it\u0026rsquo;s enough for them to simply traverse an area with poor coverage - if the HTTP requests sent by the browser don\u0026rsquo;t complete in time, they timeout, and the hits are lost.\nFor Google Analytics pageviews it\u0026rsquo;s not such a big deal, because if the user sees the web page, it\u0026rsquo;s very likely the first pageview has completed. However, what about all the other interactions that we want to track, and the user doesn\u0026rsquo;t have an internet connection to boot? That\u0026rsquo;s right - we\u0026rsquo;ll lose these interactions, since the requests are dropped by the browser and never picked up, even when the user gets their connection back.\nIn this article, the brilliant David Vallejo and I will offer a solution for retrieving these hits initially dropped by the browser due to the internet connection being offline. OK, who am I kidding, this is all David. I\u0026rsquo;m just a glorified editor at this point.\n  Anyway, let\u0026rsquo;s frame it like this: the visitor is viewing our contact page, and we have an event queued up for when they click on our email contact information. However, the visitor is also on the subway, and the moment they click the email, they enter a tunnel. Oh noes! They lose their internet connection (damn subways without WiFi), and we miss our vital conversion tracking.\nThat\u0026rsquo;s the premise. Here\u0026rsquo;s the execution.\nIn this article, we\u0026rsquo;ll touch upon a number of fairly technical concepts, but we\u0026rsquo;ll try to frame them so that they make sense in the big picture.\n  Universal Analytics Tasks API\n  The browser\u0026rsquo;s Storage API\n  Sending Google Analytics hits with a delay (the \u0026amp;qt parameter)\n  Sending custom POST and HEAD requests (HTTP protocol)\n  Batching Universal Analytics Hits\n  All of these concepts are very useful to know if you want to know more about the mechanisms that the web browser employs to compile and dispatch requests to Universal Analytics.\nLet\u0026rsquo;s go!\n1. Universal Analytics Tasks API Each time a send command is dispatched to the ga() global method, a series of tasks are run in sequence by the analytics.js library. These tasks do a number of things, such as construct the payload, validate it, sample the requests, and finally dispatch the requests to the Universal Analytics endpoint.\nThe neat thing about these tasks is that we can intercept them and modify them using the API provided by analytics.js.\nYou can find a list of all available tasks in the API here. However, in this article we will focus on just a very special, very significant task: customTask. It\u0026rsquo;s a new task introduced very recently (here\u0026rsquo;s a guide by Simo).\nThis task is true to its name - it\u0026rsquo;s entirely customizable. It runs first in the task queue, so you can use it to configure the tracker object or even to set other tasks up for customization.\nIn this guide, we\u0026rsquo;ll use customTask to modify the sendHitTask. This way we can check if the user has internet connectivity when sendHitTask is run, and we can do a number of things if the user has dropped their connection. We use customTask instead of directly accessing sendHitTask simply because this way is much more Google Tag Manager-friendly.\nIn short, here\u0026rsquo;s the process:\ncustomTask modifies sendHitTask with OUR CODE before hit is sent.\nIn order to detect if the user has internet connectivity, we could simply poll our own web server endpoint. However, that wouldn\u0026rsquo;t be a good litmus test since it could be just Google\u0026rsquo;s servers that are not responding. That\u0026rsquo;s why we\u0026rsquo;ll actually poll Google\u0026rsquo;s endpoint to check if the user has the connectivity required for Google Analytics tracking.\n2. The offline tracker The solution is that with every single request to Universal Analytics, we\u0026rsquo;ll send an HTTP HEAD request to the Universal Analytics endpoint. If the endpoint isn\u0026rsquo;t responding, we can infer that the user does not have connectivity for communicating with Google Analytics, and so we\u0026rsquo;ll store the hit into browser storage until such a time that internet coverage is restored.\n  We\u0026rsquo;ll use localStorage for the queue, but we\u0026rsquo;ll need to cheat a little. localStorage itself doesn\u0026rsquo;t introduce any structure or a deeper data model - it just processes key-value pairs. So, to give us some additional flexibility, we\u0026rsquo;ll use the Lockr open-source database layer. It\u0026rsquo;s a simple and efficient framework, and it has pretty solid browser support.\nThis solution picks up the Universal Analytics hit payload from sendHitTask, and if there is no internet connection, this hit is stored in localStorage with the timestamp of the request. Thus, when we later do manage to send the stored hit, we can send it at its original timestamp to Universal Analytics.\n2.1. The \u0026amp;qt parameter The \u0026amp;qt Measurement Protocol stands for queue time. Basically, you can set a number in milliseconds in that parameter, and the hit will be sent with a timestamp that many milliseconds in the past. For example, if I know that the hit I want to send actually happened 45 minutes ago, I can set the parameter to:\n\u0026amp;qt=2700000\nThere\u0026rsquo;s just an odd little quirk you need to know about \u0026amp;qt. The latest you can send the displaced hit is at 03:59:59 the following day (in the timezone of the Google Analytics view the hit is being sent to). Thus, the maximum value for \u0026amp;qt is 27 hours, 59 minutes, and 59 seconds (in milliseconds), if the hit occurred at exactly midnight, and you then send it the following morning, just before 4 AM.\nYes, it might be difficult to grasp, so we\u0026rsquo;ll go with the official recommendation: avoid sending hits more than 4 hours in the past, since there\u0026rsquo;s no guarantee they will get sent.\n3. The HTTP HEAD request So what is this HEAD request and why are we using it? Well, it\u0026rsquo;s identical to GET, except it only returns the HTTP headers (and associated metadata), never any content.\nIt\u0026rsquo;s thus a great method to use if we only want to test an endpoint, and not get into the expensive process of actually retrieving data from it.\nSince we are only interested in knowing if the Universal Analytics endpoint responds, the HTTP HEAD request is perfect for this purpose. Also, see how efficient it is compared to POST and GET:\n  4. The JavaScript code The code comes in three parts. First is the library for extending the database: Lockr. Next we have the customTask execution, and finally we\u0026rsquo;ll chain the HTTP HEAD and batch requests together to make the whole thing click.\n4.1. Lockr download To get started, go ahead and load Lockr on your site in any way you want. If you\u0026rsquo;re using Google Tag Manager, we recommend loading the following code in a Custom HTML Tag that fires on All Pages with the highest possible Tag Priority. Alternatively, if you accept some overhead and redundancy, you can just add the library to the top of the Custom JavaScript Variable itself, as in the example in the next chapter.\nHere\u0026rsquo;s the minified JavaScript code - it should be executed by the browser before the offline tracking solution is run:\n!function(e,t){\u0026#34;undefined\u0026#34;!=typeof exports?\u0026#34;undefined\u0026#34;!=typeof module\u0026amp;\u0026amp;module.exports\u0026amp;\u0026amp;(exports=module.exports=t(e,exports)):\u0026#34;function\u0026#34;==typeof define\u0026amp;\u0026amp;define.amd?define([\u0026#34;exports\u0026#34;],function(r){e.Lockr=t(e,r)}):e.Lockr=t(e,{})}(this,function(e,t){\u0026#34;use strict\u0026#34;;return Array.prototype.indexOf||(Array.prototype.indexOf=function(e){var t=this.length\u0026gt;\u0026gt;\u0026gt;0,r=Number(arguments[1])||0;for((r=r\u0026lt;0?Math.ceil(r):Math.floor(r))\u0026lt;0\u0026amp;\u0026amp;(r+=t);r\u0026lt;t;r++)if(r in this\u0026amp;\u0026amp;this[r]===e)return r;return-1}),t.prefix=\u0026#34;\u0026#34;,t._getPrefixedKey=function(e,t){return(t=t||{}).noPrefix?e:this.prefix+e},t.set=function(e,t,r){var o=this._getPrefixedKey(e,r);try{localStorage.setItem(o,JSON.stringify({data:t}))}catch(r){console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr didn\u0026#39;t successfully save the \u0026#39;{\u0026#34;+e+\u0026#34;: \u0026#34;+t+\u0026#34;}\u0026#39; pair, because the localStorage is full.\u0026#34;)}},t.get=function(e,t,r){var o,n=this._getPrefixedKey(e,r);try{o=JSON.parse(localStorage.getItem(n))}catch(e){o=localStorage[n]?{data:localStorage.getItem(n)}:null}return o?\u0026#34;object\u0026#34;==typeof o\u0026amp;\u0026amp;void 0!==o.data?o.data:void 0:t},t.sadd=function(e,r,o){var n,a=this._getPrefixedKey(e,o),i=t.smembers(e);if(i.indexOf(r)\u0026gt;-1)return null;try{i.push(r),n=JSON.stringify({data:i}),localStorage.setItem(a,n)}catch(t){console.log(t),console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr didn\u0026#39;t successfully add the \u0026#34;+r+\u0026#34; to \u0026#34;+e+\u0026#34; set, because the localStorage is full.\u0026#34;)}},t.smembers=function(e,t){var r,o=this._getPrefixedKey(e,t);try{r=JSON.parse(localStorage.getItem(o))}catch(e){r=null}return r\u0026amp;\u0026amp;r.data?r.data:[]},t.sismember=function(e,r,o){return t.smembers(e).indexOf(r)\u0026gt;-1},t.keys=function(){var e=[],r=Object.keys(localStorage);return 0===t.prefix.length?r:(r.forEach(function(r){-1!==r.indexOf(t.prefix)\u0026amp;\u0026amp;e.push(r.replace(t.prefix,\u0026#34;\u0026#34;))}),e)},t.getAll=function(e){var r=t.keys();return e?r.reduce(function(e,r){var o={};return o[r]=t.get(r),e.push(o),e},[]):r.map(function(e){return t.get(e)})},t.srem=function(e,r,o){var n,a,i=this._getPrefixedKey(e,o),c=t.smembers(e,r);(a=c.indexOf(r))\u0026gt;-1\u0026amp;\u0026amp;c.splice(a,1),n=JSON.stringify({data:c});try{localStorage.setItem(i,n)}catch(t){console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr couldn\u0026#39;t remove the \u0026#34;+r+\u0026#34; from the set \u0026#34;+e)}},t.rm=function(e){var t=this._getPrefixedKey(e);localStorage.removeItem(t)},t.flush=function(){t.prefix.length?t.keys().forEach(function(e){localStorage.removeItem(t._getPrefixedKey(e))}):localStorage.clear()},t});  After this code puke, we\u0026rsquo;re ready to jump in the deep end with some offline hit tracking!\n4.2. Offline hit tracker for on-page Universal Analytics The following JavaScript runs with the default Universal Analytics tracker, and thus any hits sent with the ga('send', '...'); will be included in the process.\nTo make the whole thing work, you should setup your code in the following order:\n\u0026lt;head\u0026gt; ... \u0026lt;script\u0026gt; // Put the Lockr code here first  \u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var _offlineTracker = function(customTaskModel) { // _offlineTracker (see below) here  }; \u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Universal Analytics snippet here  ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;); // Add the following line AFTER the \u0026#39;create\u0026#39; command and BEFORE the first \u0026#39;send\u0026#39; command  ga(\u0026#39;set\u0026#39;, \u0026#39;customTask\u0026#39;, _offlineTracker); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; ... \u0026lt;/head\u0026gt; And here\u0026rsquo;s the code for the _offlineTracker callback function.\nvar _offlineTracker = function(customTaskModel) { Lockr.prefix = \u0026#39;ga_\u0026#39;; // Grab the original sentHitTask Function from the first tracker. to kept the original hit sending function.  var originalSendHitTask = customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); customTaskModel.set(\u0026#39;sendHitTask\u0026#39;, function(model) { // Let\u0026#39;s send the original hit using the native functionality  originalSendHitTask(model); // Grab the hit Payload  var payload_lz = model.get(\u0026#39;hitPayload\u0026#39;); // Check if GA Endpoint is Ready  var http = new XMLHttpRequest(); http.open(\u0026#39;HEAD\u0026#39;, \u0026#39;https://www.google-analytics.com/collect\u0026#39;); http.onreadystatechange = function() { // Google Analytics endpoint is not reachable, let\u0026#39;s save the hit  if (this.readyState === this.DONE \u0026amp;\u0026amp; this.status !== 200) { Lockr.sadd(\u0026#39;hits\u0026#39;, payload_lz + \u0026#34;\u0026amp;qt=\u0026#34; + (new Date() * 1)); } else { // Google Analytics endpoint is available, let\u0026#39;s check if there are any unsent hits  if (Lockr.smembers(\u0026#34;hits\u0026#34;).length \u0026gt; 0) { // Process hits in queue  var current_ts = new Date() * 1 / 1000; var hits = Lockr.smembers(\u0026#34;hits\u0026#34;); // ./batch endpoint only allows 20 hits per batch, let\u0026#39;s chunk the hits array.  var chunk_size = 20; var chunked_hits = Lockr.smembers(\u0026#34;hits\u0026#34;).map(function(e, i) { return i % chunk_size === 0 ? hits.slice(i, i + chunk_size) : null; }).filter(function(e) { return e; }); // Let\u0026#39;s loop thru the chunks array and send the hits to GA  for (var i = 0; i \u0026lt; chunked_hits.length; i++) { var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;https://www.google-analytics.com/batch\u0026#39;, true); // Build the Batch Payload and Take care of calculating the Queue Time  xhr.send(chunked_hits[i].map(function(x) { if (x.indexOf(\u0026#34;\u0026amp;qt=\u0026#34;) \u0026gt; -1) { return x.replace(/qt=([^\u0026amp;]*)/, \u0026#34;qt=\u0026#34; + Math.round(current_ts - x.match(/qt=([^\u0026amp;]*)/)[1] / 1000) * 1000); } else return x; }).join(\u0026#34;\\n\u0026#34;)); } //Hits sent, flush the Storage  Lockr.flush(); } } }; http.send(); }); };  Once you create this _offlineTracker and invoke it in the ga('set', 'customTask', _offlineTracker) command, every single hit that uses this tracker will be stored in the queue if there is no internet connectivity. Once a hit is sent with a solid connection, all hits in the queue are sent as well.\n4.3. Offline hit tracker for Google Tag Manager With Google Tag Manager, you can get by with a single Custom JavaScript variable. This variable can be configured to include the Lockr code as well, so it\u0026rsquo;s completely self-contained. Give the variable a descriptive name, e.g. {{JS - customTask Offline Hit Tracker}} and put the following code within:\nfunction() { return function(customTaskModel) { // Load Lockr if it hasn\u0026#39;t already been loaded  if (!window.Lockr) { !function(e,t){\u0026#34;undefined\u0026#34;!=typeof exports?\u0026#34;undefined\u0026#34;!=typeof module\u0026amp;\u0026amp;module.exports\u0026amp;\u0026amp;(exports=module.exports=t(e,exports)):\u0026#34;function\u0026#34;==typeof define\u0026amp;\u0026amp;define.amd?define([\u0026#34;exports\u0026#34;],function(r){e.Lockr=t(e,r)}):e.Lockr=t(e,{})}(this,function(e,t){\u0026#34;use strict\u0026#34;;return Array.prototype.indexOf||(Array.prototype.indexOf=function(e){var t=this.length\u0026gt;\u0026gt;\u0026gt;0,r=Number(arguments[1])||0;for((r=r\u0026lt;0?Math.ceil(r):Math.floor(r))\u0026lt;0\u0026amp;\u0026amp;(r+=t);r\u0026lt;t;r++)if(r in this\u0026amp;\u0026amp;this[r]===e)return r;return-1}),t.prefix=\u0026#34;\u0026#34;,t._getPrefixedKey=function(e,t){return(t=t||{}).noPrefix?e:this.prefix+e},t.set=function(e,t,r){var o=this._getPrefixedKey(e,r);try{localStorage.setItem(o,JSON.stringify({data:t}))}catch(r){console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr didn\u0026#39;t successfully save the \u0026#39;{\u0026#34;+e+\u0026#34;: \u0026#34;+t+\u0026#34;}\u0026#39; pair, because the localStorage is full.\u0026#34;)}},t.get=function(e,t,r){var o,n=this._getPrefixedKey(e,r);try{o=JSON.parse(localStorage.getItem(n))}catch(e){o=localStorage[n]?{data:localStorage.getItem(n)}:null}return o?\u0026#34;object\u0026#34;==typeof o\u0026amp;\u0026amp;void 0!==o.data?o.data:void 0:t},t.sadd=function(e,r,o){var n,a=this._getPrefixedKey(e,o),i=t.smembers(e);if(i.indexOf(r)\u0026gt;-1)return null;try{i.push(r),n=JSON.stringify({data:i}),localStorage.setItem(a,n)}catch(t){console.log(t),console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr didn\u0026#39;t successfully add the \u0026#34;+r+\u0026#34; to \u0026#34;+e+\u0026#34; set, because the localStorage is full.\u0026#34;)}},t.smembers=function(e,t){var r,o=this._getPrefixedKey(e,t);try{r=JSON.parse(localStorage.getItem(o))}catch(e){r=null}return r\u0026amp;\u0026amp;r.data?r.data:[]},t.sismember=function(e,r,o){return t.smembers(e).indexOf(r)\u0026gt;-1},t.keys=function(){var e=[],r=Object.keys(localStorage);return 0===t.prefix.length?r:(r.forEach(function(r){-1!==r.indexOf(t.prefix)\u0026amp;\u0026amp;e.push(r.replace(t.prefix,\u0026#34;\u0026#34;))}),e)},t.getAll=function(e){var r=t.keys();return e?r.reduce(function(e,r){var o={};return o[r]=t.get(r),e.push(o),e},[]):r.map(function(e){return t.get(e)})},t.srem=function(e,r,o){var n,a,i=this._getPrefixedKey(e,o),c=t.smembers(e,r);(a=c.indexOf(r))\u0026gt;-1\u0026amp;\u0026amp;c.splice(a,1),n=JSON.stringify({data:c});try{localStorage.setItem(i,n)}catch(t){console\u0026amp;\u0026amp;console.warn(\u0026#34;Lockr couldn\u0026#39;t remove the \u0026#34;+r+\u0026#34; from the set \u0026#34;+e)}},t.rm=function(e){var t=this._getPrefixedKey(e);localStorage.removeItem(t)},t.flush=function(){t.prefix.length?t.keys().forEach(function(e){localStorage.removeItem(t._getPrefixedKey(e))}):localStorage.clear()},t}); } Lockr.prefix = \u0026#39;ga_\u0026#39;; // Grab the original sentHitTask Function from the first tracker. to kept the original hit sending function.  var originalSendHitTask = customTaskModel.get(\u0026#39;sendHitTask\u0026#39;); customTaskModel.set(\u0026#39;sendHitTask\u0026#39;, function(model) { // Let\u0026#39;s send the original hit using the native functionality  originalSendHitTask(model); // Grab the hit Payload  var payload_lz = model.get(\u0026#39;hitPayload\u0026#39;); // Check if GA Endpoint is Ready  var http = new XMLHttpRequest(); http.open(\u0026#39;HEAD\u0026#39;, \u0026#39;https://www.google-analytics.com/collect\u0026#39;); http.onreadystatechange = function() { // Google Analytics endpoint is not reachable, let\u0026#39;s save the hit  if (this.readyState === this.DONE \u0026amp;\u0026amp; this.status !== 200) { Lockr.sadd(\u0026#39;hits\u0026#39;, payload_lz + \u0026#34;\u0026amp;qt=\u0026#34; + (new Date() * 1)); } else { // Google Analytics endpoint is available, let\u0026#39;s check if there are any unsent hits  if (Lockr.smembers(\u0026#34;hits\u0026#34;).length \u0026gt; 0) { // Process hits in queue  var current_ts = new Date() * 1 / 1000; var hits = Lockr.smembers(\u0026#34;hits\u0026#34;); // ./batch endpoint only allows 20 hits per batch, let\u0026#39;s chunk the hits array.  var chunk_size = 20; var chunked_hits = Lockr.smembers(\u0026#34;hits\u0026#34;).map(function(e, i) { return i % chunk_size === 0 ? hits.slice(i, i + chunk_size) : null; }).filter(function(e) { return e; }); // Let\u0026#39;s loop thru the chunks array and send the hits to GA  for (var i = 0; i \u0026lt; chunked_hits.length; i++) { var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;https://www.google-analytics.com/batch\u0026#39;, true); // Build the Batch Payload and Take care of calculating the Queue Time  xhr.send(chunked_hits[i].map(function(x) { if (x.indexOf(\u0026#34;\u0026amp;qt=\u0026#34;) \u0026gt; -1) { return x.replace(/qt=([^\u0026amp;]*)/, \u0026#34;qt=\u0026#34; + Math.round(current_ts - x.match(/qt=([^\u0026amp;]*)/)[1] / 1000) * 1000); } else return x; }).join(\u0026#34;\\n\u0026#34;)); } //Hits sent, flush the Storage  Lockr.flush(); } } }; http.send(); }); }; }  Add this code to all your Universal Analytics tags by scrolling to More settings -\u0026gt; Fields to set. Here you add a new field with:\nField name: customTask\nValue: {{JS - customTask Offline Hit Tracker}}\n  Once you\u0026rsquo;ve done this, then all your tags with this customTask setting are protected against poor connectivity, and whenever the connection is restored, the batch queue is processed.\n4.4. About batching Universal Analytics lets you send hits to its endpoint in batches. This is used mostly by the iOS and Android SDKs. The main point in using the batch endpoint (/batch) is to have as few HTTP requests dispatched as possible. Here, batching means that we can send multiple Universal Analytics payloads in a single HTTP request.\nBatching does have some limitations we\u0026rsquo;ll need to consider:\n  A maximum of 20 hits can be specified per request.\n  The combined size of all hit payloads cannot be greater than 16K bytes.\n  No single hit payload can be greater than 8K bytes.\n  The request needs to be done using POST\n  For our solution, each time the number of stored hits is more than 1, we send the payloads using the batch endpoint. In case there are too many hits stored in the queue, we\u0026rsquo;re chunking them so that multiple batch requests are sent in succession until the entire queue is processed.\n5. Improvements Keep in mind that this current post is meant to show how to track the hits that may happen while the user is offline (due to a connectivity gap). At the same time, we\u0026rsquo;ve taken the opportunity to showcase some cool and relatively little-known Google Analytics JavaScript API functionalities.\nA solid improvement would be to skip the originalSendTask part, and just overwrite the sendHitTask task entirely. This way you can skip the HTTP HEAD request, because you can just check if the initial hit to Google Analytics is dispatched successfully.\nThe one thing you need to keep in mind if you want to overwrite the sendHitTask is that you\u0026rsquo;ll need to replicate the transport logic for the request. The analytics.js library supports three different ways to dispatch the requests:\n  'image' - max 2K payload size sent with a GET request to a pixel endpoint\n  'xhr' - max 8K payload size sent as an HTTP POST request\n  'beacon' - POST request that uses the navigator.sendBeacon() to make sure your requests are sent even if the user has already navigated away from the page\n  So you\u0026rsquo;ll need to replicate this logic in your custom sendHitTask method. It\u0026rsquo;s not exactly trivial to do, but an able developer should be able to do it, especially once the constraints (see previous paragraph) are known.\nAnother thing you might want to do is add a Custom Dimension to all the payloads that are stored in the queue. This Custom Dimension could be named something like Offline hit, and you should set the value to true if the hit was sent from the offline queue. Thus you can monitor how many hits in your data were initially aborted due to poor internet connectivity.\n6. Summary I\u0026rsquo;m really glad to have David guest star on this blog - it\u0026rsquo;s been a long time coming! This solution is great for two reasons. First, it\u0026rsquo;s actually very usable, especially if your site caters to mobile visitors. Second, it showcases a number of features of the web browser and the analytics.js library that can be extended to other purposes, too.\nThe Tasks API is really interesting, as it allows you to manipulate the payloads dispatched by the website. And with the introduction of customTask, we finally have a very handy way of accessing tasks with Google Tag Manager.\nNote that if you have a web application with offline capabilities, and you are using service workers to manage this functionality, Google has released a useful library for employing service workers to do precisely the same thing we\u0026rsquo;re doing in this article.\nWe hope you enjoyed this solution! Let us know in the comments\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/pagination-and-quick-search/",
	"title": "#GTMTips: Pagination And Quick Search",
	"tags": ["Google Tag Manager", "gtmtips", "search", "user interface"],
	"description": "Introducing new Google Tag Manager user interface features - pagination and quick search.",
	"content": "If you open the Google Tag Manager user interface and browser your tags, triggers, and variables, you might notice that the UI now has two new features:\n  Pagination, where only 50 results are shown per page\n  A quick search / filter bar at the top of each list, which lets you narrow the list down to results that match your query\n  Pagination might be a nuisance in large containers, but it was implemented to improve performance. Fetching the list of assets and rendering them on the screen can be a pretty hefty amount of API work, especially if the container is bloated. By introducing pagination, only 50 items are processed at a time, which speeds up the UI. However, being forced to plow through pages of assets can get tedious.\nThat\u0026rsquo;s why the quick search / filter bar is so useful. It lets you filter the page to show results that match your query, regardless of what page the result is actually on. So it\u0026rsquo;s a great way to quickly access your results.\nTip 63: Filtering through pages of data   This tip is as simple as it gets. If you have more than 50 tags, or 50 triggers, or 50 variables, then the respective lists in your Google Tag Manager user interface will be paginated so that only 50 results are shown per page.\nTo find what you want quickly, you can use the quick search / filter bar at the top of each list table. This will filter the results to show only those that match your query.\nAnd here\u0026rsquo;s another tip. You can use the forward slash (/) key to place focus on the search field. You can try this with other products, too, as it\u0026rsquo;s a fairly universal way of focusing on a search / filter box. Note that this requires that you have a \u0026ldquo;natural\u0026rdquo; forward slash key in your keyboard layout. So folks in Finland, for example, are somewhat out of luck, because forward slash is typed with SHIFT+7, which doesn\u0026rsquo;t work in the GTM UI. Nevertheless, if your keyboard has a number pad, you should find the forward slash key on that. Let me know in the comments if you can access the quick search some other way, and if it\u0026rsquo;s been localized in your own keyboard layout.\n"
},
{
	"uri": "https://www.simoahava.com/tags/search/",
	"title": "search",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/user-interface/",
	"title": "user interface",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/add-on/",
	"title": "add-on",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/two-new-unofficial-ga-add-ons-for-google-sheets/",
	"title": "Two New Unofficial GA Add-ons For Google Sheets",
	"tags": ["add-on", "apps script", "google analytics", "google sheets", "JavaScript", "open source"],
	"description": "Introducing two unofficial Google Analytics add-ons for Google Sheets. You can manage Custom Dimensions and validate your Google Analytics account hierarchy, right from Google Sheets.",
	"content": "More often than not, much of what we do in web analytics can be automated. This applies especially to implementations, audits, configurations, and reporting. So when I\u0026rsquo;m faced with a menial, manual task that might take hours for me to complete if done by hand, I always look at what could be done with some scripting and API work. I want to introduce a couple of Google Sheets add-ons I\u0026rsquo;ve written and released to the public. They both automate tasks which I found completely ridiculous to do by hand.\n  The two add-ons, Google Analytics Validator and Google Analytics Custom Dimension Manager, are unofficial, free-to-use, and open-source. They were created initially for my own benefit alone, but at some point I wondered if others might find them useful, too. That\u0026rsquo;s why I ended up publishing them.\nGoogle Analytics Validator You can add Google Analytics Validator to Google Sheets via this link, and you can access the open-source code in this GitHub repo.\n  Google Analytics Validator has three purposes:\n  You can use it to create a sheet with a master list of all the accounts, properties, and views your Google ID has access to.\n  From this list, you can then choose any number of properties/views for Custom Dimension analysis. This means that a new sheet is built, with the name, scope, and active status of all the Custom Dimensions created for each property.\n  Finally, you can fetch the number of hits collected in the last 7 days for any selected view. This tells you if there are Custom Dimensions that haven\u0026rsquo;t collected any data recently.\n  It\u0026rsquo;s a simple tool, but it should give you some insight to how Custom Dimensions are aligned across all your accounts, properties, and views.\nGoogle Analytics Custom Dimension Manager You can add Google Analytics Custom Dimension Manager to Google Sheets via this link, and you can access the open-source code in this GitHub repo.\n  The Google Analytics Custom Dimension Manager lets you mass update Custom Dimensions in any account/property you have required access rights to. The values that you use to update/create the dimensions with are populated in a special Source Data sheet.\nNOTE! There\u0026rsquo;s a limit of 500 write requests per day for this project. Naturally, this is nowhere near enough to cater to multiple users who might be using the add-on. I am applying for an increase in the quota, but if this tool is something you\u0026rsquo;ll need consistently, you might want to grab the open-source code and recreate the add-on as a project of your own.\nThis should save you some time if you need to update 200 dimensions across multiple properties, for example.\nFeedback If you want to leave me feedback, you can do it either via email (simo at simoahava.com), or you can open an issue in either GitHub repo. I appreciate any feedback I can get!\n"
},
{
	"uri": "https://www.simoahava.com/tools/google-analytics-custom-dimension-manager/",
	"title": "Custom Dimension Manager For Google Analytics",
	"tags": [],
	"description": "",
	"content": "The Custom Dimension Manager For Google Analytics is a Google Sheets add-on. It lets you update Custom Dimensions in any given Google Analytics property en masse.   When updating, new dimensions are created automatically with values from a source data sheet (or using a default value if no value is specified), and for existing dimensions you have the choice to update or skip.   The add-on is completely free to use. It\u0026rsquo;s a hobby project, but I would still welcome any feedback. You can send the feedback to simo (at) simoahava.com.\nShoutout Mr. Pedro Ávila from Google published in 2015 a very similar Google Sheets extension. His extension and mine serve a very similar purpose, and since I did read Pedro\u0026rsquo;s article when it was released, the work he did for his extension has certainly inspired the work I put into mine, even if both extensions were developed completely independently and have a completely different codebase.\nI recommend checking out Pedro\u0026rsquo;s work in the dedicated GitHub repo, and you can download his extension from this link.\nHow to get it In Google Sheets, open the Add-ons menu, and click Get add-ons.   You should see the Chrome web store window open. Enter \u0026ldquo;google analytics custom dimension manager\u0026rdquo; into the search field and press Enter. You should see the add-on appear in the search results.   Click the + FREE button. You will be asked to sign in with your Google ID, and then approve the add-on access to your data. The add-on requires read and write access to your Google Analytics management data, and read and write access to your Google Sheets account.\nYou are then ready to use the tool!\nGitHub repo You can download the open-source code from the GitHub repo: ga-custom-dimension-manager.\nHow it works You should see the Custom Dimension Manager For Google Analytics menu item in the Add-ons menu. You might need to reload the page if it isn\u0026rsquo;t there, or if it seems to be missing all its menu items.\nFirst, click on 1. Build/reformat Source Data sheet. This creates a new sheet named \u0026ldquo;Source Data\u0026rdquo;. It is populated with Custom Dimensions from 1-200, as well as a row where you can set the default value for empty rows. For each dimension, you can now write the name, scope (HIT or PRODUCT or SESSION or USER), and active status (true or false). If you leave a row empty, the tool will use what you specified in the \u0026ldquo;DEFAULT/EMPTY\u0026rdquo; row.\nIf the sheet already existed, the tool only updates the header and the dimension ID column.   Once you\u0026rsquo;re done, click on 2. Manage Custom Dimensions in the menu. You\u0026rsquo;ll see a dialog open, where you need to choose the property whose dimensions to update. Note the warning. To UPDATE a Custom Dimension, only EDIT access to the Property is needed. To CREATE a new Custom Dimension, EDIT access is required for both Account and Property.\nOnce you\u0026rsquo;ve selected the Property to update, you\u0026rsquo;ll need to also choose whether to update only dimensions 1-20, or whether to update all the dimensions (1-200). The latter option only works with Google Analytics 360 accounts, so be mindful of that when working with the tool.   When you\u0026rsquo;re ready, press START, and the tool will instantly begin to update/create the Custom Dimensions based on the Source Data sheet.\nThree things can happen now.\n  If the dimension to be updated already exists, and its values differ from those in the Source Data sheet, you have the choice to either UPDATE your Google Analytics data with that from the Source Data, or you can SKIP this dimension, not making any changes to GA.\n  If the dimension to be updated already exists, but it has identical values to those in the Source Data sheet, the dimension is automatically skipped.\n  If the dimension to be updated doesn\u0026rsquo;t exist yet, it is automatically created with the values from the Source Data sheet.\n  Once the process is over, the tool will inform you of this, and you can visit Google Analytics to check if the dimensions were created properly.\nPrivacy Policy What information do you collect? The Google Analytics Custom Dimension Manager collects no information from its users. It is an API tool used for managing and updating Custom Dimensions in the user\u0026rsquo;s Google Analytics account and property hierarchy.\nThe only thing the add-on logs is the generic Google Cloud Console API usage statistics, which tells the owner how much the enabled APIs are being used, but this data cannot be used to identify users or individual use patterns.\nHow do you use the information? No user or usage information is used. The only thing the owner monitors is API usage, so that it can be determined if quotas need to be increased to ensure the tool works smoothly.\nWhat information do you share? No information is shared with third parties, with other users, with analytics tools, with marketing partners, or any other party.\nTerms of Service Terms of Service (\u0026ldquo;Terms\u0026rdquo;) Last updated: October 30, 2018\nPlease read these Terms of Service (\u0026ldquo;Terms\u0026rdquo;, \u0026ldquo;Terms of Service\u0026rdquo;) carefully before using the Custom Dimension Manager For Google Analytics extension (the \u0026ldquo;Service\u0026rdquo;) operated by Simo Ahava (\u0026ldquo;us\u0026rdquo;, \u0026ldquo;we\u0026rdquo;, or \u0026ldquo;our\u0026rdquo;).\nYour access to and use of the Service is conditioned on your acceptance of and compliance with these Terms. These Terms apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service you agree to be bound by these Terms. If you disagree with any part of the terms then you may not access the Service.\nLinks To Other Web Sites Our Service may contain links to third-party web sites or services that are not owned or controlled by us.\nWe have no control over, and assume no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that we shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services.\nWe strongly advise you to read the terms and conditions and privacy policies of any third-party web sites or services that you visit.\nGoverning Law These Terms shall be governed and construed in accordance with the laws of Finland, without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect. These Terms constitute the entire agreement between us regarding our Service, and supersede and replace any prior agreements we might have between us regarding the Service.\nChanges We reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.\nBy continuing to access or use our Service after those revisions become effective, you agree to be bound by the revised terms. If you do not agree to the new terms, please stop using the Service.\nContact Us If you have any questions about these Terms, please contact us.\n"
},
{
	"uri": "https://www.simoahava.com/tools/google-analytics-validator/",
	"title": "Google Analytics Validator",
	"tags": [],
	"description": "",
	"content": "The Google Analytics Validator is a Google Sheets add-on. It lets you build a spreadsheet of all the accounts, properties, and views you have access to in Google Analytics. Then, you can select any number of these properties to populate a second sheet, where each Custom Dimension configured in the property is displayed with its name, scope, and active status.   Finally, you can poll for the last 7 days data for any selected property/view to see if the Custom Dimension has collected any hits.   The add-on is completely free to use. It\u0026rsquo;s a hobby project, but I would still welcome any feedback. You can send the feedback to simo (at) simoahava.com.\nHow to get it In Google Sheets, open the Add-ons menu, and click Get add-ons.   You should see the Chrome web store window open. Enter \u0026ldquo;google analytics validator\u0026rdquo; into the search field and press Enter. You should see the add-on appear in the search results.   Click the + FREE button. You will be asked to sign in with your Google ID, and then approve the add-on access to your data. The add-on requires read-only access to your Google Analytics data, and read and write access to your Google Sheets account.\nYou are then ready to use the tool!\nGitHub repo You can download the open-source code from the GitHub repo: ga-validator-apps-script.\nHow it works You should see the Google Analytics Validator menu item in the Add-ons menu. You might need to reload the page if it isn\u0026rsquo;t there, or if it seems to be missing all its menu items.\nFirst, click on 1. Build Google Analytics hierarchy. This collects all the Google Analytics accounts, properties, and views you have access to, and builds them into a new sheet with the name \u0026ldquo;GA Hierarchy\u0026rdquo;.   NOTE! If this sheet already exists, it is overwritten.\nNext, type the letter x or X in the Select for analysis (x/X) column for each property/view that you want to include in the Custom Dimension analysis.\nOnce you\u0026rsquo;re done, click on 2. Run validator in the menu. This creates a new sheet named \u0026ldquo;GA Dimensions\u0026rdquo; (overwriting any existing sheet with the name), where all the dimensions from 1-200 are populated with values drawn from the properties you selected for analysis.\nFinally, you can select any cell in a data column, and then click 3. Fetch last 7 days data\u0026hellip; in the menu. This populates the LAST 7 DAYS column for the given view, fetching data from the last 7 days for each dimension that is active. The data that is fetched is the ga:hits metric.   This tool has three purposes:\n  It gives you a master list of accounts, properties, and views you have access to.\n  It lets you see how Custom Dimensions are configured across properties.\n  It lets you analyze which Custom Dimensions have not collected any data recently.\n  Privacy Policy What information do you collect? The Google Analytics Validator collects no information from its users. It is an API tool for building sheets of data based on your Google Analytics account and data hierarchy, and no usage information is collected or used in any way.\nThe only thing the add-on logs is the generic Google Cloud Console API usage statistics, which tells the owner how much the enabled APIs are being used, but this data cannot be use to identify users or individual use patterns.\nHow do you use the information? No user or usage information is used. The only thing the owner monitors is API usage, so that it can be determined if quotas need to be increased to ensure the tool works smoothly.\nWhat information do you share? No information is shared with third parties, with other users, with analytics tools, with marketing partners, or any other party.\nTerms of Service Terms of Service (\u0026ldquo;Terms\u0026rdquo;) Last updated: October 30, 2018\nPlease read these Terms of Service (\u0026ldquo;Terms\u0026rdquo;, \u0026ldquo;Terms of Service\u0026rdquo;) carefully before using the Google Analytics Validator extension (the \u0026ldquo;Service\u0026rdquo;) operated by Simo Ahava (\u0026ldquo;us\u0026rdquo;, \u0026ldquo;we\u0026rdquo;, or \u0026ldquo;our\u0026rdquo;).\nYour access to and use of the Service is conditioned on your acceptance of and compliance with these Terms. These Terms apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service you agree to be bound by these Terms. If you disagree with any part of the terms then you may not access the Service.\nLinks To Other Web Sites Our Service may contain links to third-party web sites or services that are not owned or controlled by us.\nWe have no control over, and assume no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that we shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with use of or reliance on any such content, goods or services available on or through any such web sites or services.\nWe strongly advise you to read the terms and conditions and privacy policies of any third-party web sites or services that you visit.\nGoverning Law These Terms shall be governed and construed in accordance with the laws of Finland, without regard to its conflict of law provisions.\nOur failure to enforce any right or provision of these Terms will not be considered a waiver of those rights. If any provision of these Terms is held to be invalid or unenforceable by a court, the remaining provisions of these Terms will remain in effect. These Terms constitute the entire agreement between us regarding our Service, and supersede and replace any prior agreements we might have between us regarding the Service.\nChanges We reserve the right, at our sole discretion, to modify or replace these Terms at any time. If a revision is material we will try to provide at least 30 days notice prior to any new terms taking effect. What constitutes a material change will be determined at our sole discretion.\nBy continuing to access or use our Service after those revisions become effective, you agree to be bound by the revised terms. If you do not agree to the new terms, please stop using the Service.\nContact Us If you have any questions about these Terms, please contact us.\n"
},
{
	"uri": "https://www.simoahava.com/tools/chrome-extensions/",
	"title": "Chrome Extensions",
	"tags": [],
	"description": "",
	"content": "Here are the two Chrome Extensions I\u0026rsquo;ve written.\n  GTM Sonar - Debug your on-site JavaScript to see if it\u0026rsquo;s compatible with the event listeners Google Tag Manager leverages\n  Internalize for Google Analytics - Send a Custom Dimension hit at the click of a button. This tool is intended to annotate traffic in a certain way, letting you filter it out or process it in any way you choose\n     GTM Sonar is especially useful for debugging potential problems with interfering JavaScript.\nRelated posts: \u0026ndash; Internalize for Google Analytics v1.0 \u0026ndash; Google Tag Manager Sonar v1.2\nFeedback Feel free to send me any feedback relating to the tools. I don\u0026rsquo;t do extensive testing, nor are the tools especially optimized performance-wise. These are issues I would love to tackle had I more time, but I\u0026rsquo;m really grateful for any ideas, feedback, or criticism you might want to throw my way.\n"
},
{
	"uri": "https://www.simoahava.com/tools/gtm-tools/",
	"title": "GTM Tools",
	"tags": [],
	"description": "",
	"content": "   GTM Tools are utilities created for managing Google Tag Manager containers. The main features are:\n  Clone containers from one account to another (or within the same account)\n  Visualize containers\n  Inspect containers, and add individual assets (Tags, Triggers, Variables) to a \u0026ldquo;Shopping cart\u0026rdquo;\n  Clone Shopping cart contents as a new container in a GTM account\n  Save Shopping cart contents into an asset library for future use\n  The toolset can be found at:\n\u0026raquo; www.gtmtools.com \u0026laquo; The release notes and user guide can be found here: GTM Tools: Release notes and user guide.\nFeedback Feel free to send me any feedback relating to the tools. I don\u0026rsquo;t do extensive testing, nor are the tools especially optimized performance-wise. These are issues I would love to tackle had I more time, but I\u0026rsquo;m really grateful for any ideas, feedback, or criticism you might want to throw my way.\nDEPRECATED: GTM Tools @SimoAhava Deprecated, use the new version at www.gtmtools.com\nThe GTM Tools @SimoAhava is my pet project which I started working on the minute I was allowed access to Google Tag Manager\u0026rsquo;s API. It includes a number of cloner utilities, which allow you to transfer assets from one container to another in your Google Tag Manager ecosystem. Also, there\u0026rsquo;s the Container Visualizer, of which I\u0026rsquo;m definitely most proud.\n   Included are:\n  Container Cloner - Lets you copy tag(s) from one account to another\n  Tag Cloner - Lets you copy tag(s) from one container to another\n  Rule Cloner - Lets you copy rule(s) from one container to another\n  Macro Cloner - Lets you copy macro(s) from one container to another\n  Container Visualizer - Draws a pretty visualization of a container, showing the links between different assets (tags, rules, macros)\n  I have a pretty long list of planned features, but most importantly I\u0026rsquo;m working on providing support for the new UI features (triggers, variables), and I\u0026rsquo;m also looking into bundling assets together in the hopes of letting users download, update, and upload these bundles en masse to containers.\nRelated posts: \u0026ndash; Introducing GTM Tools \u0026ndash; Container Visualizer for Google Tag Manager\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-selection-drop-list/",
	"title": "#GTMTips: Track Selection In Drop-Down List",
	"tags": ["event listener", "form", "Google Tag Manager", "selection"],
	"description": "How to track when users select something in a drop-down list using Google Tag Manager.",
	"content": "Tracking what a user selects in a drop-down (or select) list/menu can be very useful. This is particularly the case when the selection immediately does something, such as initiate a download or navigate the user to another page. But even if there is no immediate action, it\u0026rsquo;s still interesting to know what selections users might be doing, if only to uncover yet another piece of the engagement puzzle. Here\u0026rsquo;s the Google Tag Manager way to do it!\nTip 62: Listen for changes in a drop-down menu   The setup is somewhat complicated, and requires a Custom HTML Tag together with some variables. But the end result is that when a user makes a selection, the web page pushes an object into dataLayer, which you can then use to track the results.\nHere\u0026rsquo;s what the Custom HTML Tag should look like:\n\u0026lt;script\u0026gt; (function() { // Change the CSS selector in the parenthesis to match your select menu  var selectMenu = document.querySelector(\u0026#39;select#selectMenu\u0026#39;); var callback = function(e) { var selectedOption = e.target.options[e.target.selectedIndex]; window.dataLayer.push({ event: \u0026#39;selectionMade\u0026#39;, selectedElement: selectedOption }); }; selectMenu.addEventListener(\u0026#39;change\u0026#39;, callback, true); })(); \u0026lt;/script\u0026gt; Set this tag to fire on a DOM Ready trigger.\nYou\u0026rsquo;ll need to modify the line which begins with var selectMenu = ..., so that the query selector matches the select HTML element you actually want to track. If CSS selectors are unfamiliar to you, you can read up on them here, or you can use some other DOM method like document.getElementById(). Regardless, you will need to store a reference to the select element in the selectMenu variable for this solution to work.\nThe code creates a custom 'change' listener, which is triggered when the value in the element being monitored changes. This is very useful with forms, since you can attach this type of listener to any form field to see if the user changed the value within.\nFinally, when a 'change' event is detected, the callback pushes an object into dataLayer, which contains the custom event selectionMade as well as an object reference to the option the user selected.\nWhen you want to fire your tag upon a selection, you will need a Custom Event Trigger that looks like this:\n  Then, depending on how the actual option element has been configured, you could use one of these Data Layer Variables to collect the information:\n    The first grabs the value attribute from the selected option, or if there is no value it grabs the text content. The second grabs the text content even if the option had a value attribute.\nPut these all together, and you\u0026rsquo;ll be tracking those drop-down / select list selections in no time!\n"
},
{
	"uri": "https://www.simoahava.com/tags/form/",
	"title": "form",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/selection/",
	"title": "selection",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/create-string-multiple-object-properties/",
	"title": "#GTMTips: Create String From Multiple Object Properties",
	"tags": ["array", "facebook", "Google Tag Manager", "gtmtips", "JavaScript"],
	"description": "Use array methods to combine multiple properties in different objects into a single string or some other set. This tip is for Google Tag Manager.",
	"content": "Facebook\u0026rsquo;s pixel has an attribute named content_ids (required for Dynamic Ads), which requires an Array of content IDs as its value. It\u0026rsquo;s very possible you\u0026rsquo;re running this pixel on a site which already has Enhanced Ecommerce for Universal Analytics implemented, and now you want to use the same Enhanced Ecommerce data that your developers have already made available in this Facebook pixel.\nOr perhaps you want to concatenate a list of strings, such as article tags (['culture', 'politics']), and send it as a comma-separated string to Google Analytics ('culture,politics').\nThis is easy to do - just follow this tip!\nTip 61: Build an Array or concatenated string from multiple object properties   With JavaScript\u0026rsquo;s Array methods, this is easy to do. Here\u0026rsquo;s a walkthrough using an actual example.\nLet\u0026rsquo;s say you have an order receipt page on your ecommerce store, where you have implemented the following Enhanced Ecommerce purchase setup:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;ecommerce\u0026#39;, ecommerce: { purchase: { actionField: { id: \u0026#39;order1\u0026#39;, revenue: \u0026#39;10.00\u0026#39; }, products: [{ id: \u0026#39;product1\u0026#39;, price: \u0026#39;6.00\u0026#39;, quantity: 1 },{ id: \u0026#39;product2\u0026#39;, price: \u0026#39;4.00\u0026#39;, quantity: 1 }] } } });  If you know your Enhanced Ecommerce, this code represents a well-formed purchase object, and if you have an Enhanced Ecommerce enabled tag firing on an ecommerce Custom Event Trigger, you should be able to collect this purchase data in your Universal Analytics property.\nBut now we have a pixel (such as Facebook), which requires an Array of only the product IDs (i.e. ['product1', 'product2']. Or, you might have a tracker, which requires a comma-separated string of the same (i.e. 'product1,product2'].\nFirst, you\u0026rsquo;ll need to create a Data Layer Variable which points to the Array that holds the objects whose properties you want to access. In this case, it would be ecommerce.purchase.products:\n  I\u0026rsquo;ve chosen to name this {{DLV - ecommerce.purchase.products}}.\nNext, you\u0026rsquo;ll need a Custom JavaScript Variable where the magic happens. This variable returns an Array of all the product IDs (i.e. ['product1', 'product2']).\nfunction() { var products = {{DLV - ecommerce.purchase.products}}; return products.reduce(function(arr, prod) { return arr.concat(prod.id); }, []); }  The reduce() method basically takes an Array (the products Array), and for every member in the Array you can choose to increment an accumulator (an empty Array in this case) with the data of your choice. In this case, we\u0026rsquo;re taking the id value from every item in the Array, and building a new Array with this information.\nIf you want to turn this into a comma-separated string (i.e. 'product1,product2'), you only need to chain another Array method to the return statement:\nfunction() { var products = {{DLV - ecommerce.purchase.products}}; return products.reduce(function(arr, prod) { return arr.concat(prod.id); }, []).join(); }  The join() method flattens any Array into a string, using whatever you pass into the parentheses as the separator. If you don\u0026rsquo;t explicitly define a delimiter, a comma is automatically used.\nThese two functions should let you easily turn any complex Array of objects into a single, straightforward data structure - whatever you need in any given situation.\nUPDATE: Thanks to postman31\u0026rsquo;s comment below, you can also use the Array.prototype.map() method to perform these tasks even more elegantly.\nfunction() { var products = {{DLV - ecommerce.purchase.products}}; return products.map(function(prod) { return prod.id; }); // OR:  // return products.map(function(prod) { return prod.id; }).join(); }  "
},
{
	"uri": "https://www.simoahava.com/tags/array/",
	"title": "array",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/send-google-analytics-tag-multiple-properties/",
	"title": "#GTMTips: Send Google Analytics Tag To Multiple Properties",
	"tags": ["duplicate", "gtmtips", "tasks", "universal analytics", "customtask"],
	"description": "Use customTask to automatically send each Google Analytics tag to multiple properties. Everything is done with Google Tag Manager.",
	"content": "Here we are again, revisiting an old theme. When using Google Tag Manager, we often want to send the contents of the same tag to multiple Universal Analytics properties. With on-page GA, this used to be quite simple, as all you had to do was create a new tracker and then just remember to run the ga('trackerName.send'...) commands to all the trackers (or you could use my duplicator plugin). With GTM, your options are more limited, since Google Tag Manager abstracts the tracker object, giving you far fewer tools to work with.\nEven though there are workarounds, only the very recent release of the customTask gave us a way to do this economically with minimum risk to our existing tracking.\nTip 60: Use customTask to duplicate your GA hits   With this solution, you\u0026rsquo;re overriding customTask in the GA tags that you want to distribute to multiple properties. The new customTask modifies the Universal Analytics task queue, so that the original hit is first sent, then the payload is duplicated with a new tracking ID, and then the modified payload is sent to GA, too.\nThe way it works is simple. In all your Universal Analytics tags that you want to duplicate, scroll down to Fields to set, and add a new field:\nField name: customTask\nValue: {{JS - customTask hit duplicator}}\nThe {{JS - customTask hit duplicator}} is a new, user-defined Custom JavaScript Variable that you need to create. The variable content should look like this:\nfunction() { // Replace newTrackingId value with the UA property to which you want to duplicate this tag  var newTrackingId = \u0026#39;UA-XXXXX-Y\u0026#39;; var globalSendTaskName = \u0026#39;_\u0026#39; + newTrackingId + \u0026#39;_originalSendTask\u0026#39;; return function(customModel) { window[globalSendTaskName] = window[globalSendTaskName] || customModel.get(\u0026#39;sendHitTask\u0026#39;); customModel.set(\u0026#39;sendHitTask\u0026#39;, function(sendModel) { var hitPayload = sendModel.get(\u0026#39;hitPayload\u0026#39;); var trackingId = new RegExp(sendModel.get(\u0026#39;trackingId\u0026#39;), \u0026#39;gi\u0026#39;); window[globalSendTaskName](sendModel); sendModel.set(\u0026#39;hitPayload\u0026#39;, hitPayload.replace(trackingId, newTrackingId), true); window[globalSendTaskName](sendModel); }); }; }  Remember to change the value of the newTrackingId variable to contain the Property ID you want to send your duplicated hit data to!\nNow, when your Universal Analytics tag with this customTask field is run, the solution runs through the following steps:\n  First, the tag with its original settings is executed (line 10).\n  Next, the original Tracking ID in the hit payload is replaced with the newTrackingId (line 11).\n  Finally, the modified hit payload is sent to Universal Analytics (line 12).\n  If you want, you can run this sendModel.set('hitPayload'...) --\u0026gt; window[globalSendTaskName](sendModel) multiple times to send the data to even more Universal Analytics properties.\nWhen you add the customTask field with this variable to your Universal Analytics tags, these tags will automatically duplicate the entire hit payload to the new tracking ID you specified.\nBecause you are overwriting the sendHitTask task, the hit payload has already been generated, which is why you need to modify it with regular expressions rather than being able to simply change values in the model object itself. It\u0026rsquo;s not the most elegant option when you want to make other changes to the hit payload (e.g. change the index of a custom dimension), but it does its job.\n"
},
{
	"uri": "https://www.simoahava.com/tags/duplicate/",
	"title": "duplicate",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/use-customtask-access-tracker-values-google-tag-manager/",
	"title": "#GTMTips: Use customTask To Access Tracker Values In Google Tag Manager",
	"tags": ["client id", "Google Tag Manager", "gtmtips", "tasks", "universal analytics", "customtask"],
	"description": "Set the Client ID, for example, in all your Google Analytics hits as a Custom Dimension. You can do this with Google Tag Manager and customTask.",
	"content": "One of the things I\u0026rsquo;ve recommended from the get-go is to always send the Client ID to Google Analytics with your users\u0026rsquo; hits. This is very useful for adding a level of granularity to your tracking. At first, I recommended using an Event tag to do this. Then I modified my approach a little so that you could send it with your initial Page View (thus not inflating your hit counts).\nHowever, Universal Analytics recently released a new task API, customTask, which lets you access the model object mid-tag, thus letting you modify the payload that is dispatched to Google Analytics. In this article, I\u0026rsquo;ll show you how this works by using the classic example of sending the Client ID to Google Analytics.\nTip 59: Access the model object mid-tag in Google Tag Manager The setup is really simple. You need a Custom Dimension setup in Universal Analytics, and then you simply need to add a new Field to set in your Page View tag (or whatever you want to use to send the data to GA). Remember to read my article on sending this type of metadata to Google Analytics, if you\u0026rsquo;re unsure why you would want to do this in the first place.\nThe tag setup would look like this:\n  Field name should be set to customTask, and as its value you need to use a Custom JavaScript Variable. The variable looks like this:\nfunction() { // Modify customDimensionIndex to match the index number you want to send the data to  var customDimensionIndex = 5; return function(model) { model.set(\u0026#39;dimension\u0026#39; + customDimensionIndex, model.get(\u0026#39;clientId\u0026#39;)); } }  What happens is that once Google Tag Manager starts executing the tag code, it first encounters the customTask field. It resolves the variable to a closure, which is basically a function that automatically receives a model object as a parameter. The model object can be manipulated using the get and set methods.\nNext, we set the Custom Dimension at index 5 (as determined by the value of customDimensionIndex) to the Client ID, which we retrieve with the get method of the model object.\nThis little trick means that we can tell the GA tag to fetch the Client ID from the tracker object and send it in a Custom Dimension without any extra hacks or workarounds that we had to employ previously. The fact that customTask has no other function in Universal Analytics means that we don\u0026rsquo;t have to mind the fact that we\u0026rsquo;re overwriting a task method with this tag.\nYou can use this for any fields in the model/tracker object if you wish.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-settings-variable-in-gtm/",
	"title": "Google Analytics Settings Variable In GTM",
	"tags": ["google analytics settings", "Google Tag Manager", "universal analytics", "variable"],
	"description": "Introduction and guide to the Google Analytics Settings variable in Google Tag Manager.",
	"content": "Let\u0026rsquo;s face it - most of us use Google Tag Manager for one main purpose: to deploy and configure Google Analytics tracking on a website. I\u0026rsquo;d wager that once you start using GTM, you won\u0026rsquo;t be implementing Universal Analytics the old-fashioned way, with on-page code, any more. But running Universal Analytics tags through GTM isn\u0026rsquo;t yet a perfect workflow. We\u0026rsquo;re still missing things like proper plugin support and the option to properly differentiate between the tracker and the hit - both of which are easy to do with an on-page implementation.\nFor me, and I\u0026rsquo;m sure for many others, one of the biggest problems with Universal Analytics has been how to manage scale. Once you have dozens of tags that have some settings configured in a specific way, e.g. cross-domain tracking settings or custom page paths, it becomes a nightmare to update each and every one when you want to make a change to one of these generic settings. Well, it\u0026rsquo;s been a long time coming, but Google Tag Manager just introduced a feature to help us with this: the Google Analytics Settings variable.\n  The variable contains the fields you\u0026rsquo;ll find under \u0026ldquo;More Settings\u0026rdquo; of Universal Analytics tags, combined with some set fields that are (typically) absolutely necessary to include in all tags anyway (Tracking ID and cookieDomain, specifically). The advent of this new variable type gives you three ways to create and manage Universal Analytics tags:\n  Copy all settings from the Google Analytics Settings variable.\n  Don\u0026rsquo;t use a Google Analytics Settings variable at all, and instead set all fields manually.\n  Use a combination of both: set the fields with a Google Analytics Settings variable, and then overwrite some of the fields with custom values on a tag-by-tag basis.\n  This gives you a lot of flexibility to work with.\nThe Google Analytics Settings variable is available in your web containers as well as your mobile Firebase containers.\nThe Google Analytics Settings variable You can find the new variable in the variable creation menu that pops open when you start building a new User-Defined Variable.\n  The first thing you\u0026rsquo;ll notice is that the Cookie Domain field is auto-populated with the value auto. This is an excellent change, and removes one of the key differences between the recommended analytics.js tracking code (which has auto as the default value for cookieDomain) and the default Universal Analytics tag template in Google Tag Manager (which does not have a cookieDomain value set at all).\nNote that the Tracking ID field is required, so add your Universal Analytics property tracking ID or a variable you\u0026rsquo;ve used before to it before saving the new variable.\nIf you click More Settings you\u0026rsquo;ll find all the fields available for configuration in Universal Analytics tag templates. You can set these as you wish, remembering that they will be applied to all tags that use this variable.\n  Note that obvious conflicts are resolved automatically. If you Enable Enhanced Ecommerce Features in your Google Analytics Settings variable, and then add this variable to a Timing type Universal Analytics tag, the Timing tag won\u0026rsquo;t magically turn into an Enhanced Ecommerce enabled tag. Since Enhanced Ecommerce hits can only be sent with Page View and Event tags, this particular setting is simply ignored in incompatible tag types.\nAdding the settings to your tags To add your settings to tags, you\u0026rsquo;ll find a new drop-down under the heading Google Analytics Settings that instructs you to select a variable from the list. You can also start a new variable creation workflow from this menu.\n  As you can see, you can still configure the Advanced Settings individually for this tag. That\u0026rsquo;s because Advanced Settings have nothing to do with Google Analytics - all tags in Google Tag Manager have these same Advanced Settings fields.\nYou\u0026rsquo;ll still need to add individual triggers to this tag. Again, triggers are not specific to Universal Analytics tags. The Google Analytics Settings variable only conflates all Google Analytics settings into a single variable configuration.\nAs I mentioned in the introduction of this article, there are three ways to implement a Google Analytics Settings variable in your Universal Analytics tags. The first one is in the screenshot above. When you apply the variable and save the tag, all Google Analytics settings are derived from the variable. This is definitely the most light-weight and straightforward way to implement the variable.\nThe second way is to ignore the variable completely, and just create an individual, one-off tag. In this case, you need to do two things:\n  Leave Select Settings Variable\u0026hellip; as the value of the drop-down.\n  Check the box next to Enable overriding settings in this tag.\n  Once you do these two things, you\u0026rsquo;ll completely ignore any Google Analytics Settings variables and just configure the tag independently. A very useful thing to still have, especially if you want to do some quick prototyping.\nThe third way to implement the settings variable is a combination of the two methods above. This is useful if you want the benefit of the settings, but you want to make some adjustments for this tag in particular. For example, I might want to use my \u0026ldquo;GA Settings - Enhanced Ecommerce\u0026rdquo; settings in my Add To Cart event tag, so that it makes use of all the settings I\u0026rsquo;ve configured for my Enhanced Ecommerce tags. But I want to make a small adjustment: I want to use a Custom JavaScript Variable for the Enhanced Ecommerce payload instead of dataLayer. This is what this modification would look like:\n  If you use a Google Analytics Settings variable, all fields that you set in that variable are inherited in the tag that uses the variable. You can always override the fields, but it might be difficult to remember just which fields have been set.\nFor this conundrum, there\u0026rsquo;s a brilliant UI workflow update. See the little (I) icons in the screenshot above, marked with a yellow star? When you click that icon, an overlay opens with the variable in question for you to review or even edit! You can now modify or edit variables mid-workflow, without having to leave the tag settings to make adjustments.\nThis is, naturally, vital for the Google Analytics Settings variable as well, because you want to be careful you\u0026rsquo;re not overriding fields that should not be overridden. It would be helpful to see the actual modified values and settings inherited from the variable in the tag itself, but I guess there are technical limitations why this is not (yet) possible.\nIdeas for use Here are three types of Google Analytics Settings variables I use in my projects.\n1. General settings This is the one I use as a generic settings variable in tags that have no special function (e.g. regular Page View and Event tags).\n  These are fields I use in all my tags.\n2. Cross-domain tracking If I have a rollup property to which I collect cross-domain tracking data, I have a Google Analytics Settings variable for that, too.\n  And in all my rollup tags, I would have the settings from above. If I had rollup tags that have BOTH Enhanced Ecommerce and cross-domain requirements, I would set them up with cross-domain tracking settings and then manually add the Enhanced Ecommerce settings.\n3. Enhanced Ecommerce settings And finally, I have a generic settings template for all Enhanced Ecommerce tags.\n  With this tag, I can setup simple Enhanced Ecommerce settings for all my EEC tags. In some special cases, I can override these default settings with custom stuff.\nSummary This is a pretty smooth feature for managing your Universal Analytics tags. I firmly believe this is a great timesaver, as it will help you avoid typical data quality mishaps with misconfigured tags. With the Google Analytics Settings variable, it\u0026rsquo;s also easy to help new users become accustomed with your data collection configuration. New users, typically, are the weakest link for data quality, as they might not know all the settings the Universal Analytics tags in your organization should be configured with.\nIf there\u0026rsquo;s one thing I\u0026rsquo;m missing it\u0026rsquo;s variable chaining. I would like to be able to use a Google Analytics Settings variable within a Google Analytics Settings variable. In the three examples of the previous chapter, you can see that all these settings variables share the same Custom Dimensions. I would like to specify that all these three settings variable use another settings variable as the base, and then simply add/modify all the necessary settings. This way I could create a hierarchy of settings variables, making it even more unlikely that misconfigured fields lead to data quality issues.\nNaturally, chaining variables can lead to issues, too, so proper governance and common sense would still rule.\nDo you have some \u0026ldquo;go-to\u0026rdquo; settings configurations in your tags, for which you could leverage the new settings variable?\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/enable-fire-triggers-google-tag-manager/",
	"title": "#GTMTips: Enable And Fire Triggers In Google Tag Manager",
	"tags": ["forms", "gtmtips", "links", "triggers"],
	"description": "Difference between enabling and firing triggers in Google Tag Manager&#39;s Just Links and Form triggers.",
	"content": "This is, by no means, a novel topic in this blog. I\u0026rsquo;ve covered Google Tag Manager\u0026rsquo;s event tracking and triggers numerous times before (see below).\n  Auto-Event Tracking In GTM 2.0\n  #GTMtips: Track Outbound Links In GTM V2\n  #GTMtips: Fix Problems With GTM Listeners\n  Trigger Guide For Google Tag Manager\n  However, based on the number of queries we still see in the Google Tag Manager Product Forums about event tracking, I believe one particular aspect of GTM\u0026rsquo;s triggers invites revisiting. I\u0026rsquo;m talking about the way that Just Links, Form, and Timer triggers can be both Enabled and Fired. This can easily lead to some confusion.\nTip 58: Difference between enabling a trigger and firing a trigger   With Just Links and Form triggers, if you check either Wait for Tags or Check Validation in the trigger settings, you will see the following condition appear:\n  With the Timer trigger, you will always need to specify an enabling condition.\nThe Enable this trigger when\u0026hellip; condition is for determining on which pages the trigger should work in the first place. It\u0026rsquo;s sole purpose is to delimit the trigger itself to listening to events only on pages that you allow it to.\nIn contrast, when you choose Some Link Clicks, for example, and the Fire this trigger when.. condition appears, THAT\u0026rsquo;S where you specify conditions for the tag to fire.\nA common mistake is to use something like {{Click Classes}} or {{Click ID}} in the Enable this trigger when\u0026hellip; condition. This is a mistake because Click variables are only produced after a Click trigger fires. But the Click trigger won\u0026rsquo;t fire if it\u0026rsquo;s not enabled on the page. So by expecting a Click variable to have some value in the enabling condition of trigger would be counter-intuitive. It might work, if you have some other trigger that\u0026rsquo;s already created the Click variable, but it won\u0026rsquo;t work as you expect it to.\nWhen implementing a Just Links, Form, or Timer trigger with the Enable this trigger when\u0026hellip; specified, you\u0026rsquo;ll want to test first with as broad a condition as you can. This would be something like Page URL contains /, which would enable the trigger on every single page that has the Google Tag Manager container snippet. Then, if you run into issues when testing the links and forms on the site, you can either add some page conditions to the triggers to ensure the trigger is only enabled on pages where it works, or you can uncheck Wait for Tags and/or Check Validation to improve compatibility.\nWhy have \u0026ldquo;Enable\u0026hellip;\u0026rdquo; optional in the first place? So why this complexity? Why can\u0026rsquo;t listeners just be active on all pages? Well, typically they COULD be active, since there\u0026rsquo;s very little overhead from a listener that doesn\u0026rsquo;t really do anything.\nHowever, the Wait for Tags setting in particular can be hazardous in some contexts (e.g. a React-driven single-page site), as it does some pretty invasive things to ensure your tags are being sent. In these cases, you can set the trigger settings so that pages where you know it causes problems are excluded from the Enable this trigger when\u0026hellip; setting.\nShould this be easier? Yes, it should. Judging by the number of times the trigger user interface has changed, and considering the number of problems people still have with these triggers, I can\u0026rsquo;t help but think there must be a more user-friendly way of approaching this dichotomy. In a sense, the first version of auto-event tracking (in GTM v1) was better, since back then event tracking was enabled by using special \u0026ldquo;Listener tags\u0026rdquo; that required triggers (or rules, as they were called then) themselves. In those cases, it was very easy to understand the difference between enabling and firing a trigger.\nPart of the problem, I think, is that triggers of the same type have both an enabling and a firing condition. So you could have a Just Links trigger that is only enabled on page X and another Just Links trigger that is enabled on all pages. GTM handles this conflict by using a special gtm.triggers key in the Data Layer, which basically implements the enabling condition of each respective trigger.\nIn other words, the enabling condition morphs pretty naturally into another firing condition when you have multiple triggers of the same type set up in your container. This can easily lead to confusion, since the Enable this trigger when.. setting should only be used to avoid conflicts with other JavaScript on the website and not to control when triggers fire tags (since that\u0026rsquo;s what the firing condition is for).\nWhat do you think? How could the trigger UI and user experience be improved?\n"
},
{
	"uri": "https://www.simoahava.com/tags/forms/",
	"title": "forms",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/approval/",
	"title": "approval",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/new-approval-workflow-gtm-360/",
	"title": "New Approval Workflow In GTM 360",
	"tags": ["approval", "Google Tag Manager", "user permissions", "workflow"],
	"description": "Introduction and guide to the new approval workflow in Google Tag Manager 360.",
	"content": "Apart from the unlimited number of workspaces, the 360 Suite version of Google Tag Manager didn\u0026rsquo;t have any differentiation from the free version feature-wise, until today.\n  GTM just introduced an approval workflow, which allows you to exercise some additional control over what changes are pushed to the live site, or created into versions.\nNote that this update also introduced a small change in the regular GTM UI - mainly, the menu that used to have \u0026ldquo;Create Version\u0026rdquo;, \u0026ldquo;Publish\u0026rdquo; and \u0026ldquo;Preview\u0026rdquo; is now changed into a dual button layout with just Preview and Submit as the options.\nWhen you click Submit, a new overlay opens with tabs for creating a version, publishing the workspace, and, if you are using GTM 360, for requesting approval of your changes.\n  In this article, I want to take a quick look at the new approval workflow and how it works.\nChanges to user permissions First of all, user permissions are more useful now. The permission levels per container haven\u0026rsquo;t changed in name, but their functionality has changed somewhat if you are using GTM 360.\n  View still lets you only view the container and preview older versions. Nothing\u0026rsquo;s changed here.\n  Edit lets you make changes to a workspace, but instead of creating a version and publishing a version, you can only submit a request for approval.\n  Approve lets you create versions and approve requests, but you still can\u0026rsquo;t publish a version.\n  Publish gives you full access in the container.\n    If you are using GTM 360, the new way to sort these permissions would be to allow Edit access for users who should be able to contribute to the container, but who must always submit their changes into the approval queue before they can be created into a version (or published).\nYou can give Approve access to users who can process this queue, comment on and approve the requests, but who still don\u0026rsquo;t have the power to actually update the live container on the site by publishing a version.\nThis new duality is interesting, as it can definitely be used for good and bad. Restricting access can create friction in an organization, which is why, I guess, the approval workflow is an optional feature that you can choose to ignore if you want. Personally I would have liked to see a flag in the settings that sets the approval process as mandatory or optional. By mandatory I mean that even users with full access would still need one other member to approve their changes.\nHow the process works First, take a look at my patented, crappy-as-hell, Powerpoint illustration below:\n  The workflow starts after you\u0026rsquo;ve made changes to the workspace, and you are ready to submit the workspace draft for approval. You can start the approval process by clicking the Submit button and opening the Request Approval tab.\n  In this tab, you can choose a user from whom you are requesting approval by clicking the Choose Approver button. Note that this is optional. If you do not choose an approver, the request can be processed by any user with at least Approve user permissions.\nNext, you can add a Comment to the request. This is highly recommended, as typing a comment actually starts a dialogue between you and any approvers. It\u0026rsquo;s a good way to keep tabs on why a request was rejected, for example, and the comment thread can also include notes on how to improve the quality of the changes.\nAs before when creating or publishing a version, you can see all changes to the workspace in the bottom of the approval request.\nSo, what happens when you click the Request button?\nWorkspace submitted for approval First of all, the workspaces selector will show a gavel symbol next to the workspace name, indicating that there is a pending approval request for this workspace.\n  Next, all users will see a blue or red notification symbol next to the Approvals menu item, which tells them that a new approval request has been submitted. The symbol is red if you are the named approver who received the request, and blue otherwise.\n  There will also be a new notification just below the main menu, indicating that there is a pending approval, and the name of the user who requested the approval.\nAll these UI signals are used to make sure the workflow doesn\u0026rsquo;t end up being an obstruction in your regular GTM use.\nNote that an approval workflow doesn\u0026rsquo;t prevent changes to the workspace. The approval workflow isn\u0026rsquo;t supposed to be a gatekeeper - rather, it\u0026rsquo;s a way for you to communicate changes you have committed to the workspace to other users of GTM.\nThus, you can continue making changes to the workspace, and the approval request will simply be updated with the changes you make.\nIf you are the user who submitted the request, you can always choose to Withdraw the request, i.e. remove it from the approval workflow. This won\u0026rsquo;t delete your workspace changes, just the approval request.\n  If you are someone with at least Approve rights, this is what you\u0026rsquo;ll see when you open the approval for processing:\n  Send Back request When the approver clicks Send Back, hopefully after having added some comments to the request, the approval request is returned to the user who made the request, and its state will be set to Needs work.\n  At this point, you should open the approval request and see what comments were left by the person who sent it back (hopefully they left a comment!).\nOnce you\u0026rsquo;re done with your changes, you can Resubmit the request (after adding a comment yourself), or you can simply delete the request via the dot menu.\n  By clicking Resubmit, you simply submit the request again to the user you\u0026rsquo;re requesting approval from.\nApprove request If you have at least Approve user permissions, you can choose to approve the request (even if you\u0026rsquo;re not the user approval was requested from).\nWhen you click Approve, you\u0026rsquo;ll see the following dialog:\n  At this point you can choose to Submit the changes, meaning the overlay with Create a version and Publish a version flies out, and you can commit the changes to the latest and/or the live container version.\nIf you choose to Leave Unsubmitted, the request will be approved, but the workspace will not be created into a version.\n  At this point, the workspace can still be worked on, but the changes have been approved, meaning the next time a user clicks Submit they can only create a version or publish the workspace. This is an odd state of limbo, and I would personally recommend to have version creation or publish as the final step of all approval workflows.\nThis is the extent of the approval workflow, for now. As said, the UI doesn\u0026rsquo;t enforce the workflow - you can continue using GTM as you have before. However, if you want added transparency, and if you want to leverage the user permission levels more efficiently, I recommend at least taking a look at the workflow.\nSummary In many ways, this change is in line with the direction Google Tag Manager has been moving towards ever since Environments was released a long while ago. It\u0026rsquo;s clear that GTM containers have many uses and users, and often there\u0026rsquo;s a lot of friction in how these moving parts mesh together. With things like Environments, Workspaces, and now the Approval workflow, you have more control over what takes place, by whom, and in what order.\nIt\u0026rsquo;s still just a feature. You still need to have an understanding of how it works as well as a need to use the Approval feature. I think it\u0026rsquo;s very useful in multi-user projects, especially when the container is operated on by users who might not have a clear idea of how the container works in the current organization.\nNaturally, I would have enjoyed this more had it been a feature in the free version of Google Tag Manager, and hopefully at some point it will trickle to free GTM as well.\n"
},
{
	"uri": "https://www.simoahava.com/tags/user-permissions/",
	"title": "user permissions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/workflow/",
	"title": "workflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/site-search/",
	"title": "site search",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-autocomplete-search-google-tag-manager/",
	"title": "Track Autocomplete Search In Google Tag Manager",
	"tags": ["google analytics", "Google Tag Manager", "search", "site search"],
	"description": "How to track auto-complete search to Google Analytics using Google Tag Manager.",
	"content": "Autocomplete search is a tricky thing to track. The underlying logic is that when the user starts feeding characters into a search form, the search suggests results based on a limited input. If the user is not satisfied with the results, they can continue adding characters to the search, thus increasing the accuracy. Often there\u0026rsquo;s also the option to revert to a regular search with what they\u0026rsquo;ve already written. Tracking this logic in tools like Google Analytics is difficult, because there\u0026rsquo;s really no way to know if the search was successful.\n  Take the example in the screenshot above. The user has written ban into the search field, and they\u0026rsquo;ve received a list of search items to proceed with. If they now chose any item in that list, would the search be considered successful?\nYes. And (maybe) no. Yes, it would be a successful search because the user clicked an item in the list, thus validating the suggestion. And (maybe) no, because we have no idea if that is the item the user was searching for in the first place. If the purpose of our search is to match a response to a query, then any click on the search results is a success. If the purpose of our search is to match a response to the original intent of the query, then only a certain type of search result click is a success.\n  This is going too far into the ontology of search and intent, so I\u0026rsquo;m going to stop with the philosophy here. However, what I do want to show you is a pretty simple way of tracking what users do search in the autocomplete search field, using Google Tag Manager. It\u0026rsquo;s up to you to decipher if the searches are successful, though.\nHow it works The setup is basically just a Custom HTML tag. In this tag, we\u0026rsquo;ll write a custom listener for the keydown browser event, which is registered when the user presses (down) a key. You must provide a minimum length of search for this handler, as well as the timeout you want the browser to wait before sending a search term to Google Analytics (or whatever platform you want to use this with).\nFor example, if you set minLength = 3 and timeout = 2000, the handler will only send searches that are at least three characters in length, and where 2 seconds have elapsed since the last character was written. This is necessary to eliminate accidental keystrokes and search terms with far too little information to be useful.\nThe search event thus occurs when two seconds have passed since the last character has been typed OR when the user presses the Enter key OR when the focus leaves the search field.\nThe code is all contained in a single Custom HTML tag which fires on the DOM Ready trigger. You\u0026rsquo;ll then need a Custom Event trigger to fire when a valid search is recorded, as well as a Data Layer Variable to grab that search term.\n1. The Custom HTML tag Here\u0026rsquo;s the code that runs the whole thing:\n\u0026lt;script\u0026gt; (function() { // Set searchField to the search input field.  // Set timeout to the time you want to wait after the last character in milliseconds.  // Set minLength to the minimum number of characters that constitutes a valid search.  var searchField = document.querySelector(\u0026#39;input#search-field\u0026#39;), timeout = 2000, minLength = 3; var textEntered = false; var timer, searchText; var handleInput = function() { searchText = searchField ? searchField.value : \u0026#39;\u0026#39;; if (searchText.length \u0026lt; minLength) { return; } window.dataLayer.push({ event: \u0026#39;customSearch\u0026#39;, customSearchInput: searchText }); textEntered = false; }; var startTimer = function(e) { textEntered = true; window.clearTimeout(timer); if (e.keyCode === 13) { handleInput(); return; } timer = setTimeout(handleInput, timeout); }; if (searchField !== null) { searchField.addEventListener(\u0026#39;keydown\u0026#39;, startTimer, true); searchField.addEventListener(\u0026#39;blur\u0026#39;, function() { if (textEntered) { window.clearTimeout(timer); handleInput(); } }, true); } })(); \u0026lt;/script\u0026gt; First, you need to define some utility variables.\n  Set searchField to capture the search field HTML element. This is the element to which the handlers will be attached to.\n  Set timeout to the time in milliseconds you want the script to wait after the last character has been typed into the field. This is to prevent the search event from happening too often, especially when people type slowly.\n  Set minLength to the minimum number of characters that constitutes a search. If the search is less than this number in length, the search will not be recorded.\n  The handleInput method checks if the search is long enough. If it is, an object is pushed to dataLayer with the following key-value pairs:\n  event: 'customSearch', which we\u0026rsquo;ll use to build the Custom Event Trigger that fires any tags you want when a search is recorded.\n  customSearchInput: searchText, which grabs the text the user wrote into the search field.\n  The startTimer method is run each time the user types something into the search field. First, it stops the current timer (which is counting the time to the timeout limit), because we want to reset the timer with each key press.\nNext, it checks if the key that was pressed was the Enter key (keyCode === 13). If it was, then we interpret this as a valid search and run the handleInput method. Finally, the timer is started again, and after the timer hits the timeout limit, handleInput is called.\nThe final rows of the script add the keydown and blur listeners to the search field to track keystrokes and if the focus leaves the search field, respectively.\n2. The triggers To fire the Custom HTML tag above, use a DOM Ready trigger. It makes sense to only fire the trigger on pages where you have the search form, so adding a DOM Variable condition to the trigger is not an altogether bad idea.\n  This way your search listener will only fire on pages with the search field (sensible!).\nNext, you\u0026rsquo;ll need a Custom Event trigger to fire your tags when a successful search is registered:\n  As you can see, I\u0026rsquo;m also checking if the search field had some text. This is, again, a sensible precaution to avoid false positives if something goes wrong with the script.\n3. The Data Layer Variable The Data Layer Variable you\u0026rsquo;ll need to capture the search term is simple:\n  This Data Layer Variable pulls the value of the key customSearchInput, pushed by the script upon a successfully recorded search.\nPutting it all together So now you have the components, and it\u0026rsquo;s time to put this all together. Below is an example, where I use a Universal Analytics tag to send the search term to Google Analytics using a custom query parameter.\n  The key is the page field, which is overwritten with:\n/search/?q={{DLV - customSearchInput}}\nThus, when the Universal Analytics fires with the Event - customSearch trigger, the page field is sent with the custom path /search/?q=searchterm. So if I wrote uunilohi into the search field, the page path sent to GA would be /search/?q=uunilohi.\nAfter this, all you need to do is go to View Settings in Google Analytics, and set the parameter q to be the site search parameter Google Analytics uses to build the site search reports.\nSummary Tracking autocomplete search is difficult not just because of the technical restrictions but because it\u0026rsquo;s difficult to determine original intent with just a few characters to work with.\nIt can be argued that ANY search that precedes a click of search results or even a conversion is successful, and I tend to agree with this. However, especially for content creators it\u0026rsquo;s important to align searches with relevant results, not just any old result that the user finds attractive. For this reason, it\u0026rsquo;s important to track partial searches as well as searches with refined inputs.\nHow have you solved the problem of autocomplete search? Do you trust the site search reports when you see only partial searches, and do you find this information useful when optimizing the site to be more responsive to complex searches?\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/user-permissions/",
	"title": "#GTMTips: User Permissions",
	"tags": ["Google Tag Manager", "gtmtips", "permissions"],
	"description": "How user permissions work in the different parts of Google Tag Manager.",
	"content": "In this #GTMTips article, we\u0026rsquo;ll take a look at user permissions and access control levels that Google Tag Manager lets you set today. Doing access control right from a user interface AND user experience perspective is really difficult, and GTM is no exception. Nevertheless, there are several levels of user control that you can modify from account and container settings, and it\u0026rsquo;s useful to familiarize with these so that managing a big, sprawling account hierarchy would be just a bit easier.\nTip 57: User Permissions And Access Control Levels   Since the concept of an account in Google Tag Manager is not as important as, say, in Google Analytics, the access levels for accounts come in two sorts:\n  Admin access lets the user modify other users\u0026rsquo; permission levels, and Admin level has always at least Read access to each container.\n  User access prevents the user from modifying other users\u0026rsquo; permission levels, and they can also have the No Access level for containers.\n  Note that there is no owner in Google Tag Manager. Admin access can be freely distributed, and it can be revoked, even from the user who created the account.\nOn Container level, the available permission levels are:\n  No Access means the user will not even see this container in the list of containers in the UI or via API queries.\n  Read gives the user view-only access to the container. This includes browsing workspace drafts, opening tags, triggers, and variables, but not being able to edit them in any way. One feature of Read access that is often ignored is that users can Preview container versions by going to the Versions page and choosing Preview from the version action menu (see screenshot below). Note that users can\u0026rsquo;t preview workspace drafts.\n  Edit access lets users create workspaces and edit assets within those workspaces, but they cannot Create Versions or Publish those workspaces. Note that Edit access CAN Preview workspace drafts.\n  Approve is almost the same as Edit except the user has now permission to Create Versions out of workspace drafts. They still can\u0026rsquo;t Publish anything, though.\n  Publish is the highest access level for containers. It gives you full access to the container, including ability to create and modify Environments, and even to delete containers.\n    With these access control levels, you can distribute access within a single container pretty granularly. However, there\u0026rsquo;s a bunch of things many users would still love to see with permission distribution, most notably involving folders. Also, a nice, juicy approval queue would be great to have, so that users with limited permissions could still submit a workspace draft for approval programmatically, rather than having to sort out the publish workflow in person (I know, social interaction, YUCH!).\nBONUS: I\u0026rsquo;ve lost account access / I don\u0026rsquo;t know who has access to GTM-XXXXX - what do I do? This must be one of the most frequently asked questions in the Product Forums. Typically it\u0026rsquo;s a case of a GTM container being deployed on the site, but no one has access to it, nor does anyone know WHO is the current admin. (Folks, this is what lack of governance does. Learn from it!).\nAnyway, I\u0026rsquo;m going to quote Googler Andrew Lanzone, who had the perfect answer for what to do in case you need to retrieve access information for any given container or account.\n The easiest option is to track down the person with admin rights on the account. If you can, contact a user who has admin rights on the GTM account and ask them to add you to the account (as admin as well). If you have access to the email accounts of the departed employees, you can try logging in with those accounts and adding yourself as an admin.\nIf that is not possible, file a feedback request (choose \u0026ldquo;Send feedback\u0026rdquo; from the \u0026hellip; menu in the GTM header) and we can try and contact the account admins on your behalf.\nIn general, we recommend:\n  You should have multiple admins on your account.\n  At least one email address on the account should be monitored on a daily basis if possible.\n  You should have a handoff plan for when people leave the company and/or end business relationships.\n   Here\u0026rsquo;s the source for Andrew\u0026rsquo;s tips.\n"
},
{
	"uri": "https://www.simoahava.com/tags/permissions/",
	"title": "permissions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/gtm-tools-release-notes-and-user-guide/",
	"title": "GTM Tools: Release Notes And User Guide",
	"tags": ["api", "Google Tag Manager", "gtm tools"],
	"description": "Release notes and user guide to GTM Tools for Google Tag Manager. Use it to manage, clone, and create containers.",
	"content": "With the release of the latest Google Tag Manager API version, it\u0026rsquo;s time to release the new version of GTM Tools. Most of the changes have been done under the hood, with the entire codebase refactored for improved stability.\nI released the first toolset in October 2014, and quickly released an updated UI a few months later. Aside from a few bug fixes and stability improvements, the tools have remained largely unchanged since then. Now with the new API version, it was time to update the tools again.\nThus this current version is actually the third iteration of the tools. The changes might not seem that dramatic, but trust me when I say this is the best version of the tools so far. The added features should make it easier to use the tools, the stability upgrades should prevent the tools from bugging out as often as before, and there are plenty of improvements that make it easier for me to manage and release new versions of the tools.\n  This article will be the closest thing to a \u0026ldquo;user guide\u0026rdquo; the tools can have, so I\u0026rsquo;ll jump straight to the subject matter right after this disclaimer:\nGTM Tools is not a commercial product.\nThis toolset is my own, personal, intellectual property, with no purpose of making money out of it or making it the best possible product out there. It\u0026rsquo;s got bugs, the code is still pretty convoluted in places, and I haven\u0026rsquo;t done nearly enough testing. It\u0026rsquo;s a toolset that you might find useful or then you might not.\nWhen you use GTM Tools you need to accept this.\nNote also that when you use the Library feature, the assets you save into the library will be viewable by me, as I need to access this information for debugging and management purposes. I will, under no circumstances, use or copy your data. I will only read the data in situations where I deem it necessary to keep GTM Tools functional.\nI would also appreciate if you sent me email (simo(at)simoahava.com) when encountering any bugs, errors, or freezes. Just remember to check the Known Issues part of this guide first.\nThe tool is located in this URL:\nhttps://www.gtmtools.com/\nLatest releases 21 Feb 2020  Removed the Slack integration. It was a fun experiment, but it turned out to be a resource hog. GTM doesn\u0026rsquo;t have a notification system, so the solution was to poll GTM periodically to see if there\u0026rsquo;s a new container version. This is expensive and full of memory leaks, so I decided to abandon this proof-of-concept.  27 Aug 2019   Added an option in Workspace mode where you can show a list of all links to the selected item.\n  If no links are found, a Delete button appears in the item row.\n  If the button is clicked, the item is marked for deletion.\n  Once the workspace is updated, the item is deleted.\n  The purpose of this feature is to let users easily delete items that are not used in the container in any way.\n26 Jun 2019   Added support for Custom Templates.\n  Custom Templates are included when cloning an entire container.\n  If cloning a tag from cart that is based on a Custom Template, the clone will fail unless the template is included in the cart OR the target container already has the exact required template.\n  Templates are included in Workspace mode and in Visualizations.\n  9 Jun 2019  Set an expiration of three months (since creation or last time the container was cloned) to stored containers. The container must be cloned every three months at the latest or it will be automatically deleted from the library.  16 Oct 2018   Added the option to choose container version when cloning a container.\n  Change all progress indicators to follow the pattern of Workspace mode, i.e. list all assets and show a green checkmark next to each when it\u0026rsquo;s cloned.\n  Added better error handling - now an error will simply skip the item that caused the error rather than cause the whole process to abort.\n  Prevent closing of a modal if a cloning process is in progress.\n  Added the possibility to flush the cart from anywhere, not just the cart page.\n  Force reload of the page after a cloning process is complete - this is necessary so that the user doesn\u0026rsquo;t return to the main page and work with outdated data.\n  2 Oct 2018   Released Workspace mode.\n  With Workspace mode, you can mass edit items in the selected Workspace.\n  30 Sept 2018   Improve support for Tag Sequencing in Inspect mode (sequences are preserved as long as all sequenced tags are in the cart).\n  Add button to clear cart directly from the nav bar.\n  6 Sept 2018   List folders in Inspect view.\n  Clicking a folder row highlights all items in the folder.\n  You can add all items in a folder into the cart.\n  Added Expand/collapse all toggle to item list in Inspect view.\n  5 Sept 2018  Preserve Tag Sequencing details for tags when cloning an entire container from one account to another.  11 Jan 2018   Added Slack integration to container pages.\n  Fixed \u0026ldquo;Last Modified\u0026rdquo; when viewing version details to match the last modified of the version and not the container.\n  29 Dec 2017  Switched to HTTPS  3 Apr 2017  Published the new version of the tools. Main changes are listed in 1. What\u0026rsquo;s changed?.  1. What\u0026rsquo;s changed? Here are the main changes to this latest version of GTM Tools:\n  You can now revoke GTM Tools\u0026rsquo; access to your data via the profile drop-down (Revoke access).\n  Click Show container type in any container list to see what type each container is.\n  You can only Save to cart an asset if it matches the container type already saved in cart. In other words, you can\u0026rsquo;t combine assets of different container types (e.g. Web and Firebase) in the cart.\n  You can only Clone cart contents to container if the cart type matches the container type. Thus you can\u0026rsquo;t clone Web assets to a Firebase container, for example.\n  This means that you can only Save to library a cart of a single type. And this, in turn, means that if you want to clone a saved container to an existing container, they must have the same type.\n  You can now Display notes in any Inspect container view.\n  You can export a Container overview as CSV in any Inspect container view.\n  You can select the Container version you want to inspect.\n  You can\u0026rsquo;t have multiple assets with the same name in the cart. If you do, these conflicting assets are flagged in the cart page.\n  Added Workspaces to all clone operations. Basically, you need to select a Workspace you are cloning into. You can also create a new Workspace as the target, if you wish.\n  Improved stability and error handling.\n  I rewrote almost the entire codebase, so stability and performance should have improved. This doesn\u0026rsquo;t mean it\u0026rsquo;s error-free (far from it).\n2. Login And Authentication GTM Tools uses your Google Account for authentication. This means that when you first open the tool website, you will need to Sign in with your Google credentials. Note that GTM Tools has no sign in of its own. The tools use your Google sign-in to access your data. The only data that is permanently stored is your Asset Library data.\n  Once you\u0026rsquo;ve signed in, you will need to authorize GTM Tools for access to Google Tag Manager and your Google profile. Specifically, here are the authorization scopes you allow access to:\n  If you refuse to allow access, you will not be able to use the toolset.\nOnce you\u0026rsquo;re in the actual tool interface, you can see the profile you\u0026rsquo;ve logged in with in the upper right corner of the page. You can click this link and select Revoke access to revoke the tool\u0026rsquo;s access to your data at any time. This will not sign you out of your Google account. You can click My Account to access your Google Account settings.\n  3. Home Page The first page you\u0026rsquo;ll see in the tools is the home page. This page is a placeholder, and you should use the navigation bar in the top of the page to move on in the site.\n  The navigation has the following selections in the home page:\n  Home – Takes you back to the home page\n  GTM Account – This lists all the GTM accounts you have access to with the signed in Google Account.\n  Library – Takes you to the Asset Library where you can find your stored containers\n  Cart – Shows you how many items you have in your cart, and by clicking the button you will be taken to the Cart page\n  Your Profile – Clicking this shows a drop-down menu, where you can choose to access your Google Account settings and/or revoke GTM Tools\u0026rsquo; access to your data.\n  4. Account Page When you choose a GTM account in the Accounts navigation, you will be taken to a page that lists all the containers in the selected GTM account. If you click a container name, you will be taken to the respective Container page.\n  If you click the small down arrow next to a container name, you\u0026rsquo;ll see quick links for the following container actions:\n  Inspect - Takes you to the Inspect Container page, where you can view information about the container, and where you can add / remove assets from the container into the cart\n  Visualize - Takes you to the Visualize Container page, where you can view a visualization of the Latest Version of the container\n  Clone - Opens a modal dialog that lets you clone the Latest Version of the container\n  Note that only Inspect supports accessing other versions than the Latest Version. This is something I intend to fix in a future release. Until then, Clone and Visualize only access the latest container version.\nIf you click Show container type, you\u0026rsquo;ll see a label indicating what type each container is. This is relevant for cart interactions.\n  Read more about these actions in the following sections of this guide.\n5. Container Page The container page is here more for navigational reasons than to add any extra value. You can move through it to the individual actions (Inspect, Visualize, Clone), which you can also do from the account page, as you just learned.\n  The following chapters will include details about the various actions you can take.\nInspect Container On the Inspect Container page, you can see a list of all tags, triggers, and variables in a container. You will also see information about the selected version by clicking the Version Information panel.\n  The number on the right-hand-side of a panel title tells you how many assets are in each respective category.\nBy expanding an asset category, you\u0026rsquo;ll see a list of all the assets under that category.\n  If you click Display notes, an additional column is added, where you can see the notes added for each asset (if any).\n  You will also see two selections:\n  Green plus + for adding the asset to your cart (if not added yet)\n  Red minus - for removing the asset from your cart (if already added)\n  Icon in the Links column if there are dependencies (i.e. linked assets) that you should probably add to your cart as well\n  The green plus or red minus will appear depending on if the asset is in the cart or not. If there are no dependencies, you will only see a dash in the \u0026ldquo;Links\u0026rdquo; column.\n  When you click on the Links icon for dependencies, a modal dialog will open up which lists all the dependencies of the current asset. This means that these dependencies are linked to directly from the asset itself, or from one of its linked assets (it\u0026rsquo;s a recursive check). It\u0026rsquo;s strongly recommended that you include all linked dependencies when adding an asset to the Cart.\n  You can add a dependency to the cart by clicking the Add link, after which you\u0026rsquo;ll see the text Added next to the dependency.\nYou can change the version you are inspecting from the drop-down in the header of the page.\n  By clicking Export overview as CSV, you can export a CSV file that has the current container information in CSV format.\n  Folders The Folders panel lists all the folders you have created in the container.\nHere you can highlight all the items in any given folder (by clicking a folder row in the list).\nYou can also add all items in a folder to the cart by clicking the respective button.\nVisualize Container The Visualize Container page first shows you a brief description of what the tool does. Once you click the Start visualization button, a full-screen modal dialog will open, and you will be able to see a visualization of all the assets in the container as well as any links between them.\nThe asset colors are:\n  Grey - Built-In Variables\n  Green - Tags\n  Blue - Variables\n  Red - Triggers\n    If you hover your mouse over an asset, any links to or from that asset will be highlighted. The path color is red if the link is from the selected asset, and the path color is green if the link is to the selected asset.\nHovering over the asset will also show information about it in the small box that appears in the center of the visualization.\nClicking an asset name freezes the paths, so that it\u0026rsquo;s easier for you to navigate to the other end of the path.\nClicking Select Hermit Nodes will highlight all the assets that have no links to or from other assets.\n  You can use the search box to find assets. Start typing, and the assets that match whatever you\u0026rsquo;ve typed will be highlighted as you type.\n  Clone Container There are two ways to clone a container in GTM Tools. Due to architectural reasons, they are a bit different.\nThe first way is through the Account page and the Container page. So either you choose Clone from the drop-down menu next to the container name in the account page, or you click the Clone button on the container page itself.\n  When you choose this clone option, you will be able to choose the GTM account where this container will be cloned to. You can also choose the same GTM account where the container originates from.\n  Once you\u0026rsquo;ve chosen the account and clicked Clone, the process begins, and the source container with all its assets is cloned to the target account.\nIf there already is a container with this name in the target account, the container name will be prefixed with \u0026ldquo;copy of \u0026ldquo; during the process.\nThe second way of cloning a container is with your custom-created containers. This means that you choose to Clone either directly from the Cart page or from your Asset Library page.\n  If you choose this option, it will be possible to merge the stored container with an existing container, or you can choose to create an entirely new container, if you wish.\nNote that you will need to specify to which Workspace you want to clone the contents to. Alternatively, you can also create a new workspace.\nAdditionally, you can only clone the cart or library container to a container that matches the type of the container you are cloning. So you cannot, for example, clone a Firebase cart into a Web container.\n  When you choose to create a New Container, a new container with the name (and workspace name) you give will be created, and all the assets will be copied to that container.\nIf a container already exists with the name you gave, the new container name will be prefixed with \u0026ldquo;copy of\u0026rdquo; during the cloning process.\nIf you choose to clone the assets to an existing container, the contents in your cart or in the stored container will be merged with the assets in the target container. This means that if there is a naming conflict, i.e. an asset with the same name already exists in the target container, the asset\u0026rsquo;s name will be prefixed with \u0026ldquo;copy of\u0026rdquo;, and any links to the asset in other cloned assets will be updated accordingly.\nRenaming containers and assets like this makes merging containers possible while still preserving the established links between assets in the source container.\nIf you choose to merge the assets to an existing container, no existing assets in the target container are modified in any way, so you don\u0026rsquo;t have to worry about data or integrity loss.\nOnce the cloning process begins, there\u0026rsquo;s no way to interrupt it except by refreshing the page or moving away from the page.\n6. Cart Page On the cart page you can see all the assets that you have stored in your cart. You store assets in the cart through the Inspect Container page. The assets are listed first by GTM account name, then by container name, and finally by asset type.\n  Clicking the Remove link next to an asset removes the asset from your Cart.\nClicking the Clone to container button opens a modal dialog that lets you clone the cart contents into an existing container or a new container. If you choose an existing container, the target container must match the container type of the cart.\nClicking the Save cart button opens a modal dialog that lets you save the cart contents into your Asset Library. This way you can save your favorite container configurations to be used later.\n  Clicking the Empty cart button flushes the cart contents.\nIf you have multiple items in the cart with the same name, they will be flagged with details about the container version from which the items were added. You must resolve these conflicts before doing anything with the cart. Currently GTM Tools does not support storing or cloning multiple assets with the same name, so you will need to remove the items from the cart until only one asset per name remains.\n  7. Asset Library The Asset Library page shows you all your stored containers. When you click a container name, you will see how many tags, triggers, and variables are in the stored container. You\u0026rsquo;ll also be able to see when the container was created, as well as the description you gave the container when you saved it.\n  If the library already (globally) has a container of the name you tried to save, the container name will automatically be prefixed with \u0026ldquo;copy of\u0026rdquo;, so don\u0026rsquo;t be surprised if you see this in your stored container name.\nClicking the Clone button lets you clone this container into an existing container or a new container.\nClicking the Visualize button takes you to the Visualize Container page, where you can see a visualization of all the assets stored in the container.\n  Clicking the Delete button opens a modal dialog which confirms this action. If you choose to delete the Container, you\u0026rsquo;ll see a success message shortly, after which the page will automatically reload.\n8. Known Issues Here are some of the issues I know exist in the toolset.\n1) You can add all items in a folder to the cart, but the folder designation itself doesn\u0026rsquo;t carry over. So when you clone the cart, these items will not be put into any folders in the target container. I\u0026rsquo;m trying to figure out a way to do this elegantly. When cloning an entire container, the folder structure is preserved.\n2) Version selection should be added to the Visualize function of containers as well.\n3) There are lots of little features that I\u0026rsquo;d like to work on at some point, such as: clone to multiple containers, interact with cart without having to move to the cart page, modify asset settings, etc.\n10. Summary As I hopefully made clear in the introduction, this toolset is my own playing ground. It\u0026rsquo;s not a fully-formed platform, it\u0026rsquo;s not a sponsored product, and it doesn\u0026rsquo;t have a team of engineers working on it 24/7. Thus I hope you will find it useful, and I\u0026rsquo;ll do my best to fix bugs and new features, but don\u0026rsquo;t expect Premium-level support from me in making things right.\nUse the toolset at your risk.\nThere\u0026rsquo;s no risk to your existing assets, since I don\u0026rsquo;t have any overwrite features in the toolset. The only thing you can botch up is cloning something into something else, and in that case only the thing you were cloning will suffer. Easy enough to clean up afterward in your GTM account.\nI still hope you find the toolset useful, and I would very much appreciate any feedback that you might want to direct to my developer team (i.e. me).\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-api-v2-released/",
	"title": "Google Tag Manager API V2 Released",
	"tags": ["api", "Google Tag Manager"],
	"description": "Introduction to the latest version of the Google Tag Manager programmatic API, and especially the changes compared to version 1.",
	"content": "Google Tag Manager has a very nifty programmatic API that lets you do almost anything that\u0026rsquo;s also possible within the GTM UI. I\u0026rsquo;ve used the API a lot, most notably for my GTM Tools, which might be getting a new release soon, too!\nThe API was recently updated to its second release version (V2), and in this article I want to go over the additions, removals, and changes that the new version introduced.\n  First of all, V1 of the API is still fully functional, so you don\u0026rsquo;t have to migrate until you\u0026rsquo;re ready. That being said, the new version brings about a number of changes I\u0026rsquo;m sure you\u0026rsquo;ll find helpful when working with the API.\nBiggest changes The most significant change, in my opinion, is the introduction of workspace as the central focus in almost all interactions with anything on the container level. It\u0026rsquo;s not just that there\u0026rsquo;s a new namespace for workspaces in the API, but that you must always reference a workspace when interacting with container assets. This is going to be the biggest hurdle in a migration, I believe. Anything that happens in a workspace (pretty much all UI interactions with a single container draft) must now be accompanied by the workspace ID of the workspace you want to modify. This, in turn, means a round trip via the workspaces API to get the IDs you need.\n  Another very visible change is the separation of Built-in Variables into its own namespace. Luckily there\u0026rsquo;s also a simple batching shorthand for enabling / deleting multiple Built-in Variables in a single request.\nOn a technical level, when using client libraries you now provide the path to the asset as a single parameter (basically path or parent) instead of using multiple named parameters. The new resource representations include the path key, which you can then use to easily chain API commands together.\nFinally, and I love this, there\u0026rsquo;s support for adding notes to pretty much any resource available via the API. Even though the UI doesn\u0026rsquo;t yet support the notes field as widely, you can now add descriptive text to all assets via the API. Very useful for documenting the container.\n(UPDATE: a short while after writing this article, GTM released support for notes in the UI too, yay!)\nDetailed changes What follows is a walkthrough of all the namespaces in the new version of the API, with information on what changed from V1.\n1. Accounts Resource changes: The Accounts resource has the following new fields:\n path: path of the Account, e.g. accounts/12345 tagManagerUrl: direct URL to your account in the GTM API  Method changes: There are no significant changes in Accounts methods.\n2. Built-in Variables This is a completely new namespace in the API. Instead of passing a list of Built-in Variables in the container resource itself, you now need to explicitly enable Built-in Variables in the workspace of your choice. You can read the full description of this new feature here.\nNote that if you want to enable multiple Built-in Variables in a single request (strongly recommended), you can use a shorthand batch format to do so.\naccounts() .containers() .workspaces() .built_in_variables() .create( parent=\u0026#39;accounts/%s/containers/%s/workspaces/%s\u0026#39; % (accountId, containerId, workspaceId), type=[\u0026#39;clickClasses\u0026#39;, \u0026#39;clickElement\u0026#39;, \u0026#39;clickId\u0026#39;, \u0026#39;clickUrl\u0026#39;, \u0026#39;clickText\u0026#39;, \u0026#39;clickTarget\u0026#39;] ) 3. Containers Resource changes: The Containers namespace hasn\u0026rsquo;t changed that much. The new fields are the same as with Accounts: path and tagManagerUrl. Fields that were removed from V2 are the redundant timeZoneCountryId and timeZoneId. The enabledBuiltInVariable field has been replaced by the Built-in Variables namespace introduced above.\nMethod changes: There are no significant changes to the Containers methods.\n4. Environments Resource changes: The main changes to the Environments resource are the following:\n  path: relative path in the API to the given environment\n  authorizationTimestamp: supports both nanos and seconds as values\n  workspaceId: link to preview a given workspace in the environment\n  tagManagerUrl: link to the Environments page in the container\n  Method changes: The most significant change is that the old environments.reauthorize_environments namespace has been removed in favor of simply adding a reauthorize() method to the main Environments API.\n5. Folders Resource changes: The big changes to the Folders resource are:\n  path: relative API path to the folder\n  workspaceId: ID of the workspace from which the folder was fetched\n  tagManagerUrl: link to the folder in the GTM UI\n  notes: notes about the folder\n  Method changes: The main changes to the methods are that both folders.entities and folders.move_folders have been replaced with their own dedicated API methods in entities() and move_entities_to_folders(), respectively.\nThere\u0026rsquo;s also a new method, revert(), which lets you revert changes to a folder in the given workspace.\n6. Tags Resource changes: The only changes are the inclusion of path, workspaceId and tagManagerUrl. Otherwise the resource representation has remained relatively unchanged.\nMethod changes: The only big change is the introduction of the revert() method, which lets you cancel any changes to the tag in the given workspace.\n7. Triggers Triggers was affected in exactly the same way as Tags. The changes are identical, with the exception of notes now introduced as a writable field for Triggers, too.\n8. User Permissions (Note, this used to be called just Permissions in the previous API version documentation.) Resource changes: The main changes to User Permissions are:\n  path: relative API path to the permission entity, includes the permissionId\n  accountAccess.permission: now a string rather than a list/array\n  containerAccess[].permission: now a string rather than a list/array\n  Method changes: No significant changes to methods.\n8. Variables Changes to Variables are pretty much on par with what\u0026rsquo;s done to Tags, so check the relevant section above for more details.\n9. Version Headers The Version Headers API is a new addition to the GTM API. Basically, it\u0026rsquo;s a shorthand for accessing versions of any given container. Unlike the Versions API itself, Version Headers only returns, surprise surprise, headers. This keeps the API calls really lean, and lets you quickly get the necessary information, such as the version ID.\nYou can check the documentation for more details on the resource representation.\nThe API has two methods.\n  latest() returns the version header of the Latest Container Version. The Latest version is the most recently created version in Google Tag Manager, and is very relevant to Workspaces, since Workspaces need to be synchronized with the Latest container version before they can be published or turned into versions themselves.\n  list() returns a list of all container versions in any given container.\n  10. Versions (Note, this used to be called Container Versions in the previous API version documentation.) Resource changes: Here are the changes to Versions in the GTM API:\n  path: relative API path to the container version\n  description: used to be called notes in the previous API version\n  builtInVariable: list of Built-in Variables enabled in the version\n  tagManagerUrl: link to the container version in the GTM UI\n  Note that the macro and rule parameters have been deprecated in the new API version.\nMethod changes: Here are the main changes to Versions methods:\n  create(): deprecated - container version creation is done via Workspaces\n  list(): deprecated - version list is now done via Version Headers\n  live(): retrieves the container version that is currently live in the container\n  set_latest(): sets the given version as the Latest version in the container - replaces restore() in the previous API version\n  11. Workspaces The Workspaces API was introduced in V2 of the GTM API. You will definitely want to follow this link and familiarize yourself with the new API. Understanding how workspaces function in the container is integral to understanding how the GTM API works.\n  create_version() creates a version out of a workspace but only if the version passes all GTM\u0026rsquo;s syntax and validation checks.\n  getStatus() returns all the conflicting and/or modified entities in the workspace. This is useful if you want to check if the workspace is ready to be turned into a version.\n  resolve_conflict() lets you resolve conflicts in favor of the workspace or the latest version.\n  sync() lets you synchronize the workspace with the latest container version - a necessary step to take before creating the version.\n  Workspaces are fundamental to many interactions with the API. Basically, when you work with tags, triggers, variables, folders, and built-in variables, you always need to provide the workspace ID with which you are interacting. There\u0026rsquo;s no single \u0026ldquo;Container draft\u0026rdquo; anymore. There\u0026rsquo;s always a workspace you\u0026rsquo;re working in.\nIn addition to having to update your methods, this also means that you need to rethink some of the API flows you have been using thus far. For example, instead of just creating new items in a container, you now need to specify the workspace you want to work with. This means you might first need to list() the available workspaces to get the ID you are looking for. Or perhaps you want to create() a new workspace so that you don\u0026rsquo;t mess with other people\u0026rsquo;s unfinished work.\nSummary The new API certainly has a lot of stuff to wrap your head around. The introduction of workspaces, for one, is certain to make migration a bit of a headache. However, it would be equally awkward to work with an API that isn\u0026rsquo;t in sync with the feature set of the UI it is managing. For this reason, I think it makes a lot of sense to upgrade to the latest API version as soon as possible.\nMost of the stuff is easy to figure out, such as how to move from accountId= and containerId= to path=accounts/accountId/containers/containerId. In fact, I think changes like this make it easier to work with programming languages that make use of string interpolation (Python, for example). There are some conventions that might take time to understand routinely, such as how to use versionheaders.list instead of versions.list.\nOne thing that\u0026rsquo;s a constant feature request from me is that Google would update their error messages. It\u0026rsquo;s very difficult to understand what\u0026rsquo;s wrong when all you see in the logs is \u0026ldquo;400 Bad Request\u0026rdquo; or \u0026ldquo;500 Backend Error\u0026rdquo;.\nWhat do you think about the new API? Have you written tools for the GTM API you want to share? Please do!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/cross-domain-tracking-with-multiple-ga-trackers/",
	"title": "#GTMTips: Cross-domain Tracking With Multiple GA Trackers",
	"tags": ["cross-domain tracking", "google analytics", "Google Tag Manager", "gtmtips"],
	"description": "How to setup cross-domain tracking to Google Analytics when you have more than one tracker running on the page.",
	"content": "To be fair, this tip isn\u0026rsquo;t just for Google Tag Manager but for regular old on-page Google Analytics as well. It\u0026rsquo;s one of those little things that\u0026rsquo;s corroding your data quality without you ever realizing it. Namely, this tip is about how to handle cross-domain tracking in situations where you are sending data to multiple Google Analytics properties on the same page.\nIt\u0026rsquo;s a very typical scenario - you have a \u0026ldquo;local\u0026rdquo; property, which tracks only the traffic of the current site, and then a \u0026ldquo;rollup\u0026rdquo; property, where you send data from all your organization\u0026rsquo;s websites. The rollup property would need cross-domain tracking enabled, since you want to track users across your organization\u0026rsquo;s many website domains.\nTip 56: Manage Cross-domain Tracking In Multi-Property Setups   The problem, in a nutshell, is that your cross-domain tracking property has the power to overwrite the value of the _ga cookie. It does this with the combination of the allowLinker: true field and a linker parameter in the URL of the page.\nSince all of your trackers and tags, by default, use the _ga, it\u0026rsquo;s possible this is wreaking havoc on your data quality.\nYou see, when a URL is loaded with the linker parameter, such as when traffic from another domain to the current page is decorated with cross-domain linker parameters, any tracker on the page with allowLinker: true checks if the linker parameter is valid. If it is, the Client ID is grabbed from the linker parameter and used on the current page to replace the value of the _ga cookie.\nThus, if there already WAS a _ga cookie with a different Client ID, tough luck. It\u0026rsquo;s now overwritten with what was in the linker parameter.\nThis means that any user who used to have a Client ID of X in your GA tracking will now have a Client ID of Y, meaning they are effectively treated as completely different users.\nAnnoying, right?\nWell, there\u0026rsquo;s a way to fix this. Basically, in all the trackers and tags that DO accept cross-domain tracking, you will need to use a different cookie name than _ga to store the Client ID! This way the cross-domain Client ID will not overwrite any pre-existing user data stored in the browser, but will be isolated in its own cookie where it will harm no one.\nHere are the basic steps.\n  In every single tag (GTM) or tracker (regular GA) that is not used for cross-domain tracking, make sure to either leave out the allowLinker field or set its value to false. This is very important.\n  In every single tag (GTM) or tracker (regular GA) used for cross-domain tracking, make sure to set the cookieName field to something other than _ga, e.g. _rollupGa, and make sure to set the allowLinker field to true.\n  So in the end you should have two types of field configurations in your GA trackers and tags. One set has no cross-domain settings, no special cookie settings, and no allowLinker field set. The other set has a new cookie name and has the allowLinker field set to true. Here\u0026rsquo;s an example using regular, on-page GA:\n// Regular GA tracker, uses _ga cookie ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {allowLinker: false}); // Rollup GA tracker ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-2\u0026#39;, {name: \u0026#39;rollup\u0026#39;, cookieName: \u0026#39;_rollupGa\u0026#39;, allowLinker: true});  It\u0026rsquo;s important that you audit all your tags and trackers and take extra care to see that not a single one of your non-cross-domain tags has the allowLinker field set to true, and that every single one of your cross-domain tags has the cookieName field set to the custom cookie name.\nThere, I think I\u0026rsquo;ve repeated myself sufficiently to impress you with what you should do the next time you setup cross-domain tracking through GTM or on-page GA.\nGood luck!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/100-google-tag-manager-learnings/",
	"title": "100+ Google Tag Manager Learnings",
	"tags": ["Google Tag Manager", "JavaScript", "Tips"],
	"description": "Over 100 tips and lessons learned after five years of using Google Tag Manager.",
	"content": "I\u0026rsquo;ve always been proud to avoid the typical headline clickbait of \u0026ldquo;Ultimate guide to pigeon care\u0026rdquo;, \u0026ldquo;All you need to know about the Great Vowel Shift\u0026rdquo;, \u0026ldquo;Did you know that you\u0026rsquo;ve been smoking peyote wrong your whole life?\u0026rdquo;. I\u0026rsquo;m ready to make an exception now by adding a BIG WHOPPING NUMBER to the title. You see, the amount of knowledge one can accumulate about anything they do on a daily basis is mind-blowing. It helps if you write a blog about the topic, since creative output is a great way to organize your thoughts. It also helps to be active in community support, since problem-solving is an excellent way to accumulate new skills and to hone the edge of your existing talent.\n  Now, I already have 50+ GTM Tips written, so it\u0026rsquo;s not like this is a novel idea, even on this blog. But this time I just wanted to write short, byte-sized things I\u0026rsquo;ve learned along the way, and I want to share them with you.\nAs you can read from the outrageously baiting title, there should be 100+ tips, but I only enumerated an even 100. That\u0026rsquo;s because I want YOU to add your ideas to the end of this post, and let\u0026rsquo;s see if we can keep it going. Yes, it\u0026rsquo;s my shameful attempt to delegate content creation to the community. I am guilty of that, too, now.\nContainer JavaScript Snippet 1. Initializes the dataLayer The JavaScript snippet part of the GTM container has one very important function (among others). It initializes the window.dataLayer array. Thus, if you haven\u0026rsquo;t initialized a dataLayer object yourself, the container snippet will do this for you. This ensures that dataLayer.push() works within GTM.\n2. Creates the script loader for the GTM library Perhaps even more importantly, the JavaScript container snippet creates a \u0026lt;script\u0026gt; element, which loads the Google Tag Manager container library for your GTM container ID from Google\u0026rsquo;s servers.\n  3. JavaScript snippet should be in \u0026lt;head\u0026gt; but can be (almost) anywhere The latest (and best) recommendation for placing the JavaScript snippet is to put it in the \u0026lt;head\u0026gt; of the document. This helps GTM load as early as possible, resulting in greater tracking accuracy. However, you can execute the JavaScript snippet pretty much any time during the page load and anywhere in your site code where execution of JavaScript is possible. The sooner the library loads, though, the more accurate your data collection will be.\n4. Pushes the initial event: 'gtm.js' The JavaScript snippet also pushes the initial event: 'gtm.js' into dataLayer. This is an important GTM event. It is used by the All Pages and Page View triggers. Any Data Layer variables you want to use with these triggers must be added to dataLayer before the JavaScript container snippet is executed.\n  5. Multiple container snippets on a page are supported You can add multiple JavaScript container snippets on a page. This is officially supported. The caveat is that they all need to use the same dataLayer name.\n Container \u0026lt;noscript\u0026gt; snippet 6. The \u0026lt;noscript\u0026gt; block should be at the very beginning of \u0026lt;body\u0026gt; At the time of writing, the \u0026lt;noscript\u0026gt; block should be added to the very beginning of \u0026lt;body\u0026gt;. This is the only way that Search Console Verification using the Google Tag Manager method will work. Naturally, if you don\u0026rsquo;t care about verifying the site using the GTM method, nor do you have any use for tracking non-JavaScript visits, you can leave the \u0026lt;noscript\u0026gt; block out altogether. Just don\u0026rsquo;t place it in \u0026lt;head\u0026gt; as that would result in HTML validation issues.\n7. Only executed by browsers with JavaScript disabled The \u0026lt;noscript\u0026gt; snippet is only executed by browsers with JavaScript disabled. If you want to test it, you can disable JavaScript using your browser\u0026rsquo;s developer tools (e.g. Chrome).\n8. Loads an HTML page in an \u0026lt;iframe\u0026gt; The block loads an \u0026lt;iframe\u0026gt; element, which fetches its data as an HTML file from Google Tag Manager\u0026rsquo;s servers. In essence, this HTML file is your container. The HTML will contain the image elements you have configured to fire for JavaScript-disabled visitors.\n9. Only the Page View trigger works Because the JavaScript-less GTM can\u0026rsquo;t run JavaScript (d\u0026rsquo;oh), only the Page View trigger is at your disposal. Thus, there\u0026rsquo;s no dynamic triggers, and no way to wait for the page to load or anything like that. The Page View trigger is fired when the \u0026lt;iframe\u0026gt; contents are fetched.\n10. Use a function() { return true; } Custom JavaScript variable in the trigger A very handy way to fire tags only when executed in the \u0026lt;iframe\u0026gt; is to create a Custom JavaScript Variable with the following content:\nfunction() { return true; }  This variable will only return true if the browser executes it, i.e. executes JavaScript. By adding {{Variable}} does not equal true as a trigger condition fires the trigger only in browsers where JavaScript is disabled.\n11. Only the Custom Image tag is useful Since the JavaScript-less container can\u0026rsquo;t execute JavaScript, you are left with just the Custom Image tag. In other words, you can create image elements that are added directly into the container HTML. These image elements will then be rendered by the browser. In fact, you can even do some basic Google Analytics tracking using an image tag, since GA requests are basically image pixels. See this LunaMetric guide for inspiration.\n12. Can utilize \u0026ldquo;Data Layer\u0026rdquo; parameters via query parameters You can feed \u0026ldquo;Data Layer\u0026rdquo; values to the container HTML using query parameters in the \u0026lt;iframe\u0026gt; src attribute value. The query parameters need to be added as key-value pairs, and the keys that you add can then be used in Data Layer variables. For further details, see the Bounteous guide linked to in the previous paragraph, or check the guide I\u0026rsquo;ve written.\n The dataLayer structure 13. Global JavaScript array The dataLayer structure is a global JavaScript array, and can thus be accessed in any site code that can also access the window object. It\u0026rsquo;s a good idea to always prefix the dataLayer name with window. to avoid conflicts with any locally scoped structures that use the same name.\n14. You can use a different name than dataLayer You can change the name of this global structure in the JavaScript container snippet. Just remember to always use this new name when adding messages to dataLayer!\n  15. Only the .push() method works with GTM Google Tag Manager only reacts to the .push() method. You can .splice(), .slice(), .shift() and .pop() all you like. GTM only listens for .push() commands.\n16. Typically only plain objects work with GTM The most common way to feed data to Google Tag Manager is using plain objects. Each object contains one or more key-value pairs. These key-value pairs are then translated into Data Layer variables, which you can create in Google Tag Manager to fetch values from the Data Layer.\nvar plainObject = { someKey: \u0026#39;someValue\u0026#39;, someOtherKey: \u0026#39;someOtherValue\u0026#39; }; window.dataLayer.push(plainObject);  17. You can use any JavaScript type as a value of a key All JavaScript types are supported as values when you push your dataLayer messages. When you create a Data Layer variable in GTM, it will contain a reference to whatever the value of the key is, regardless of type.\nwindow.dataLayer.push({ type_number: 5, type_string: \u0026#39;hello\u0026#39;, type_object: { someKey: \u0026#39;someValue\u0026#39; }, type_array: [1,2,3,4], type_function: function() { return \u0026#39;hello\u0026#39;!; }, type_boolean: true });  18. Only event key can trigger tags Only a message with an event: 'someValue' key-value pair can trigger tags. Any object without an 'event' key is treated as just a \u0026ldquo;message\u0026rdquo;, and has no triggering power of its own.\n19. You can also .push() a command array There\u0026rsquo;s a special command array you can .push() into dataLayer if you want to execute methods for values already in the data model. So technically it\u0026rsquo;s not just plain objects that dataLayer digests. There\u0026rsquo;s more about this in tip #26.\n20. Never overwrite, always .push() I usually hate to dole out best practices, so consider this a fact of life instead. Never, ever, ever, ever use this syntax:\nvar dataLayer = [{...}];  It\u0026rsquo;s destructive. If this command is executed after the GTM container snippet or after you\u0026rsquo;ve already established a dataLayer object, you will end up overwriting the existing object with this newly initialized structure. Worst-case scenario (surprisingly common) is that you\u0026rsquo;ll end up breaking GTM, since you also overwrite the custom .push() listener added by the container library.\nPrefer this syntax instead:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({...});  21. The array is capped at 300 This is perhaps more obscure, but in addition to adding a .push() listener, the GTM container library also caps the length of the dataLayer structure at 300. This means that when you .push() the 301st item into dataLayer, the first/oldest item in dataLayer is removed. As you will learn in the next section, this has no impact on GTM\u0026rsquo;s data model. It just caps the dataLayer structure itself.\n GTM\u0026rsquo;s data model 22. Copies messages queued via dataLayer.push() When you use the Data Layer Variable, Google Tag Manager doesn\u0026rsquo;t fetch the value from the dataLayer array. Instead, it polls its own internal data model to see if any value has been pushed to the given key (or variable name). If a value is found, GTM returns it. This means that GTM\u0026rsquo;s internal data model has access to the most recently pushed value for any given key.\n23. GTM freezes variable values when a trigger fires Triggers only fire when the key 'event' is pushed into dataLayer. When this happens, GTM \u0026ldquo;freezes\u0026rdquo; the state of the container, and any tags that fire on this trigger will only have access to the current state of the internal data model. Thus, if you want to push values into dataLayer so that they become available to a tag that triggers on the site, these values need to be pushed before or in the same object as the 'event' that triggers the tag.\n  24. Objects are recursively merged Recursive merge is one of the more complex concepts to understand. When you work with primitive values (strings, numbers, booleans, for example), the internal data model of GTM only has access to whatever was most recently pushed into a key whose value is one of these primitive types. However, when you work with structured objects and arrays, it\u0026rsquo;s more complicated.\nWhen you push an object into dataLayer, GTM goes through each key in this object, and only overwrites those that have shared keys and primitive values (or where the type changes). New keys are simply added to the existing object value.\n  When pushing an object to a key that already contains an object with the same keys, only the keys that have primitive values or a different type are overwritten. All others are simply updated.\n25. Arrays are recursively merged In JavaScript, arrays are structured objects, too, where the keys are index numbers that start from 0. So when you push an array into a key that already had an array, these two arrays are recursively merged starting from index 0, and any indices that are not modified remain the same.\n  26. You can run JavaScript methods on existing Data Layer values with a command array What if you already have an array in a key, but instead of merging or overwriting you want to add values to it, i.e. push items to the end of the array? You can use a special command array that you push into dataLayer. The first element in the array is a string that contains the key name and the command you want to execute, and all the other items are passed as arguments to the command.\n  27. Version 1 vs. Version 2 of the Data Layer Variable You\u0026rsquo;ve probably noticed that you can choose a version when using the Data Layer Variable. There are some very important differences between the two.\nVersion 2 supports deep structures with dot notation. If you want to access array indices, you need to use dot notation too (products.0.name rather than products[0].name). Only Version 2 supports recursive merge.\nVersion 1 does not support dot notation, and it only fetches the most recently pushed value whether it\u0026rsquo;s an object or not. Thus there\u0026rsquo;s no recursive merge - what you push is what you get.\n28. google_tag_manager['GTM-XXXX'].dataLayer methods If you want to access values stored in Google Tag Manager\u0026rsquo;s data model from outside GTM or without using a Data Layer Variable, you can use the google_tag_manager interface.\ngoogle_tag_manager['GTM-XXXX'].dataLayer.get('keyName') fetches the value stored in GTM\u0026rsquo;s data model for variable name keyName.\ngoogle_tag_manager['GTM-XXXX'].dataLayer.set('keyName', 'someValue') sets the value of keyName to someValue in GTM\u0026rsquo;s data model. This is the equivalent to using dataLayer.push({keyName: 'someValue'});\ngoogle_tag_manager['GTM-XXXX'].dataLayer.reset() purges GTM\u0026rsquo;s data model, removing all stored keys.\n Preview mode 29. Preview mode works with a cookie on www.googletagmanager.com in the preview browser When you enter GTM\u0026rsquo;s Preview Mode, you are transported through the domain www.googletagmanager.com (the same domain that serves the gtm.js library), during which a cookie is written in your browser for that domain.\n  When you then visit your website, the request for the gtm.js library identifies that you have the Preview mode cookie written on www.googletagmanager.com, and the preview container library is returned instead. So what you\u0026rsquo;re basically dealing with is a third-party cookie, even though the cookie isn\u0026rsquo;t set while browsing the site itself.\n30. Shows the state of tags, triggers, variables, and Data Layer at each Data Layer message Preview mode is a great way to understand how Google Tag Manager works. The navigation in the left column is a chronological (oldest at the bottom) list of messages that have been pushed into dataLayer. By selecting a message, you can see if any tags fired during that message, and you can see the state of tags, variables, and triggers at the time of the message.\n  What you see is what you get. If the variable does not have a value at the time of the message, it means that any tags that fire for that message will not have access to any value if using that variable. This is why it\u0026rsquo;s important to understand that if you want to use a variable, it must have a value when the message that triggers the tag is pushed into dataLayer.\n31. Summary shows the state at the latest message Summary is not a message itself. It\u0026rsquo;s a recap of what the state of the container is after the latest message has fired. Note that if you have Summary (or any other message, for that matter) selected, and you select a tag that fired in an earlier message, the tag might have different values than what you\u0026rsquo;d expect. That\u0026rsquo;s because when you select a message (or Summary), the variables reflect what their values are at the time of the selected message. This way tags can show different values from those that were actually used.\nThat\u0026rsquo;s why it\u0026rsquo;s really important to start debugging by selecting the message that fired the tag. Any other message and you might see confusing data.\n32. Variables are resolved multiple times - at least once per message If you\u0026rsquo;ve ever created a variable with side effects and then gone to preview mode, you might have been surprised at what happens. For example, create a Custom JavaScript Variable with this:\nfunction() { window._testCount = window._testCount || 1; return window.alert(window._testCount++); }  Now when you go to Preview mode, you\u0026rsquo;ll see a bunch of alerts with a number that increments with each alert. Depending on how many messages are pushed into dataLayer and how many tags use this variable, you might see a huge number in the last alert box.\nThis is because GTM resolves variables in Preview mode multiple times. Preview mode needs to resolve the variables at least once per message pushed into dataLayer. Why? Because Preview mode must be able to tell you the value of each variable in each tag, trigger, variable, and message.\nIn a live container, variables won\u0026rsquo;t be resolve this many times. Most likely they are only resolved when they are directly invoked, e.g. in triggers and tags upon injection.\n33. Preview can be minimized The Preview mode panel can be visually obstructive, so it\u0026rsquo;s a good thing the developers added a minimize button some time ago:\n  After clicking it, you can bring the panel back up by clicking the small DEBUG ^ icon in the lower right corner of the window.\n34. To quit preview, you need to exit preview mode via the GTM UI The easiest way to quit Preview mode is to go to the Google Tag Manager user interface and click the \u0026ldquo;Leave preview mode\u0026rdquo; link:\n  You can also go to your browser\u0026rsquo;s content settings, and delete all cookies written on the www.googletagmanager.com domain. This works with Shared Preview, too.\n  Wouldn\u0026rsquo;t it be handy if you could just quit Preview mode from the panel itself on the site? Yes, I think so too.\n35. To quit a shared preview, you need to follow the original link If you want to quit Preview mode that has been shared with you, you should follow the original Share Preview link and click \u0026ldquo;Exit preview and debug mode\u0026rdquo;.\n  Note that you can also delete the cookies as described in the previous tip.\n36. Problems with the preview mode not showing correctly are most typically due to CSS conflicts Sometimes you might not see a Preview mode on a website at all. Other times the panel might be buggy, such as being partly transparent or completely white.\nIn these cases, it\u0026rsquo;s most often a CSS conflict with the site code. GTM loads the panel on the website itself, so style conflicts can arise if they share the same namespace.\nIf this happens, your best bet is to contact the developer team via the Send Feedback link in the UI, or by posting the issue in the Product Forums.\n  37. You can also preview without the debug panel Note that you can also preview a container on the site without the benefit of the debug panel. Why you\u0026rsquo;d want to do this when you can minimize the debug panel escapes me, but to do so you need to click the Share Preview link in the GTM UI, uncheck \u0026ldquo;Turn on debugging when previewing\u0026rdquo;, and then follow the link in your browser. This sets your browser into Preview mode without showing the debug panel.\n  38. Preview must be refreshed after every change GTM doesn\u0026rsquo;t auto-refresh the Preview mode when you save changes in your container. You need to click the \u0026ldquo;Refresh\u0026rdquo; link to update the preview mode for yourself and anyone with the preview link.\n   Universal Analytics 39. GTM creates a new tracker with every tag instance Unlike on-page Universal Analytics (analytics.js), Google Tag Manager creates a new, unique tracker object with every single tag that fires, even if they use the same template.\n  This might not be the most elegant technical design ever, but it\u0026rsquo;s necessary in how GTM structures Universal Analytics tags. Basically each tag is its own sandbox, and settings are not shared from tag to tag.\n40. Settings are not shared across tags Because each tag has a unique tracker, no settings are shared from tag to tag. This is very much unlike on-page Universal Analytics, where you create a single tracker and then invoke that tracker in commands like ga('trackerName.send', 'pageview');.\nIf you want to share settings of a single tag with other tags, currently you need to set the Tracker Name field in the tag settings. But before you do, read the next tip.\n41. You can set a tracker name, but most often this is risky and unnecessary If you do set the Tracker Name, you are likely to run into a host of problems. First of all - ALL settings are shared across the two tags. This is because GTM sets all fields and Custom Dimension / Metrics on the tracker object itself rather than just the hit. So you\u0026rsquo;ll need to take great care to reset any fields that you don\u0026rsquo;t want values to leak into.\nUntil GTM introduces some type of shared tag settings feature, I suggest avoiding the tracker name setting and working with GTM variables instead. If you want a setting to apply across two or more tags, just replicate the setting in each tag and use a variable to populate the same value in all the tags.\n42. Use Fields to Set for setting any analytics.js fields You can use Fields to Set to set any analytics.js fields. These fields are set on the tracker object itself (see previous tip), but will work as if set on the hit itself.\nYou can also add Measurement Protocol parameters to Fields to Set, but this is, in most cases, unnecessary.\n43. If a field has the variable icon, you can use variables in it Fields in Google Tag Manager support adding a variable if the field has the variable icon next to it.\n  By clicking the icon, a list of all available variables pops up. You can also use the auto-complete feature by typing {{ into the field, after which an auto-complete menu shows, and you can continue typing to find the variable you\u0026rsquo;re looking for.\n Enhanced Ecommerce 44. Use Data Layer option uses Version 1 of the Data Layer Variable When you select the Use Data Layer option in your Enhanced Ecommerce enabled Universal Analytics tags, GTM uses the Version 1 of the Data Layer Variable to locate the most recently pushed ecommerce key from the Data Layer.\n  Read that again. GTM only has access to the most recently pushed Enhanced Ecommerce payload in dataLayer. This means that if you first push impressions, for example, but don\u0026rsquo;t fire a tag, and then you push a Product Detail View which does fire a tag, that tag will only access the Product Detail View data. The impressions data is lost in cyberspace, due to no tag firing when it was pushed to dataLayer. To avert this, either make sure you always add an event to all your Enhanced Ecommerce pushes, and always use a Custom Event Trigger to fire an Enhanced Ecommerce enable tag.\nAlternatively, you can use the far more flexible Custom JavaScript variable method.\n45. Requires Data Layer object to be syntactically flawless Enhanced Ecommerce is a bit different from how Data Layer typically works. Generally, you can push any key-value pairs into Data Layer, because you can always transform and mutate them in the GTM UI later on. However, when working with Enhanced Ecommerce, either via \u0026ldquo;Use Data Layer\u0026rdquo; or the Custom JavaScript Variable method, the payload must be syntactically accurate. It must have all the required keys, it must be structured correctly, and it must obey certain limitations to the structure (more details about structure).\nYou should always make sure you\u0026rsquo;re following the official developer guide to the letter.\n46. The Currency type is just a string with a monetary value If you read the official Enhanced Ecommerce developer guide, you might have noticed references to a type called \u0026ldquo;currency\u0026rdquo;.\n  Well, there\u0026rsquo;s no such data type in JavaScript. What the guide means is a string that has a monetary value (without currency symbol). Don\u0026rsquo;t use a thousand separator (e.g. \u0026ldquo;1 045.99\u0026rdquo;), and use the period as the decimal character.\nA valid \u0026ldquo;currency\u0026rdquo; type would be \u0026quot;1045.99\u0026quot;. Invalid types would be \u0026quot;1 045.99\u0026quot; and \u0026quot;1045,99\u0026quot;. Due to loose typing in JavaScript, you could just as well pass it as a number 1045.99, but that will definitely lead to problems if the number is incorrectly formatted.\n47. Product-scoped Custom Dimensions and Metrics need to be formatted correctly Product-scoped Custom Dimensions and Metrics need to be formatted in a certain way to work. With regular Custom Dimensions and Metrics, all you need to do is add them to the tags under the respective tag settings.\nHowever, in Enhanced Ecommerce, all the information must be in the payload. With Product-scoped Custom Dimensions and Metrics, this data must be in the products array, under each individual product you want to add the dimensions and metrics to. The dimensions and metrics must be named dimensionX and metricX where X is the index number for the given custom variable.\n{ ecommerce: { purchase: { actionField: { ... }, products: [{ id: \u0026#39;1\u0026#39;, name: \u0026#39;Shirt\u0026#39;, dimension1: \u0026#39;Red\u0026#39;, metric1: 132, quantity: 1, price: \u0026#39;10.99\u0026#39; },{ id: \u0026#39;2\u0026#39;, name: \u0026#39;Pants\u0026#39;, dimension1: \u0026#39;Black\u0026#39;, dimension2: \u0026#39;Adidas\u0026#39;, metric1: 133, quantity: 1, price: \u0026#39;13.99\u0026#39; }] } } }  48. Custom JavaScript variable method is more flexible than Use Data Layer I always implement Enhanced Ecommerce using the Custom JavaScript variable method. It gives me so much more flexibility, as I can simply create the original dataLayer object as semantically unambiguous as possible, and then use the Custom JavaScript variable to mutate the object into the state the Enhanced Ecommerce requires. Why? Because I have plenty of other platforms that need the ecommerce data, too, and they might not be happy with the way that Google Tag Manager enforces a specific structure.\nfunction() { var order = {{DLV - Order}}; return { ecommerce: { purchase: { actionField: { id: order.orderId, revenue: order.price.totalWithTax, tax: order.price.taxValue, affiliation: order.store.name }, products: [order.productsForGTM] } } }; }    The Custom JavaScript variable itself is simple. All you need to do is make sure it returns a valid Enhanced Ecommerce object as required by Google Tag Manager.\n Triggers 49. Variables can only be used to check against This is perhaps slightly oddly worded, but what I mean is that you can only use a variable as the thing in the trigger whose value you are checking. You can\u0026rsquo;t use a variable as the condition value itself.\n  50. Use a Custom JavaScript variable to check for dynamic values If you DO want to check against dynamic values in your triggers, you can always use a Custom JavaScript variable. Let\u0026rsquo;s say you want to check if the clicked URL contains the current page hostname. Why? Because you want a trigger that fires only for clicks on links that take the user away from the website. This is what the Custom JavaScript variable might look like:\nfunction() { return {{Click URL}}.indexOf({{Page Hostname}}) \u0026gt; -1; }  This variable returns true if the clicked URL contains the current page hostname, and false otherwise. Now you can use a trigger like this:\n  51. 'event' is implicit in all but the Custom Event trigger All triggers require an event key in dataLayer to fire. Thus, when you create a trigger, they check for the value of event, and if there\u0026rsquo;s a match the trigger fires. Only the Custom Event trigger requires you to explicitly state the value of event you want to fire against. Here are the basic trigger types and their implicit event values:\n  DOM Ready - gtm.dom\n  Page View - gtm.js\n  Window Loaded - gtm.load\n  Click / All Elements - gtm.click\n  Click / Just Links - gtm.linkClick\n  Form submission - gtm.formSubmit\n  History Change - gtm.historyChange\n  JavaScript Error - gtm.pageError\n  Timer - gtm.timer\n  Scroll Depth - gtm.scrollDepth\n  YouTube Video - gtm.video\n  52. Multiple trigger conditions are AND, multiple triggers are OR Multiple conditions in a single trigger must ALL match for the trigger to fire. Thus a trigger like this should never work:\n  Why won\u0026rsquo;t it work? Because the hostname of the current page can\u0026rsquo;t be two things at once.\nIf you add multiple triggers to a tag, then any one of these will fire the tag. So, if you want your tag to fire when the page hostname is either www.domain.com or www.other-domain.com, you can create two triggers, one for each hostname, and add both to the tag.\n53. Use regular expressions or Custom JavaScript variables to add optionality in a single trigger There\u0026rsquo;s an easier way to introduce optionality, though. First, if it\u0026rsquo;s a simple string check, you can always use regular expressions.\n  If you have more complex logic, a Custom JavaScript variable is your best friend, again.\nfunction() { var hn = {{Page Hostname}}, ut = {{DLV - userType}}; if (hn === \u0026#39;www.mydomain.com\u0026#39; \u0026amp;\u0026amp; ut === \u0026#39;visitor\u0026#39;) { return \u0026#39;visitor\u0026#39;; } if (hn === \u0026#39;www.mydomain.com\u0026#39; \u0026amp;\u0026amp; ut === \u0026#39;member\u0026#39;) { return \u0026#39;member\u0026#39;; } if (hn === \u0026#39;www.other-domain.com\u0026#39; \u0026amp;\u0026amp; ut === \u0026#39;loyal\u0026#39;) { return \u0026#39;loyal\u0026#39;; } return \u0026#39;other\u0026#39;; }   Auto-event trigger 54. Just Links listens to clicks on \u0026lt;a\u0026gt; elements and their descendants When you create a Just Links trigger, it listens to clicks on \u0026lt;a\u0026gt; elements and their descendants. When a click is registered, Google Tag Manager checks if there is a link node wrapping the clicked element, and if there is, GTM stores a reference to the link in the dataLayer.\nFor example, say the page HTML looks like this:\n\u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;https://www.google.com/\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Google\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; If someone clicks on the link, the click actually falls on the \u0026lt;span\u0026gt; element, but the Just Links trigger propagates the click to the \u0026lt;a\u0026gt; element, and returns that for you to leverage with Auto-event variables.\n55. All Elements listens to all clicks All Elements, on the other hand, listens to all clicks and returns the element that was actually clicked. In the HTML example above, the All Elements trigger would return the \u0026lt;span\u0026gt; element because that\u0026rsquo;s the element that was actually clicked.\n56. Form listens to a submit event dispatched by a \u0026lt;form\u0026gt; element The Form trigger only works with an actual \u0026lt;form\u0026gt; element submitted with default HTML form functionality. This means that there actually needs to be a submit event dispatched by the form, and it must be allowed to bubble up.\nAny custom server-side validation, suppressing of the submit event, or customized form handling will result in the Form trigger not working. Thus, the Form trigger is typically the trigger you\u0026rsquo;ll have the most difficulties with due to the ridiculously diverse number of ways that forms can be handled with JavaScript.\n57. History Change listens to interactions with the browser history API The History Change trigger listens for the following browser history API events: hashchange, pushState, replaceState and popstate.\nTypically these are used on single-page websites, where page transitions are done without a page refresh.\nWhen creating History Change triggers, you\u0026rsquo;ll typically want to work mainly with pushState and replaceState, as those are managed by the by the site code itself. popstate and hashchange can be triggered automatically by the web browser (and there are differences between browsers), which might lead to inaccuracies in your tracking.\n58. Error listens to uncaught JavaScript exceptions The Error triggers listens to uncaught JavaScript errors that occur on the website. If there\u0026rsquo;s a try...catch block anywhere in the error path, this trigger will not react to it.\nDo note that Custom JavaScript variables automatically catch all exceptions, so this trigger will not help you debug errors in Custom JavaScript variables.\n59. All triggers but the Click / All Elements trigger require that the original event bubble up Google Tag Manager attaches its auto-event listeners to the document element of the page. This is the highest node in the web document. The reason GTM does this is because it allows you to handle events that take place anywhere on the page, even for elements that don\u0026rsquo;t exist when GTM first loads.\nFor this type of event delegation to work, the event must bubble up to the top of the document. It\u0026rsquo;s surprising how often bubbling is cancelled, leading to GTM\u0026rsquo;s events not working.\nThe only exception is the All Elements trigger, which uses the capture phase of the event path. This means that even if bubbling is stopped, the capture phase can still record the click. So if you find your Just Links trigger isn\u0026rsquo;t working, you can recreate the same logic with an All Elements trigger and some clever CSS selector / Custom JavaScript variable work.\nFor more information, see e.g. this article on GTM listener issues, and this on element capturing with the All Elements trigger.\n60. Check Validation checks for event.preventDefault() If you have Check Validation checked in your Just Links or Form trigger, the trigger will only fire if the event\u0026rsquo;s default action is not cancelled.\n  This is a useful feature to leverage, since often the default action of a link (redirect) or a form (submit) is prevented due to the link simply changing a content tab, or the form being incorrectly filled, for example. In these cases, you\u0026rsquo;ll want to have Check Validation checked, because you probably don\u0026rsquo;t want to track clicks on links that don\u0026rsquo;t redirect or submissions of forms that don\u0026rsquo;t actually submit.\n61. Wait for Tags pauses the original event, but be careful If you use the Wait for Tags option in the Just Links or Form triggers, Google Tag Manager halts the action of the link or form, respectively, to wait until all tags that use the trigger have fired. After tags signal completion, GTM allows the original event to continue.\nThis is a great feature. It mitigates the risk of losing data due to the link or form redirecting the user to a new page before the tags that use the trigger have fired.\nHowever, there are many ways to do custom link redirects and form submissions. When GTM pauses the event, it\u0026rsquo;s not 100% reliable GTM understands what the custom behavior was. Thus, when GTM then proceeds with the paused action, it might be a different action altogether that GTM resumes.\nThis is typical in single-page apps, where internal links have a lot of complex logic added to them to prevent links from redirecting the user to new URLs.\nSo, when you use the Wait for Tags option, always remember to test the pages where the trigger is active. Test links and forms, both, to make sure that their functionalities are not compromised. If you are in doubt, simply uncheck Wait for Tags and accept a certain level of inaccuracy in your tracking.\n62. Enable this trigger when\u0026hellip; vs. This trigger fires on\u0026hellip; If you do check Wait for Tags or Check Validation, you\u0026rsquo;ll see the \u0026ldquo;Enable this trigger when\u0026hellip;\u0026rdquo; option appear in the trigger settings.\n  The \u0026ldquo;Enable this trigger when\u0026hellip;\u0026rdquo; option is for determining on which pages the trigger should listen to actions. Thus you can use it to have the trigger listen only on pages where you have thoroughly tested the trigger doesn\u0026rsquo;t mess with site functionality.\nIt\u0026rsquo;s a good idea to start with a generic Page URL contains / condition here, as it will simply set the trigger to listen to user actions on all pages of the site. If you do run into trouble, you can modify this condition to only activate the trigger on a specific subset of pages.\nThe \u0026ldquo;This trigger fires on\u0026hellip;\u0026rdquo; option is for determining what conditions OTHER than the trigger event itself need to exist for any tags which use the trigger to fire. This is where you\u0026rsquo;ll add your \u0026ldquo;Click URL\u0026rdquo; and \u0026ldquo;Form ID\u0026rdquo; conditions, for example.\n63. Data Layer object composition of an auto-event When an auto-event trigger fires, the following items are pushed into dataLayer:\n  event - gets the value of the trigger event that was registered.\n  gtm.element - reference to the HTML element that was the target of the event.\n  gtm.elementClasses - string with the value from the class attribute of the target element (if any).\n  gtm.elementId - string with the value from the id attribute of the target element (if any).\n  gtm.elementTarget - string with the value from the target attribute of the target element (if any).\n  gtm.elementUrl - string with the value from the href or action attribute of the target element (if any).\n  gtm.triggers - a regular expression which determines if the trigger that activated is enabled on the current page.\n  In addition to these, you might see variables like gtm.uniqueEventId, eventCallback, eventTimeout and eventReporter. These are internal to GTM, but suffice to say that they govern how the Wait for Tags option works, among other things.\n64. Auto-event variable If you\u0026rsquo;re not satisfied with the Built-in variables (Click / Form) for auto-events, you can create your own. Google Tag Manager has a user-defined variable type called Auto-Event Variable, which lets you analyse pretty much any part of the auto-event target element you want.\nFor example, if you\u0026rsquo;ve added a custom data attribute data-gtm-cta=\u0026quot;Subscribe call to action\u0026quot;, and you want to check if the clicked link has this data attribute, you can create a new Auto-Event Variable that looks like this:\n  This comes in very handy, as you won\u0026rsquo;t be limited to the rather small number of available built-in variables.\n65. Matches CSS selector with the Click / Form Element built-in variable The matches CSS selector is one of the most effective ways you can create triggers in Google Tag Manager. When combined with the Click / Form Element Built-in variable, it gains a whole new level of awesomeness.\nYou can write complex CSS selectors to check whether the element that was the target of the auto-event matches what you expect. For example, let\u0026rsquo;s say you have the following HTML structure:\n\u0026lt;div id=\u0026#34;products\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;a href=\u0026#34;some-product.html\u0026#34;\u0026gt;Product/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;services\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;a href=\u0026#34;some-service.html\u0026#34;\u0026gt;Service\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Now, you only want your Just Links trigger to fire when the user clicks the \u0026ldquo;Product\u0026rdquo; link. However, as you can see, the HTML structures between \u0026ldquo;Product\u0026rdquo; and \u0026ldquo;Service\u0026rdquo; are almost identical, making it difficult to pinpoint the exact element without having to resort to suboptimal selection mechanisms. Well, CSS selectors to the rescue. The following Just Links trigger will only fire when the click is on a link that is the descendant of a div with the id \u0026quot;products\u0026quot;:\n  Just remember: matches CSS selector only works against an HTML element. So don\u0026rsquo;t try to match a selector against the Click ID or Click URL variables, for example. Only Click / Form Element will do.\n Custom HTML tags 66. Code is automatically minified Custom HTML tags automatically minify / uglify the JavaScript, so no need to do so yourself.\n  67. Code is injected to the end of \u0026lt;body\u0026gt; When you write HTML code in a Custom HTML tag, the entire code block is always injected to the end of \u0026lt;body\u0026gt; when the trigger fires for the tag.\nIf you want to place the HTML code somewhere else, you need to write JavaScript (within the Custom HTML tag) that creates a new HTML element with your specifications, and then uses DOM methods to place it wherever you like on the page.\n68. Can be used to add any HTML elements You can use the Custom HTML tag to add any supported HTML5 code to the page. This means that tags such as these are all supported: meta, link, style, video, script, and so forth.\nNaturally, with some JavaScript DOM magic, you can also modify existing elements or even remove them entirely from the page.\n69. The document.write option is fixed to prevent the site from breaking If you are adding a script which makes use of document.write to place elements on the page, you will need to check the \u0026ldquo;Support document.write\u0026rdquo; option in the Custom HTML tag editor.\n  If you don\u0026rsquo;t check this option, you run the risk of the document.write command clearing the entire page of all contents because of how document.write works post-page-load.\nDan Wilkerson has written a great guest post on this topic, so remember to check it out for more information.\n70. Variables are automatically renamed in Custom HTML tags, too Did you know that when you create a variable reference in GTM with {{Variable Name}} and you then change the variable name to {{Some Other Variable Name}}, all references are automatically updated?\nWell this applies to Custom HTML tags, too, so you don\u0026rsquo;t have to worry about syntax errors when renaming variables. The new name will be automatically applied to all places in the container where the variable is referred to.\n Built-in variables 71. Need to be enabled The only Built-in Variables enabled by default in your web container are Event, Page URL, Page Path, Page Hostname and Referrer.\nTo enable others, go to Variables and click the red CONFIGURE under the heading \u0026ldquo;Built-In Variables\u0026rdquo;.\n  In the overlay that flies out, check the box next to each Built-in variable you want to enable. Only after enabling the variables will they be available in variable selection drop-downs.\n72. Click and Form variables are copies of each other You might have noticed that there\u0026rsquo;s a very similar set of six auto-event Built-in variables: Click/Form Element, Click/Form ID, Click/Form URL, Click/Form Target, Click/Form Classes, Click/Form Text.\nThese are identical in functionality, so you really only need to enable either Click or Form Built-in variables.\n Custom JavaScript variables 73. Must be anonymous functions When you create a Custom JavaScript variable, it should be an anonymous function. Yes, you can name it, but it doesn\u0026rsquo;t matter since there\u0026rsquo;s no way to call the variable with its name, as it\u0026rsquo;s locally scoped to whatever execution context invokes the variable when your container needs to do so.\n// Not ideal: function clickTextLowercase() { return {{Click Text}}.toLowerCase(); } // Use this: function() { return {{Click Text}}.toLowerCase(); }  To avoid any potential namespace conflicts, just use an anonymous function, since you call GTM\u0026rsquo;s variables with the syntax and not with whatever name you give the method itself.\n74. Must have a return statement All Custom JavaScript variables must have a return statement or Google Tag Manager will throw an error when you try to create a version of the container.\n// Will not work: function() { if (true) {} } // Works: function() { if (true) {} return; }  75. Can be used to return another function Remember that a Custom JavaScript variable is just regular JavaScript, and as such you can use it to return another function. This is also called returning a closure.\nFor example, let\u0026rsquo;s say I want to create a utility function that takes a string as an argument and reverses all letters in it. First, I\u0026rsquo;d create a Custom JavaScript variable like this:\nfunction() { return function(str) { return str.split(\u0026#39;\u0026#39;).reverse().join(\u0026#39;\u0026#39;); }; }  Then, when I want to use this utility in a Custom HTML tag or another variable, I can pass any string to the GTM variable as an attribute:\n\u0026lt;script\u0026gt; (function() { // Create a reference to the Custom JavaScript variable we just created  var reverseString = {{JS - ReverseString}} window.dataLayer.push({ event: \u0026#39;reverseComplete\u0026#39;, reversedString: reverseString(\u0026#34;Reverse me!\u0026#34;) }); })(); \u0026lt;/script\u0026gt; This Custom HTML tag calls the function returned by the Custom JavaScript variable with the string \u0026ldquo;Reverse me!\u0026rdquo;, resulting in a dataLayer.push() where the key reversedString will have the value \u0026ldquo;!em esreveR\u0026rdquo;.\n76. Should avoid side effects Custom JavaScript variables should avoid side effects. In other words, they shouldn\u0026rsquo;t do anything except process an input and produce an output. Any transformations or mutations should happen in local scope alone.\nIf you don\u0026rsquo;t respect this, you might run into all sorts of issues due to the fact that Google Tag Manager\u0026rsquo;s resolution of variables is unpredictable and can\u0026rsquo;t be counted on to only happen once per tag.\nHere are some things to avoid:\n  Modifying, creating, or deleting items in the global window namespace.\n  Creating or processing custom HTTP requests.\n  Setting or removing items in globally available APIs, such as dataLayer, google_tag_manager, document.cookie and localStorage.\n  If you do want to modify global state, use Custom HTML tags or closures instead.\n77. Can refer to other variables This might seem like a no-brainer, but you can freely refer to other variables in Custom JavaScript variables, too.\nfunction() { var hostname = {{Page Hostname}}; return hostname === \u0026#39;www.domain.com\u0026#39; ? \u0026#39;Main domain\u0026#39; : \u0026#39;Other domain\u0026#39;; }   Tag sequencing 78. Setup and cleanup are fired with the main tag, regardless of their own triggers When you add a Setup or Cleanup tag to a sequence with a main tag, the Setup and Cleanup will be fired with the main tag regardless of what triggers they might actually have themselves.\nSo here\u0026rsquo;s a tip: When creating Setup and Cleanup tags, use them only for that purpose and do not add any triggers to them. This way you will never run the risk of these tags firing when they shouldn\u0026rsquo;t, as they are strictly bound to whatever sequence they are in.\n79. Use onHtmlSuccess() and onHtmlFailure() to signal that the sequence can continue When you have a Custom HTML tag as a Setup or Main tag, you can let Google Tag Manager know if the tag completed successfully or failed by using the onHtmlSuccess() and onHtmlFailure() methods.\nFor these to work, you first need to enable Built-in variables Container ID and HTML ID.\nHere\u0026rsquo;s an example of a Custom HTML Setup tag, where we wait for the asynchronous POST request to complete before telling GTM to continue to the main tag. If the asynchronous request fails, we tell GTM that the Setup tag was a failure.\n\u0026lt;script\u0026gt; (function($) { $.post(\u0026#39;/test.php\u0026#39;) .done(function() { google_tag_manager[{{Container ID}}].onHtmlSuccess({{HTML ID}}); }) .fail(function() { google_tag_manager[{{Container ID}}].onHtmlFailure({{HTML ID}}); }); })(jQuery); \u0026lt;/script\u0026gt; If you didn\u0026rsquo;t have the onHtmlSuccess/Failure() methods there, Google Tag Manager would simply signal completion as soon as it reaches the last line of code in the tag. Thus the browser won\u0026rsquo;t wait for any asynchronous requests to complete, and you might end up with a nasty race condition if you need the request to complete before proceeding with the Main tag.\n80. Use dataLayer.set() to change Data Layer variable values mid-sequence If you want to change some value in GTM\u0026rsquo;s Data Layer while in the middle of a sequence, you can\u0026rsquo;t use dataLayer.push(), because GTM freezes its state for the duration of each message that is pushed into dataLayer.\nHowever, there\u0026rsquo;s a workaround. You CAN update the value of GTM\u0026rsquo;s Data Layer variables, even though the same dataLayer message is still being processed. To do this, you need to use the google_tag_manager[{{Container ID}}].dataLayer.set('keyName', 'value') interface you learned of here.\n Tag settings 81. Once per page is once per page load, once per event is once per GTM event There are three options in the Tag firing options menu that you can find at the end of each tag\u0026rsquo;s settings.\n  Unlimited means the tag will fire whenever its triggers fire - no restrictions.\nOnce per event means that the tag will fire just once per the trigger event that caused it to fire. Thus if you have, for example, multiple Click triggers attached to the tag, and there\u0026rsquo;s a chance that some of these have overlapping trigger conditions, Once per event ensures that the tag fires just once per click.\nOnce per page means that the tag will fire only once per page load. No matter what triggers you have attached to it - if the tag fires on the page, it will not fire again until the page is reloaded from the web server. Tag Sequencing respects this, too, so if a Setup or Cleanup tag are set to fire just once per page, they will not fire multiple times even if the main tag does.\n82. Tag priority is for a single GTM event - doesn\u0026rsquo;t necessarily mean tags are completed in the given order The Tag priority field can be used to establish an order execution for tags that fire on the same trigger event. When a trigger event happens, Google Tag Manager executes tags in order of priority.\nNote that Tag priority only establishes the order in which tags begin execution. It has no implications on when they complete. Thus, even if a tag starts its execution first, it might have a long, complex, asynchronous process involved, and it still ends up completing only after tags later in the priority order have already finished.\n Workspaces 83. You will always have at least one workspace Workspaces have come to stay, and it\u0026rsquo;s impossible to use Google Tag Manager without a workspace.\nThus, even if you try to delete all workspaces, GTM will always leave you with one.\n84. When you create a version out of a workspace, that workspace is deleted Workspaces are ephemeral - they only exist until a version is created from them. Once you create a version, it\u0026rsquo;s as if the workspace never existed. Thus workspaces shouldn\u0026rsquo;t be used as permanent container subsets that you could, for example, delegate to a certain user group only. Instead, workspaces should be approached as branches of a version control system. When you start working on a new feature, create a workspace first so that any changes you make are contained until you choose to create a version and merge the changes to the latest container version.\n85. You don\u0026rsquo;t have to update your workspace until you are ready to create a version When someone else updates the latest container version, all the workspaces in the container need to be updated. You\u0026rsquo;ll know this has happened when you see the following notice in the GTM UI:\n  You don\u0026rsquo;t have to update the workspace the minute you see this warning. You can bide your time, and only sync the changes once you are ready to create a version out of your workspace.\nNevertheless, the sooner you update the better. Why? Because once merges start piling up, you\u0026rsquo;ll have a hard time resolving all the conflicts at once.\n AMP container 86. No Custom JavaScript or Custom HTML Accelerated Mobile Pages place some pretty strict restrictions on what you can use Google Tag Manager for. You can\u0026rsquo;t execute arbitrary JavaScript code anymore, nor can you simply inject HTML via Google Tag Manager. In fact, all dynamic operations need to be handled server-side in GTM, because all that GTM returns is the AMP configuration JSON object, which has very limited capabilities for any advanced tracking.\nSo no Custom JavaScript variables or Custom HTML tags in the AMP container, unfortunately.\n87. AMP is very restricting As said, AMP is very restricting. Only JavaScript sanctioned by AMP and provided as an AMP module can be executed. This means that the development of the GTM AMP container is bound very closely to the roadmap of AMP itself. Any attempts to deviate would result in an invalid configuration JSON, which would, in turn, lead to an invalid AMP page.\nIt\u0026rsquo;s a good idea to closely follow the amp-analytics project, as it contains the latest release details for the amp-analytics module. This is the module used by Google Tag Manager\u0026rsquo;s AMP container.\n88. Client ID is (currently) ambiguous Universal Analytics\u0026rsquo; Client ID is problematic with AMP, especially when run through the GTM container. There are actually four different Client ID scenarios for AMP analytics.\n  When visiting the regular site (no AMP): _ga cookie stores the Universal Analytics Client ID.\n  When visiting the regular site (AMP): AMP_ECID_GOOGLE cookie stores a randomly generated AMP Client ID.\n  When visiting the site via the AMP cache: the AMP client ID is stored in localStorage on the ampproject.org domain.\n  When visiting the site via Google search: the AMP client ID is stored on google.com and ampproject.org in localStorage.\n  In all of these scenarios, it\u0026rsquo;s possible that the Client ID is different, which means that you\u0026rsquo;ll have quite a bit of difficulty in tracking the same user across AMP and non-AMP sites, or between Google search and direct access to the site.\nSome time ago, Dan Wilkerson and I wrote a hack around this, which does require some web server muscle, but it will make sure that all the scenarios above use the same Universal Analytics client ID in the _ga cookie.\n89. New triggers, e.g. scroll AMP introduces a bunch of really useful triggers, which I hope we\u0026rsquo;ll see in GTM for web soon, too.\n  Since you can\u0026rsquo;t execute arbitrary JavaScript, AMP provides stuff like scroll and visibility tracking out-of-the-box with amp-analytics. Google Tag Manager gives you a way to easily configure the triggers, but it should be noted that there\u0026rsquo;s not much room to configure the triggers. AMP is restricted in this way, too.\n  For example, I would like to configure the scroll trigger to track scroll-to-element rather than scroll-to-percentage. Unfortunately, this is not possible.\n90. Can be augmented with the on-page JSON configuration Remember that even if you use the AMP Google Tag Manager container, you can also add a JSON configuration block to the page. You can use this extra configuration to complement the tracking you setup via Google Tag Manager.\n... \u0026lt;!-- AMP Analytics --\u0026gt;\u0026lt;script async custom-element=\u0026#34;amp-analytics\u0026#34; src=\u0026#34;https://cdn.ampproject.org/v0/amp-analytics-0.1.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;amp-analytics config=\u0026#34;https://www.googletagmanager.com/amp.json?id=GTM-XXXXX\u0026amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt;\u0026lt;/amp-analytics\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;pageType\u0026#34;: \u0026#34;profile\u0026#34;, \u0026#34;userType\u0026#34;: \u0026#34;member\u0026#34; } } \u0026lt;/script\u0026gt; This is a pretty simple way to add some web server logic to your GTM AMP container.\n GTM for mobile 91. Not the same as GTM for web Let\u0026rsquo;s address the elephant in the room first. If AMP was restricted, then GTM for mobile is doubly so. Why? Because you\u0026rsquo;re no longer working with a website that self-corrects any JavaScript mistakes you happen to have in the site code. When working with mobile apps, the app needs to be well-formed without exceptions, and you can\u0026rsquo;t add runtime code to it without it being sanctioned by the platform the app is running on.\nAlso, since this isn\u0026rsquo;t the web anymore, there\u0026rsquo;s no \u0026ldquo;Custom HTML\u0026rdquo; or \u0026ldquo;Custom JavaScript\u0026rdquo; here, either. Android apps use Java, and iOS relies on Swift and Objective-C.\nDue to these restrictions, it\u0026rsquo;s a valid concern to raise whether you need Google Tag Manager at all. It takes pretty much the same effort as simply using the analytics SDK directly.\n  The benefit of using GTM is that it will always leverage the latest analytics SDKs. Also, with the Firebase integration you have access to the entire Firebase suite of services, and most of them are really useful for app developers.\nFinally, GTM is still GTM. You can implement tracking tags without having to worry too much about how the app itself works. Support for different tag templates isn\u0026rsquo;t too impressive yet, but this will surely change in the future.\n92. An SDK you need to download and add to the project Google Tag Manager isn\u0026rsquo;t a container file that is just fetched from the web and then everything works nicely. No, it\u0026rsquo;s actually a combination of SDKs (software development kit) and container binaries / JSON files that you need to integrate into the app itself.\nWith the latest version of Google Tag Manager for mobile (Firebase), implementing GTM to a project is really simple. You basically need to include just two podfiles: Firebase/Core and GoogleTagManager. The rest is added automatically via dependencies.\n  The app fetches the latest container version if a \u0026ldquo;fresh\u0026rdquo; container is available. You can\u0026rsquo;t always trust that a fresh container can be fetched, so it\u0026rsquo;s a good idea to always keep a recent version of the container in the app assets. So, when publishing a new release of the app, make sure to check if there\u0026rsquo;s an updated container version that needs to be added to the project before going live.\n93. GA tracker object is not exposed Google Tag Manager doesn\u0026rsquo;t expose the GA tracker object in the same way that GTM for web does. In other words, if you want to check what the user\u0026rsquo;s Client ID is, for example, there\u0026rsquo;s no command that lets you dig that information from the available GTM interfaces.\nA hacky but working solution in these cases is to create a dummy tracker using the regular Google Analytics SDK, and mining tracker-specific information from this to be used in your Google Tag Manager setup.\n94. GTM legacy uses dataLayer GTM for mobile, before Firebase, is now called \u0026ldquo;Legacy\u0026rdquo;. If you\u0026rsquo;re used to GTM for web, the legacy SDK should be quite familiar to you, since it also uses a dataLayer construct to pass messages to Google Tag Manager. As before, there is the event key you need to use to fire the triggers, and there are Data Layer Variables you can use to access values in this message queue.\n  Note that unlike with GTM for web, there is no differentiation between dataLayer the message queue and GTM\u0026rsquo;s internal data model. The dataLayer structure acts as both.\n95. Latest version of GTM for mobile uses Firebase The most recent incarnation of GTM for mobile uses Firebase. Firebase is a cloud-based application framework that provides a number of services your apps can use, such as Firebase Analytics.\n  When you want to use GTM with Firebase, you need to actually use Firebase. You add tracking using regular Firebase Analytics tracking methods, which means that unless you use GTM to radically change the tracking, you will end up using Firebase Analytics in addition to the other analytics tools you want to track via Google Tag Manager.\n96. You can intercept Firebase events using GTM The way that Google Tag Manager works with Firebase is that it listens to all the events you log into Firebase. Once it detects an event, it goes through the triggers and tags you\u0026rsquo;ve configured in the container, and if a tag is set to fire on a specific Firebase event, it will go off.\n  And as with Google Tag Manager for web, the more tag endpoints you have, the more useful GTM becomes. You can use a single event stream (the logged Firebase events) to send data to multiple endpoints, such as Google Analytics, Firebase, and AppsFlyer.\n97. Firebase GTM has (imperfect) support for GA Enhanced Ecommerce At the time of writing, you can\u0026rsquo;t configure Universal Analytics Ecommerce tags via Firebase GTM. The reason is that GTM\u0026rsquo;s data model does not support array structures, which is how you\u0026rsquo;d need to send product data to Google Analytics.\nHopefully we\u0026rsquo;ll see support for Ecommerce soon. Until then, if you need to collect Ecommerce data to Google Analytics, you might want to use the GTM Legacy SDK or GA SDK.\nUpdate: Support for Enhanced Ecommerce has finally reached Firebase (iOS and Android). Still missing product-scoped custom dimensions and metrics, though! (Thanks Yuhui for the tip in the comments).\n Other stuff 98. Debugging is a complex process End-to-end debugging with Google Tag Manager isn\u0026rsquo;t just a case of opening up Preview mode and being satisfied with what you see. It\u0026rsquo;s a complex process, involving not only front-end conflicts in your site code, but also things like race conditions, unresponsive tag endpoints, and dataLayer automated tests.\nI recommend you take a look at this article: #GTMTips: Debugging Tag Execution Properly. Make sure you familiarize yourself with the Network tab of your browser\u0026rsquo;s developer tools. If Preview mode says that your tags fire but you don\u0026rsquo;t see any evidence in the endpoint you are looking at (e.g. Google Analytics reports), take a look at the Network tab to see if you are dispatching hits to /collect correctly. Google Tag Assistant Recordings is your friend, too, for all Google-related tagging issues.\nFinally, I recommend adding dataLayer to your organization\u0026rsquo;s quality assurance process. This would mean writing unit tests and browser tests that verify dataLayer has a predictable composition with each new release of the site or app. I\u0026rsquo;ve written a simple framework for running automated tests against dataLayer, and you can read about it here.\n99. Load sequence of GTM\u0026rsquo;s default events is important to understand When Google Tag Manager loads on a site, it pushes three default events into dataLayer: gtm.js, gtm.dom and gtm.load.\ngtm.js is pushed in the container snippet itself. Thus it introduces a state which contains all the Data Layer variables pushed before or at the time when the container snippet runs. This event is used by the Page View trigger. In other words, if you have tags that fire on the Page View trigger (or All Pages), any Data Layer variables you want to use need to be pushed into dataLayer before the container snippet is executed by the browser. Typically this means to physically add your dataLayer initialization in the page template above the GTM container snippet.\n\u0026lt;script\u0026gt; // Always use this syntax to initialize dataLayer  window.dataLayer = window.dataLayer || []; window.dataLayer.push({ userType: \u0026#39;member\u0026#39;, loyaltyLevel: \u0026#39;premium\u0026#39; }); \u0026lt;/script\u0026gt; ... \u0026lt;script\u0026gt; // GTM container snippet here \u0026lt;/script\u0026gt; gtm.dom is pushed into dataLayer when the browser signals the DOMContentLoaded event has taken place. This browser event happens when the browser has read the HTML template and built the Document Object Model (DOM) using the structure within. Thus, if you have tags which refer to DOM elements, you might want to make sure they don\u0026rsquo;t fire before the gtm.dom event. The trigger which uses this event is DOM Ready, so any tags firing on this trigger will have access to any elements described in the page HTML.\ngtm.load is pushed into dataLayer when the browser signals the load event, which means that the entire page, and all linked assets such as images, scripts (both asynchronous and synchronous), and videos have completed downloading. This is the event you\u0026rsquo;d need to use if you want your tags to access the result of some asynchronous process, such as the download of the jQuery library (if downloaded asynchronously, as you should). The trigger which uses this event is Window Loaded.\nIt\u0026rsquo;s important to understand how these three events work. They are not fired at the exact time the underlying browser event takes place. For example, gtm.js will fire triggers only after the GTM library has downloaded, and this might be seconds after the browser originally reads the event in the page template. Thus these events don\u0026rsquo;t describe the time something happens but rather the state. gtm.js describes the state of GTM\u0026rsquo;s data when the container snippet was first read by the browser, gtm.dom reflects GTM\u0026rsquo;s state when the browser signalled DOMContentLoaded, and gtm.load is the state of GTM when the entire page had completed loading.\n100. Some ad and content blockers block GTM from loading Call it a feature or symptom of today\u0026rsquo;s web browsing behavior, but ad and content blockers are making life for web analysts difficult. Firefox, for example, offers Tracking Protection out-of-the-box, and it blocks Google Analytics by default.\nPopular browser extensions like Ghostery and AdBlock Plus make it easy to block Google Tag Manager, too.\nWhether you like this or not, it\u0026rsquo;s a common practice and you should therefore accept a certain level of inaccuracy in your data.\nWith Google Tag Manager, however, you can actively modify and extend the site experience with Custom JavaScript. If the user\u0026rsquo;s browser doesn\u0026rsquo;t load Google Tag Manager, these modifications will not be executed.\nThus, be very wary of potential GTM blocking when running code via Google Tag Manager. Never rely on GTM to handle your link redirects or form submissions, and never use GTM to fix on-site issues that should be fixed in front-end code. You are depriving an ever-increasing subset of your visitors of a full experience, and at worst destroying the site user experience altogether for them.\n 101. Use undefined to clear individual keys from Data Layer (upepo mwindaji) The first guest tip comes from upepo mwindaji. You can clear individual values from GTM\u0026rsquo;s data model by pushing the keys with the value undefined. This effectively resets the value of the key.\nwindow.dataLayer.push({ event: \u0026#39;userLoggedOut\u0026#39;, loginStatus: \u0026#39;loggedOut\u0026#39;, userId: undefined, memberStatus: undefined });  The code above, for example, clears the values of userId and memberStatus from GTM\u0026rsquo;s data model because the user logged out.\n102. You can track to multiple Universal Analytics properties in the same container This guest tip is from samgabell. Since GTM creates a unique tracker with every Universal Analytics tag, you can freely track to multiple Universal Analytics properties in the same container. There will be no interference between sets of tags tracking to different properties.\nThe only thing that will cause issues is if one set of tags is configured for cross-domain tracking with the allowLinker field set to true. In this case, a linker parameter in the URL will overwrite the Client ID stored in the _ga cookie, impacting all trackers on the site - even those that should ignore cross-domain tracking. You can read about a solution to this issue here.\nSummary So that was my 100 tips. Some of them were indeed very simple and obvious, but I\u0026rsquo;ve run into enough people having issues with all of the things covered here to justify my adding them to the list.\nGoogle Tag Manager turns 5 years old this year. It\u0026rsquo;s been a colorful journey, and I\u0026rsquo;m sure the popularity of the tool has changed the landscape for all tag management solutions for the better. Via GTM, so many new people have had the pleasure to (or have been forced to) get acquainted with the wonderful world of JavaScript, and have thus accumulated new skills to help them on their digital journeys.\nNow is your turn - are there invaluable tips that you\u0026rsquo;d want to share with the readers? I\u0026rsquo;d love to add them to the end of the list. Thank you!\n"
},
{
	"uri": "https://www.simoahava.com/tags/tips/",
	"title": "Tips",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/mixcloud/",
	"title": "mixcloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/mixcloud-player-integration-google-tag-manager/",
	"title": "Mixcloud Player Tracking In Google Tag Manager",
	"tags": ["api", "Google Tag Manager", "JavaScript", "mixcloud"],
	"description": "How to setup embedded MixCloud player tracking in Google Tag Manager.",
	"content": "A couple of years ago I wrote an article on tracking interactions with the SoundCloud widget via Google Tag Manager. When a platform provides a JavaScript embed API, it\u0026rsquo;s surprisingly easy to track interactions with the player. You\u0026rsquo;ve seen this with YouTube, with SoundCloud, with JWPlayer, and now you\u0026rsquo;ll see how to do this with the Mixcloud player.\n  If you don\u0026rsquo;t know what Mixcloud is, well it\u0026rsquo;s a hugely popular streaming service for DJs, podcasts, radio shows, and other published radio media. Since they offer a handy embed solution for any show you like, it makes sense to start tracking if people are actually using the widget, right?\nHow to set it up First, you need to add the player widget to your site. To do this, select the show you want to embed, and click the Share button.\nAn overlay should open with both a Share and an Embed option. Choose the latter.\nSetup the player visuals however you wish. Once you\u0026rsquo;re done, uncheck the box next to Wordpress, after which the text box should contain an iframe embed tag.\n  This iframe tag is what you need to add to your website. It\u0026rsquo;s important that you use the exact format provided, so that the JavaScript API works without any problems.\nOne thing you\u0026rsquo;ll want to add to the tag, however, is a unique ID attribute. This makes it MUCH easier to target the correct player. So, if the iframe looks like this:\n\u0026lt;iframe width=\u0026#34;100%\u0026#34; height=\u0026#34;60\u0026#34; src=\u0026#34;https://www.mixcloud.com/widget/iframe/?feed=https%3A%2F%2Fwww.mixcloud.com%2Fdj_umek%2Fbehind-the-iron-curtain-with-umek-episode-294%2F\u0026amp;hide_cover=1\u0026amp;mini=1\u0026amp;light=1\u0026#34; frameborder=\u0026#34;0\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; Add id=\u0026quot;myPlayer\u0026quot; like this:\n\u0026lt;iframe id=\u0026#34;myPlayer\u0026#34; width=\u0026#34;100%\u0026#34; height=\u0026#34;60\u0026#34; src=\u0026#34;https://www.mixcloud.com/widget/iframe/?feed=https%3A%2F%2Fwww.mixcloud.com%2Fdj_umek%2Fbehind-the-iron-curtain-with-umek-episode-294%2F\u0026amp;hide_cover=1\u0026amp;mini=1\u0026amp;light=1\u0026#34; frameborder=\u0026#34;0\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; Now this particular player can be identified with the myPlayer identifier.\nThe Google Tag Manager Custom HTML Tag The whole tracking solution is contained within a single Custom HTML Tag. So, create a new Custom HTML Tag, and add the following code within:\n\u0026lt;script src=\u0026#34;//widget.mixcloud.com/media/js/widgetApi.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; (function() { /* Change the CSS selector within document.querySelector to capture * the correct player iframe. * * Available events are \u0026#39;progress\u0026#39;, \u0026#39;buffering\u0026#39;, \u0026#39;play\u0026#39;, \u0026#39;pause\u0026#39;, * \u0026#39;ended\u0026#39; and \u0026#39;error\u0026#39;. If you want to stop tracking any one of these, * just remove them from the events Array. */ var playerIframe = document.querySelector(\u0026#39;#myPlayer\u0026#39;), events = [\u0026#39;progress\u0026#39;, \u0026#39;buffering\u0026#39;, \u0026#39;play\u0026#39;, \u0026#39;pause\u0026#39;, \u0026#39;ended\u0026#39;, \u0026#39;error\u0026#39;]; if (playerIframe) { var player = Mixcloud.PlayerWidget(playerIframe); var key = \u0026#39;\u0026#39;; var sendDataLayer = function(event, key, currentProgress) { window.dataLayer.push({ event: \u0026#39;mixcloud\u0026#39;, mixcloudEvent: { name: event, key: key, progress: currentProgress } }); }; player.ready.then(function() { events.forEach(function(event) { try { player.events[event].on(function(progress, duration) { player.getCurrentKey().then(function(key) { var currentProgress; if (progress \u0026amp;\u0026amp; duration) { if (progress === Math.round(duration * 0.25)) { currentProgress = \u0026#39;25%\u0026#39;; } else if (progress === Math.round(duration * 0.5)) { currentProgress = \u0026#39;50%\u0026#39;; } else if (progress === Math.round(duration * 0.75)) { currentProgress = \u0026#39;75%\u0026#39;; } } if (event !== \u0026#39;progress\u0026#39; || !!currentProgress) { sendDataLayer(event, key, currentProgress); } }); }); } catch(e) {} }); }); } })(); \u0026lt;/script\u0026gt; Set this tag to fire on a Page View / DOM Ready trigger, and if you only want to fire this on certain pages, you can modify the trigger accordingly. Here\u0026rsquo;s an example:\n  How it works This tracker adds a listener to the given player for all the events you specified in the events Array. Whenever such an event is registered by the page, a dataLayer.push() takes place with the following structure:\n{ event: \u0026#39;mixcloud\u0026#39;, mixcloudEvent: { name: \u0026#39;\u0026#39;, key: \u0026#39;\u0026#39;, progress: \u0026#39;\u0026#39; } }  As you can see, the progress event measures how far along the show the user is. It has triggers for 25%, 50% and 75%. I didn\u0026rsquo;t add one for 100%, as the ended event is the same thing.\nTriggers and variables Next thing you\u0026rsquo;ll need to do is create a Custom Event trigger for the mixcloud event. It\u0026rsquo;s dead easy, and the trigger should look like this:\n  Finally, you\u0026rsquo;ll need three Data Layer variables. One for mixcloudEvent.name, one for mixcloudEvent.key, and one for mixcloudEvent.progress.\n  Now you should have everything you need to start tracking interactions with the widget.\nA typical Google Analytics Event tag might look like this:\n  For example, when a progress event for hitting the half-way mark is recorded, the event would look like this:\nEvent category: Mixcloud\nEvent action: progress: /syte/subfm170217/\nEvent label: 50%\nYou can make it even more versatile with Custom Dimensions and Custom Metrics.\nSummary This was (hopefully) a simple tip to get you along with Mixcloud player tracking. With a robust JavaScript API it really is easy to do.\nThe Mixcloud JavaScript API works with promises, so the functional approach exhibited in the Custom HTML Tag might look messy, and as such, there is room for improvement via refactoring.\nIf you have more than one widget on the page that you want to track, you\u0026rsquo;ll need to duplicate the relevant code for each player on the page. For example:\n... var player = document.querySelector(\u0026#39;#myPlayer\u0026#39;); var player2 = document.querySelector(\u0026#39;#myPlayer2\u0026#39;); ... if (player) { player.ready.then(function() { ... }); } if (player2) { player2.ready.then(function() { ... }); } ...  Have fun listening and tracking those tracks!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtmtips-test-multiple-urls-triggers/",
	"title": "#GTMTips: Test Against Multiple URLs In Your Triggers",
	"tags": ["Google Tag Manager", "gtmtips", "trigger"],
	"description": "Test current page URL against a list of URL strings when using Google Tag Manager&#39;s triggers.",
	"content": "The beauty of #GTMTips, at least how I\u0026rsquo;ve envisioned them, is that they can be really simple or crazy complex. The important thing is that the idea is conveyed clearly enough. That\u0026rsquo;s why so many of these tips have originated from discussions in our Google Tag Manager Google+ community, the Product Forums, and in private email correspondence with people asking for help. Today\u0026rsquo;s tip, for example, originated from a back-and-forth with Süleyman Okan, so thank you for the inspiration!\nThis tip lets you take list of seemingly unrelated URLs, and group them together so that any given trigger will only activate if the current page URL contains one of these URL strings.\nTip 55: Test current page URL against a list of URL strings   First, why is this necessary? Well, let\u0026rsquo;s say you have the following URLs, and you want your trigger to only fire when the page the visitor is on matches one of these:\n /page/login.html /login/ /my-account/ mydomain.com/users/signin/ mydomain.co.uk/en/profile/  Your first impulse would be to create a trigger that looks like the following. This will not work, because in a trigger with multiple conditions, all of the conditions must match.\n  So unless the current URL looks something like http://www.mydomain.com/users/signin/login/my-account/mydomain.co.uk/en/profile/page/login.html, this trigger will not work.\nAn alternative would be to write a long regular expression, since in regular expressions you can indicate optional patterns with the pipe symbol (|):\nPage URL matches RegEx (ignore case) /page/login.html|/login/|/my-account/|mydomain.com/users/signin/|mydomain.co.uk/en/profile/\nBut the problem with this approach is that the trigger itself becomes quite difficult to maintain, as any changes would require that you dig in to this single field value.\nThe third option would be to create a trigger for each of the URL variations individually (or maybe you can save some effort by grouping logically similar URLs together), but this is cumbersome to maintain as soon as you have more than one tag that needs these triggers.\nSo, my suggestion is to leverage a Custom JavaScript Variable to check against a list of URLs that you provide, and then it will return true if the current URL contains any one of the URL strings in the list, or false otherwise. This is what the variable might look like:\nfunction() { var urlsToTest = [ \u0026#39;/page/login.html\u0026#39;, \u0026#39;/login/\u0026#39;, \u0026#39;/my-account/\u0026#39;, \u0026#39;mydomain.com/users/signin/\u0026#39;, \u0026#39;mydomain.co.uk/en/profile/\u0026#39; ]; for (var i = 0; i \u0026lt; urlsToTest.length; i += 1) { if (document.location.href.indexOf(urlsToTest[i]) \u0026gt; -1) { return true; } }; return false; }  This code was updated thanks to David Porter in the comments below. The original code had some flaws which prevented this from working.\nIn the urlsToTest array, you provide a comma-separated list of strings that you want to test the current URL against. They can be just page paths, or they can even be complete URLs with protocol, domain name, query string, and hash in place.\nThe variable loops through each member of the list, and if the current page URL contains any one of these URL strings, the function returns true.\nTo include this in your triggers, all you need is something like:\n  Here, I\u0026rsquo;ve named the variable {{URL is login page}}, and you can see how easy it is to add this as a trigger condition.\nAnd that\u0026rsquo;s all there is to it! There are many ways to make this even more useful, such as changing from a string method (indexOf()) to regular expressions (match()). This way you\u0026rsquo;ll have more power over things like case-sensitivity, or whether the URL should match the string from the beginning, from the end, or from anywhere.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/capturing-the-correct-element-in-google-tag-manager/",
	"title": "Capturing The Correct Element In Google Tag Manager",
	"tags": ["css", "custom javascript", "dom", "Google Tag Manager"],
	"description": "A simple DOM traversal method to find the correct element when some other element is clicked using Google Tag Manager.",
	"content": "Google Tag Manager provides us with a bunch of handy triggers, designed to make capturing user interactions on the website much easier. These triggers are part of a paradigm called auto-event tracking, which comprises the Click, Form, History, and JavaScript Error trigger types.\n  Now, I\u0026rsquo;ve covered GTM\u0026rsquo;s triggers many, many times before. If you need a refresher, take a look at the following articles:\n Trigger Guide For Google Tag Manager Auto-Event Tracking In GTM 2.0 Fix Problems With GTM Listeners Why Don\u0026rsquo;t My GTM Listeners Work?  In this article, I want to tackle a specific aspect of Click triggers. If you\u0026rsquo;ve used them before, you\u0026rsquo;ll know (at least) two things:\n  The Just Links trigger captures the link element (\u0026lt;a/\u0026gt;) that is the nearest wrapping link of the clicked element.\n  The All Elements trigger captures the clicked element itself.\n  In other words, the Just Links trigger actually climbs up the document until it finds a link element, and returns that for GTM to process. The All Elements acts more like a regular handler: it simply returns the element that was clicked.\nIn this article, I want to show you how to leverage especially the All Elements trigger more effectively. Basically, you might want to use the All Elements trigger, but then mimic the Just Links trigger to find some nearest wrapping element, and return that for GTM to use.\nWhy use All Elements? Well, there\u0026rsquo;s one obvious reason: There are all sorts of elements on the page you might want to track. Even though links are a very popular tracking item, there are many elements that have nothing to do with links.\n  Perhaps you want to track a button, or a form field, or an image. In these cases, you will need the All Elements trigger.\nAnother reason you might not be aware of is very significant: The All Elements trigger uses the capture phase of the event path. The Just Links trigger, as well as pretty much all other GTM triggers, utilize the bubble phase.\nAnd why is this significant? Because event propagation is most often cancelled in the bubble phase! Many people have come across forms and links that simply refuse to fire their respective triggers in Google Tag Manager. The reason is that other JavaScript on the site can prevent the event from climbing up the document to where Google Tag Manager\u0026rsquo;s listeners are waiting.\nBut when you use the All Elements trigger, you are using the capture phase, which is cancelled far less often.\nIn other words, you can actually use the All Elements trigger to apply link tracking for links that do not create a GTM event when using the Just Links trigger!\nThe only problem with this approach is that the All Elements trigger returns the actual element that was clicked. For example, let\u0026rsquo;s say you have HTML like this on the site:\n\u0026lt;div id=\u0026#34;contact-us\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;mailto:simo@example.com\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Send me mail\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; If you used a Just Links trigger, and event propagation wasn\u0026rsquo;t stopped, clicking the link will pass the \u0026lt;a href=\u0026quot;mailto:simo@example.com\u0026quot;/\u0026gt; to Google Tag Manager as the target element of the event. But if you\u0026rsquo;re using the All Elements trigger, GTM captures the \u0026lt;span/\u0026gt;, because it\u0026rsquo;s what the click actually landed on!\nSo how do we get the benefit of All Elements with its capture phase handler, but still be able to capture any element up the DOM tree?\nWith some Custom HTML Tag and Custom JavaScript variable magic, of course!\nThe solution The solution comes in two parts. First, we need to create a Custom HTML Tag that introduces a polyfill (read: workaround) for older browsers which might not support the method we\u0026rsquo;re going to use.\nNext, we\u0026rsquo;ll use a Custom JavaScript variable to create a generic function to which we can pass a CSS selector. This function, in turn, climbs up the document tree and returns the element that matches the selector.\nNote that you can achieve most of this easily with jQuery or something similar. However, I want to show the native JavaScript way of doing it, since it\u0026rsquo;s not actually that complex.\nThe Custom HTML Tag The Custom HTML Tag should fire as early as possible on the page, so you can use the All Pages trigger, if you wish. This is what it holds:\n\u0026lt;script\u0026gt; if (!Element.prototype.matches) { Element.prototype.matches = Element.prototype.matchesSelector || Element.prototype.mozMatchesSelector || Element.prototype.msMatchesSelector || Element.prototype.oMatchesSelector || Element.prototype.webkitMatchesSelector || function(s) { var matches = (this.document || this.ownerDocument).querySelectorAll(s), i = matches.length; while (--i \u0026gt;= 0 \u0026amp;\u0026amp; matches.item(i) !== this) {} return i \u0026gt; -1; }; } \u0026lt;/script\u0026gt;  This is a good addition to the site code in general, so if possible to add it to the regular JavaScript of the site, I recommend you do!\nThe polyfill is gratefully copied from the Mozilla Developer Network pages.\nThe script checks if the browser supports the method Element.matches() or any of its alternatives. If no match is found, then it\u0026rsquo;s fixed with a custom method that scrolls through the document elements and checks if any of them matches the given CSS selector.\nOnce this is running, you can start using it!\nThe Custom JavaScript variable To make things work, we need a little utility Custom JavaScript variable. So create a new Custom JavaScript Variable, and name it {{Find closest}}. This is the code you need to put within:\nfunction() { return function(target, selector) { while (!target.matches(selector) \u0026amp;\u0026amp; !target.matches(\u0026#39;body\u0026#39;)) { target = target.parentElement; } return target.matches(selector) ? target : undefined; } }  This function takes two parameters: target and selector. The first is an HTML element such as the {{Click Element}} built-in variable. The latter is a CSS selector string. The CSS selector is what you use to tell GTM the following:\n Starting from target, start climbing up the wrapping DOM structure until you find an element that matches selector. If such an element is found, return it. If such an element is NOT found, return the original event target.\n The idea here is that even though the click landed on the element you wanted to target, you might actually want to access some other element relative to the original event target. For this, the matches() workaround is invaluable, as it lets you traverse element-relative paths.\nHow to use it Once you\u0026rsquo;ve set up the polyfill and the Custom JavaScript Variable, you can leverage the variable in all your Google Tag Manager JavaScript with the following syntax:\nvar elementFound = {{Find closest}}(someHTMLElement, 'someCSSselector');\nFor example, let\u0026rsquo;s say you have an HTML structure that looks like this:\n\u0026lt;div id=\u0026#34;product_12345\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;details\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Product 1\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;link\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;product1.html\u0026#34;\u0026gt; Product 1 \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Now, you\u0026rsquo;ve set up a Click - Just Links or Click - All Elements trigger to capture clicks on the \u0026lt;a/\u0026gt; element. Now, however, you also want to grab the value of the id attribute from the very top-most div (\u0026ldquo;product_12345\u0026rdquo;). To get that, you could use a Custom JavaScript Variable like this:\nfunction() { var el = {{Find closest}}({{Click Element}}, \u0026#39;div[id^=\u0026#34;product\u0026#34;]\u0026#39;); return typeof el !== \u0026#39;undefined\u0026#39; ? el.id : undefined; }  This Custom JavaScript Variable takes {{Click Element}} (the element that was originally clicked), and then starts walking up the DOM until it finds a div whose id attribute begins with the string \u0026ldquo;product\u0026rdquo;. If such an element is found, it returns the value of the id attribute in question. If the element isn\u0026rsquo;t found, it simply returns undefined.\nSummary Traversing the Document Object Model with CSS selectors can be a very good friend to you, indeed. Click handlers are usually too accurate, as they return elements we didn\u0026rsquo;t expect them to return, or they return elements too deep in the DOM to matter.\nThat\u0026rsquo;s why it\u0026rsquo;s good to have a tool that lets you access the DOM structure more deliberately. Traversing the DOM with the {{Find closest}} workaround is a nice way to achieve the type of freedom using native JavaScript that is typically found only within frameworks like jQuery.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/automated-tests-for-google-tag-managers-datalayer/",
	"title": "Automated Tests For Google Tag Manager&#39;s dataLayer",
	"tags": ["Google Tag Manager", "JavaScript", "testing"],
	"description": "An open-source library to help you write automated functional tests against Google Tag Manager&#39;s dataLayer object.",
	"content": "One of the biggest fears I have as a Google Tag Manager user is a broken release of the website (or app) on which I have deployed GTM. Far too often, lack of proper communication practices within an organization lead to a release being pushed out without thoroughly testing how this release impacts any existing tracking solutions.\n  Luckily there are ways to mitigate this. The most significant and impactful precautions you can take are all about process:\n  Be active in the daily development team work.\n  Introduce \u0026ldquo;analytics\u0026rdquo; as an item in the definition of done of each release.\n  Educate the developers on what Data Layer is and how a proper GTM deployment hinges on a stable Data Layer.\n  In addition to these, there are technical measures you can employ to further eliminate the risk of things breaking down when a new release is pushed into the wild.\n  Unit tests should be used to test the code itself, verifying that a given input always produces an expected output. When testing dataLayer, this is especially important for any objects you write into the page template, as they are mostly code-driven and do not depend on user input.\nHowever, if you only ran unit tests it would be difficult to cover the variety of things that can happen in the web browser. That\u0026rsquo;s where functional tests come into play, and those will be the focus of this article.\nWith functional tests, you are actually running tests on a \u0026ldquo;real\u0026rdquo; implementation of the website. Typically this means having a version of the site running on localhost or a staging environment, and then using a \u0026ldquo;virtual\u0026rdquo; browser, typically deployed via a framework such as Selenium, to perform the actual test steps.\nTo save you time and trouble, I have written an open-source, self-contained functional test suite for Google Tag Manager\u0026rsquo;s dataLayer called, imaginatively, gtm-datalayer-test.\n  Visit the GitHub repo for gtm-datalayer-test.\nThe rest of this guide covers how the solution works.\nCredit First credit where credit is due. The solution proposed in this article isn\u0026rsquo;t ground-breaking in any way, shape or form. Using JSON to control processes is very common in the development world, and there are many test frameworks that directly support JSON validation for tests.\nA direct inspiration to this article is Jan Exner\u0026rsquo;s work with a TDD setup for Adobe Analytics and DTM. You should take a look at the related GitHub repo, where you\u0026rsquo;ll find that it has many similarities to gtm-datalayer-test.\nAs often happens, David Vallejo\u0026rsquo;s been an important brainstorming partner for building the test suite. I hope he will be a regular contributor to the project!\nElevator pitch The elevator pitch for this solution is this:\n gtm-datalayer-test is a functional testing solution for the global window.dataLayer queue used by Google Tag Manager. Tests are defined, executed, and reported using a special JSON configuration file, which validates against the latest draft of the JSON Schema standard. The solution can be used to manually or automatically test if window.dataLayer has an expected structure. This is crucial especially when new releases of the website are pushed live.\n 1. Get it up and running First, let\u0026rsquo;s get things up and running. Before you start, you will need the following things:\n  NodeJS and NPM installed (here)\n  Latest version of Java (here)\n  Once you have these installed, you should be able to run the following commands in your terminal and get a valid result for each:\n  java -version\n  npm -v\n  node -v\n    Once you\u0026rsquo;ve installed these successfully, you can get gtm-datalayer-test up and running by executing the following commands. Run the first command (git clone) in a directory where you want the gtm-datalayer-test project directory to be established.\n  git clone https://github.com/sahava/gtm-datalayer-test.git\n  cd gtm-datalayer-test\n  npm install\n  npm test\n  When running the final command, you should see something like this:\n  With git clone ..., you pulled the source code from my GitHub repo into a directory named gtm-datalayer-test, which is where you then navigated to in the second step.\nNext, by running npm install, npm (package manager for Node) runs through all the dependencies listed in the package.json file, and installs them locally to the project. This is very important, as without these dependencies the code itself is useless.\nFinally, by running npm test, you are executing an npm script specifically written for this project. It first starts a simple web server, after which it runs the automated dataLayer tests against a mock index.html file on that server. Finally, it outputs the results of the test into the console.\nThe following chapters explore each of these steps in more detail.\n2. Technology stack Node.js and npm give you a wealth of pre-built modules to work with when building your web application. I mean, you could write everything from scratch, but you\u0026rsquo;ll end up spending more time, energy, and hair trying to do the stuff that existing npm packages already perform much more efficiently.\nThis project uses the following technologies / dependencies / packages:\n  Node.js and npm as the server tech and package manager, respectively.\n  Java to run Selenium.\n  ajv for validating the custom test JSON Schemas.\n  chai as the test assertion library.\n  chai-json-schema to provide custom assertions for JSON Schema validation.\n  chai-subset for performing subset checks against window.dataLayer.\n  http-server to act as a light-weight HTTP server for testing purposes.\n  wdio as a test runner, built on webdriverio.\n  wdio-mocha-framework to enable Mocha as a test framework.\n  wdio-phantomjs-service for installing and running the PhantomJS headless browser.\n  wdio-selenium-standalone-service for installing and running the Selenium framework.\n  wdio-spec-reporter (forked from the original) as a custom spec test reporter.\n  webdriverio for running Selenium via Node.js.\n  You don\u0026rsquo;t have to know what these modules do, but by looking at the list, one thing should jump out: I\u0026rsquo;m gunning for a self-contained solution here. I\u0026rsquo;m sacrificing customization for ease of use, but even in doing so the modularity of gtm-datalayer-test hasn\u0026rsquo;t been compromised. All the packages above contribute to the framework I have built, but nothing\u0026rsquo;s stopping you from extracting just some components and integrating them into your own test suite. In fact, I strongly recommend you do so if you already have a functional testing framework set up!\nAs you might have gathered, the whole thing runs on JavaScript. There\u0026rsquo;s not much you need to customize, but if you do wish to get your hands dirty, some knowledge of JavaScript, especially ECMAScript 6 and Node.js, is required.\n3. How it works Each test setup comprises some moving parts. There\u0026rsquo;s the wdio configuration file you feed to wdio, there\u0026rsquo;s the JavaScript test spec itself, and then there\u0026rsquo;s a configuration JSON you use to define your expectations for each test.\n3.1. wdio configuration file This solution uses WebdriverIO to manage much of the legwork. WebdriverIO offers a bunch of bindings we can use together with the virtual browsers operated by Selenium. Together with Selenium, WebdriverIO gives us all the tools we need to define, execute, and report on tests we want to run on any given application or web property.\nTo make running WebdriverIO as smooth as possible, we use a module called wdio to run the tests. The great thing about wdio is that it\u0026rsquo;s very extendable, and there are already lots of great modules and plugins we can use with it to make our setup purr.\nThe wdio configuration JavaScript is basically what runs this whole show. The JavaScript exports a config object, which holds all the settings you want to define for the test. This includes what browser drivers you want to use, in which directories to look for spec files, what test runner to use, etc.\nYou can find the example configuration, used when running npm test, in the ./examples directory, with the name examples.conf.js. Here\u0026rsquo;s what it looks like:\nconst enhancedEcommerceSchema = require(\u0026#39;../lib/enhancedEcommerceSchema.json\u0026#39;) exports.config = { specs: [ \u0026#39;./examples/spec/basic_example.js\u0026#39; ], maxInstances: 10, capabilities: [{ maxInstances: 5, browserName: \u0026#39;phantomjs\u0026#39;, \u0026#39;phantomjs.binary.path\u0026#39;: \u0026#39;./node_modules/phantomjs-prebuilt/bin/phantomjs\u0026#39; }], sync: true, logLevel: \u0026#39;silent\u0026#39;, coloredLogs: true, bail: 0, waitforTimeout: 10000, connectionRetryTimeout: 90000, connectionRetryCount: 3, services: [\u0026#39;selenium-standalone\u0026#39;], seleniumInstallArgs: { version: \u0026#39;3.0.1\u0026#39; }, seleniumArgs: { version: \u0026#39;3.0.1\u0026#39; }, framework: \u0026#39;mocha\u0026#39;, reporters: [\u0026#39;spec\u0026#39;], mochaOpts: { ui: \u0026#39;bdd\u0026#39; }, before: function() { const chai = require(\u0026#39;chai\u0026#39;); chai.use(require(\u0026#39;chai-json-schema\u0026#39;)) chai.use(require(\u0026#39;chai-subset\u0026#39;)) chai.tv4.addSchema(\u0026#39;/enhancedEcommerceSchema.json\u0026#39;, enhancedEcommerceSchema) global.expect = chai.expect global.assert = chai.assert } }  First, be sure to check here for the full range of options you can set in the configuration file. It\u0026rsquo;s a good idea to familiarize yourself with the available options, since you\u0026rsquo;ll definitely need to modify some of them when extending these examples to your actual use cases.\nconst enhancedEcommerceSchema = require(\u0026#39;../lib/enhancedEcommerceSchema.json\u0026#39;)  This command loads a custom Enhanced Ecommerce JSON Schema which you can refer to when you want to test if your dataLayer has valid Enhanced Ecommerce objects within. There\u0026rsquo;s more on this in a later chapter.\nspecs: [ \u0026#39;./examples/spec/basic_example.js\u0026#39; ]  Use the specs keyword to list the locations with test files you want wdio to run through. Whenever you run wdio without the --spec command-line parameter, it will run through all the tests listed in this array.\nYou can use wildcards, too. For example, all my tests are stored in various places within a directory named /spec/. To get wdio to run through all tests within, regardless of directory or file name, my specs configuration looks like this:\nspecs: [ \u0026#39;./spec/**/*.js\u0026#39; ]  This means that in the director /spec/, look through all the files and directories for any items that have the .js extension, and use them as test specifications.\ncapabilities: [{ browserName: \u0026#39;phantomjs\u0026#39; },{ browserName: \u0026#39;chrome\u0026#39; }]  This is where you define the browser instances you want to fire with each test. Basically, any browser you list here will be launched when the test runner starts, and the tests will be run through each respective browser. Chrome, Firefox and PhantomJS have support out of the box in gtm-datalayer-test.\nBy the way, PhantomJS is a very popular headless browser. Basically, it\u0026rsquo;s a browser without a graphical user interface. It\u0026rsquo;s thus a very lightweight, quick solution for running your tests on. However, with every major release you should probably run the tests through \u0026ldquo;real\u0026rdquo; browsers, too.\nframework: \u0026#39;mocha\u0026#39;, reporters: [\u0026#39;spec\u0026#39;]  Here you define that you want to use Mocha as the test framework, and spec as the reporter. With wdio, you can also use Jasmine or Cucumber instead of Mocha. You also have a bunch of reporters to choose from, or you can build your own (quite simple, actually). For more details, see here for test frameworks and here for reporters.\nbefore: function() { const chai = require(\u0026#39;chai\u0026#39;); chai.use(require(\u0026#39;chai-json-schema\u0026#39;)) chai.use(require(\u0026#39;chai-subset\u0026#39;)) chai.tv4.addSchema(\u0026#39;/enhancedEcommerceSchema.json\u0026#39;, enhancedEcommerceSchema) global.expect = chai.expect global.assert = chai.assert }  The before hook is executed before any test specs are run. In this hook, I basically tell wdio to use Chai as the assertion library of choice. Chai is a very popular assertion library, and it\u0026rsquo;s often used together with Mocha to provide a more expressive language for describing your tests. You can use any assertion library you want, or you can use the default Node.js assertion library, if you wish.\nAgain, remember to read through the wdio documentation. It\u0026rsquo;s very important to understand what you can customize and how. I hope to add more examples to the GitHub repo in the future, but I still recommend you take a look at the documentation.\n3.2. JavaScript test specification Because this is a self-contained solution (or so I hope), you don\u0026rsquo;t really need to touch the JavaScript test spec at all. The whole thing is orchestrated with the wdio configuration file and the JSON configuration file.\nEach JavaScript test specification executes its respective configuration JSON, running a bunch of tests against the window.dataLayer object on any page it visits.\nIf you want to run multiple specifications, you will need to copy both the test specification JavaScript (e.g. basic_example.js) as well as the configuration JSON (e.g. basic_example.conf.json) to the spec directory. You can also have all your spec files in a single directory, if you wish. The wdio configuration file simply looks through the /examples/spec directory for any JavaScript test files.\nI\u0026rsquo;m not going to walk you through the entire file, but there are some things you should know about.\n// Set the file in require() to point to this test\u0026#39;s configuration JSON const dataLayerConf = require(\u0026#39;./basic_example.conf.json\u0026#39;)  If you create a new test specification, the JSON file name you point to in the require() method should refer to the configuration JSON you\u0026rsquo;ve created for this test.\nThe test specification runs through two main suites of tests:\n  \u0026ldquo;Generic\u0026rdquo; tests for window.dataLayer, where certain keys should be found on every page the test visits.\n  \u0026ldquo;Page-specific\u0026rdquo; configurations, where you test window.dataLayer on each given page for page-specific key-value pairs.\n  All the reports are generated automatically based on certain values in the configuration JSON. Before the test runs, however, the configuration JSON itself is validated against what I\u0026rsquo;ve defined as a valid JSON Schema for this particular project (you can find this in /lib/validTestConfSchema.json). If anything is missing or incorrectly encoded, the test reporter will stop with an error.\n3.3. JSON configuration file The JSON configuration is what the whole test hinges on. All the parameters you provide govern not only which pages the test loads in the virtual browser, but also what you expect to find in window.dataLayer on any given page.\nThis is what a fully loaded JSON configuration might look like:\n{ \u0026#34;baseUrl\u0026#34; : \u0026#34;https://www.simoahava.com\u0026#34;, \u0026#34;dataLayerName\u0026#34; : \u0026#34;dataLayer\u0026#34;, \u0026#34;multipleContainers\u0026#34; : true, \u0026#34;dataLayer\u0026#34; : [{ \u0026#34;@json\u0026#34; : false, \u0026#34;visitorLoginState\u0026#34; : \u0026#34;logged-out\u0026#34; },{ \u0026#34;event\u0026#34; : { \u0026#34;pattern\u0026#34; : \u0026#34;^gtm.dom$\u0026#34; } },{ \u0026#34;event\u0026#34; : { \u0026#34;pattern\u0026#34; : \u0026#34;^gtm.load$\u0026#34; } }], \u0026#34;page\u0026#34; : [{ \u0026#34;path\u0026#34; : \u0026#34;/gtm-tips/10-useful-css-selectors/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have basic article variables\u0026#34;, \u0026#34;pageAttributes\u0026#34; : { \u0026#34;enum\u0026#34; : [[\u0026#34;css-selectors\u0026#34;, \u0026#34;google-tag-manager\u0026#34;, \u0026#34;gtmtips\u0026#34;]] }, \u0026#34;pageCategory\u0026#34; : { \u0026#34;enum\u0026#34;: [[\u0026#34;gtm-tips\u0026#34;]] }, \u0026#34;pagePostType\u0026#34; : { \u0026#34;pattern\u0026#34; : \u0026#34;^post$\u0026#34; }, \u0026#34;pagePostType2\u0026#34; : { \u0026#34;pattern\u0026#34;: \u0026#34;^single-post$\u0026#34; }, \u0026#34;postCountOnPage\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;number\u0026#34; }, \u0026#34;postCountTotal\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;number\u0026#34; } },{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have article impressions\u0026#34;, \u0026#34;event\u0026#34; : { \u0026#34;pattern\u0026#34;: \u0026#34;^impressionsPushed$\u0026#34; }, \u0026#34;ecommerce\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34; : { \u0026#34;impressions\u0026#34;: { \u0026#34;$ref\u0026#34; : \u0026#34;/enhancedEcommerceSchema.json#/definitions/impressions\u0026#34; } }, \u0026#34;required\u0026#34; : [\u0026#34;impressions\u0026#34;] } },{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have clientId\u0026#34;, \u0026#34;event\u0026#34; : { \u0026#34;pattern\u0026#34; : \u0026#34;^trackerReady$\u0026#34; }, \u0026#34;cid\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34; : \u0026#34;[0-9]+\\\\.[0-9]+\u0026#34; } },{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have product detail view\u0026#34;, \u0026#34;ecommerce\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34; : { \u0026#34;detail\u0026#34; : { \u0026#34;$ref\u0026#34; : \u0026#34;/enhancedEcommerceSchema.json#/definitions/detail\u0026#34; } }, \u0026#34;required\u0026#34; : [\u0026#34;detail\u0026#34;] } }] }] }  Here are the main keys you can work with:\n  baseUrl (required) - a string containing the full protocol, domain, and port of the site where all the tests are run in this specification. Note! Leave the trailing slash out (https://www.simoahava.com, NOT https://www.simoahava.com/).\n  dataLayerName - if you\u0026rsquo;re using some other name for the global dataLayer object than \u0026ldquo;dataLayer\u0026rdquo;, remember to specify it here. You can leave this out otherwise (the test defaults to dataLayer).\n  multipleContainers - set this to true if you have multiple containers on the page. Otherwise leave this out.\n  In addition to this, there\u0026rsquo;s dataLayer, which is an array of objects you expect to find on every single page the test visits (generic configuration).\nThere\u0026rsquo;s also page, which is where you specify each page you want the test to visit, with any page-specific dataLayer configurations defined within.\nThere\u0026rsquo;s more on the configuration file in the following chapter.\n4. JSON configuration Picking up where we left off in the previous chapter, let\u0026rsquo;s focus on the two important configuration arrays we haven\u0026rsquo;t covered yet: dataLayer[] and page[].\n4.1. Generic dataLayer The dataLayer key in the root of the JSON configuration is where you list objects you expect to find on every single page the test visits. These are so called \u0026ldquo;generic\u0026rdquo; keys. So, let\u0026rsquo;s say you expect to find an object that contains the key-value pair \u0026quot;event\u0026quot; : \u0026quot;gtm.dom\u0026quot;, as well as an optional object where they key is \u0026quot;visitorLoginState\u0026quot; whose value is any string, this is what the generic object might look like:\n{ ... dataLayer : [{ \u0026#34;@json\u0026#34; : false, \u0026#34;event\u0026#34; : \u0026#34;gtm.dom\u0026#34; },{ \u0026#34;visitorLoginState\u0026#34; : { \u0026#34;@rootRequired\u0026#34; : false, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34; } }] ... } Let\u0026rsquo;s see what these keys mean.\n@json is a special configuration that can only have the false value. Any other value and the schema will not validate. If a dataLayer object has this key in the configuration, it means that any key-value pairs within that object will be looked for verbatim within the global window.dataLayer object. In other words, if window.dataLayer doesn\u0026rsquo;t have at least one object with the exact key-value pair \u0026quot;event\u0026quot; : \u0026quot;gtm.dom\u0026quot;, the test will fail.\nYou can thus use the \u0026quot;@json\u0026quot; : false setting to run simple subset checks. You can define complex objects, arrays, or any available data types, but they need to be found in that exact format within window.dataLayer. This subset check is a great way to test key-value pairs which you expect to be immutable across releases.\nIf you don\u0026rsquo;t provide the @json key, the test will be validated against JSON Schema logic. JSON Schema is a (draft) standard used to describe JSON documents. The window.dataLayer object can be stringified into an imperfect JSON representation, though it\u0026rsquo;s typically good enough to run JSON validation tests against.\nJSON Schema gives you a lot of tools to work with when describing the complexity of window.dataLayer. You can create tests where you expect keys to have a certain range of values, a certain data type, optional or required parameters, et cetera.\nIn the example above, \u0026quot;visitorLoginState\u0026quot; is simply defined as an optional string. You don\u0026rsquo;t require window.dataLayer to have it, but if it is found, you expect it to be a string. Thus the test would fail if window.dataLayer had an object with \u0026quot;visitorLoginState\u0026quot; : false.\nThe special @rootRequired key is used only for keys in the root of the dataLayer object. If you add this key with value false, it means that the parameter it\u0026rsquo;s attached to is optional. You don\u0026rsquo;t require window.dataLayer to have that key in the root of any object. The default is that each key you define in the root of the dataLayer object is required, so using \u0026quot;@rootRequired\u0026quot; : false is the only way to impact this. For keys deeper in the structure you can use regular JSON Schema syntax (e.g. \u0026quot;required\u0026quot; : [\u0026quot;someProperty\u0026quot;, \u0026quot;someOtherProperty\u0026quot;]).\n4.2. Page-specific configurations The page-specific configurations contain one extra configuration level, after which you define their own dataLayer objects using the methods you read about in the previous chapter.\nThe first page-specific key you need to define is path. This should be in the root of each object in the page array. The value should be a proper URL path, starting with \u0026ldquo;/\u0026rdquo;. For example, to visit three different pages in the test, you would configure the page object like this:\n{ ... \u0026#34;dataLayer\u0026#34; : [{ .... }], \u0026#34;page\u0026#34; : [{ \u0026#34;path\u0026#34; : \u0026#34;/first-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ ... }] },{ \u0026#34;path\u0026#34; : \u0026#34;/second-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ ... }] },{ \u0026#34;path\u0026#34; : \u0026#34;/third-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ ... }] } } For each given page object, you can define the dataLayer composition you expect to find on that particular page. The configuration of each dataLayer object is exactly the same as explored in the previous chapter with one important addition.\nIn the root of each dataLayer object, you need to define the key @expect with a textual description of this test. In other words, use this key to describe why you are expecting this particular dataLayer object to be found in the global window.dataLayer.\nWhatever you type in this string will be prefixed with the verb \u0026ldquo;expect\u0026rdquo;, so you can thus use a partial sentence. This is what your configuration might look like:\n{ ... \u0026#34;dataLayer\u0026#34; : [{ .... }], \u0026#34;page\u0026#34; : [{ \u0026#34;path\u0026#34; : \u0026#34;/first-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have the userState key\u0026#34;, \u0026#34;userState\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34; } }] },{ \u0026#34;path\u0026#34; : \u0026#34;/second-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to optionally contain userCount\u0026#34;, \u0026#34;userCount\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;number\u0026#34;, \u0026#34;@rootRequired\u0026#34; : false } }] },{ \u0026#34;path\u0026#34; : \u0026#34;/third-page-to-test/\u0026#34;, \u0026#34;dataLayer\u0026#34; : [{ \u0026#34;@expect\u0026#34; : \u0026#34;dataLayer to have the Enhanced Ecommerce detail object\u0026#34;, \u0026#34;ecommerce\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34; : { \u0026#34;detail\u0026#34; : { \u0026#34;$ref\u0026#34; : \u0026#34;/enhancedEcommerceSchema.json#/definitions/detail\u0026#34; } }, \u0026#34;required\u0026#34; : [\u0026#34;detail\u0026#34;] }] } } See the last object? Interesting, eh? First, you expect the global window.dataLayer structure to contain an object named ecommerce with one required property: detail. Next, you\u0026rsquo;re using the $ref JSON Schema keyword to link to an external definition. In fact, in the directory /lib/ you can find the schema named enhancedEcommerceSchema.json, which I created to make it easier to describe a valid Enhanced Ecommerce object.\nYou can link to the definitions within from your test configuration JSON files. Hopefully we\u0026rsquo;ll have more schemas for other complex objects soon, and nothing\u0026rsquo;s stopping you from creating and linking your own custom JSON for schemas you expect to use over and over again!\n5. Running your tests If you\u0026rsquo;ve been modifying the basic_example.conf.json, you can use the predefined npm script npm test to run your setup. When you execute that command, the following things happen:\n  The script fires up an http-server instance on http://localhost:8080, which loads the index.html file from ./examples/.\n  The script runs ./node_modules/.bin/wdio ./examples/examples.conf.js\n  The test runner automatically starts an instance of the Selenium server, which then proceeds to fire up a phantomjs browser driver.\n  The test runner starts reading your basic_example.js test specification, and opens the test browser on URL http://localhost:8080/index.html.\n  The test runner proceeds to run through all the tests on this URL, keeping tabs on which tests passed and which failed.\n  The test runner then reports the results of the test, using green checkmarks for passed tests and red numbers for failed tests.\n  If any tests failed, they are reported as AssertionErrors with each error associated with its respective test number.\n  Finally, wdio automatically shuts down the Selenium instance, and http-server is killed, too.\n    Once you\u0026rsquo;ve graduated beyond the basic_example.js file, you might want to create your own tests, store them in their own folders, and create new wdio configuration files for each.\nThe command for running wdio against your own wdio configuration files is this (executed in the root of your project):\n./node_modules/.bin/wdio \u0026lt;wdio configuration file\u0026gt;\nThis runs the wdio executable against your configuration file, which in turn passes each specification defined in the configuration for the test runner. The test runner, then, starts the browser instances you have selected, running the tests, and channeling the output to the test reporter.\nYou might want to start using a task / workflow runner such as Grunt or Gulp to manage your test suites. It will make it easier to run arbitrary tests, as you can specify things like custom parameters, and you can chain multiple tests together, if you wish.\n6. Contribute This is an open-source project. I don\u0026rsquo;t expect you to contribute, but if you do I am very grateful indeed.\nTo contribute, head on over to the GitHub repo. You can fork the repo, make modifications, and then submit those modifications as Pull Requests. It would be best, however, if you first introduce the thing you want to change as an Issue to make sure we all agree that it\u0026rsquo;s a good feature to focus on.\nAlternatively, you can add your ideas, questions, or bugs in the comments below. I\u0026rsquo;m fully aware this isn\u0026rsquo;t a \u0026ldquo;plug-and-play\u0026rdquo; solution for testing GTM, since I never intended it to be one. Thus I expect there to be issues that I haven\u0026rsquo;t considered (I should write tests for this, too!), and I would be very grateful if you\u0026rsquo;d let me know about any trouble you\u0026rsquo;ve come across with this solution.\nFinally, the latest version of this solution AND the documentation is only maintained in the GitHub repo. It\u0026rsquo;s possible that this article is already outdated as you\u0026rsquo;re reading this, so I hope you head on over to the repo to see the latest changes.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/10-useful-custom-javascript-tricks/",
	"title": "#GTMTips: 10 Useful Custom JavaScript Tricks",
	"tags": ["Google Tag Manager", "gtmtips", "JavaScript"],
	"description": "Introducing 10 useful Custom JavasScript tricks for Google Tag Manager.",
	"content": "I recently published a #GTMTips guide called 10 Useful CSS Selectors, and it was very well received. Inspired by the feedback, here\u0026rsquo;s the next instalment. This time, we\u0026rsquo;re going over some useful JavaScript tips and tricks that you can use to make your Google Tag Manager deployment even more efficient. I\u0026rsquo;ve written a lot about JavaScript in this blog, and I intend to keep on doing so in the future. As always, if JavaScript is somewhat of a mystery to you, I strongly recommend you take the Codecademy (free) course on JS, and take a look at the other available web technology tracks while you\u0026rsquo;re there!\nTip 54: 10 Useful Custom JavaScript Tricks   You can deploy these tricks in Custom HTML Tags or Custom JavaScript Variables, since they are the only contexts within Google Tag Manager where you can execute arbitrary JavaScript. Note that some of the tricks are just code snippets, so you will need to understand enough of how Google Tag Manager and JavaScript mesh together to be able to deploy them successfully.\nBefore adding any of these to your deployments, remember to use caniuse.com to check for browser compatibility, and the MDN JavaScript Reference to find alternative ways (AKA polyfills) for writing the unsupported methods.\n1. String methods String methods are utilities that you can use to modify any given string. Here are some of the most useful ones, in my opinion.\n// Use .trim() to strip leading and trailing whitespace from a string. \u0026#34; Oh no! Leading AND trailing whitespace!! \u0026#34;.trim(); // Result: \u0026#34;Oh no! Leading AND trailing whitespace!!\u0026#34;  // Use .replace() to replace characters or regular expressions with something else. // .replace() without a regular expression replaces the first instance. \u0026#34;Food\u0026#34;.replace(\u0026#39;o\u0026#39;, \u0026#39;e\u0026#39;); // Result: \u0026#34;Feod\u0026#34; \u0026#34;Food\u0026#34;.replace(/o/g, \u0026#39;e\u0026#39;); // Result: \u0026#34;Feed\u0026#34;  // Use .toUpperCase() and .toLowerCase() to change the case of the entire string \u0026#34;MixED CaSe String\u0026#34;.toLowerCase(); // Result: \u0026#34;mixed case string\u0026#34;  // Use .substring() to return only part of the string. \u0026#34;?some-query-key=some-query-value\u0026#34;.substring(1); // Returns: \u0026#34;some-query-key=some-query-value\u0026#34; \u0026#34;id: 12345-12345\u0026#34;.substring(4,9); // Returns: \u0026#34;12345\u0026#34;  // Use .split() to split the string into its constituents \u0026#34;get the second word of this sentence\u0026#34;.split(\u0026#39; \u0026#39;)[1]; // Returns \u0026#34;the\u0026#34;  Naturally, you can combine these in inventive ways. For example, to capitalize the first letter of any string you could do this:\nvar str = \u0026#34;capitalize the first letter of this string, please!\u0026#34;; str = str.replace(/^./, str.substring(0,1).toUpperCase());  Here we first identify the first letter of the string using a regular expression, after which we replace it with the first letter of the string that has been converted to upper case.\n2. Array methods Array methods are really powerful in any programming language. Mastering methods such as filter() and forEach() is critical if you want to make your JavaScript more compact and often more readable.\nfilter() filter() goes through each element in the Array, and returns a new Array for every element that passes the check you provide in the callback. Here\u0026rsquo;s the syntax:\nsomeArray.filter(function(eachItem) { return eachItem === someCondition; });  So eachItem is the variable where the iterator stores each member of the Array as it is processed. If the callback returns true, it means that the item is added to the returned, new Array. If it returns false, it\u0026rsquo;s dropped.\nHere\u0026rsquo;s an example:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;addMe!\u0026#39; },{ \u0026#39;event\u0026#39; : \u0026#39;doNotAddMe!\u0026#39; }); var newArray = window.dataLayer.filter(function(item) { return item.event === \u0026#39;addMe!\u0026#39;; }); // Returns: [{\u0026#39;event\u0026#39; : \u0026#39;addMe!\u0026#39;}]  The iterator checks every single item for the property event, and returns true if that property has value addMe!. Thus the returned array only has those elements that have the key-value pair \u0026quot;event\u0026quot; : \u0026quot;addMe!\u0026quot;.\nforEach() Remember the clumsy for-loop for iterating over an Array? Yuck! Instead, you can use the forEach() iterator.\nforEach() receives each item in the array one-by-one, and you can then do whatever you wish with this item. The syntax is very simple and intuitive, and thus should be preferred over the confusing for-loop.\nvar array = [\u0026#34;I\u0026#34;, 4, 2, true, \u0026#34;love\u0026#34;, [1,2,3], {chocolate: \u0026#39;too\u0026#39;}, \u0026#34;you\u0026#34;]; var newArray = []; array.forEach(function(item) { if (typeof item === \u0026#39;string\u0026#39;) { newArray.push(item); } }); newArray.join(\u0026#34; \u0026#34;); // Result: \u0026#34;I love you\u0026#34;  As you can see, it\u0026rsquo;s more readable than a for-loop, as you don\u0026rsquo;t have to access the original array in the iterator.\nmap() The map() iterates over each member in the array, again, but this time the code in the callback is executed against each member of the array, and a new array is returned with the results. Here\u0026rsquo;s how to set it up:\narray.map(function(item) { return doSomething(item); });  In other words, you are mapping each element in the array against the result of the callback function. Here\u0026rsquo;s are some examples:\nvar array = [1,2,3,4,5]; array.map(function(item) { return item * 2; }); // Result: [2,4,6,8,10]  var array = [\u0026#34; please \u0026#34;, \u0026#34; trim\u0026#34;, \u0026#34; us \u0026#34;]; array.map(function(item) { return item.trim(); }); // Result: [\u0026#34;please\u0026#34;, \u0026#34;trim\u0026#34;, \u0026#34;us\u0026#34;];  reduce() The reduce() method is often the most complex one, but it actually has a very simple principle: You provide the function with an accumulator, and each member of the array is then operated against this accumulator. You can also provide an initial value to the accumulator. Here\u0026rsquo;s what the basic structure looks like:\narray.reduce(function(accumulator, item) { accumulator.doSomethingWith(item); return accumulator; }, initialValue);  This time, it\u0026rsquo;s definitely easiest to learn via examples:\n// Example: calculate the sum of all even numbers in the array var array = [1,6,3,4,12,17,21,27,30]; array.reduce(function(accumulator, item) { if (item % 2 === 0) { accumulator += item; } return accumulator; }, 0); // Returns: 52  // Example, concatenate a string of all product IDs in array var array = [{ \u0026#34;id\u0026#34; : \u0026#34;firstId\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;T-shirts\u0026#34; },{ \u0026#34;id\u0026#34; : \u0026#34;secondId\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;Pants\u0026#34; },{ \u0026#34;id\u0026#34; : \u0026#34;thirdId\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;shoes\u0026#34; }]; array.reduce(function(accumulator, item) { accumulator.push(item.id); return accumulator; }, []).join(); // Returns: \u0026#34;firstId,secondId,thirdId\u0026#34;  3. Ternary operator The ternary operator is just a very simple shorthand for running conditional checks in JavaScript. Here\u0026rsquo;s an example:\n// BEFORE: if (something) { somethingElse(); } else { somethingDifferent(); } // AFTER: something ? somethingElse() : somethingDifferent();  The ternary operator is thus used to combine an if-statement into a simple expression. First you provide an expression that evaluates to a truthy or falsy value, such as me.name() === \u0026quot;Simo\u0026quot;. Then you type the question mark, after which you write an expression that is executed if the first item evaluates to a truthy value. Finally, you type the colon :, after which you type the expression that is executed if the first item evaluates to a falsy value.\n// BEFORE: if (document.querySelector(\u0026#39;#findThisId\u0026#39;) !== null) { return document.querySelector(\u0026#39;#findThisId\u0026#39;); } else { return \u0026#34;Not found!\u0026#34;; } // AFTER: return document.querySelector(\u0026#39;#findThisId\u0026#39;) ? document.querySelector(\u0026#39;#findThisId\u0026#39;) : \u0026#34;Not found!\u0026#34;; // EVEN BETTER: return document.querySelector(\u0026#39;#findThisId\u0026#39;) || \u0026#34;Not found!\u0026#34;;  As you can see, sometimes there are even more efficient ways to process JavaScript statements than the ternary operator. Especially when working with simple binary checks (if value exists, return it), it might be better to just use basic logical operators instead of complex statements or expressions.\n4. return {{Click URL}}.indexOf({{Page Hostname}}) \u0026gt; -1 This is very Google Tag Managerish. It\u0026rsquo;s a simple Custom JavaScript Variable that returns true if the clicked element URL contains the current page hostname, and false otherwise. In other words, it returns true if the clicked link is internal, and false if it takes the user away from the website.\nfunction() { return {{Click URL}}.indexOf({{Page Hostname}}) \u0026gt; -1; }  5. return {{Click URL}}.split('/').pop() Again, a simple Custom JavaScript Variable. This is especially useful when tracking file downloads, as it returns the actual filename of the downloaded item. It does this by returning whatever is in the clicked URL after the last \u0026lsquo;/\u0026rsquo;.\nfunction() { // Example: https://www.simoahava.com/downloads/download_me.pdf  return {{Click URL}}.split(\u0026#39;/\u0026#39;).pop(); // Returns: download_me.pdf }  6. Create a random, unique GUID Every now and then it\u0026rsquo;s useful to create a random ID in GTM. For example, if you want to measure session IDs, or if you want to assign a unique identifier to each page hit, you can achieve this with the following Custom JavaScript Variable.\nThe variable creates a GUID string (\u0026ldquo;Globally Unique Identifier\u0026rdquo;), and even though uniqueness isn\u0026rsquo;t guaranteed, it\u0026rsquo;s still very likely. There\u0026rsquo;s only a microscopically small chance of collision.\nThis solution is gratefully adapted from this StackOverflow post.\nfunction() { return \u0026#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\u0026#39;.replace(/[xy]/g, function(c) { var r = Math.random()*16|0, v = c == \u0026#39;x\u0026#39; ? r : (r\u0026amp;0x3|0x8); return v.toString(16); }); }  7. Return an ISO-formatted timestamp This is one of my favorite solutions, as it lets you convert the current client time to a proper, readable timestamp. In addition, it has the timezone offset included, so you\u0026rsquo;ll know just how much the users\u0026rsquo; local times differ from your own timezone. I send this to Google Analytics with every single hit, so that I can create a timeline of events when analyzing the data.\nThis solution is gratefully adapted from this StackOverflow post.\nfunction() { var now = new Date(); var tzo = -now.getTimezoneOffset(); var dif = tzo \u0026gt;= 0 ? \u0026#39;+\u0026#39; : \u0026#39;-\u0026#39;; var pad = function(num) { var norm = Math.abs(Math.floor(num)); return (norm \u0026lt; 10 ? \u0026#39;0\u0026#39; : \u0026#39;\u0026#39;) + norm; }; return now.getFullYear() + \u0026#39;-\u0026#39; + pad(now.getMonth()+1) + \u0026#39;-\u0026#39; + pad(now.getDate()) + \u0026#39;T\u0026#39; + pad(now.getHours()) + \u0026#39;:\u0026#39; + pad(now.getMinutes()) + \u0026#39;:\u0026#39; + pad(now.getSeconds()) + \u0026#39;.\u0026#39; + pad(now.getMilliseconds()) + dif + pad(tzo / 60) + \u0026#39;:\u0026#39; + pad(tzo % 60); // Returns, for example: 2017-01-18T11:58:32.977+02:00 }  8. .matches() polyfill When working with the Document Object Model (DOM), being able to identify elements is crucial. We already have a bunch of wonderful CSS selectors at our disposal, but now we just need a method we can use to check if any given element matches one of these selectors.\nWell, there\u0026rsquo;s the Element.matches(someSelector) method that you can use, but it doesn\u0026rsquo;t have stellar browser support, even with prefixes. With this solution, you can always use .matches() without having to worry about browser support. This trick is called a polyfill, as it patches lack of feature support with a workaround using JavaScript that is universally supported.\nFirst, here\u0026rsquo;s how the method works in general:\n// Check if the parent of the clicked element has ID #testMe var el = {{Click Element}}; console.log(el.parentElement.matches(\u0026#39;#testMe\u0026#39;)); // RESULT: true or false, depending on if the parent element matches the selector.  To implement the polyfill, either ask your developers to add it to the site JavaScript as early as possible in the page load sequence, or use Google Tag Manager.\nIn Google Tag Manager, you\u0026rsquo;ll need a Custom HTML Tag that fires as early as possible in the container load sequence (i.e. All Pages trigger with a high tag priority).\nHere\u0026rsquo;s the code you need to add to the Custom HTML Tag. It\u0026rsquo;s gratefully adapted from this MDN reference page.\n\u0026lt;script\u0026gt; if (!Element.prototype.matches) { Element.prototype.matches = Element.prototype.matchesSelector || Element.prototype.mozMatchesSelector || Element.prototype.msMatchesSelector || Element.prototype.oMatchesSelector || Element.prototype.webkitMatchesSelector || function(s) { var matches = (this.document || this.ownerDocument).querySelectorAll(s), i = matches.length; while (--i \u0026gt;= 0 \u0026amp;\u0026amp; matches.item(i) !== this) {} return i \u0026gt; -1; }; } \u0026lt;/script\u0026gt; The polyfill modifies the actual prototype of the Element object, which all HTML and DOM elements inherit from. After modifying the prototype, you can use the matches() method with confidence in all your GTM and site JavaScript.\n9. DOM traversal Sometimes it\u0026rsquo;s necessary to climb up (or down) the Document Object Model. For example, if you\u0026rsquo;re using a Click / All Elements trigger, it always targets the actual element that was clicked. But that\u0026rsquo;s not always necessarily the element you want to track! Say you have an HTML structure like this:\n\u0026lt;a href=\u0026#34;takemeaway.html\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;clickMe\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Click Me!\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/a\u0026gt; Now, if you use a Click / All Elements trigger, the element that is captured in the click is the \u0026lt;span/\u0026gt;. But I\u0026rsquo;m guessing you actually want to use the \u0026lt;a href=\u0026quot;takemeaway.html\u0026quot;\u0026gt; element, since you\u0026rsquo;re more interested in knowing what happens after the click. So, you can use this Custom JavaScript Variable to return the nearest link above the clicked element in the DOM tree:\nfunction() { var el = {{Click Element}}; while (!el.matches(\u0026#39;a\u0026#39;) \u0026amp;\u0026amp; !el.matches(\u0026#39;body\u0026#39;)) { el = el.parentElement; } return el.matches(\u0026#39;a\u0026#39;) ? el : undefined; }  NOTE! This relies on the matches() method, so don\u0026rsquo;t forget to implement the polyfill from above, first!\nThis Custom JavaScript Variable climbs up the DOM until it reaches the first link element it finds ('a'), after which it returns this element. If it doesn\u0026rsquo;t find a link, it returns undefined instead.\n10. Set browser cookies with ease Cookies are a great, if somewhat outdated, way of storing information in the browser. Since Google Tag Manager operates in the context of a web page, it is essentially stateless. Thus any information you want to persist from one page to another must be stored either in the server or the browser itself. The latter is far easier to do, and with browser cookies it\u0026rsquo;s just a question of adding a couple of lines of code to your GTM deployment.\nFirst, you need a Custom JavaScript Variable. You can name it {{Set Cookie}}, for example.\nfunction() { return function(name, value, ms, path, domain) { if (!name || !value) { return; } var d; var cpath = path ? \u0026#39;; path=\u0026#39; + path : \u0026#39;\u0026#39;; var cdomain = domain ? \u0026#39;; domain=\u0026#39; + domain : \u0026#39;\u0026#39;; var expires = \u0026#39;\u0026#39;; if (ms) { d = new Date(); d.setTime(d.getTime() + ms); expires = \u0026#39;; expires=\u0026#39; + d.toUTCString(); } document.cookie = name + \u0026#34;=\u0026#34; + value + expires + cpath + cdomain; } }  This Custom JavaScript Variable returns a function that takes five parameters:\n  name (required): the name of the cookie (string)\n  value (required): the value of the cookie (string)\n  ms: expiration time of the cookie in milliseconds. If unset, defaults to a Session cookie (expires when the browser is closed).\n  path: the path of the cookie. If unset, defaults to the current page path.\n  domain: the domain of the cookie. If unset, defaults to the current domain.\n  To use the cookie, you invoke it with:\n{{Set Cookie}}(\u0026#39;test\u0026#39;, \u0026#39;true\u0026#39;, 10000, \u0026#39;/\u0026#39;, \u0026#39;simoahava.com\u0026#39;);  The code above, when run in GTM, sets a cookie with name \u0026quot;test\u0026quot;, value \u0026quot;true\u0026quot;, expiration time of ten seconds, and it\u0026rsquo;s set on the root of the simoahava.com domain.\nWith this helper, setting cookies is a breeze. Remember that you can then use the handy 1st Party Cookie variable in GTM to retrieve values from set cookies.\nSummary Here I listed 10 JavaScript tricks that I use (almost) all the time. There\u0026rsquo;s plenty more to JavaScript, but with these methods you can get started on making your clunky Google Tag Manager deployment a thing of the past.\nDo you have any favorite methods, tips, or tricks you want to share? Please do so in the comments below.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/10-useful-css-selectors/",
	"title": "#GTMTips: 10 Useful CSS Selectors",
	"tags": ["css selectors", "Google Tag Manager", "gtmtips"],
	"description": "10 useful CSS selectors you can use in Google Tag Manager.",
	"content": "Without a doubt, the possibility to leverage CSS selectors in Google Tag Manager\u0026rsquo;s trigger conditions is one of the most useful features of the platform. It gives you an amazing amount of flexibility, especially when combined with GTM\u0026rsquo;s click and form triggers.\nEssentially, CSS selectors let you test an HTML Element against a selector string. This check verifies that the element matches the given selector. In practice, this would mean that when you use the click or form trigger, you can check if the Click Element or Form Element built-in variables match a specific selector, allowing you to confirm that the action happened on the correct element. I\u0026rsquo;ll explain this with more detail further down.\nIn this #GTMTips article, I\u0026rsquo;ll showcase ten useful CSS selectors which might come in handy in your Google Tag Manager setups.\nTip 53: 10 Useful CSS Selectors   Even though you can use CSS selectors in any JavaScript (or CSS) you deploy in your site or in GTM, I\u0026rsquo;m guessing your most typical scenario would be the matches CSS selector trigger condition.\nThis trigger condition is used when you want to evaluate the element that was clicked (or submitted, in case it was a form). In other words, to use a CSS selector against the clicked element, you would first need a trigger which records clicks or forms, and then you\u0026rsquo;d need to append it with a condition like so:\n  For example, the trigger above would only fire if the clicked element was a direct child of an element with the ID myDiv.\nAnd that\u0026rsquo;s how CSS selectors can be used with triggers. Note that CSS selectors are very useful in general, too. It goes without saying that you\u0026rsquo;ll need them in stylesheets, but they can also be used with the querySelector() and querySelectorAll() DOM methods, as well as with matches() (cross-browser support might need some tweaks).\nAnyway, without further ado, here are 10 useful CSS selectors for your viewing pleasure.\n1. Generic selectors The following selectors are used to pinpoint elements based on their attributes or their position in the DOM. Selectors can be combined by putting them one after the other. For example, div[title=\u0026quot;someTitle\u0026quot;][data-gtm-event=\u0026quot;someEvent\u0026quot;] would match any div element that has both the title and data-gtm-event attributes.\n  .someClass - matches an element with class \u0026ldquo;someClass\u0026rdquo;, e.g. \u0026lt;div class=\u0026quot;someClass\u0026quot;\u0026gt;.\n  #someId - matches an element with id \u0026ldquo;someId\u0026rdquo;, e.g. \u0026lt;span id=\u0026quot;someId\u0026quot;\u0026gt;.\n  element - matches any HTML element named \u0026ldquo;element\u0026rdquo;. For example, \u0026ldquo;div\u0026rdquo; would match all div elements on page, and \u0026ldquo;div#myId\u0026rdquo; would match \u0026lt;div id=\u0026quot;myId\u0026quot;\u0026gt;.\n  element element - matches any HTML element that is the descendant of the preceding element. Doesn\u0026rsquo;t need to be a parent-child relationship - the first element just needs to precede the second one in the same tree. For example, \u0026ldquo;span.myClass div#myId\u0026rdquo; would match any div#myId that is the descendant of a span.myClass. You can add as many links to the chain as you want: \u0026ldquo;div#main ol li\u0026rdquo;, for example, would match any li that is the descendant of an ol that is the descendant of div#main.\n  element \u0026gt; element - matches any HTML element that is the direct child of the preceding element. For example, \u0026ldquo;div#myId \u0026gt; a#contactUs\u0026rdquo; would match \u0026lt;a id=\u0026quot;contactUs\u0026quot;\u0026gt; that is the direct child of \u0026lt;div id=\u0026quot;myId\u0026quot;\u0026gt;.\n  selector, selector - two selectors separated by a comma are evaluated with EITHER-OR logic when used in a trigger. So you can specify multiple selectors, and as long as one of them matches, the trigger will fire.\n  These generic selectors are the basis for pretty much everything you do with CSS selectors, so it\u0026rsquo;s a good idea to learn how they work.\n2. a[href^=\u0026quot;tel:\u0026quot;] This selector matches any link element (\u0026lt;a\u0026gt;) whose href attribute begins with the string \u0026ldquo;tel:\u0026rdquo;, such as: \u0026lt;a href=\u0026quot;tel:01010101\u0026quot;\u0026gt;. This is useful for tracking clicks on telephone numbers that have been encoded to use the \u0026ldquo;tel:\u0026rdquo; protocol.\nYou can also make it work with email links: a[href^=\u0026quot;mailto:\u0026quot;], the SMS protocol: a[href^=\u0026quot;sms:\u0026quot;], and the outdated but still prevailing JavaScript protocol: a[href^=\u0026quot;javascript:\u0026quot;].\n3. a[href*=\u0026quot;simoahava.com\u0026rdquo;] This selector matches any link element whose href attribute contains \u0026ldquo;simoahava.com\u0026rdquo;. Thus I can use it to weed out (or include) clicks on internal links on my website.\n4. a[href$=\u0026rdquo;.pdf\u0026rdquo;] This selector matches any link element whose href attribute ends with \u0026ldquo;.pdf\u0026rdquo;. This is useful for tracking PDF links. To measure other filenames, you can simply replace \u0026ldquo;.pdf\u0026rdquo; with whatever filetype you want to track.\n5. div.someElement a I already covered this in the generic selectors, but there\u0026rsquo;s a very important use case I should highlight.\nWhen working with the Click / All Elements trigger, it\u0026rsquo;s a good idea to add a wildcard check for every element you want to track:\nClick Element matches CSS selector a[href*=\u0026quot;simoahava.com\u0026quot;], a[href*=\u0026quot;simoahava.com\u0026quot;] *\nIn other words, after the actual selector, add a second selector that matches any descendant of that selector. This is useful because the All Elements trigger captures the very element that was clicked. With a nested DOM structure, this might often be something unexpected. For example, if you have a link that looks like this:\n\u0026lt;a href=\u0026#34;mailto:some@email.com\u0026#34;\u0026gt; \u0026lt;span\u0026gt;some@email.com\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; A click on the link element above will actually land on the \u0026lt;span/\u0026gt;. By setting the selector to a[href=\u0026quot;mailto:some@email.com\u0026quot;], a[href=\u0026quot;mailto:some@email.com\u0026quot;] *, you\u0026rsquo;re capturing clicks on the link element itself as well as any descendants (including the \u0026lt;span\u0026gt;).\n6. form#myForm option:checked You can use the pseudo-selector :checked to match any checked element. For example, form#myForm option:checked looks for any selected \u0026lt;option\u0026gt; element(s) in the form. This is useful when you want to identify which element in a drop-down list is currently selected.\n7. a:not() The :not pseudo-selector matches if the opposite of the given selector holds true. So, a selector like a:not([href*=\u0026quot;simoahava.com\u0026quot;]) will match clicks on any links that do not have \u0026ldquo;simoahava.com\u0026rdquo; in their href attribute value.\n8. ol \u0026gt; li:first-child The :first-child selector will match the given element that is the first child of its parent. So ol \u0026gt; li:first-child will match the first \u0026lt;li\u0026gt; element of an \u0026lt;ol\u0026gt; list.\nOther similar selectors are :last-child (matches the last child of its parent) and :nth-child(N) (matches the Nth child of its parent, so :nth-child(3) would match the element that is the third child of its parent).\n9. a[data-gtm-event] Square brackets denote attributes, and if you leave out the equals sign (=), you can simply check if an element has the given attribute. a[data-gtm-event] will match any link element that has the attribute data-gtm-event regardless of what the value of that attribute is.\n10. body \u0026gt; div.site-container \u0026gt; div \u0026gt; div \u0026gt; main\u0026hellip; ARGH This is actually a tip rather than a useful selector. Try to avoid really long, complex selector chains. The longer chain it is, and the more you insist on direct parent-child relationships (\u0026gt;), the more points of failure you introduce into the selector.\nAll it takes is one element to change along that DOM path and your selector will stop working. Thus, try to always find the most generic selector that is still specific enough to match exactly what you are trying to capture. This requires some knowledge of the HTML structure of your templates.\nThe long, complex selector in this post\u0026rsquo;s feature image could be replaced simply with:\nheader \u0026gt; h2 \u0026gt; a\nand it will be just as accurate, because I know for a fact that my HTML reserves that DOM sequence for the article titles you see on the main page.\nSummary And there you have it! These selectors should come in handy when you are tweaking your GTM setup.\nDo you have any other really useful selectors you\u0026rsquo;d like to share with others? If you do, please share them in the blog comments!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/change-management-workspaces/",
	"title": "#GTMTips: Change Management With Workspaces",
	"tags": ["Google Tag Manager", "gtmtips", "workspaces"],
	"description": "Guide and introduction to Google Tag Manager&#39;s Workspaces feature.",
	"content": "A while ago, the Google Tag Manager team published one of my favorite feature releases in the history of GTM: Workspaces. I was so thrilled by this release that I went ahead and published a guide on how to implement and leverage this new feature.\nWorkspaces is a very comprehensive feature in Google Tag Manager. This is because it changed the entire underlying data model. We no longer work directly with a single container draft. Instead, our work is done in a cordoned section of the container, protected from changes made to other workspaces until such a time that we choose to sync the latest container version to our workspace, or vice versa.\nThis #GTMTips article is short and simple: it\u0026rsquo;s a reminder and a nudge to leverage workspaces efficiently.\nTip 52: Use Workspaces To Keep Track Of Change   First of all, if workspaces are completely alien to you as a concept, please take a look at my Workspaces Guide. It\u0026rsquo;s long and probably quite confusing (as my guides typically are), but it should get you up to speed with this enterprise-friendly feature.\nIn a nutshell, workspaces provides you the opportunity to work on multiple container drafts at the same time. Once the feature set is complete in a single workspace, you can Create a Version or Publish the workspace, so that it becomes the new Latest Container Version. After the Latest Container Version is updated, all existing workspaces will be alerted of this change, and you will then need to synchronize these new changes into all the other workspaces.\nIf you\u0026rsquo;re using the free version of Google Tag Manager, you have three workspaces that you can use concurrently. In Google Tag Manager 360, you have an unlimited number of workspaces at your disposal. This article mainly targets the former group, since necessity is the mother of innovation. Nevertheless, even if you have access to GTM 360 you should consider utilizing workspaces in such a manner that best relieves friction in your Google Tag Manager process.\nWhat follows are some practices that I have found very useful when working with workspaces.\n1. Use workspaces for feature updates This might seem self-evident, but when workspaces was first released, a common reaction was to use them to isolate a part of the container for some team, and another part of the container for another team.\nWorkspaces isn\u0026rsquo;t a user management feature - it\u0026rsquo;s change management through-and-through. When you publish or create a version of your changes, the underlying workspace is deleted when the new version is created. This means that the feature doesn\u0026rsquo;t really lend itself for consistent use within a team.\nNothing\u0026rsquo;s stopping you from reserving one of the three workspaces to one team, another for some other team, and a third one for generic, incremental changes (see the next tip). However, you should be aware that the feature itself has no built-in functionality to support user management in that way, so you\u0026rsquo;ll have to make sure to use a naming convention \u0026ldquo;Team Name - Workspace Name\u0026rdquo;, for example, to make it clear that the workspace is for a specific team.\n2. Always have one workspace available for small, incremental changes One of the main benefits of workspaces is that you can finally commit your small change to a single tag and publish it without having to worry about taking a bunch of unfinished work with the update into the live container.\nFor this reason and for the sake of agility, it is important to always have one workspace available for small, incremental changes.\nOnce you\u0026rsquo;ve done the change, you can either Create a Version or directly Publish the workspace, depending on what the appropriate workflow is in your organization.\nThe best part about this is that even when you publish your small, incremental change, any other workspace in the container doesn\u0026rsquo;t need to merge the changes you made until they are ready to do so. This deferred synchronization is another great thing about the feature.\n3. Defer synchronization until you are ready to do so   When someone has updated the Latest Container Version by publishing their changes, you\u0026rsquo;ll see a small banner in the bottom of the screen as well as the \u0026ldquo;UPDATE\u0026rdquo; button in the Overview Dashboard (see image above).\nThis means that there are changes to the Latest Container Version that should be merged with your workspace before you can Publish or Create a Version of your workspace.\nIt\u0026rsquo;s good to know that you don\u0026rsquo;t have to do this until you\u0026rsquo;re ready to merge!. The note is there to remind you that you need to merge before you\u0026rsquo;re done with your workspace, but you might want to audit the changes before you sync them with your workspace.\nTo check what changes would be merged, click the \u0026ldquo;UPDATE\u0026rdquo; button in the Overview Dashboard. A new fly-out will appear, where you\u0026rsquo;ll see all the versions that have been created after your workspace was detached from the version branch. You can click these versions to see what changes have been done, and thus you can prepare your workspace to accommodate these disruptions if necessary.\n  Remember that even if there are conflicts, Google Tag Manager will warn you of these and force you to resolve them before the sync is complete. Even so, it\u0026rsquo;s still a good idea to briefly check each update to see if there\u0026rsquo;s something you should consider in your workspace, conflicts or not.\n4. Name your workspaces clearly I\u0026rsquo;m not usually one to froth at the mouth over naming conventions, but with workspaces I might consider an exception. By naming the workspace, you are actually giving a name to the version you\u0026rsquo;ll inevitably create from the workspace, too. Yes, you can rename the container version at any time, but it cuts one step out of the workflow if you name the workspace with the future version in mind.\nConsider naming the workspace with a compact but comprehensive description of what the feature increment that you\u0026rsquo;re doing is. Typically, if you find it difficult to come up with a name that covers all the changes introduced in the workspace, it means that you\u0026rsquo;re doing too much in a single go, and you might consider rethinking your approach in the future. I always prefer small, incremental changes over big, whopping, world-altering feature blasts. Small updates also make it easier to keep all other workspaces in sync.\nRemember that naming versions is purely for your own benefit. If you ever need to roll back, or if you need to find a specific version, the version name (and sometimes the description) is pretty much all you have to work with. Because of this, it\u0026rsquo;s important to name every single version you create so that anyone glancing at the version name would have even just a cursory idea of what was updated in that version.\nSummary It would be silly to say \u0026ldquo;I hope you\u0026rsquo;re using workspaces already\u0026rdquo;, since everyone using Google Tag Manager is, by default, using workspaces already.\nIt might be a bummer to some to only have three workspaces available in the free version of GTM. However, apart from initial implementation or larger, migration-level projects, three workspaces should always be enough for the type of iterative, agile change management that Google Tag Manager necessitates, at least in my experience.\n"
},
{
	"uri": "https://www.simoahava.com/tags/workspaces/",
	"title": "workspaces",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/ga-spy/",
	"title": "ga spy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/log-failed-google-analytics-requests-google-analytics/",
	"title": "Log Failed Google Analytics Requests In Google Analytics",
	"tags": ["debug", "ga spy", "google analytics", "Google Tag Manager"],
	"description": "You can use Google Analytics to collect information on other Google Analytics requests that failed to send correctly, for whatever reason.",
	"content": " READ THIS (26 Aug 2019)!! Unfortunately, the update I made in 2016 contained code that was incomplete and broken. I nevert noticed this until it was pointed out to me almost three years later. At this point, I don\u0026rsquo;t have a working backup of the solution, so unless some internet archive / cache service manages to surface the code, this article is basically lost.\n UPDATE 20 December 2016: I made some fixes to the solution - be sure to grab the latest code snippet from below!\nHaving worked in all sorts of Google Analytics projects over the years, I\u0026rsquo;ve found myself more and more interested in the technical underpinnings of digital analytics rather than the actual analytical work. We all play to our respective strengths, I guess. For example, there are still many purely technical / technological mysteries surrounding Google Analytics, because much of the data processing is done server-side.\nOne of these mysteries is a proper feedback loop for request quality. When you send a request to Google Analytics such as a Page View hit, you can verify that it works in multiple ways:\n  Google Tag Manager Debug mode\n  Google Analytics Debugger\n  Google Analytics Real Time reports\n  Google Tag Assistant recordings\n  Web browser\u0026rsquo;s own developer tools\n  I\u0026rsquo;ve covered much of Google Analytics debugging in this article.\nHowever, no matter how successful your own tests are, one thing is missing: a solid, reliable way of collecting data from all failed requests from your site visitors. Google Analytics doesn\u0026rsquo;t automatically log problems with data collection, even if Analytics notifications do reveal some issues.\nThis can, and should, lead to healthy doubt concerning the veracity of your data set - is it a representative sample of all hits sent from your digital property, or is some business critical information (such as Ecommerce) being dropped more than average?\nIn this article, I want to show a simple way of logging all failed requests in Google Analytics as new events. The solution builds on Stephen Harris\u0026rsquo; awesome GA Spy, and the technical approach is encapsulated in a single Custom HTML Tag in Google Tag Manager.\n  Read on, my friend!\nThe solution Here\u0026rsquo;s how it works.\n  GA Spy to processes every single call to the ga() global function.\n  For each call with the send command, send the hit payload to https://www.google-analytics.com/debug/collect (read more about the /debug/ endpoint).\n  Don\u0026rsquo;t forget to send the regular hit to Google Analytics.\n  If the /debug/ returns a failed request, push the error message and the failed hit payload into dataLayer.\n  In a nutshell, for every hit sent to Google Analytics, you\u0026rsquo;re also sending the same hit to the GA debug endpoint. This endpoint returns information on whether or not the request was a success, and if it wasn\u0026rsquo;t, what was wrong with it.\n{ \u0026#34;hitParsingResult\u0026#34;: [ { \u0026#34;valid\u0026#34;: false, \u0026#34;parserMessage\u0026#34;: [ { \u0026#34;messageType\u0026#34;: \u0026#34;WARN\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The value provided for parameter \u0026#39;ev\u0026#39; is invalid. Please see http://goo.gl/a8d4RP#ev for details.\u0026#34;, \u0026#34;messageCode\u0026#34;: \u0026#34;VALUE_INVALID\u0026#34;, \u0026#34;parameter\u0026#34;: \u0026#34;ev\u0026#34; } ], \u0026#34;hit\u0026#34;: \u0026#34;...\u0026#34; } ], \u0026#34;parserMessage\u0026#34;: [ { \u0026#34;messageType\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Found 1 hit in the request.\u0026#34; } ] } From dataLayer, I\u0026rsquo;m actually sending this information to Google Analytics via GTM. This way you\u0026rsquo;ll end up with a report that looks like this:\n  Heck, you can even click one of the error messages to drill down to the actual payloads. For example, if I choose \u0026lsquo;ea\u0026rsquo; error, I can see the payloads and verify that indeed, there\u0026rsquo;s no Event Action parameter with these hits.\n  I can then go to GTM, find the offending tag, and fix it to always send some value in Event Action.\nDue to the abstraction of dataLayer, you don\u0026rsquo;t have to log this information in Google Analytics. In fact, you might not want to, since you might already have some sort of logging rig setup in your development process. Thanks to Google Tag Manager, you can push the data to any endpoint that supports JavaScript-generated data transfer (e.g. HTTP requests).\nHowever, I\u0026rsquo;ve elected to send the information to Google Analytics. Doing so, I can align these validation errors with all the other hits that this particular user has sent to GA, and I can thus potentially identify bigger issues that these validation errors might only be a symptom of. For example, it\u0026rsquo;s possible that an Ecommerce tag only fails if the user has followed a specific funnel on the site. By sending the validation error to Google Analytics, I can see if it occurred during one of these broken funnels, giving me more information to fix it in the end.\nHow to do it You basically need a Custom HTML Tag that fires on the All Pages trigger, with a higher Tag Priority than any of your Google Analytics tags that might also fire on this event.\nIt\u0026rsquo;s important that this code starts its execution before your Google Analytics tags have a chance to start up. This code overwrites the ga() method, and any tag that has already fired will not be debugged with this solution.\nYou\u0026rsquo;ll first need the actual code for GA Spy, and you can download the latest version from Stephen\u0026rsquo;s repository. You need to copy-paste the JavaScript into a Custom HTML Tag, and then add the custom listener below it. Here\u0026rsquo;s the full example, where I\u0026rsquo;ve minified the GA Spy code to make it more compact. Scroll down past the GA Spy part to find the actual magic.\n\u0026lt;script\u0026gt; /* Minified GA Spy starts */ window.gaSpy=window.gaSpy||function(b){var j,k,c=function(a){if(b=null,a.debugLogPrefix=a.debugLogPrefix||\u0026#34;gaSpy\u0026#34;,!a.callback||\u0026#34;function\u0026#34;!=typeof a.callback)throw new Error(\u0026#34;[\u0026#34;+a.debugLogPrefix+\u0026#34;] Aborting; No listener callback provided.\u0026#34;);return a.gaObjName=a.gaObjName||window.GoogleAnalyticsObject||\u0026#34;ga\u0026#34;,a.debug=!!a.debug,a}(\u0026#34;function\u0026#34;==typeof b?{callback:b}:b),d=c.gaObjName,e=window[d],f=window.console\u0026amp;\u0026amp;c.debug;?function(){var a=[].slice.call(arguments);a.unshift(\u0026#34;[\u0026#34;+c.debugLogPrefix+\u0026#34;]\u0026#34;),console.log.apply(console,a)}:function(){},g=function(a){var b,d={args:a,the:{}},e=d.the;return c.debug\u0026amp;\u0026amp;function;(b,c){for(b=\u0026#34;Intercepted: ga(\u0026#34;,c=0;c1?b[0]:\u0026#34;t0\u0026#34;,e.command=b.length\u0026gt;1?b[1]:b[0],b=b[b.length-1].split(\u0026#34;:\u0026#34;),e.pluginName=b.length\u0026gt;1?b[0]:void 0,e.pluginMethodName=b.length\u0026gt;1?b[1]:void 0,\u0026#34;require\u0026#34;===e.command||\u0026#34;provide\u0026#34;===e.command?(e.pluginName=a[1],\u0026#34;provide\u0026#34;===e.command\u0026amp;\u0026amp;(e.pluginConstructor=a[2])):(\u0026#34;send\u0026#34;===e.command\u0026amp;\u0026amp;(e.hitType=a[a.length-1]\u0026amp;\u0026amp;a;[a.length-1].hitType||a[1]),\u0026#34;object\u0026#34;==typeof a[a.length-1]\u0026amp;\u0026amp;(e.trackerName=a[a.length-1].name||e.trackerName))),f(\u0026#34;Run listener callback\u0026#34;,e),!1!==c.callback(d)},h=function(){var a=[].slice.call(arguments);if(c.debug){if(!g(a))return f(\u0026#34;Command blocked.\u0026#34;)}else try{if(!g(a))return}catch(a){}return f(\u0026#34;Command allowed:\u0026#34;,a),h._gaOrig.apply(h._gaOrig,a)},i=function(){var a,b=h._gaOrig=window[d];f(\u0026#34;Hijack\u0026#34;,b._gaOrig?\u0026#34;(already hijacked)\u0026#34;:\u0026#34;\u0026#34;),window[d]=h;for(a in b)b.hasOwnProperty(a)\u0026amp;\u0026amp;(window[d][a]=b[a])};if(f(\u0026#34;Config:\u0026#34;,c),e||(f(\u0026#34;Instantiate GA command queue\u0026#34;),e=window[d]=function(){(window[d].q=window[d].q||[]).push(arguments)},e.l=1*new Date),e.getAll)f(\u0026#34;GA already loaded; cannot see previous commands\u0026#34;),i();else{if(!e.l)throw new Error(\u0026#34;[\u0026#34;+c.debugLogPrefix+\u0026#34;] Aborting; `\u0026#34;+d+\u0026#34;` not the GA object.\u0026#34;);if(f(\u0026#34;Command queue instantiated, but library not yet loaded\u0026#34;),e.q\u0026amp;\u0026amp;e.q.length;){for(f(\u0026#34;Applying listener to\u0026#34;,e.q.length,\u0026#34; queued commands\u0026#34;),j=[],k=0;k -1; })[0] .description; var errorHit = data.hitParsingResult[0].hit; window.dataLayer.push({ event: \u0026#39;gaValidationError\u0026#39;, gaValidationError: { description: errorDescription, hit: errorHit } }); }; // If a \u0026#39;send\u0026#39; command is registered, start the process  if (typeof ga === \u0026#39;function\u0026#39; \u0026amp;\u0026amp; trackerName \u0026amp;\u0026amp; gaCommand === \u0026#39;send\u0026#39;) { ga(function() { tracker = ga.getByName(trackerName); if (!tracker.get(\u0026#39;debugDone\u0026#39;)) { originalSendTask = tracker.get(\u0026#39;sendHitTask\u0026#39;); tracker.set(\u0026#39;sendHitTask\u0026#39;, buildDebugHit); } }); } } catch(e) { // Error handling  } }); \u0026lt;/script\u0026gt; This solution uses Universal Analytics Tasks API to copy the hit payload sent to Google Analytics, and to send it then to the debugger endpoint.\nSince you\u0026rsquo;re actually \u0026ldquo;hijacking\u0026rdquo; Google Analytics here, it\u0026rsquo;s very important that you test this thoroughly. To see if it works, you should see a POST request to /debug/collect for each actual hit to /collect. You can find this in the Network debugger of your browser\u0026rsquo;s developer tools. Here\u0026rsquo;s what the output looks like in Chrome:\n  Again, remember to test it.\nSend the information to Google Analytics To send the validation error hits to Google Analytics, you\u0026rsquo;ll need a Universal Analytics tag, two Data Layer variables and a Custom Event trigger.\nThe Data Layer variables should point to variable names gaValidationError.description and gaValidationError.hit. They might thus look something like this:\n  Next, the Custom Event Trigger is a simple affair, and looks like this:\n  Finally, the Universal Analytics tag is your run-of-the-mill Event tag, with just one important modification. You need to set a custom Fields to set field to:\nField name: debugDone\nValue: true\nDon\u0026rsquo;t bother looking it up in the supported field reference for analytics.js, it\u0026rsquo;s not there. It\u0026rsquo;s a custom field I created only for this solution. It prevents two things: 1) Calls to the debug endpoint from multiplying when the same tracker is used, and 2) calls to the debug endpoint for the validation error events.\nHere\u0026rsquo;s what the Event tag might look like:\n  This particular tag will send each validation error as a non-interaction event to Google Analytics, with the error description as the Event Action and the broken hit payload as the Event Label.\nSummary This solution relies on the awesomeness of GA Spy. The script basically hijacks the GA global method, and copies all commands to the debug endpoint.\nWhat this solution does is give you yet another tool for validating your Google Analytics setup. When working with complex Google Tag Manager setups, it might be difficult to keep tabs on all the variables you are using. This might lead to problems in your tags, when a required field ends up with a blank value just because a variable didn\u0026rsquo;t resolve in an expected way. This solution lets you find these cases with ease, giving you a clear path to fixing them before they become a real data quality problem.\nIt would be pretty neat to have the /debug/ interface as a tool you could install locally. That way you wouldn\u0026rsquo;t need to do the trip to Google Analytics servers, and instead validate the hits in your own web server. On the other hand, the logic isn\u0026rsquo;t probably that complex, so having it as a dedicated JavaScript library would be great as well. In any case, the endpoint debugger is a really smooth tool, especially when combined with GA Spy as illustrated in this article.\nAnother thing that would make this whole thing easier was if the original request to /collect simply returned the debug payload automatically. That way you could just look at the responses to the GA requests without having to do the extra trip to /debug/. I understand this doesn\u0026rsquo;t exist because of latency and how most hits to GA are still done with a GET request. Still, Google could at least make it a configurable setting in the GA request.\n"
},
{
	"uri": "https://www.simoahava.com/spam-filter/",
	"title": "Spam Filter Insertion Tool",
	"tags": [],
	"description": "",
	"content": "I took my Spam Filter Insertion Tool down since it\u0026rsquo;s very ineffective for tackling referral spam in Google Analytics. There are far better ways to combat spam.\nYou can still download the source in the GitHub repository if you want to build the tool yourself.\n"
},
{
	"uri": "https://www.simoahava.com/tags/amp/",
	"title": "amp",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/dan-wilkerson/",
	"title": "dan wilkerson",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-client-id-amp-pages/",
	"title": "Google Analytics Client ID In AMP Pages",
	"tags": ["amp", "client id", "dan wilkerson", "google analytics", "nodejs", "guest post"],
	"description": "How to setup Google Analytics Client ID bridging across AMP and non-AMP pages. You can use Google Tag Manager to set it up.",
	"content": "This article is a collaboration between Simo and Dan Wilkerson. Dan\u0026rsquo;s one of the smartest analytics developers out there, and he\u0026rsquo;s already contributed a great #GTMTips guest post. It\u0026rsquo;s great to have him back here sharing his insight on working with Accelerated Mobile Pages (AMP).\nSo, we\u0026rsquo;re back on AMP! Simo wrote a long, sprawling AMP for Google Tag Manager guide a while ago, and Dan has also contributed to the space with his guide for AMP and Google Analytics. Both of these guides touched upon a subject that might be one of the reasons to stay away from AMP for now: Client ID matching across AMP, your regular website, and any caches or CDNs that serve the AMP version.\n  Not only does AMP have its own, unique syntax for the Client ID, it also stores it in different ways. For example, in the AMP Cache (and thus via Google search, too), it\u0026rsquo;s stored in localStorage. If someone visits your site directly and accesses AMP pages, the Client ID is stored in the AMP_ECID_GOOGLE cookie. And then if someone visits a regular page on your site, the Client ID can be found in the _ga cookie.\nIn this article, Dan and Simo tackle this issue by showing how to serve the Client ID consistently from the _ga cookie written on your website\u0026rsquo;s actual domain. This way, AMP pages will use the same Client ID that regular Google Analytics uses, and you will be able to identify customer journeys that may pass through both your regular site as well as your AMP content.\nThe examples are provided in NodeJS and PHP (WordPress), but the methodology is universal and quite simple to do with any web server software.\nOverview To recap, Google Analytics stores a unique identifier for every user who visits your site within a first-party cookie. This value, in GA parlance, is called the Client ID, and the name of the cookie is _ga. Here\u0026rsquo;s what the entire cookie string looks like, with the actual Client ID bolded.\nGA1.2.1789536866.1471440764\nThe GA1 value denotes this is version 1 of the _ga cookie. The 2 denotes the number of dot-separated components in the URL the cookie is stored at. For example, if GA were instructed to store the cookie at shop.example.com, the number would be 3, instead.\nBecause AMP pages can be cached and served on many different domains, users who might have had a single Client ID could wind up being split into several users instead.\n  This appears to be at least part of the reason behind why Google Analytics officially recommends using a separate GA Property for AMP data.\nHowever, there\u0026rsquo;s a way to use only the _ga cookie for all sources that serve your web content. To do this, we\u0026rsquo;ll need to customize our amp-analytics component.\nCustom AMP configuration amp-analytics is the component that AMP uses for tracking user interactions. If you\u0026rsquo;d like to learn more about all the features it supports, check out the AMP for GTM guide in this blog; it should have all the information you need.\nFor our purposes, let\u0026rsquo;s focus on the config attribute. It\u0026rsquo;s an optional attribute that we can use to tell AMP: \u0026ldquo;Hey, we\u0026rsquo;ve got some additional configurations that we need to you fetch from this location\u0026rdquo;. This attribute should be set to a URL where additional analytics configurations should be retrieved from. In other words, you need to specify an HTTP request endpoint which returns a valid, AMP-compliant JSON configuration file.\n\u0026lt;amp-analytics config=\u0026#34;//example.com/analytics.config.json\u0026#34;\u0026gt;\u0026lt;/amp-analytics\u0026gt; Once the browser has loaded the AMP page, and the page is visible in the browser window, AMP will fetch this external configuration file and use it to supplement any configurations that might already be established on the page. Unlike other resources in your content, your amp-analytics config will always point to your server and the result isn\u0026rsquo;t automatically cached. When the request to your server comes in, it will include all of the cookies set on the user\u0026rsquo;s browser on your domain.\nNOTE! If, and when, your content is served through the Google AMP Cache, any external configuration must be downloaded from an HTTPS source. Thus if your web server is behind HTTP, you\u0026rsquo;ll need to either only serve your custom configuration when visitors are on your domain OR use just the default configuration template instead. Or, you know, switch to HTTPS as soon as possible.\nGrabbing the _ga cookie When the request for the custom AMP config is received by your web server, you can check the cookies in the HTTP request to see if a _ga cookie is already set for the user on the website domain. This would be the case if the user has visited your content before (and hasn\u0026rsquo;t flushed cookies). If the cookie is found, you can use a specific HTTP header (see below) in the response to pass this cookie to the domain where the request originated from, e.g. Google\u0026rsquo;s AMP cache.\nIf the cookie is not found, you can generate a new _ga cookie, following the same pattern analytics.js uses - a random unsigned 32-bit integer coupled with a timestamp rounded to the nearest second, like this:\n1789536866.1471440764\nYou can then leverage the Set-Cookie header in the HTTP Response, and instruct the browser to store the new Client ID in a _ga cookie, making sure to set the domain, path, and expiration date to match the domain the request originated from (e.g. cdn.ampproject.org), just like analytics.js does. You\u0026rsquo;ll need to add the same GA1.X. prefix that GA uses, too. Furthermore, you\u0026rsquo;ll need to do this on every single request, so that the lifetime of the _ga cookie continues to be extended on each page load. The Set-Cookie should end up looking something like this:\nSet-Cookie: _ga=GA1.1.18347199128.1478727498; Domain=example.com; Path=/; Expires=Sat, 10 Nov 2018 21:06:48 GMT;\nFinally, you can return the Client ID in the JSON response as a custom AMP variable for use with our AMP requests:\n{ \u0026#34;vars\u0026#34;: { \u0026#34;clientId\u0026#34;: \u0026#34;18347199128.1478727498\u0026#34; } } Checklist for setting up the request handler Of course, it\u0026rsquo;s not quite that simple. In addition to transposing our Client ID and setting it as a cookie, we need to ensure that we\u0026rsquo;ve dotted a few i\u0026rsquo;s. Here\u0026rsquo;s a handy checklist of the configuration steps that need to be taken to ensure everything works in the wild.\n  In the site HTML:\n  Add the amp-analytics script tag and amp-analytics component to your AMP templates.\n  Configure component JSON for desired triggers and requests, using ${clientId} for the \u0026amp;cid; parameter. Alternatively, you can use the pre-built Google Analytics vendor template by adding type=\u0026quot;googleanalytics\u0026quot; to the component.\n  Point config attribute to an endpoint or API on your own server.\n  Set data-credentials=\u0026quot;include\u0026quot;.\n    In your web server:\n  In the request handler in your web server, extract the Client ID from the _ga cookie or generate a new one.\n  Add the clientId parameter to the vars object in the JSON configuration. Set it to the Client ID from the _ga cookie.\n  Add the Set-Cookie header with the _ga cookie, set to expire in two years.\n  Set the Access-Control-Allow-Origin header to https://cdn.ampproject.org. Note: Wildcards (*) are invalid in this context.\n  Set the Access-Control-Expose-Headers header to AMP-Access-Control-Allow-Source-Origin.\n  Set the Access-Control-Allow-Credentials header to true.\n  Set the header AMP-Access-Control-Allow-Source-Origin to the source domain of the document (e.g. https://mysite.com).\n  Return the JSON configuration in the response body.\n    For a full example, check out this repository on GitHub. If you\u0026rsquo;re using the Google Analytics vendor configuration, this is all you need to do. If you\u0026rsquo;d like to combine this concept with Google Tag Manager, read on!\nGoogle Tag Manager proxy using NodeJS As Simo covered in his GTM/AMP guide, Google Tag Manager allows us to build Tags and Triggers in a web UI, then compile those down into a JSON configuration in the format AMP expects. If we used the standard GTM implementation, however, the container request is done directly to GTM\u0026rsquo;s server, which means our custom API wouldn\u0026rsquo;t be able to serve the proper Client ID. That said, you can still combine both techniques. You\u0026rsquo;ve just got to roll up your sleeves a little.\nHere\u0026rsquo;s a truncated example, using Node and Express. For the complete code, visit this GitHub repository.\napp.get(\u0026#39;/gtm-analytics.config.json\u0026#39;, (req, res) =\u0026gt; { const domain = req.headers.host.split(\u0026#39;:\u0026#39;)[0] const gaCookie = req.cookies._ga || generateGaCookie(domain) const clientId = parseClientIdFromGaCookie(gaCookie) const cookieString = generateCookieString({ name: \u0026#39;_ga\u0026#39;, value: gaCookie, domain: domain.replace(\u0026#39;www.\u0026#39;, \u0026#39;\u0026#39;), path: \u0026#39;/\u0026#39;, expires: new Date(1000 * 60 * 60 * 24 * 365 * 2 + (+new Date)).toGMTString() }) res.setHeader(\u0026#39;Set-Cookie\u0026#39;, cookieString) res.setHeader(\u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;https://cdn.ampproject.org\u0026#39;) res.setHeader(\u0026#39;Access-Control-Expose-Headers\u0026#39;, \u0026#39;AMP-Access-Control-Allow-Source-Origin\u0026#39;) res.setHeader(\u0026#39;Access-Control-Allow-Credentials\u0026#39;, \u0026#39;true\u0026#39;) // AMP-specific header, check your protocol  res.setHeader(\u0026#39;AMP-Access-Control-Allow-Source-Origin\u0026#39;, \u0026#39;https://\u0026#39; + domain) request.get({ url: \u0026#39;https://www.googletagmanager.com/amp.json\u0026#39;, qs: req.query, json: true }, (err, response, data) =\u0026gt; { if (err) data = {\u0026#34;vars\u0026#34;: {}} // Add additional error handling here  data.vars.clientId = clientId data.requests = Object.keys(data.requests) .reduce((map, key) =\u0026gt; { map[key] = data.requests[key].replace(/(\u0026amp;cid=)[^\u0026amp;]+/, \u0026#39;$1${clientId}\u0026#39;) return map }, {}) res.json(data) }) })  Here\u0026rsquo;s another checklist, this time for combining your custom Client ID workaround with the Google Tag Manager AMP container. I\u0026rsquo;ve bolded the new steps.\n  In your page HTML:\n  Add the amp-analytics script tag and amp-analytics component to your AMP templates.\n  Point config attribute to an endpoint or API on your own server.\n  Set data-credentials attribute to include.\n    In your web server:\n  In the request handler in your web server, extract the Client ID from the _ga cookie or generate a new one\n  Request the container JSON from GTM, passing along all query parameters from the original amp-analytics request.\n  Replace all instances of \u0026lsquo;CLIENT_ID(AMP_ECID_GOOGLE)\u0026rsquo; in the request with \u0026lsquo;${clientId}\u0026rsquo;.\n  Add the clientId parameter to the vars object in the JSON configuration. Set it to the Client ID from the _ga cookie.\n  Add the Set-Cookie header with the _ga cookie, set to expire in two years.\n  Set the Access-Control-Allow-Origin header to https://cdn.ampproject.org. Note: Wildcards (*) are invalid in this context.\n  Set the Access-Control-Expose-Headers header to AMP-Access-Control-Allow-Source-Origin.\n  Set the Access-Control-Allow-Credentials header to true.\n  Set the header AMP-Access-Control-Allow-Source-Origin to the source domain of the document (e.g. https://mysite.com).\n  Return the JSON configuration in the response body.\n    In your site code you\u0026rsquo;ll need:\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;amp-analytics config=\u0026#34;//www.yourdomain.com/gtm-analytics.config.json?id=GTM-XXXXX\u0026amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt;\u0026lt;/amp-analytics\u0026gt; Congratulations, you have created a web proxy that fetches the Google Tag Manager container from Google\u0026rsquo;s servers and modifies the JSON to leverage the value stored in the _ga cookie.\nGoogle Tag Manager proxy with WordPress This blog is running on WordPress, so Simo wanted to see how trivial it would be to create the endpoint. As it turns out, it\u0026rsquo;s very simple indeed.\nWordPress provides the rest_api_init hook, which lets you create an HTTP request endpoint on your web server:\n// REST API for GTM container add_action( \u0026#39;rest_api_init\u0026#39;, function() { register_rest_route( \u0026#39;amp-gtm\u0026#39;, \u0026#39;/amp.json\u0026#39;, array( \u0026#39;methods\u0026#39; =\u0026gt; \u0026#39;GET\u0026#39;, \u0026#39;callback\u0026#39; =\u0026gt; \u0026#39;retrieve_gtm_json\u0026#39;, ) ); }); That piece of code in your functions.php would create a GET request endpoint in your web domain path /wp-json/amp-gtm/amp.json. If a GET request to this endpoint is recorded, the callback function named retrieve_gtm_json is then invoked:\n// Generate random Client ID function generate_ga_client_id() { return rand(100000000,999999999) . \u0026#39;.\u0026#39; . time(); } // Set cookie to expire in 2 years function getCookieExpirationDate() { return date(\u0026#39;D, j F Y H:i:s\u0026#39;, time() + 60*60*24*365*2); } // Callback for the GET request function retrieve_gtm_json( $data ) { /* Get the hostname of the request origin, and parse it for the * pure domain name. */ $domain = explode(\u0026#39;:\u0026#39;, $data-\u0026gt;get_header(\u0026#39;Host\u0026#39;))[0]; $domainName = str_replace(\u0026#39;www.\u0026#39;, \u0026#39;\u0026#39;, $domain); // Get the number of parts in the domain name $domainLength = count(explode(\u0026#39;.\u0026#39;, $domainName)); /* Check if the browser already has the _ga cookie. * If not, generate a new cookie. */ $cid = $_COOKIE[\u0026#39;_ga\u0026#39;]; if (!isset($cid)) { $cid = \u0026#34;GA1.{$domainLength}.\u0026#34; . generate_ga_client_id(); } /* Store the actual Client ID (last two numbers) of the * _ga cookie value in the $cidNumber variable */ $cidNumber = preg_replace(\u0026#39;/^GA.\\.[^.]+\\./\u0026#39;,\u0026#39;\u0026#39;,$cid); // Get all HTTP request parameters $query = $_SERVER[\u0026#39;QUERY_STRING\u0026#39;]; /* Fetch the actual GTM container, by passing the valid query parameters from * the original request. */ $container = file_get_contents(\u0026#34;https://www.googletagmanager.com/amp.json?{$query}\u0026#34;); // Replace the \u0026amp;cid; parameter value with ${clientId} $container = preg_replace(\u0026#39;/(\u0026amp;cid=)[^\u0026amp;]+/\u0026#39;,\u0026#39;${1}${clientId}\u0026#39;, $container); // Add the clientId to the \u0026#34;vars\u0026#34; object in the container JSON. $container = json_decode($container); $container-\u0026gt;vars-\u0026gt;clientId = $cidNumber; // Build a new HTTP response from the modified configuration file. $response = new WP_REST_RESPONSE( $container ); // Add the required headers (Set-Cookie, most importantly) to the Request $response-\u0026gt;header( \u0026#39;Set-Cookie\u0026#39;, \u0026#34;_ga={$cid}; Path=/; Expires=\u0026#34; . getcookieExpirationDate() . \u0026#34; GMT; Domain={$domainName};\u0026#34;); $response-\u0026gt;header( \u0026#39;Access-Control-Allow-Origin\u0026#39;, \u0026#39;https://cdn.ampproject.org\u0026#39;); // Remember to check the protocol and change to http if that\u0026#39;s where your domain is $response-\u0026gt;header( \u0026#39;AMP-Access-Control-Allow-Source-Origin\u0026#39;, \u0026#34;https://{$domain}\u0026#34;); $response-\u0026gt;header( \u0026#39;Access-Control-Expose-Headers\u0026#39;, \u0026#39;AMP-Access-Control-Allow-Source-Origin\u0026#39;); // Return the HTTP response. return $response; } This is the API script that handles requests for the Google Tag Manager container. The proxy fetches the GTM container, and replaces the default Client ID with the AMP variable ${clientId}. This, in turn, is added to the configuration JSON with the value retrieved from the _ga cookie. If the _ga cookie doesn\u0026rsquo;t exist, a new one is generated.\nIn your site code, you\u0026rsquo;ll need:\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;amp-analytics config=\u0026#34;//www.yourdomain.com/wp-json/amp-gtm/amp.json?id=GTM-XXXXX\u0026amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt;\u0026lt;/amp-analytics\u0026gt; This request is then passed to your endpoint, and the process described above will take place.\nGoogle Analytics setup Not much has to be done in Google Analytics, but you will want to add ampproject.org into the Referral Exclusion List of your Google Analytics property settings. Otherwise, if the user follows any link from the cached AMP page to the rest of your site, the click will start a new session with a referral from ampproject.org.\nThanks to Adrian Vender for this tip!\nSummary This is a fairly technical topic, but we, the authors, found it necessary to point out this potential flaw in how doing analytics in Accelerated Mobile Pages might be detrimental to your overall tracking plan.\nThe fact that AMP doesn\u0026rsquo;t automatically leverage the _ga cookie if the request is to Google Analytics, for example, is a bit odd. Similarly, Google Tag Manager defaulting to AMP_ECID_GOOGLE is weird too, considering how much easier things would be if you could provide a cookie name or an AMP variable for the Client ID in the request.\nBecause Google\u0026rsquo;s AMP cache is a different domain from your own, there\u0026rsquo;s really no way around Client ID stitching that wouldn\u0026rsquo;t involve the type of third-party cookie scheme as described in this guide. The request must be allowed to process the cookies written on your domain, so that the same _ga cookie value can be used on the pages in the external domain.\nLuckily the technical solution to this dilemma is not too complicated. The proxy you create in your web server is simple, and should be easy to configure with any web server software. You might want to add some enhancements of your own, such as caching the Google Tag Manager container locally, and we\u0026rsquo;d love to hear your tips and experiences in the comments below.\nWe hope this article gets your stAMP of approval, and that you\u0026rsquo;ve learned an AMPle amount of new things. Sorry for the puns.\n"
},
{
	"uri": "https://www.simoahava.com/tags/accelerated-mobile-pages/",
	"title": "accelerated mobile pages",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/accelerated-mobile-pages-via-google-tag-manager/",
	"title": "Accelerated Mobile Pages Via Google Tag Manager",
	"tags": ["accelerated mobile pages", "amp", "google analytics", "Google Tag Manager", "mobile"],
	"description": "Guide on how to set up Accelerated Mobile Pages (AMP) tracking using Google Tag Manager&#39;s new AMP container.",
	"content": "Google Tag Manager recently published support for Accelerated Mobile Pages (AMP). This support comes in the form of a new Container type in Google Tag Manager.\n  When you create an AMP container in GTM, you are actually setting up an external configuration for AMP, which leverages AMP\u0026rsquo;s own analytics module. As befits Google Tag Manager, creating the configuration is done in the familiar Google Tag Manager user interface, and you have (almost) all the tools of regular Google Tag Manager at your disposal.\n  In this article, I want to go over how AMP and GTM mesh together to provide your mobile pages with improved tracking capabilities.\n1. AMP Overview Accelerated Mobile Pages is an open-source project, and you can read all about it at https://www.ampproject.org/. In a nutshell, it\u0026rsquo;s a set of structural instructions for building web pages, where focus is on speed and performance, without sacrificing too much UI/UX along the way.\nAMP is based on a number of design principles that might ring true to you if you\u0026rsquo;ve ever pondered about website performance. Things like asynchronous resource loading, inline CSS styling, web font optimization and optimized pre-rendering of pages are some of the features that AMP relies on to provide users with content super fast.\nCreating an AMP site Creating an AMP version of your site isn\u0026rsquo;t just a plug-and-play affair. You need to rewrite the HTML, JavaScript, and CSS styles to match the AMP design principles. If you\u0026rsquo;re using a platform like WordPress, there are plugins available that do most of the configuration for you. And here\u0026rsquo;s a great tip for AMP development work: anytime you are browsing an AMP page, you can add the URL hash #development=1 to the URL to validate your AMP page, outputting the result of the validation into your browser\u0026rsquo;s JavaScript Console.\n  AMP and Google search Finally, if you\u0026rsquo;ve setup your site with AMP pages, it\u0026rsquo;s a good idea to follow the Google Search Guidelines for informing the search engine about your site\u0026rsquo;s new mobile structure. Google will attempt to direct mobile searches of your site to the corresponding AMP pages, thus providing mobile visitors with fast, optimized access to your precious content.\nIf you want to see what AMP pages look like, you can visit any article on this site, and add /amp/ to the URL (e.g. https://www.simoahava.com/amp/analytics/accelerated-mobile-pages-via-google-tag-manager/.\n2. AMP Analytics The Google Tag Manager AMP container leverages the amp-analytics component. This component, developed within the AMP project, provides a light-weight framework for analytics requests sent either via a number of built-in vendor templates, or to a custom endpoint of your choosing.\nThe amp-analytics framework is managed by a JSON configuration object, where you specify details of the endpoint you want to send the data to, as well as variables and triggers (sound familiar?) that govern what analytics requests are sent and when.\nEven though the amp-analytics documentation isn\u0026rsquo;t particularly long or complex, there\u0026rsquo;s still many things to consider when configuring a custom tracking scheme. That\u0026rsquo;s why it\u0026rsquo;s very useful to have pre-built templates for a number of analytics vendors (e.g. Google Analytics, Adobe Analytics, Snowplow Analytics).\nAdd amp-analytics to your website To add support for the analytics component on your site, you need to add the following line of code into the \u0026lt;head\u0026gt; of your document:\n\u0026lt;script async custom-element=\u0026#34;amp-analytics\u0026#34; src=\u0026#34;https://cdn.ampproject.org/v0/amp-analytics-0.1.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; This loads the AMP analytics library, and instructs the page to look for an element named \u0026lt;amp-analytics\u0026gt;. This custom element is where you introduce the JSON configuration object that will eventually govern how the site is tracked.\nGoogle Analytics has a great developer guide for setting up the JSON configuration object manually. For example, to send a Page View hit to Google Analytics when the page is loaded, you\u0026rsquo;d add the following element to the \u0026lt;body\u0026gt; of the site:\n\u0026lt;amp-analytics type=\u0026#34;googleanalytics\u0026#34; id=\u0026#34;analytics1\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;account\u0026#34;: \u0026#34;UA-XXXXX-Y\u0026#34; }, \u0026#34;triggers\u0026#34;: { \u0026#34;trackPageview\u0026#34;: { \u0026#34;on\u0026#34;: \u0026#34;visible\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;pageview\u0026#34; } } } \u0026lt;/script\u0026gt; \u0026lt;/amp-analytics\u0026gt; As you can see, you\u0026rsquo;re using the custom element \u0026lt;amp-analytics\u0026gt; that you specified when adding the initial script loader to the \u0026lt;head\u0026gt; of your page. The type parameter of the element specifies that you are using the built-in googleanalytics vendor template.\nBe sure to check out Bounteous\u0026rsquo; excellent guide for Google Analytics and AMP integration!\nLoad the JSON configuration object as an external resource You can also load the JSON configuration object from an external source (as long as the request adheres to AMP CORS security guidelines). For example, the Google Tag Manager \u0026ldquo;container\u0026rdquo; is actually a request to an external config file that you specify like this:\n\u0026lt;amp-analytics config=\u0026#34;https://www.googletagmanager.com/amp.json?id=GTM-XXXXXX\u0026amp;amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;someCustomAmpVar\u0026#34;: \u0026#34;someValue\u0026#34; } } \u0026lt;/script\u0026gt; \u0026lt;/amp-analytics\u0026gt; This downloads a file named amp.json, specified by using your container ID, and the data-credentials attribute allows the resource request to read and write cookies as needed. The \u0026lt;script\u0026gt; block within the amp-analytics element can be used to use your own custom AMP variables on the page.\nNote that the external config file download will only begin if the page is visible in the viewport! For example, if you open an AMP page with a remote configuration link in a new browser window without making that window active, the configuration file download will wait until such a moment that the page becomes visible. This means that your analytics tracking will not commence until the page is visible in the browser.\nAnyway, back to the JSON configuration object. It details what interactions and events are tracked to the analytics endpoint of your choosing, when they are tracked, and how. I recommend you read through the AMP Analytics documentation, as it sheds quite a bit of light on how the GTM AMP container works, too.\nAMP Analytics with its JSON configuration object is quite far removed from the dynamic execution context of JavaScript and the Document Object Model. However, you should respect the fact that all compromises are done in favor of improved performance. I hope some of the AMP Analytics methodology would rub off on the bloated, performance-killing, dynamic mess that \u0026ldquo;modern\u0026rdquo;, JavaScript-based web analytics so often is.\n2.1. Client ID Before we move on to how the GTM AMP container works, we need to talk about Client ID. You can skip this chapter if all you want to do is get Google Tag Manager up and running on your AMP site.\nGoogle Analytics uses the Client ID parameter to align hits with the users who sent the hits. This ID is stored in a first-party cookie named _ga. Thus, every time you visit a website running Google Analytics, this cookie is used to make sure all the hits you send are tied together with your previous visits.\nFirst-party cookies can only be written on the domain you are currently on. Consequently, if you want to travel from one domain to another and still maintain all your hits under the same Client ID, you will need to somehow pass your cookie value from one domain to another without violating the restriction outlined in the first sentence of this paragraph.\nClient ID and AMP With AMP, things are slightly more difficult. AMP Analytics does not use the _ga cookie by default, even though you can set it up so that AMP falls back to _ga if one is found. But even if you do set it up to use _ga, what if the user browses the AMP page via Google search or the AMP CDN? Both cache your content in an external domain (www.google.com and cdn.ampproject.org, respectively), which means that they will not be able to access any cookies written on your domain. Also, AMP\u0026rsquo;s default Client ID syntax is vastly different from the one used by Google Analytics.\nThis all means that even if you did manage to use your existing _ga cookie as the Client ID in AMP pages on your site, it\u0026rsquo;s not enough. You see, if the user lands on your AMP page via Google search, which is probably the most typical use case, they\u0026rsquo;ll actually visit a cached version of your page on www.google.com. This means that no _ga cookie is found, and AMP defaults back to a random, unique ID.\nHacking to create a single, unified Client ID There are workarounds, and thanks to some brainstorming with the inimitable Dan Wilkerson from Bounteous (who already wrote an excellent guide for Google Analytics and AMP), here\u0026rsquo;s one suggestion that I\u0026rsquo;ll probably expand in a later article.\n  Here\u0026rsquo;s how this particular solution works:\n  Instead of fetching the GTM container directly, the request is sent to a custom HTTP endpoint you need to set up on your website\u0026rsquo;s domain.\n  This API serves a cached version of the GTM container, or fetches the most recent version if the cache has expired.\n  Instead of using the default, random Client ID which AMP Analytics uses, Client ID is retrieved from the _ga cookie written on the domain of the REST API, i.e. your website.\n  If no cookie is found, a new one is created.\n  The _ga value is returned in the Set-Cookie HTTP Response Header, so that the cookie will be written on the domain which originated the request (e.g. www.google.com or cdn.ampproject.org).\n  The HTTP Response also contains the JSON configuration object (with the new Client ID) used by the site to set AMP Analytics up.\n  In other words, you\u0026rsquo;re creating a proxy on your web server, which relays requests for the Google Tag Manager container, while modifying the container so that it includes (or creates) the Client ID used by Google Analytics.\n  It\u0026rsquo;s convoluted, yes, and there might be easier ways to handle this, but for now it lets you stitch together AMP traffic with other on-site traffic, such as desktop visits to non-AMP pages. For some publishers, and especially for ecommerce sites, this is vital.\n3. Create and implement GTM/AMP container As mentioned earlier, the Google Tag Manager AMP container is essentially a remote JSON configuration object that AMP Analytics uses for tracking interactions on your page.\nAlso, there is no dataLayer. Any custom variables you want to pass from the page to the JSON configuration object need to be included in the \u0026lt;script\u0026gt; block within the amp-analytics element:\n\u0026lt;amp-analytics config=\u0026#34;https://www.googletagmanager.com/amp.json?id=GTM-5BH2HM\u0026amp;amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;gaTrackingId\u0026#34;: \u0026#34;UA-12345-1\u0026#34; } } \u0026lt;/script\u0026gt; \u0026lt;/amp-analytics\u0026gt;   The benefit of using Google Tag Manager is that you can use the trusted user interface to configure your AMP container just as you\u0026rsquo;d configure your web container. All the supported vendor templates are available as tags, so creating a request for your endpoint is quite simple.\n  Once you\u0026rsquo;re done configuring, you need to hit the PUBLISH button, and the AMP Analytics JSON configuration object will be updated automatically for all web visitors.\nAMP containers are a new selection option in the container creation dialog:\n  Once you\u0026rsquo;ve created the container, you might want to click the PUBLISH button, and publish a base container version. It\u0026rsquo;s OK that you\u0026rsquo;re not serving any tags yet; you just need to publish the container once to make sure it doesn\u0026rsquo;t respond with a 404 when accessed by your site.\nNow that you\u0026rsquo;ve created your AMP container, you need to deploy it.\nDeploying the AMP container AMP Analytics consists of two parts: the amp-analytics JavaScript library and the JSON configuration object. To deploy AMP Analytics via Google Tag Manager, you will need to implement both in your page templates.\nFirst, the library. In AMP, it\u0026rsquo;s actually called an Extended Component of the AMP HTML schema. For AMP Analytics to work, you must deploy the following in the \u0026lt;head\u0026gt; of your website:\n\u0026lt;script async custom-element=\u0026#34;amp-analytics\u0026#34; src=\u0026#34;https://cdn.ampproject.org/v0/amp-analytics-0.1.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; This loads the AMP Analytics library asynchronously. As you can see, you are also foreshadowing the existence of a custom AMP HTML element named amp-analytics.\nThe next thing you need to do is tell the page where to load the remote JSON configuration object from. If you recall, this is your Google Tag Manager AMP container, so you must add the following code into the  of your website:\n\u0026lt;amp-analytics config=\u0026#34;https://www.googletagmanager.com/amp.json?id=GTM-5BH2HM\u0026amp;amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;someAmpVariable\u0026#34;: \u0026#34;someValue\u0026#34; } } \u0026lt;/script\u0026gt; \u0026lt;/amp-analytics\u0026gt; This is the custom amp-analytics element we were just talking about. The value of the config attribute is a reference to your GTM container, and the data-credentials=\u0026quot;include\u0026quot; part is important, as it gives permission for the downloaded configuration to read and write cookies on your domain. You can pass custom AMP variables to the JSON configuration object by using the embedded \u0026lt;script\u0026gt; block as shown.\nNote that if you went ahead and implemented the Client ID hack I introduced earlier (if even possible from my crazy complicated description), you\u0026rsquo;d need to replace the reference to Google Tag Manager\u0026rsquo;s servers with your own custom endpoint, e.g.:\n\u0026lt;amp-analytics config=\u0026#34;https://www.simoahava.com/wp-json/amp-gtm/amp.json?id=GTM-5BH2HM\u0026amp;amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt;\u0026lt;/amp-analytics\u0026gt; Do not forget gtm.url The \u0026amp;gtm.url parameter is important. By adding SOURCE_URL as its value, you are instructing AMP to resolve a platform variable as the value of the gtm.url parameter. This particular platform variable is automatically substituted with the URL of the website from which the request was made. If the page is served from a known proxy, such as the AMP Cache, it will still resolve to the actual URL of the website whose content is being served.\nThis parameter is significant, as it provides a source URL for the GTM container, which you can then utilize the enable triggers only on certain pages of your site.\nOnce you\u0026rsquo;ve done these steps, you are ready start configuring your tracking! On the surface and in the UI, things might seem all too familiar. You\u0026rsquo;ll create your tags just as you used to, filling the fields as before, so it\u0026rsquo;s as if nothing\u0026rsquo;s changed.\nBy the way, if this is all new to you, remember to check out these excellent GTM learning resources.\nThere are lots of differences compared to GTM for the web, however. For one, there really is no dataLayer anymore, since AMP doesn\u0026rsquo;t let you run arbitrary JavaScript. This severely delimits what you can do with triggers and variables.\nOn that note, most of the triggers and variables that are available to you are either completely new or behave differently from before. Also, the way the container is built and validated server-side has changed dramatically.\nThe following sections of this guide will take a closer look at all these changes.\n4. Triggers To understand how AMP triggers work, you must unlearn most of what you know about Google Tag Manager\u0026rsquo;s triggers.\nIn ye olde Google Tag Manager, triggers are dynamic conditions that react to dataLayer.push() commands. If the dataLayer.push() had a specific key-value structure, the trigger would \u0026ldquo;fire\u0026rdquo;, executing any tag to which it was attached.\nThis meant that the GTM container contained all the tags that had any trigger attached to them.\nIn the AMP GTM container, the JSON configuration object only includes those tag requests whose triggers pass a server-side validation check! In other words, you can forget any triggers whose enabling conditions depend on a dynamic value. No more \u0026ldquo;Data Layer variable equals this\u0026rdquo; or \u0026ldquo;Custom JavaScript variable returns that\u0026rdquo;. The only Built-In variables you can check against in a trigger enabling condition are:\n  Container ID\n  Container Version\n  Environment Name\n  Page Hostname\n  Page Path\n  Page URL\n  Random Number\n  The User-defined types (in addition to e.g. Random Number and Container Version, which are also variable types) you can utilize are URL (as long as it uses Page URL as the source), Constant and Lookup Table.\nWhy just these? Because all of these (and only these) can be resolved server-side by Google when the HTTP request for the JSON configuration object comes in. In other words, you can only use variables whose value is known by Google when the request for the JSON configuration object is received. This rules out all AMP variables, for example, as they are resolved in the browser.\nBecause validation is done server-side, GTM does not have access to the URL of the page that made the request automatically. You need to explicitly add this information into the container snippet!\nLet me show you an example. Say you create a trigger like this in your AMP container:\n  Any tag that has this trigger attached to it will be added to the JSON configuration object if the HTTP Request for the AMP configuration has the current domain in the gtm.url parameter. If you remember, you need to add this parameter to the container request, and set its value to SOURCE_URL.\n  The SOURCE_URL AMP variable is resolved to the actual URL of the page when the request for the container JSON is made. Without this parameter, server-side validation against the Page Hostname, Page Path, and Page URL variables will not most likely not work as you expect them to.\nIf at least one trigger that you have added to the tag passes server-side validation, the end result is something like this in the JSON configuration object:\n\u0026#34;5\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.pageview\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;visible\u0026#34; } This simply means that on the initial page load, this trigger should fire request number \u0026ldquo;2\u0026rdquo;, which is the Page View tag turned into a request to GA. As you can see, there\u0026rsquo;s nothing about trigger conditions in this JSON block.\nIn fact, all the five (at the time of writing) available trigger types have the same binary setting:\n  If the trigger does not pass server-side validation, the tag is dropped from the JSON configuration object, and there\u0026rsquo;s no way to dynamically alter page conditions to fire it in the browser.\nI\u0026rsquo;ve spent quite a bit of time with this change in the basic functionality of GTM triggers, but deservedly so. Server-side validation is a huge difference to how GTM used to work. Again, this ensures that AMP pages are lightning-fast, as the JSON configuration object is basically just a few lines of code. That\u0026rsquo;s light-weight compared to the complexity of a \u0026ldquo;regular\u0026rdquo; GTM container.\nNow, let\u0026rsquo;s take a look at the available triggers themselves.\n4.1. Click The Click trigger in the AMP container is pretty much the same thing as the All Elements trigger in regular Google Tag Manager. In other words, the target of the click (if valid) is the element that is captured. The Just Links trigger, on the other hand, does not exist in the AMP container, and you need to configure link capturing yourself.\n  The Click trigger has just one setting: CSS Selector. With this setting, you specify which clicks you want to listen to. If you want a quick refresher on CSS Selectors, read my article on the topic, or check out this excellent CSS Selectors Reference.\nTo help you get going, here are some useful CSS Selectors for you:\nOutbound Link Clicks\na:not([href*=\u0026quot;mydomain.com\u0026quot;]), a:not([href*=\u0026quot;mydomain.com\u0026quot;]) *\nMailto: Link Clicks\na[href^=\u0026quot;mailto:\u0026quot;], a[href^=\u0026quot;mailto:\u0026quot;] *\nTel: Link Clicks\na[href^=\u0026quot;tel:\u0026quot;], a[href^=\u0026quot;tel:\u0026quot;] *\nWhen you add a tag that fires on a click to a container, this is what the ensuing JSON looks like:\n\u0026#34;triggers\u0026#34;: { \u0026#34;11\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;selector\u0026#34;: \u0026#34;:not(*)${gtm_css_2_0}\u0026#34;, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.click\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;click\u0026#34; } }, \u0026#34;vars\u0026#34;: { \u0026#34;gtm_css_2_0\u0026#34;: \u0026#34;,a:not([href*=\\\u0026#34;simoahava.com\\\u0026#34;]), a:not([href*=\\\u0026#34;simoahava.com\\\u0026#34;]) *\u0026#34; } The selector is included in the trigger JSON with the following syntax: :not(*)${gtm_css_2_0}. The part in the curly brackets is an AMP variable substitution, which refers to a key in the \u0026quot;vars\u0026quot; object further down the JSON. So, if you replace ${gtm_css_2_0} with the value of vars.gtm_css_2_0, you get the following string:\n:not(*),a:not([href*=\u0026quot;simoahava.com\u0026quot;]), a:not([href*=\u0026quot;simoahava.com\u0026quot;]) *\nThe first selector, :not(*), clears the selector chain, matching no element on the page. After that, the selectors you specified for the trigger are included. The amp-analytics code then listens for clicks on the page, matching them against the selector, and fires the respective request if the clicked element matches the selectors you have defined for each request.\n4.2. Page View The Page View trigger is very simple. If you have All Page Views selected, or the Some Page Views condition matches the value retrieved from the gtm.url parameter of the incoming HTTP Request, then any tags attached with the Page View trigger will be included in the JSON configuration object. The Page View trigger has no \u0026ldquo;DOM Ready\u0026rdquo; or \u0026ldquo;Window Loaded\u0026rdquo; differentiation any more - it will fire as soon as possible (see below).\n  When the trigger passes server-side validation, it is added to the JSON configuration object like this:\n\u0026#34;triggers\u0026#34;: { \u0026#34;5\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.pageview\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;visible\u0026#34; } } The \u0026quot;on\u0026quot;: \u0026quot;visible\u0026quot; is quite literal - if the page is visible in the current browser window, the trigger will fire as soon as the AMP analytics JavaScript has loaded. Otherwise, the trigger will wait until such a moment as the page becomes visible. Some time ago, I wrote a custom visibility trigger setup for regular Google Tag Manager, so I\u0026rsquo;m very happy to see it on by default in AMP containers.\nThere\u0026rsquo;s more on the Visibility trigger below.\n4.3. Scroll The Scroll trigger is a very useful little tool. It lets you trigger tag requests on certain scroll thresholds, which are represented by percentage of vertical or horizontal scrolling. The percentage is calculated from the maximum available height or width of the page, respectively.\nScroll tracking has been instrumental in content engagement measurement over the years, so it\u0026rsquo;s a welcome addition to the default trigger set in amp-analytics, and Google Tag Manager by extension.\n  It\u0026rsquo;s very simple to set up. A single Scroll trigger can handle all the thresholds that you indicate with a comma-separated list of percentages. For example, to fire the Scroll trigger for each 25% increment of vertical scrolling, you would add 25, 50, 75, 100 into the respective field. This will naturally work for both mobile and desktop browsing.\nWhen you add a scroll trigger, it looks like this in the JSON configuration object:\n\u0026#34;triggers\u0026#34;: { \u0026#34;3\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;scrollSpec\u0026#34;: { \u0026#34;verticalBoundaries\u0026#34;: [25, 50, 75, 100], \u0026#34;horizontalBoundaries\u0026#34;: [100] }, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.scroll\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;scroll\u0026#34; } } The problem with the Scroll trigger is that there are no dynamic variables in the AMP GTM container that you can use to detect which increment triggered the tag. So if you have a single Scroll tracker for 25%, 50%, 75%, and 100%, your tag will trigger four times on the page, but you can\u0026rsquo;t simply use a variable to tell you directly which increment it was that triggered the tag.\nNow, AMP knows how far you scrolled by calculating a bunch of document and event properties. Each threshold in your Scroll trigger is checked against the following calculation:\n(scrollTop + viewportHeight) / scrollHeight\nCoincidentally, all of these are available as AMP variables. Here, scrollTop is the number of pixels the top of your viewport is from the top of the document, viewportHeight is the height of the current viewport in pixels, and scrollHeight is the height of the entire document in pixels.\nSo the calculation checks how far from the top of the page the current viewport bottom, i.e. the very bottom edge of the content page you see in your browser window, is. If it hits 25%, the tag is triggered, and then again at 50%, 75% and 100%.\nIf in your analytics solution you want to know which of these thresholds was crossed, here are some solutions:\n  Create a separate tag and trigger for each threshold. So if you want to track 25%, 50%, 75%, and 100%, you would end up with four triggers (one per threshold), and four tags. In each of the tags, you can now tell precisely which threshold was crossed.\n  Use just a single trigger, but send the scrollTop, viewportHeight and scrollHeight as Custom Dimensions (Google Analytics example), and then do the calculations in a spreadsheet or something else where you export the data to.\n  There\u0026rsquo;s a nifty AMP variable named counter, which lets you create a counter which increments each time the variable is resolved. This would be the solution for tracking the thresholds, but unfortunately it isn\u0026rsquo;t supported in the GTM AMP container (yet).\n4.4. Timer The Timer trigger is something you might be intuitively familiar with if you\u0026rsquo;ve ever worked with Google Tag Manager. When the container is loaded, any Timer triggers in the JSON configuration object are initiated. The firing pattern is determined by the settings you input into the trigger:\n  Interval is the number of seconds that needs to pass before the trigger is fired.\nLimit is the maximum amount of time that the trigger loops.\nFire Immediately When Triggered when checked fires the request once as soon as the container is loaded.\nThe Interval and Limit can be quite confusing. First of all, if you want the Timer to have an unlimited amount of activations, just leave the Limit field empty. Otherwise, you should use a multiple of Interval in the Limit field, so that you can specify how many times the trigger goes off.\nFor example, if you\u0026rsquo;ve set the Interval to 10 seconds, and you want the trigger to fire six times, you would set the Limit to 60.\nThe Fire Immediately When Triggered setting governs whether or not the first iteration of the trigger should fire as soon as the container is downloaded (checked), or whether the trigger should wait for one interval before firing (unchecked).\nNote that the same caveat about visibility that applied to the Page View trigger is relevant here, too. The Timer trigger will only fire a request if the page is visible. However, the timer will not pause while the page is not visible. Thus, all the timer hits that should have fired within the limitations of the Interval and Limit settings, but were deferred due to the page not being visible, will be fired the moment the page becomes visible again.\nWhen you create a Timer trigger, it will look like this in the JSON configuration object:\n\u0026#34;triggers\u0026#34;: { \u0026#34;3\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;timerSpec\u0026#34;: { \u0026#34;interval\u0026#34;: 10, \u0026#34;maxTimerLength\u0026#34;: 20 }, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.timer\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;timer\u0026#34; } } If you left the \u0026ldquo;Fire Immediately When Triggered\u0026rdquo; unchecked, under \u0026quot;maxTimerLength\u0026quot;: 20 you would have the setting \u0026quot;immediate\u0026quot;: false.\n4.5. Visibility Now here\u0026rsquo;s an interesting trigger! The Visibility trigger lets you fire a tag when a specific element has been visible in the browser window a set amount of time. Heck, you can even define that a certain minimum amount of the element has to be visible for the trigger to fire. That\u0026rsquo;s really useful for ad impression tracking!\n  Element ID is the unique identifier of the element. It\u0026rsquo;s required, so you can\u0026rsquo;t track the visibility of an element without an ID attribute.\nMinimum Percent Visible is the minimum amount of the element that needs to be visible in the browser viewport for the trigger to be valid for firing. If you leave this out, it means that the trigger will fire even if the element is not in the viewport! Kind of defeats the purpose, so try to remember to have at least 1 in this field.\nMaximum Percent Visible is the maximum amount of the element that can be visible in the browser viewport. So if you set 10 as the Minimum Percent Visible and 50 as the Maximum Percent Visible, the trigger will only fire when you have between 10 and 50 percent of the element in the viewport.\nMinimum Continuous Time is the minimum amount of time in one stretch that the element must be visible in the viewport (in milliseconds). So if you have 2000 in this field, it means that the element must be in the viewport for 2 seconds without interruption for this condition to pass.\nMinimum Total Time is the minimum total time that the element needs to be visible in the viewport. If you have 2000 in this field, it\u0026rsquo;s enough to have the element visible for a total of 2 seconds, but it doesn\u0026rsquo;t have to be continuous. The user can reveal the element for 500 milliseconds a time, and after four such times the Minimum Total Time of 2000 has been reached.\nAll of these conditions stack. So, let\u0026rsquo;s say you have a trigger that looks like the one in the screenshot above. What it translates to is this:\nAn element with ID AMP_2 must have at least 10 percent of its total area in the browser viewport for at least 5 seconds altogether. Also, the trigger will only fire if the element is in the viewport without interruption for at least 1 second.\nWhen you publish a trigger like this, it will translate into the following JSON:\n\u0026#34;triggers\u0026#34;: { \u0026#34;3\u0026#34;: { \u0026#34;request\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;visibilitySpec\u0026#34;: { \u0026#34;selector\u0026#34;: \u0026#34;#AMP_2\u0026#34;, \u0026#34;visiblePercentageMin\u0026#34;: 10, \u0026#34;continuousTimeMin\u0026#34;: 1000, \u0026#34;totalTimeMin\u0026#34;: 5000 }, \u0026#34;vars\u0026#34;: { \u0026#34;gtm.event\u0026#34;: \u0026#34;gtm.visible\u0026#34; }, \u0026#34;on\u0026#34;: \u0026#34;visible\u0026#34; } } I hope that support for other CSS selectors than just IDs would become available for selecting the element. I guess the reasoning is that such a complex trigger would be difficult to maintain consistently if it matched multiple elements on the page.\n5. Variables AMP Analytics supports two types of variables: platform and page-defined variables. Platform variables are default variables that do not need to be specified in the JSON configuration object. They are automatically resolved based on information available in the Document Object Model.\nPage-defined variables are added to the JSON configuration object. They can be defined in multiple places, such as a remote configuration file, top-level of an embedded configuration object, trigger-specific configuration, or even in special data attributes configured in elements themselves. There is an order of priority in variables which share the same name, and they are resolved in the following order of importance:\nRemote configuration file \u0026gt; element level data attributes \u0026gt; triggers \u0026gt; top-level of configuration object \u0026gt; platform.\nSo if you had \u0026amp;cid=${clientId(some-cid-cookie)} in your request to Google Analytics (platform variable), and also \u0026quot;vars\u0026quot;: {\u0026quot;clientId\u0026quot;: \u0026quot;12345\u0026quot;} in the root of the amp-analytics configuration object, you\u0026rsquo;d end up with \u0026amp;cid=12345 in the actual request, since the configuration object trumps platform variables in priority.\nAll variables are resolved when the request to any given endpoint is compiled, so they might well have different values for each request.\nIn the AMP Google Tag Manager container, you can create variable references to these AMP Analytics variables using the AMP variable type. However, there are other variables at your disposal, too, and we\u0026rsquo;ll take a look at these as well in this chapter.\n5.1. Built-In Variables The Built-In variables in the GTM container are variables that you do not need to manually create. To enable them, you need to go to Variables in the container, and under the \u0026ldquo;Built-In Variables\u0026rdquo; heading click the large red CONFIGURE button to enable / disable any variables. It\u0026rsquo;s weird that the Built-In variables aren\u0026rsquo;t all enabled by default, since there\u0026rsquo;s no harm in having them all available. In regular GTM, enabling Built-In variables meant some extra clutter in the Google Tag Manager library, but here they are only available in the UI to make it easier to configure your tags.\n  Here are all the Built-In variables with a short description of each.\n  Total Engaged Time (AMP variable): The total time the page has been visible in the viewport in seconds.\n  Scroll Width (AMP variable): Total width of the page in pixels.\n  Scroll Top (AMP variable): Number of pixels between the top of the viewport and the top of the page.\n  Scroll Left (AMP variable): Number of pixels between the left edge of the viewport and the left edge of the page.\n  Scroll Height (AMP variable): Total height of the page in pixels.\n  Screen Width (AMP variable): Total width of the screen. Does not take into account stuff like widgets, scroll bars, etc.\n  Screen Height (AMP variable): Total height of the screen. Does not take into account stuff like bookmark and address bars.\n  Random Number: Generates a random number between 0 and 2147483647. Note that this number is generated server-side by GTM. Once the JSON configuration object is downloaded, any references to this variable will have the same generated number.\n  Page View ID (AMP variable): Random number that is unique per user, URL, and day. Each refresh of a page sets a new random number.\n  Page URL: This is the URL passed to GTM with the gtm.url=SOURCE_URL query parameter in the GTM container snippet. This, Page Path, and Page Hostname are the only variables you can use in GTM to delimit tags from triggering only on certain pages.\n  Page Path: Returns the path (e.g. /analytics/accelerated-mobile-pages-via-google-tag-manager/) of the URL in the gtm.url query parameter of the GTM container request.\n  Page Load Time (AMP variable): This is the entire load time of the page in milliseconds. It is calculated from the moment the previous page started its unload to the moment the current page has fully loaded. If there is no previous page, the timer starts from the moment when the HTTP request for the page content was ready to dispatch.\n  Page Hostname: Returns the hostname (e.g. www.simoahava.com) of the URL in the gtm.url query parameter of the GTM container request.\n  Page Download Time (AMP variable): Time in milliseconds from the moment the request for the page content was dispatched to the moment the last byte of the document was downloaded from the web server.\n  Environment Name: The name of the custom created GTM Environment (if any) that the current GTM JSON configuration object is downloaded from. Yes, Environments work in the AMP container, too.\n  Document Title (AMP variable): Title of the current document.\n  Document Referrer (AMP variable): The URL of the page the user navigated from. This variable will resolve to an empty string if no referrer information is available.\n  Container Version: The version number of the container that received the request. If you are in Preview mode, it returns QUICK_PREVIEW.\n  Container ID: Returns your GTM Container ID (GTM-XXXXX).\n  Client Timezone (AMP variable): Returns the timezone offset in minutes from UTC based on client\u0026rsquo;s system time. For example, if you are in New York City, Client Timezone would return 300, because New York is 5 hours behind UTC. In Helsinki, which is two hours ahead of UTC, Client Timezone would return -120.\n  Client Timestamp (AMP variable): Returns the number of seconds that have passed since Jan 1, 1970 (epoch time).\n  Canonical URL (AMP variable): Returns the full canonical URL of the page. Typically this should be the URL without /amp/ or any other AMP parameter.\n  Canonical Path (AMP variable): Returns just the pathname of the canonical URL of the page.\n  Canonical Host (AMP variable): Returns the complete host of the canonical URL of the page (e.g. http://www.mydomain.com:9000).\n  Browser Language (AMP variable): Returns the language of the browser UI, e.g. en-us or fi.\n  AMP Event (AMP variable): Returns the name of the event that triggered the tag. GTM automatically adds event names like gtm.click (Click trigger) and gtm.pageview (Page View) to this variable.\n  Once you\u0026rsquo;ve enabled these variables, you can refer to them in your tags, triggers, and variables by using the {{Variable Name}} syntax.\n5.2. User-Defined variables In addition to Built-In variables, you can also create variables of your own. These can be added in the variables page of your container, under the heading \u0026ldquo;User-Defined variables\u0026rdquo;.\nRemember that you can reference any GTM variable with the {{Variable Name}} syntax.\nAt the time of writing, here are the variable types you can configure.\nURL URL variables exist in regular Google Tag Manager, too. However, in AMP, the URL variable can only be used to provide data about the URL included in the container HTTP request. If you remember, this URL is resolved in the gtm.url=SOURCE_URL parameter of the request. There are already Built-In variables for Page Hostname, Page URL, and Page Path, so the component types that you could configure variables for are Protocol (e.g. http or https), Port, and Query (e.g. ?test=true).\nAMP variable Jump to the next section to read more about AMP variables.\nConstant This should be familiar from regular Google Tag Manager. A Constant variable can be set to any string value you wish, and any time this variable is referenced, the string you input into the field will be returned.\n  A typical use case would be to set your Google Analytics tracking ID as a Constant String variable.\nEnvironment Name This isn\u0026rsquo;t particularly useful, as there already is a Built-In variable for this. It returns the name of the Google Tag Manager Environment where the JSON configuration object is loaded from.\nLookup Table The Lookup Table variable is a staple from regular Google Tag Manager. It\u0026rsquo;s very useful, as it lets you do value transformations with dynamic input variables.\nHowever, in AMP GTM it\u0026rsquo;s not that useful anymore. The Lookup Table must resolve server-side, because AMP doesn\u0026rsquo;t support the type of dynamic calculation that a Lookup Table would require if resolved in the client. Thus, the only types of input variables you can use are those which have a value within the Google Tag Manager container. This is, unfortunately, a very short list:\n  Container ID\n  Container Version\n  Environment Name\n  Page Hostname\n  Page Path\n  Page URL\n  Random Number\n  One use case would be to send Preview hits to another tracking endpoint. For that, the Lookup Table would look like this:\n  If you add this variable to the Tracking ID field of your Google Analytics tag, it will return \u0026ldquo;UA-12345-2\u0026rdquo; if Google Tag Manager is in Preview mode, and \u0026ldquo;UA-12345-1\u0026rdquo; otherwise.\nRandom Number Redundant. There already is a Built-In variable for this. It returns a random number between 0 and 2147483647.\nContainer ID Redundant. There already is a Built-In variable for this. It returns the ID of the GTM container (GTM-XXXXX).\nContainer Version Redundant. There already is a Built-In variable for this. It returns the version of the published GTM container, or QUICK_PREVIEW if in Preview mode.\n5.3. AMP variables I\u0026rsquo;ve already mentioned AMP variables here and there. They are built into the AMP Analytics framework, and comprise two types of variables: platform variables and page-defined variables. Platform variables are sort of \u0026ldquo;built-in\u0026rdquo; variables for AMP, in that you don\u0026rsquo;t need to manually specify them in the JSON configuration object. Page-defined variables, on the other hand, need to be included in the JSON configuration object itself.\nTo refer to AMP variables, you need to use the syntax ${variableName}. Naturally, Google Tag Manager does this for you when you create a user-defined variable of type AMP variable.\n  When you add an AMP variable to your Google Tag Manager tag, it becomes part of the request when the JSON is downloaded.\n  An AMP variable added like this to a Custom Dimension field in a GA tag would look like this in the JSON configuration object:\n\u0026#34;requests\u0026#34;: { \u0026#34;4\u0026#34;: \u0026#34;https://www.google-analytics.com/r/collect?...\u0026amp;cd1=${backgroundState}\u0026#34; } As you can see, the variable name is still in the request itself, meaning that it isn\u0026rsquo;t resolved when the JSON configuration object is downloaded. Only once the actual request dispatches will the AMP variable be resolved, which means, in turn, that its value is generated dynamically.\nMost of the useful AMP variables are already available as Built-In variables in GTM. In fact, I find it curious that Built-In variables don\u0026rsquo;t simply include all the pre-defined AMP variables, as that would cut down time required to configure the container.\nCreating custom AMP variables If you want to create your own AMP variables, you can. It\u0026rsquo;s sort of similar to working with dataLayer, in the sense that you are passing semantic information from the page to Google Tag Manager. However, the difference is that you won\u0026rsquo;t be able to utilize these Variables within GTM itself. So you can\u0026rsquo;t create trigger conditions based on AMP variables, nor can you use Lookup Tables with AMP variables as the input variable.\nTo create custom AMP variables, you include them like this:\n\u0026lt;amp-analytics config=\u0026#34;https://www.googletagmanager.com/amp.json?id=GTM-XXXXXX\u0026amp;gtm.url=SOURCE_URL\u0026#34; data-credentials=\u0026#34;include\u0026#34;\u0026gt; \u0026lt;script type=\u0026#34;application/json\u0026#34;\u0026gt; { \u0026#34;vars\u0026#34;: { \u0026#34;someAmpVar\u0026#34;: \u0026#34;someValue\u0026#34;, \u0026#34;someOtherAmpVar\u0026#34;: \u0026#34;someValue\u0026#34; } } \u0026lt;/script\u0026gt; \u0026lt;/amp-analytics\u0026gt; So you include them in the \u0026quot;vars\u0026quot; block as key-value pairs. Once you\u0026rsquo;ve done this, you can create AMP variables for these keys, and then include the custom AMP variables in tag fields.\n  The thing to remember about AMP variables and GTM is that AMP variables are resolved in the client when the requests are dispatched. Thus you can\u0026rsquo;t use them in any operations within GTM that would require server-side validation. The most notable cases are trigger enabling conditions and the Lookup Table variable.\n  There\u0026rsquo;s a great reference guide for all the available AMP variables in the GitHub project itself: AMP HTML URL Variable Substitutions. Read through that document, and create User-Defined variables for any AMP variables that don\u0026rsquo;t already exist as Built-In variables.\n6. Other Google Tag Manager quirks Google Tag Manager is pretty smooth with AMP. The fact that it compiles the configuration object for you is a huge plus, and the ease of creating a tracking schema for your AMP pages is very much in line with how \u0026ldquo;regular\u0026rdquo; Google Tag Manager works, too.\nThere are some hiccups, though. Most notably, server-side validation can be a difficult thing to come to terms with. GTM needs to build a configuration object dynamically, but because the object is built within Google\u0026rsquo;s servers, it doesn\u0026rsquo;t have any access to the variables that would exist on your page. Thus the only dynamic variables it has access to need to be passed in the URL request for the container, gtm.url being the most notable one.\nGoogle Tag Manager for AMP does not support the Debug mode we\u0026rsquo;re used to with regular Google Tag Manager. The debug panel would require that the AMP page supported the type of JavaScript that is very performance-killing.\nHowever, GTM does work in Preview Mode. So when you enter Preview Mode in Google Tag Manager, once you reload the site with the same web browser you entered Preview Mode in, the container that is downloaded from Google Tag Manager will actually be the one you are previewing and not the one that is published.\nThis is very useful, even though I know you\u0026rsquo;ll miss having the excellent Debug panel at your disposal.\nBecause Preview Mode doesn\u0026rsquo;t have any verbose logging or anything like that, you should get familiar with other debug tools at your disposal.\n7. Summary I\u0026rsquo;m a big fan of Google Tag Manager\u0026rsquo;s AMP container. It\u0026rsquo;s not just because I\u0026rsquo;m such a fanboy (I am), nor because I\u0026rsquo;m typically devoid of a critical voice when it comes to GTM (I am this, too). The reason I really like it is because it does what GTM has always done best: it abstracts a somewhat complex operation of compiling the JSON configuration object, and lets you create it using GTM\u0026rsquo;s UI, knowing full-well that whatever you create will be syntactically valid for AMP.\nThat\u0026rsquo;s the modus operandi for GTM, and it\u0026rsquo;s refreshing to see how it\u0026rsquo;s still respected by the development team. On top of that, AMP introduces a number of triggers and variables that I really hope will make their way into regular GTM, too. The scroll and visibility triggers alone would be really useful in regular Google Tag Manager.\nMost of my issues with the AMP container have to do with AMP rather than with GTM. Managing a single Client ID across your website and the available AMP proxies is a pain, and I really hope that AMP introduces some methods to facilitate extending a single Client ID (e.g. _ga) across your website and the various proxies AMP might be served through.\nI foresee some great applications for AMP through GTM, and I also look forward to some feature leakage to regular Google Tag Manager, too.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/fix-rogue-referral-problem-single-page-sites/",
	"title": "#GTMTips: Fix The Rogue Referral Problem In Single-Page Sites",
	"tags": ["google analytics", "Google Tag Manager", "gtmtips", "referral", "tracker"],
	"description": "How to fix referrer issues, especially when recording paid vs. organic search sessions, in Google Analytics when using Google Tag Manager on single-page sites.",
	"content": "Single-page sites (or single-page apps) typically have just one page load. When navigating the site, subsequent content is either uncovered from the DOM, where it\u0026rsquo;s been in a hidden state, or loaded from the server using HTTP requests that do not invoke a new page refresh. This behavior, however, has some implications for Google Analytics tracking, especially when configured via Google Tag Manager.\nThe crux of the problem is this: When you create a Google Analytics tracker, the URL of the page (without a possible #hash) from when the tracker was created is sent as the value of the Document Location field with every hit that uses this tracker. This is used for a number of things, most significantly attributing the session to the campaign specified by URL parameters such as gclid (AdWords) or utm_source, utm_medium.\nNow, on single-page sites you send \u0026ldquo;virtual\u0026rdquo; pageviews whenever new content is loaded from the server. The reason this works fine with on-page GA is because you\u0026rsquo;re always using the same tracker object to send the hits. Google even recommends this in their developer guide. Thus the Document Location field stays the same, and campaigns are attributed correctly.\nWith Google Tag Manager, every single Universal Analytics Tag that fires on the site creates a new, unique tracker object. This means that the Document Location field is updated with every Tag you fire, which is a problem if the URL changes due to browser history manipulation. Thus you can end up with a situation where the first Universal Analytics Tag has gclid in the URL, attributing the session to AdWords, but the next pageview doesn\u0026rsquo;t have this in the URL anymore, as you would not include it in the \u0026ldquo;virtual\u0026rdquo; pageview path names. Instead, since gclid is no longer in the URL, GA looks at the HTTP referrer of the page to see what the previous page was for attribution. It finds google.com, as you came from the search engine (HTTP referrer is not updated when manipulating the URL with the browser History API). Thus a new session starts with attribution to Google Organic! I\u0026rsquo;ve dubbed this as the Rogue Referral problem.\nThere are ways to combat this. David Vallejo\u0026rsquo;s written a great article on setting the Tracker Name in your GTM Tags. This will effectively work like on-page GA, maintaining the initial value of Document Location throughout the page load. However, there are some risks with the tracker name setting, so I wanted to offer an alternative.\nTip 51: Manually Set Document Location To Prevent Rogue Referrals   The way this works is that you store the initial page URL in a global variable such as dataLayer, and then manually set the Document Location field in all your Universal Analytics tags to use this variable.\nThe most robust way to do this would be to have the following in the page HTML before the GTM container snippet:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ originalLocation: document.location.protocol + \u0026#39;//\u0026#39; + document.location.hostname + document.location.pathname + document.location.search });  This would store the original URL of the page (without #hash) into a dataLayer variable named originalLocation. Then, you\u0026rsquo;d add this to all your Universal Analytics tags by browsing to Fields to Set and adding a new field:\nField name: location\nValue: {{Data Layer Variable - originalLocation}}\nHere, {{Data Layer Variable - originalLocation}} would be a Data Layer Variable you\u0026rsquo;ve created, pointing to the originalLocation you store when the page is first loaded.\n(UPDATE: Note that if you add the location field, you must also specify the page, or else all pages will use what\u0026rsquo;s stored in location as the page path sent to GA! If you have a single-page site, you probably already have the page field set to a virtual page path, but if not, you can always use something like:\nField name: page\nValue: {{JS - Get Page URL}}\nWhere the variable {{JS - Get Page URL}} is a Custom JavaScript variable with:\nfunction() { return document.location.pathname + document.location.search; }  This would send the current page pathname with any query parameters as the virtual page path dispatched with your GA Tags. Thank you Brian Clifton for pointing out that query parameters should be sent, too.)\nIf you can\u0026rsquo;t or don\u0026rsquo;t want to edit the page HTML, you can also use Tag Sequencing. First, you would need to create a Custom HTML Tag with the same code as above (enclosed in \u0026lt;script\u0026gt; and \u0026lt;/script\u0026gt; tags). Then, you would need to identify the first Universal Analytics Tag that fires on the site. This would typically be a Page View Tag with something like All Pages or some other Page View Trigger attached to it. Then, you\u0026rsquo;d need to add the new Custom HTML Tag to this Page View Tag\u0026rsquo;s sequence, by firing it before the Page View Tag.\n(UPDATE: Read the following Caveat chapter if you choose to do this all via GTM and not the page template!)\n  That way the original URL is stored into dataLayer before the Page View Tag fires, and is thus available for all the Universal Analytics tags that fire on the page.\nCaveat If you\u0026rsquo;re pushing the originalLocation via GTM and not the page template, there might be a race condition between when the originalLocation variable is pushed into dataLayer, and when Tags try to access it. In these cases, analytics.js does not default to the current URL, resulting in a missing Document Location field! To fix this, instead of adding {{Data Layer Variable - originalLocation}} directly to the location field in your GA Tags, you might want to add a Custom JavaScript Variable instead:\nfunction() { return {{Data Layer Variable - originalLocation}} || window.location.protocol + \u0026#39;//\u0026#39; + window.location.hostname + window.location.pathname + window.location.search; }  This returns either {{Data Layer Variable - originalLocation}} or, if that hasn\u0026rsquo;t been set yet, the current URL without hash.\nSummary If you have a single-page site and you\u0026rsquo;re sending \u0026ldquo;virtual\u0026rdquo; pageviews, you might want to check if you have the rogue referral problem. A quick way to identify it is to use the new User Explorer reports, looking for sessions which start with an AdWords hit, but then quickly turn into a new session with Google Organic as the campaign.\nActually, if you\u0026rsquo;re using Google Tag Manager and you\u0026rsquo;re sending virtual pageviews, you will most certainly suffer from the rogue referral problem, unless you\u0026rsquo;ve set the Tracker Name or the Document Location as instructed in this guide.\n"
},
{
	"uri": "https://www.simoahava.com/tags/referral/",
	"title": "referral",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/tracker/",
	"title": "tracker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/site-speed/",
	"title": "site speed",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/site-speed-sample-rate-multiple-page-views/",
	"title": "Site Speed Sample Rate And Multiple Page Views",
	"tags": ["data quality", "Google Tag Manager", "site speed", "universal analytics"],
	"description": "If you&#39;re collecting page timings automatically to Google Analytics, you might have some data quality issues especially if collecting virtual page views using Google Tag Manager.",
	"content": "Google Analytics\u0026rsquo; Site Speed reports are pretty darn great. They report automatically on various milestones in the process the browser undertakes when rendering content. These reports leverage the Navigation Timing API of the web browser, and they are (typically) collected on the first Page View hit of a page.\nAnd this is all fine. As I said, it\u0026rsquo;s a great feature of Google Analytics, and lends itself handily to spotting issues in the quite complex client-server negotiation that goes on when your web browser requests content from the web server.\n  However, there\u0026rsquo;s a glitch. These automatic page timings are collected once per tracker instance per page, meaning if you have multiple trackers on the page, each set to collect site speed samples, you might inadvertently send the same page speed data multiple times. This will naturally inflate the numbers that Google Analytics reports on, and your data will be ruined, as deduplication is really difficult.\n  So if you have any of the following in place, you might be in the risk group:\n  Single-page site where you send \u0026ldquo;virtual\u0026rdquo; Page Views to Google Analytics\n  Multiple trackers on the page, each collecting to the same Universal Analytics property (UA-XXXXX-Y)\n  Manually set site speed sample rate (e.g. at 100%)\n  Google Tag Manager with a Page View Tag that has multiple Triggers attached to it\n  If any of these rang true, you might have a data quality issue that should be fixed immediately.\nFraming the problem By default, 1% of Page View hits are sampled for page timings. So if you have the following code on the site:\nga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;);  There\u0026rsquo;s a 1/100 chance that upon the first time command is executed, this particular request will grab the available Navigation Timing data and send it to Google Analytics. And, again, this is fine. That\u0026rsquo;s what we want. Now if you have the following code:\nga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;, {page: \u0026#39;/some-custom-page/\u0026#39;});  That\u0026rsquo;s two separate Page View hits firing on the same page, you\u0026rsquo;re still fine. Since they use the same tracker object (the default tracker), even if you win the odds and manage to hit the 1/100 twice on the same page, the page timing data is only sent once, since Universal Analytics only sends timing data once per tracker per page.\nHowever, if you have this:\nga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); ga(\u0026#39;secondTracker.send\u0026#39;, \u0026#39;pageview\u0026#39;, {page: \u0026#39;/some-custom-page/\u0026#39;});  If both these trackers collect to the same Google Analytics property (UA-XXXXXX-Y), and if both trackers manage to somehow make the 1/100 cut, you will be sending the same page timing data twice, just for different page paths. And that will warp your data!\nIt gets worse\u0026hellip;\nThe problem with Google Tag Manager Google Tag Manager, for all the good it does, has one complication that makes a lot of things difficult: it assigns a unique tracker object for every single tag that fires - even if it\u0026rsquo;s the same tag firing multiple times.\nTake another look at the image at the beginning of this article. There you have a Page View Tag which has the siteSpeedSampleRate set to 100%, which means that every single time the Page View hit is sent, it should be used to sample page timing data. It also has a Trigger which fires whenever a Custom Event is pushed into dataLayer. So this is your typical \u0026ldquo;virtual\u0026rdquo; Page View (man I hate that term - all Page Views are virtual!) Tag, set to fire with a custom page path whenever the event triggers.\nNow, here\u0026rsquo;s the issue: because it has siteSpeedSampleRate set to 100, and because it will fire multiple times on the page, and because each time it fires it will have a unique tracker name:\nEvery single time this Page View Tag fires, it will send the same page timing data to Google Analytics!\nThat\u0026rsquo;s multiplication on a grand level! It\u0026rsquo;s even more confusing since the same page timing data is sent to multiple page paths, making deduplication in the reports really difficult to do.\nFixes Here are some obvious fixes to this issue.\n1. Only sample a single tracker This is pretty easy if you\u0026rsquo;re using the Universal Analytics snippet. Set siteSpeedSampleRate to a proper value in only one tracker, and set all other trackers to 0.\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {siteSpeedSampleRate: 100}); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {name: \u0026#39;secondTracker\u0026#39;, siteSpeedSampleRate: 0});  This would ensure that only a single set of page timings is sent per page.\nIn GTM, by default, this would be impossible, because each tag has its own unique tracker name.\n2. Only sample the first pageview This is irrelevant in Universal Analytics, as only the first pageview per tracker is sampled anyway. In Google Tag Manager, however, this might just fix the issue for you.\nFor this to work, you need to fire the first Page View hit of a page using a unique trigger. Typically, you\u0026rsquo;d have the \u0026ldquo;All Pages\u0026rdquo; Trigger on the Page View Tag, and then another Trigger for the custom Page View. Thus, a single-page app might have the following Tag catering for all Page Views:\n  Now, this Tag will be problematic if left like this, since the Page Timing sample will be sent every single time the Tag fires - once for the All Pages Trigger, and each time the Custom Event Trigger fires.\nTo fix this, use this Custom JavaScript Variable as the value of the siteSpeedSampleRate field:\nfunction() { return {{Event}} === \u0026#39;gtm.js\u0026#39; ? 100 : 0; }  So your Tag looks like this:\n  This JavaScript returns 100 for the All Pages Event (that\u0026rsquo;s the gtm.js event name), and 0 for all other Triggers.\nIn other words, only the All Pages Trigger will send the site speed sample, ensuring it is only sent once on the page.\nIf you don\u0026rsquo;t have the means to distinguish the first Page View from all the subsequent triggerings, you could also add hitCallback to your Page View Tag, which sets a global flag (or even a dataLayer variable) to true, indicating that the tag has fired (at least) once. Then you can use this flag as a condition for the siteSpeedSampleRate field, only sending the 100 value if this flag is false.\n3. Set the Tracker Name in your GTM Tag This is the most risky but also the most effective way to fix it. Remember how GTM uses a unique tracker name for every instance of a Tag? Well you can always set the Tracker Name field with some value to make sure that every time the Tag fires it uses the same tracker object. Thus the page timing sample is only sent once. To do this, you\u0026rsquo;d have to configure one of the settings in the Page View Tag:\n  You can set it to whatever value you wish, though I advice against leaving it blank. If you do leave it blank, it uses the default Universal Analytics tracker and can lead to issues, unless you are really on top of your site\u0026rsquo;s GA implementation.\nThere are risks when setting the Tracker Name field, and I recommend you read my article on the topic before proceeding with the method.\nSummary The Site Speed reports are really great, but you might want to go over your Universal Analytics implementation to make sure you\u0026rsquo;re not involuntarily collecting bogus data.\nAs long as you keep in mind that the page speed sample is collected once per page per tracker, you should be able to identify if there are issues in your setup. In this article, I have outlined three different things you can try, but there are other methods you can employ too, with just a little creativity.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/restrict-custom-html-tag-scope/",
	"title": "#GTMTips: Restrict Custom HTML Tag Scope",
	"tags": ["custom html", "Google Tag Manager", "gtmtips", "JavaScript"],
	"description": "Always remember to restrict the scope in your Google Tag Manager Custom HTML tags, so that your locally defined variables don&#39;t pollute the global namespace.",
	"content": "What better way to celebrate the 50th #GTMTips article than, well, a really useful Google Tag Manager tip?! This tip is so useful and simple; it encapsulates everything that I had in mind when starting this series. The tip is about restricting scope of Custom HTML Tags. This is an important concept, because it\u0026rsquo;s possible that you\u0026rsquo;re stuffing your page\u0026rsquo;s global JavaScript namespace with all sorts of junk, and thus inadvertently causing conflicts.\nTip 50: Restrict Custom HTML Tag Scope   The tip is very simple. Whenever adding JavaScript code in a Custom HTML Tag between \u0026lt;script\u0026gt; and \u0026lt;/script\u0026gt; tags, add the following code around whatever you\u0026rsquo;ve written:\n(function() { // Your JavaScript here  })();  What you\u0026rsquo;re doing is creating an immediately invoked function expression, or IIFE for short. By wrapping the function() {...} in parentheses, you\u0026rsquo;re instructing the browser to treat the code within as an expression rather than a declaration. In other words, you\u0026rsquo;re instructing the browser to invoke the function, making it syntactically valid by adding the empty parentheses at the end (so that the browser knows it\u0026rsquo;a a function expression). This executes whatever code is within the \u0026lt;script/\u0026gt; block.\nSo if it does exactly what it would do even without the IIFE, why do it?\nWell, JavaScript scopes variables to their execution context. This means that if variables are declared within a function, they are only accessible inside that function. Furthermore, JavaScript has a global namespace which in the web browser is defined by the window object. If you didn\u0026rsquo;t have the IIFE around your code, then any variables you declared would be hoisted to the global window namespace, thus creating potential for conflict.\nTake a look at this example:\n\u0026lt;script\u0026gt; // No IIFE  var ga = \u0026#34;My secret \u0026#39;ga\u0026#39; code\u0026#34;; console.log(ga); // Results in an error  window.ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; By overwriting ga with your own nifty little string, you just broke Universal Analytics. Congratulations.\nWhereas if you did this:\n\u0026lt;script\u0026gt; (function() { // IIFE  var ga = \u0026#34;My secret \u0026#39;ga\u0026#39; code\u0026#34;; console.log(ga); })(); // Works  window.ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; This time you\u0026rsquo;re protecting your own little ga variable and, more importantly, you\u0026rsquo;re protecting the global ga function by declaring your variable in an IIFE.\nJust remember the following additional tips:\n  Always declare variables in GTM using the var keyword. This way they will be restricted to the current scope. If you omit this keyword, then JavaScript automatically adds them to the global window namespace. NOTE! This also applies to Custom JavaScript Variables, not just Custom HTML Tags!\n  Always refer to global variables using the window. prefix. That will make your code more readable and will prevent you from causing conflicts by accident.\n  And that\u0026rsquo;s it!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/introducing-ga-spy-for-google-analytics/",
	"title": "Introducing GA Spy For Google Analytics",
	"tags": ["google analytics", "Google Tag Manager", "guest post", "tracker", "universal analytics"],
	"description": "Introducing the GA Spy tool built by Stephen Harris. It lets you hijack the Google Analytics global function so that you can insert and execute custom code in the hit build process.",
	"content": "   This is a guest post by Stephen Harris from Seer Interactive . He was kind enough to share his awesome solution in this blog, so I\u0026rsquo;m very grateful indeed for his contribution.\nIf Google Tag Manager is loaded as the primary instrument for tracking on a webpage (as it should be), then all webpage tracking could and should be configurable via GTM. But we don\u0026rsquo;t always control the circumstances, and it\u0026rsquo;s not uncommon to face hardcoded Google Analytics tracking outside of GTM.\nPerhaps GA tracking cannot be removed from the website source code quickly enough. Maybe the website is on a publishing schedule that doesn\u0026rsquo;t suit measurement needs, or maybe the folks responsible for the website don\u0026rsquo;t have sufficient resources. Whatever the reason, you got a \u0026ldquo;no\u0026rdquo; when you requested insisted on removal. (You did insist on removal, didn\u0026rsquo;t you?)\nOr what about when there are hardcoded GA commands in a platform or plugin? It could be screwing with your data. But chances are it\u0026rsquo;s not doing anything at all (because it\u0026rsquo;s tracking to the nonexistent default GA tracker) and there seems to be no way to get it to work through GTM.\nAnd even when all tracking runs through Google Tag Manager, some things can be difficult (or impossible) to implement using the tag templates (such as adding GA plugins, or defining Universal Analytics tracker field defaults).\nHere\u0026rsquo;s a solution.\n1. Spying On GA You can grab the code on GitHub. Copy-paste it either into a Custom HTML Tag in Google Tag Manager, or load it in the page template proper.\nIt\u0026rsquo;s called GA Spy because it can silently hijack and control all tracking that relies on Google\u0026rsquo;s Universal Analytics library (analytics.js). Effectively, this is a Google Analytics listener that does karate.\n  To put this in layman\u0026rsquo;s terms, the script listens for interactions with the ga() interface, returning the arguments passed to the function for you to process for whatever purpose you want. For example, you can automatically copy all calls to ga('send'...) to dataLayer.push() syntax, allowing you to fluently replicate hardcoded Universal Analytics tracking in Google Tag Manager.\nThis blog post is written assuming the hardcoded tracking is for Universal Analytics, but there is a version of the script for the async GA library (ga.js), to which the same core concepts apply.\n  Source code for ga-spy.js\n  Source code for ga-spy_async.js\n  README\n  1.1. Spy Code 101 Let\u0026rsquo;s walk through the script logic. Before implementing, it\u0026rsquo;s important to get a basic idea of what this script actually does. Once you understand the basics, you\u0026rsquo;ll also understand why this should be considered a temporary solution. Indeed, you should document the use of this solution, so that when the hardcoded Universal Analytics code is eventually removed from the site, you won\u0026rsquo;t be left hanging dry with methods that don\u0026rsquo;t really do anything.\nThis section also explains this script\u0026rsquo;s biggest caveat: GA Spy is not guaranteed to intercept (or even detect) tracking that fires while the page is loading (e.g. the standard GA snippet) if the hack is deployed via GTM. Capturing these requests requires that you deploy the script on the page template itself, which unfortunately might detract from the usefulness for those without developer resources handy.\nFor a more technical breakdown, check out the README.\n1.2. A Good Spy Starts With Research Let\u0026rsquo;s start with an overview of how the Universal Analytics tracker works on the page.\nMany GA users are familiar with the JavaScript namespace ga. It\u0026rsquo;s the global object for tracking with the Universal Analytics library (analytics.js). When the browser loads the page, this object is actually processed twice. First, it\u0026rsquo;s created by the Universal Analytics snippet, and it queues all commands to ga() while waiting for the library to load. Then, once the library has loaded, the namespace is converted to a full interface which processes each command as it is executed.\n  So the standard Universal Analytics snippet instantiates ga as a tiny function that saves all the arguments you pass to it as an Array in ga.q. This is called the command queue. It\u0026rsquo;s created by the second line of the GA install snippet function, shown here (beautified with more readable variable names):\nwindow[gaObjName] = window[gaObjName] || function() { ( window[gaObjName].q = window[gaObjName].q || [] ).push( arguments ) }, ga.l = 1 * new Date();  Aside: Notice any similarities to the recommended format for data layer pushes?\n( window.dataLayer = window.dataLayer || [] ).push( { } )  Once this queue is established, the snippet creates a script loader, which starts an asynchronous request to download the analytics.js library from Google\u0026rsquo;s servers.\nOnce analytics.js loads, it will run the commands in the command queue and replace the queue function ga with much more robust object, which we call the ga object or function. Once this happens, ga stops keeping track of previously called commands, and will run commands as soon as they are called.\n1.3. Spy Moves The core of this script is the hijack function. What it does is simple: it saves a private reference to the global ga object, then updates the global reference (window.ga or simply ga) to point to a custom function, which we\u0026rsquo;ll call our \u0026ldquo;proxy\u0026rdquo; function. Note that if you\u0026rsquo;ve chosen to rename the global function as something other than ga, it will still work nicely with GA Spy.\nfunction hijack(){ // The current global GA object (could be GA command queue or loaded GA object).  var gaOrig = window.ga; // Replace global GA object with a proxy.  window.ga = proxy; // Maintain references to GA\u0026#39;s public interface.  for( k in gaOrig ) if( gaOrig.hasOwnProperty( k ) ) proxy[k] = gaOrig[k]; }  The proxy function mimics the data members and methods of the original ga function. Thus it behaves like the original, which we accomplish by passing its arguments back to the ga function (and returning the result). But, only if we so choose. We can reject the message, simply by not sending the message to the original ga function, and thus intercept and block the hardcoded GA from running its requests altogether.\nfunction proxy(){ // Grab all arguments as an array.  var args = [].slice.call( arguments ); // Unless processArgs() returns false ...  if( processArgs( args ) ) // ... pass through to original GA object.  return gaOrig.apply( gaOrig, args ); };  The processArgs function simply delegates the processing and filtering of all commands to you (via the custom listener function).\nfunction processArgs( a ){ // Call listener, return false only if listener returns false.  return listener( a ) !== false; }  1.4. Time To Spy As soon as GA Spy is run, it will instantiate the command queue if it doesn\u0026rsquo;t already exist, just as the Universal Analytics snippet would. Both in this script and in the Universal Analytics snippet, we\u0026rsquo;re careful not to blindly set a new variable and lose any previous data.\nif( ! window[gaObjName] ){ // If global object doesn\u0026#39;t exist ...  ga = function(){ if( ! window[gaObjName].q ) window[gaObjName].q = []; window[gaObjName].q.push( arguments ); }; ga.l = 1 * new Date(); window[gaObjName] = ga; }  Note: the above code does the exact same thing as the second line of the GA install snippet function, shown beautified under A Good Spy Starts With Research above.\nThen, regardless of whether it already existed or was just initiated, GA Spy hijacks ga.\nIf ga is found to be the fully-formed Universal Analytics tracking method, then analytics.js has already loaded and we will be tapped into all subsequent calls to ga until the page is unloaded. Job done. Since the global ga object does not keep track of previously run ga() commands, we cannot access the information passed in those commands. It\u0026rsquo;d be useful if we could access the command history, but we still would not be able to block those hits, since JavaScript does not support time reversal (not gonna work: setTimeout( hijack, -5 )).\nHowever, if GA Spy finds the command queue instantiated in ga, then we know that if/when analytics.js loads, it will replace the hijacked command queue (our proxy) with the actual Universal Analytics tracking method. We can easily hijack it again, but timing is important. We want the global ga object to be in existence for as short a period as possible to avoid missing any commands before we manage to hijack it.\nAny function in the command queue will be executed as soon as the ga object is ready. So we add the hijack function, thereby setting a trap the ga object will trigger immediately upon loading, and put our proxy in its place before the ga object is even available to other scripts.\nThe only other thing we need to do is run the stack of existing commands through the listener, and when it returns false just filter that command out of the queue.\n2. Reliability And Caveats This should be an extremely reliable solution. In fact, the method of listening by \u0026ldquo;hijacking\u0026rdquo; a function is used by a number of Google libraries, including GTM\u0026rsquo;s own Data Layer and Autotrack. Note that it\u0026rsquo;s possible GA Spy can be broken by an update to analytics.js, but the ga interface GA Spy relies on is wholly defined in the standard Universal Analytics install snippet, so breakage due to unannounced library updates is highly unlikely.\nNevertheless, there are some important caveats to note before using this.\n2.1. Uncommon Behavior For A Common Library Whenever this is used to modify or block a ga command, I would consider this a \u0026ldquo;hacky\u0026rdquo; solution. Although the code is sound, such usage modifies the typical behavior of the global ga() function, making it work differently than it works on 99% of other websites. This can impede troubleshooting and confuse those who are new to Google Analytics or JavaScript.\nTip: You can tell that you\u0026rsquo;re dealing with hijacked GA by checking for the presence of the property _gaOrig in the global Google Analytics object. By default it would be: window.ga._gaOrig.\nIn some cases, hijacking the GA function is necessitated by third-party vendors that require their JavaScript to be implemented with a hardcoded GA tracker.\nIn other cases, such as when completing a migration from hardcoded Google Analytics to Google Tag Manager, this treatment should be acceptable only as a short-term solution.\n2.2. Cannot Intercept Hardcoded Pageviews From GA If GA Spy runs before analytics.js loads, you will be able to access and block 100% of the commands queued to analytics.js. But if GA Spy runs after, then it has zero insight into what hits (if any) were previously fired. So if you want to intercept all hardcoded commands, GA Spy needs to be deployed directly on the page. However, for many purposes, this is not necessary at all.\nUnfortunately, in some cases where deploying on-page is necessary, the inability to make on-page changes is the very problem that prompted the need for GA Spy in the first place, making this solution a catch-22. For GTM migrations, one way to mitigate this is to request to have GA Spy placed on the page at the same time that GTM is added.\nNote, if synchronous loading of Google Tag Manager is ever supported, hardcoding GA Spy will no longer be required, but still recommended, because loading anything synchronously in the \u0026lt;head\u0026gt; will degrade page loading speed.\n2.3. Incomplete Currently, GA Spy only intercepts GA commands that are called upon the global ga object, but there are other ways to send commands to analytics.js. Many GA plugins and some custom implementations call commands directly upon the tracker object (i.e. ga.getAll()[0].send('event')). Support for intercepting these commands may be added in the future.\nAnother limitation is that GA Spy can process only the values passed to GA commands. It doesn\u0026rsquo;t provide access to the default values for all the fields that are populated by analytics.js.\nOne effect of these two facts is that GA Spy will not pick up hit modifications (or extra hits) done by GA plugins (and other implementations using the Tasks API). This is by design and no support for this planned. (If you see a need for this, describe the use case and upvote this enhancement here.)\n2.4. More Code! As a developer I have many protests against the following statement, but I can\u0026rsquo;t deny it\u0026rsquo;s essential truth: code is bad. Meaning, no matter how good the code, no code is better. Code leads to bugs which lead to more code, which leads to more bugs. Even bullet-proof code requires some degree of maintenance. Perhaps the biggest issue is that a code-based solution imposes a higher skill barrier on managing and customizing the solution. As a general rule, we should avoid custom code whenever there is a sound alternative.\n3. Usage 3.1. Placement If you need to intercept GA commands that run upon page load or shortly thereafter (such as the tracker creation or 'pageview' in the base GA install snippet), then you\u0026rsquo;ll need deploy GA Spy in an inline script tag or external script (with no async or defer attributes) above the first GA command (usually in the base GA snippet).\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... // GA Spy: \u0026lt;script\u0026gt; ;window.gaSpy = window.gaSpy... \u0026lt;/script\u0026gt; // Universal Analytics snippet: \u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m)... \u0026lt;/script\u0026gt; ... \u0026lt;/head\u0026gt; If the commands you need to intercept are triggered by user action, then this is not an issue and you can deploy GA Spy using GTM.\n  3.2. Your Listener This script will not do anything on its own. You need to implement all actions in your custom listener callback function. You can do things like:\n  Access and modify the arguments sent to ga().\n  Access information about the command scope (tracker name, hit type, etc.)\n  Block the hit by returning false (returning merely \u0026ldquo;falsy\u0026rdquo; values like 0 or undefined will not block the hit).\n  And everything else you can do in JavaScript :)\n  In many cases, we\u0026rsquo;ll want to push the hit values onto the dataLayer so they can be accessed in GTM.\n\u0026lt;script\u0026gt; gaSpy(function(obj) { // Do something with the arguments to ga():  var args = obj.args; // Do something with details about the tracker itself:  var details = obj.the; }); \u0026lt;/script\u0026gt; Naturally, any calls to gaSpy() need to be timed so that they take place after you\u0026rsquo;ve loaded the GA Spy code itself.\nSee the README file for more details on what data is passed to the callback function, as well as additional GA Spy configuration options.\n3.3. Listening From GTM If using GA Spy in GTM, the listener should typically ignore all commands from GTM. Failure to do this could result in blocking GTM hits or in an infinite loop. We can do this by checking the tracker name. GTM uses tracker names starting with \u0026ldquo;gtm\u0026rdquo; followed by the timestamp of when the tracker was created.\nHowever, this method will not work when callbacks are passed in place of a GA command. GTM does not appear to use these currently, but we cannot do much with a them anyway, so as a precaution any listener should ignore callbacks too.\nUse the this code at the top of your callback function to avoid issues:\ngaSpy( function( ev ){ if( ev.the.callback ) return; if( ev.the.trackerName \u0026amp;\u0026amp; ev.the.trackerName.substr( 0, 3 ) == \u0026#39;gtm\u0026#39; ) return; // Your code here });  This is not foolproof, but it\u0026rsquo;ll work unless hardcoded tracking is using a tracker name starting with \u0026ldquo;gtm\u0026rdquo; (or if the tracker names in GTM are customized, which is probably done as an alternative to this solution). For example, this method will not work for Wistia\u0026rsquo;s built-in tracking, because Wistia\u0026rsquo;s code (erroneously) sends hits through every named tracker on the page.\n4. Examples 4.1. Log GA Commands Logging GA commands is a useful way to easily see what commands are being picked up by GA Spy, letting you see which things GA Spy can block and/or latch on to in order to execute custom behavior. Compared with browser extensions and the verbose analytics_debug.js, GA Spy logging can be very minimalistic. You could even log using only emojis if you wanted! And it has the advantage of working even when the hit does not fire (such as when using GA opt-out or tracking blockers).\nThis listener callback will print ga() arguments exactly as they are given:\ngaSpy( function gaSpy_cb_( ev ){ console.debug.apply( console, ev.args ) });  Or check out one of the examples on GitHub for something a bit more robust:\n  Since this is all one script, you could turn this into a bookmarklet. This could also block all hits, so it serves as a \u0026lsquo;test mode\u0026rsquo; bookmarklet, which both logs and prevents any hits from being fired.\n4.2. Access Hardcoded GA Command Data Via GTM If you know exactly what you\u0026rsquo;re looking for, you can simply look for that format and send it to dataLayer. Remember to define an event name in the dataLayer.push(), so you can create a GTM trigger based on the event. Here\u0026rsquo;s an example that clocks hardcoded events from the social sharing plugin ShareThis and forwards them to GTM:\ngaSpy( function gaSpy_cb_( ev ){ var a = ev.args; if( a[0] == \u0026#34;send\u0026#34; \u0026amp;\u0026amp; a[1] == \u0026#34;event\u0026#34; \u0026amp;\u0026amp; a[2] == \u0026#34;ShareThis\u0026#34; ){ dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;spy.ga.socialPlugin\u0026#39;, \u0026#39;spy.ga.socialPlugin\u0026#39; : { \u0026#39;network\u0026#39; : a[3], \u0026#39;url\u0026#39; : a[4] } }); } return false; });  Here\u0026rsquo;s a more robust example that sends all non-GTM commands to dataLayer.\n  Most of the code necessary for this simply normalizes the command arguments (since GA commands have a flexible format for defining field values). It also handles data scope (different trackers, plugins) and also wipes hit-only fields after the relevant GTM event.\n5. Next Steps Even though this is a reliable and relatively simple method for working with hardcoded GA tracking, hopefully this was not your first choice. Even though this fixes real data issues and helps consolidate your implementation in GTM, it\u0026rsquo;s an awkward way of doing so. You can get the same result with a much simpler configuration: no hardcoded tracking, and no spying! Don\u0026rsquo;t be satisfied with getting tracking working; pursue a clean implementation. Even if it will take a while, plan to have the hardcoded tracking removed.\nYour script is making your Universal Analytics code work differently than it would work on virtually every other website, confusing troubleshooters and learners. So, even if you believe hardcoded tracking removal is imminent, be sure to note your use of GA Spy (prominently) in your tracking documentation.\nIf you are using GA Spy to deal with code from a third party platform or plugin, contact the developers. Let them know their script is not compatible with GTM. (Actually, you should do that before implementing GA Spy; they might be responsive and fix their code quickly!)\n6. Feedback This method was designed to be flexible and was tested with various setups, but that\u0026rsquo;s not to say it\u0026rsquo;s bulletproof. On the contrary, I\u0026rsquo;m eager to discover bugs, incompatibilities with browser plugins, and similar issues. Naturally, I admit it\u0026rsquo;d be nicer to find out there are none :)\nPlease post any problems or suggestions as a new issue on GitHub if it has not already been added. Be sure to upvote fixes and enhancements you want to see implemented (by clicking the little thumbs-up icon)!\n7. Summary (by Simo) This is some top-notch JavaScript right here! What Stephen has built is a swiss-army knife that lets you take full control over how analytics.js functions as a tracking interface on your website. Even though translating the hardcoded ga() calls to dataLayer.push() commands is the obvious choice, there are lots of use cases for this library, and you can check some of the examples out here.\nOne of the things this enables is what we\u0026rsquo;ve been waiting for so long with Google Tag Manager: the ability to intercept and modify the payload sent by GTM\u0026rsquo;s Tags to Google Analytics. There\u0026rsquo;s no way to add your own custom plugins, for example, but this library with its hijack function lets you modify the tracker object between its creation and when the data is dispatched. It\u0026rsquo;s not entirely trivial, but I might just have an article in the pipeline showing some cool uses cases for this particular library, so stay tuned!\nIn any case, thank you so much to Stephen for sharing this incredible resource with the community. If you have implementation trouble with the library or any other type of feedback to share, please sound off in the comments. And please respect Stephen\u0026rsquo;s wishes when he requested any actual bugs and issues to be contributed to the issue tracker in GitHub.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/debug-change-history-gtm/",
	"title": "#GTMTips: Debug Change History In GTM",
	"tags": ["change history", "debug", "Google Tag Manager", "gtmtips"],
	"description": "Understand what changes have been made and when in the Google Tag Manager user interface.",
	"content": "Google Tag Manager can be quite the hierarchical mess. You have accounts which comprise containers. Containers are split into container versions, and you use workspaces to create new versions. Phew! Trying to keep tabs on all the activity within these layers can be quite the chore.\nIn this #GTMTips article, I\u0026rsquo;ll do a quick walkthrough of (almost) all the places where you can review changes made in your Google Tag Manager account and container.\nTip 49: Debug Change History In GTM   Let\u0026rsquo;s start from the top and make our way all the way to the smallest increments.\nAccount Activity What it shows: Changes to user permissions (both Account and Container level)\nYou\u0026rsquo;ll find Account Activity by browsing to Admin in the GTM user interface, and looking in the Account column.\n  When you click this link, you\u0026rsquo;ll see a list of all changes made on the Account level. These include:\n  Containers created and deleted\n  User permissions created, modified, and deleted\n    Note that if you have \u0026ldquo;No Access\u0026rdquo; as your permission level for any given container, you will not see its changes in this list.\nContainer Activity What it shows: Container Version, Workspace, and Environment activity.\nYou\u0026rsquo;ll find Container Activity under the Container column of GTM\u0026rsquo;s Admin interface.\n  Under Container Activity, you\u0026rsquo;ll find:\n  Container Version creation and restoration\n  Workspace deletion\n  Environment creation and deletion\n    I think list is oddly lacking. There\u0026rsquo;s nothing about Workspace creation nor anything about Container Version deletion. I\u0026rsquo;d also like to see more granular changes, such as when an Environment link is reset.\nContainer Versions What it shows: Container Versions, with details about when they were created and published.\nThis should be a pretty familiar list. You\u0026rsquo;ll find it by clicking Versions in the main navigation.\n  In the Versions list, you\u0026rsquo;ll find:\n  Each Container Version, numbered in sequence\n  Which Environment (if any) the Version is published to\n  Who created the Version and (if published) who published it\n  When the Version was published\n  Actions menu for the version\n    Note also the Show deleted button in the top right corner. By clicking that you can see deleted container versions, and by choosing Undelete from the Actions menu you can, surprise surprise, undelete the respective container version.\nBy clicking a version in this list, you will be taken to the dedicated page for debugging the changes with more detail (see below).\nI would really like to see the Workspace name from which the version was created here, too. It would help draw a line through container activity history from the version to the draft.\nVersion Changes / Activity History What it shows: Version Changes shows the list of assets (Tags, Triggers, Variables) that were modified in this Container Version. By clicking any one of these assets, you can see what changes were made between this version and what was then the Latest Version. Activity History gives you more information about who made the changes.\nYou\u0026rsquo;ll find these details by clicking any version either in the Versions list (see above) or from the Workspace Overview screen by clicking either \u0026ldquo;Live Version\u0026rdquo; or \u0026ldquo;Latest Version\u0026rdquo;.\n  Under Version Changes you\u0026rsquo;ll see:\n  What was changed\n  How do the changes compare with the Latest Version at the time when this version was created (click an asset to see this information)\n  When clicking Activity History an overlay flies out with:\n  Who made what changes\n  Who published the version\n    Oddly enough, the Activity History doesn\u0026rsquo;t tell you who created the container version itself. So only the Publish activity is enumerated in this list.\nWorkspace Changes What it shows: List of assets that have been modified in this Workspace draft, when compared to the Latest Container Version.\nThis is the list you\u0026rsquo;ll see in the Workspace Overview screen.\n  This list will give you the following details:\n  What assets were created, modified or deleted in this workspace\n  Who made the changes and when\n  What the changes were compared to the Latest Container Version\n  By clicking the dot menu at the end of each line, you can Abandon Changes you\u0026rsquo;ve made to the asset, or you can View Changes, which lets you analyze what modifications were done compared to the Latest Container Version.\nNote that \u0026ldquo;Last Edited\u0026rdquo; is quite literal. Removing something isn\u0026rsquo;t \u0026ldquo;Editing\u0026rdquo;, so in the list itself the \u0026ldquo;Last Edited\u0026rdquo; doesn\u0026rsquo;t reflect the time when the asset was deleted.\nRemember to read my Workspaces guide for more information about debugging Workspace activity!\nActivity History What it shows: A chronological list of changes to each asset. \u0026ldquo;Workspace Changes\u0026rdquo; only has the latest state listed, but \u0026ldquo;Activity History\u0026rdquo; shows you previous states, too. Also, any changes to the Workspace itself (e.g. sync with Latest Container Version) are listed here.\nYou\u0026rsquo;ll find the link to Activity History just below Version Changes.\n  When you click the link, an overly flies out with the following details:\n  Creation, deletion, modification, and restoration of individual assets\n  Workspace synchronization\n  Workspace creation\n    Summary These are the \u0026ldquo;obvious\u0026rdquo; places for debugging activity in your Google Tag Manager domain. There are some glaring omissions (especially under Container Activity in Admin), but with the current tools you can already get pretty far in pinpointing who made what changes and when.\nOne thing I miss from the pre-Workspace era is a master list of all changes to a container. So one place which puts all the above together, in chronological sequence. A sort of \u0026ldquo;Activity History\u0026rdquo; for the entire container, with all the details in one, long list. Right now there\u0026rsquo;s a bit too much bouncing about the place if you\u0026rsquo;re trying to align individual actions within the timeline of the entire container.\nThis wasn\u0026rsquo;t an exhaustive list for debugging container activity. Do you have your own tips and tricks for uncovering more details about what\u0026rsquo;s going on your precious container?\n"
},
{
	"uri": "https://www.simoahava.com/tags/change-history/",
	"title": "change history",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/10-javascript-concepts-for-web-analytics-implementation/",
	"title": "10 JavaScript Concepts For Web Analytics Implementation",
	"tags": ["education", "google analytics", "Google Tag Manager", "JavaScript"],
	"description": "Ten JavaScript concepts that are important to understand when working with web analytics implementations.",
	"content": "I\u0026rsquo;ve already written extensively about JavaScript in web analytics implementation. Suffice to say, understanding at least the basics is absolutely necessary to survive in the technical medium of the web browser.\n  This article expands on a conference talk I gave at MeasureCamp IX, London a short while ago. I\u0026rsquo;ve always been quite single-minded about the importance of JavaScript in web analytics development, and it was a pleasure for me to get some of that off my chest. So I want to go over the ten concepts introduced in my conference talk with a little more detail, especially in the context of Google Tag Manager.\n  Get the basics right JavaScript is not the quirky animation tool from two decades ago anymore - it\u0026rsquo;s a programming language if there ever was one. It hides a lot of complexity under its seemingly simple syntax, though in the web browser much of this complexity is either abstracted or removed entirely thanks to how the client-server infrastructure of the web works.\nTo get up to speed with JavaScript, I typically recommend the following resources. Codecademy especially is something you might want to devote time and effort to. It has lots of different tracks, and some that are extremely useful for web (analytics) developers.\n1. Codecademy   Codecademy is an amazing resource. The courses are free, fully interactive, and the service keeps track of your progress as you learn new skills along the way. Codecademy has lots of tracks for web developers, but I want to highlight the following as the bare minimum for web analytics:\n  JavaScript\n  HTML \u0026amp; CSS\n  jQuery\n  I recommend making it habitual to always have a course active in Codecademy. Programming is not a skill you develop in silos - each new language and discipline you learn helps you become a better developer overall.\n2. Books   You might argue that books are not the best way to keep up-to-date with web technologies, but the following volumes have stood the test of time (with the exception of the last one which is brand new) marvellously, and even if some of the code samples might be outdated, the ideas behind the solutions remain as topical as ever.\n  Nicholas Zakas: Professional JavaScript for Web Developers (3rd Edition)\n  Cody Lindley: DOM Enlightenment\n  Jonathan Weber: Practical Google Analytics and Google Tag Manager for Developers\n  Zakas\u0026rsquo; volume is a great, if somewhat intimidating, foray into how JavaScript and the web browser interact. Needless to say, it is a goldmine for understanding the technical stack you\u0026rsquo;re working with when implementing web analytics solution. It\u0026rsquo;s the book I would most recommend you to read once you have the basics of JavaScript down.\n3. Sandbox The only way you can really learn something is by doing it. All creative skills, programming included, need honing. The best way to learn programming is to actively practice it. So in addition to taking courses and learning by reading, remember to also create stuff! A great way to do it is to deploy Google Tag Manager on your own website. If you don\u0026rsquo;t have one, you can now create one with your new-fangled skills!\nPlay around with the JavaScript Console of your web browser\u0026rsquo;s developer tools. Implement scripts in Google Tag Manager, and see how they interact with your website. Use tools like JS Lint to understand what issues your code might have.\nWhatever you do, just keep coding.\n  In the context of Google Tag Manager, functions are arguably the most useful little things when customizing implementations. That\u0026rsquo;s because the Custom JavaScript Variable is basically a function call that you use to resolve a value.\nThe Custom JavaScript Variable has a very deliberate syntax when using Google Tag Manager, and some of the restrictions might sound odd to you, so I want to go over them with some detail in this chapter.\nAnonymous function with a return statement The Custom JavaScript Variable requires an anonymous function with a return statement. An anonymous function is exactly that: it\u0026rsquo;s a function without a named identifier that you could use to refer to it. Now, you can argue that it does have a name since the Variable itself has a name, but that\u0026rsquo;s GTM\u0026rsquo;s proprietary way of resolving the Custom JavaScript Variable and has nothing to do with the underlying JavaScript syntax.\nThe Custom JavaScript Variable doesn\u0026rsquo;t take any parameters, either. All values you want to process through it need to come from the internal data model of Google Tag Manager (GTM\u0026rsquo;s \u0026ldquo;Variables\u0026rdquo;), need to be created on the spot (local scope), or need to be available in the global namespace (global scope).\nFinally, all Custom JavaScript Variables need a return statement. They must always return some value, as they are invoked in situations where a value is required.\nSo this is the minimum viable Custom JavaScript Variable you could create.\nfunction() { return; }  When invoked with the {{Variable}} syntax, this function would return undefined.\nAvoid side effects Since the Custom JavaScript Variable is a value-returning function which does not accept parameters, it should only be used for input/output operations. Thus, the purpose of the Custom JavaScript Variable is to take some value, modify it, and then return the modified value.\nA function has side effects if it does something else except the input/output operation described in the previous paragraph. Typical side effects occur when a function is used to modify something in the global scope, or it\u0026rsquo;s used to set some value elsewhere, or, in the context of Google Tag Manager, it\u0026rsquo;s used to manipulate the Data Layer.\nYou should avoid side effects because Google Tag Manager can\u0026rsquo;t guarantee that the Custom JavaScript Variable is only resolved once per activation. In fact, in Preview mode, Custom JavaScript Variables can be resolved dozens of times per dataLayer.push(), which can lead to a confusing experience.\nfunction() { // AVOID THIS:  window.dataLayer.push({\u0026#39;event\u0026#39; : \u0026#39;JavaScript executed\u0026#39;}); return; }  The main problem with side effects is that they\u0026rsquo;re hard to trace if problems arise. Since the Custom JavaScript Variables are resolved at arbitrary times, having them modify the global scope can be hazardous and can easily lead to unwanted race conditions.\nUnderstand scope, utilize closures JavaScript in the web has two types of scope: local and global.\nLocal scope, also known as function scope, is what variables are restricted to when declared within a function. When you declare a variable in a function, it will only exist for the duration of the function call (though see the part about closures below), after which the garbage collector comes and whisks it away. Locally scoped variables cannot be referred to or invoked from outside the current function context.\nfunction() { // These variables all have local scope  var hello = \u0026#34;Hi, \u0026#34;; var myname = \u0026#34;Simo!\u0026#34;; return hello + myname; }  Global scope, on the other hand, comprises variables that are declared as properties of the window object. If you don\u0026rsquo;t use the var keyword when declaring a variable in a function, it is automatically elevated into global scope.\nWhen a variable is in global scope, you can refer to it anywhere. For example, the ubiquitous document property of the browser\u0026rsquo;s Document Object Model can be used anywhere since it\u0026rsquo;s actually a global property of the window object:\nwindow.document === document; // true  Closures are an exception to how locally scoped variables are ephemeral and inaccessible from the outside. When you create a closure, you are actually creating a simple interface that lets you access the locally scoped variable from outside the function!\nfunction() { var timeNow = new Date(); // Locally scoped  return function() { return \u0026#34;Time then was: \u0026#34; + timeNow; }; }  The closure is the return function() {...} statement. Because the function is declared within same context, it has access to the locally scoped timeNow variable. And because the function is the target of the return statement, you can actually call this returned function in code! In the example below, the Custom JavaScript Variable we just created above is named Time Then.\nvar timeThen = {{Time Then}}(); console.log(timeThen);  The parentheses at the end of the Custom JavaScript Variable are significant. If you simply had {{Time Then}}, the variable assignment would store the return value of the Custom JavaScript Variable (the closure) into the timeThen variable. The console.log() command would simply output the function description itself, and not the result. By adding the parentheses, you\u0026rsquo;re telling the browser to actually invoke the function returned by the Custom JavaScript Variable. Since the timeNow is only created once (when the Custom JavaScript Variable is first resolved), the closure will always return the same date-time string.\nModify state when you are in control of resolution There is one exception to Custom JavaScript Variables having side effects. When you are in full control of when the function is resolved, it\u0026rsquo;s fine to cause side effects, since you don\u0026rsquo;t have to worry about the function resolving an unknown number of times or in unknown contexts.\nOne such example is when using a closure in a callback. Callbacks (more on these later) are execution milestones which take place after some other process has completed. So if you indicate the closure of a Custom JavaScript Variable as the value of a callback, you can rest assured that the function is only called once - when the main process has completed.\nfunction() { // Return a function to be used by hitCallback  return function() { // It\u0026#39;s OK to modify state here!  window.dataLayer.push({\u0026#39;event\u0026#39; : \u0026#39;tagHasFired\u0026#39;}); }; }  This process is exemplified in the hitCallback feature of the Google Analytics tracker. The hitCallback function is called after the request to Google Analytics has completed. Thus, you can use the closure of a Custom JavaScript Variable to perform some state-altering thing in the hitCallback field itself! This is very useful if you want to chain tags in a sequence, for example.\nFURTHER READING Take a look at the following articles for more information on how functions work:\n  w3schools: JavaScript Closures\n  MDN: Closures\n  Custom JavaScript Variable\n  #GTMtips: hitCallback And eventCallback\n    In JavaScript, you have five value-carrying data types:\nvar something = 5; // number var something = \u0026#34;five\u0026#34;; // string var something = true; // boolean var something = {five: 5}; // object var something = function() { return 5; }; // function  It gets slightly messier than this, since there are actually lots of different types of objects (all JavaScript data types are actually an extension of \u0026ldquo;object\u0026rdquo; in its fundamental sense):\nvar something = {five: 5}; // Object var something = [5,5,5]; // Array var something = new Date(); // Date  And finally we have some mystic pseudo-types which don\u0026rsquo;t carry any value:\nvar something = undefined; // undefined var something = null; // null  Dynamic type Unlike some other programming languages (e.g. Java), type is dynamic in JavaScript. That means that you can reassign a variable with a completely different type without having to worry about type conversion.\nvar something = 5; something = \u0026#34;five\u0026#34;;  The above is possible in JavaScript, because the variable assignment itself doesn\u0026rsquo;t coerce the assigned value to any specific type. This can be a weakness in the code, because it requires you to test for type to avoid errors.\n// Works because something is a number var something = 5; var somethingMultiplied = something * 2; // Won\u0026#39;t work because something is suddenly a string something = \u0026#34;five\u0026#34;; somethingMultiplied = something * 2; // NaN: Not a Number  // So you need to test if (typeof something === \u0026#39;number\u0026#39;) { somethingMultiplied = something * 2; }  The further apart your variables are from their execution context, the messier it can get with all the type checks. So try to avoid exploiting dynamic type, and do your best to maintain type when creating and reassigning variables.\nLoose type JavaScript also does type conversion for you when needed. This means that the following is possible:\nvar something = \u0026#34;5\u0026#34;; var somethingMultiplied = something * 2; // 10  In the multiplication, JavaScript expects something to resolve to the number type. Even though something is a string, JavaScript detects the number within and automatically converts the variable to a number for the multiplication statement. The variable something itself is not modified, but the value held by it is converted automatically to make resolving somethingMultiplied possible.\nNote that there are some embarrassing exceptions. The plus operator (+) is not only used for mathematical statements but also for concatenating strings. So if you try to add a number to a string, you\u0026rsquo;ll end up with a concatenated string rather than a number!\nvar a = \u0026#34;5\u0026#34;; var b = 6; var c = a + b; // \u0026#34;56\u0026#34;!  // You need to use parseInt() to manually convert to number: c = parseInt(a) + b; // 11  Weird stuff I\u0026rsquo;ll just leave this list here. It\u0026rsquo;s just a handful off the odd behavior you get when working with loose and dynamic type.\ntypeof NaN; // number NaN === NaN; // false typeof NaN; // number isNaN(NaN); // true isNaN(null); // false typeof null; // object null instanceof Object; // false typeof undefined; // undefined undefined === null; // false undefined == null; // true {simo: true} === {simo: true}; // false  Undefined and Google Analytics Remember that the undefined type has a special function in Google Analytics (and Google Tag Manager). If you refer to a variable which returns undefined, that particular setting, field, or parameter is automatically dropped from the request to Google Analytics.\nIn Google Tag Manager, undefined can also be used to purge values from the Data Layer. This is extremely useful on single-page apps, where variables persist their values throughout the visit.\nwindow.dataLayer.push({ event: \u0026#39;GAEvent\u0026#39;, eventData : { cat: \u0026#39;Category value\u0026#39;, act: \u0026#39;Action value\u0026#39;, lab: undefined, val: undefined } });  FURTHER READING The following articles shed more light on data types in JavaScript:\n  w3schools: JavaScript Data Types\n  MDN: JavaScript data types and data structures\n  #GTMtips: Undefined dimensions won\u0026rsquo;t get sent\n    When looking for things that can slow down the site, slow or poorly performing HTTP requests are usually the easy culprit to blame.\nThanks to asynchronous loading, most HTTP requests are done in such a way that they don\u0026rsquo;t block the page from rendering, but async itself doesn\u0026rsquo;t guarantee a smooth ride.\nLet\u0026rsquo;s take Google Tag Manager as an example. This is a typical container snippet:\n(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;:new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src=\u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f);})(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-W92WQQ\u0026#39;);  I\u0026rsquo;ve gone over this code in an earlier article, but suffice to say that the end result of all this code is this:\n  In other words, the minified code creates a new script HTML element, which instructs the browser to request for a file in Google\u0026rsquo;s servers, and the request is done asynchronously.\nThe script element, then, turns into an HTTP request, since the src attribute in the tag signals the browser to make an HTTP request to the URL value of the attribute.\n  So just by creating the script tag, the browser automatically executed an HTTP request to the given endpoint, and the result of this request is the gtm.js retrieved in the browser for immediate execution.\nThere are lots of elements that can produce an HTTP request. All scripts, images, and HTML files need to be retrieved from the web server, and the main channel of data transfer is the HTTP request. The following HTML tags (with the given attributes) also invoke HTTP requests:\n\u0026lt;input src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;input usemap=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;ins cite=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;link href=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;object classid=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;object codebase=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;object data=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;object usemap=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;q cite=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;url\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;audio src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;button formaction=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;command icon=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;embed src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;html manifest=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;input formaction=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;source src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;video poster=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;video src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;applet codebase=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;area href=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;base href=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;blockquote cite=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;body background=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;del cite=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;form action=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;frame longdesc=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;frame src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;head profile=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;iframe longdesc=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;iframe src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;img longdesc=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;url\u0026#34;\u0026gt; \u0026lt;img usemap=\u0026#34;url\u0026#34;\u0026gt; Yeah. That\u0026rsquo;s quite a bunch.\nNote that HTTP requests can be created manually using e.g. XMLHttpRequest or jQuery.ajax(). In fact, when you send hits to Google Analytics, you\u0026rsquo;re actually using HTTP requests created by the analytics.js library.\n  FURTHER READING Here\u0026rsquo;s more information on HTTP requests:\n  w3schools: AJAX - Create an XMLHttpRequest Object\n  w3schools: HTTP Methods: GET vs. POST\n  Measurement Protocol Reference: Transport\n    The difference between asynchronous and synchronous loading is subtle but significant.\nJavaScript in the web browser is single-threaded. That means that only one process can ever be running at a time, and thus every single line of JavaScript must be executed completely before the browser can proceed to the next line. If you make a synchronous HTTP request, it means that the browser will wait until the entire request (download, execution, render) is taken to completion before the browser proceeds to the next line of code in the page. It\u0026rsquo;s said that synchronous operations are thus blocking.\nTo counter this potentially devastating blow to page render and user experience, asynchronous loading was introduced to make the \u0026ldquo;expensive\u0026rdquo; part of the process, the download, happen so that it doesn\u0026rsquo;t block the browser.\nSo, when a request is made asynchronously, the browser initiates the download but then proceeds to the next line of code while the download goes on in the background. Only after the file is completely downloaded, does the browser return execution to this initial process, rendering the downloaded asset in the browser.\nThe problem with asynchronous execution is that even though you can pinpoint the exact moment when the request is initiated, as it\u0026rsquo;s governed by the order of the lines of code in the page template, you\u0026rsquo;ll never know the exact moment when it completes. And this can lead to something called a race condition.\nSo let\u0026rsquo;s say I have the following two lines of code in the page template:\n\u0026lt;script async src=\u0026#34;https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; src=\u0026#34;https//www.googletagmanager.com/gtm.js?id=GTM-XXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; The browser starts loading the jQuery library before GTM, so I might expect jQuery to available to GTM as soon as the latter has finished downloading. But that\u0026rsquo;s not necessarily the case! The actual download of jQuery can end long after GTM has completed, because the jQuery library is more often than not quite a bulky thing. Also, if the CDN providing the jQuery library has latency, it might affect when jQuery becomes available.\nSo, if in Google Tag Manager I try to use jQuery, there can be a race condition where GTM expects jQuery to be available but it isn\u0026rsquo;t.\nTo combat the race condition, we can use tools like callbacks, promises or, in GTM, Tag Sequencing. You can always resort to synchronous downloading if you\u0026rsquo;re really concerned about race conditions, but remember that this can have a detrimental impact on user experience.\n\u0026lt;!-- Synchronous request --\u0026gt; \u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script async=\u0026#34;\u0026#34; src=\u0026#34;https//www.googletagmanager.com/gtm.js?id=GTM-XXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Callback --\u0026gt; \u0026lt;script\u0026gt; (function() { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\u0026#39;; el.async = true; el.addEventListener(\u0026#39;load\u0026#39;, function() { window.dataLayer.push({event: \u0026#39;jQueryLoaded\u0026#39;}); }); document.head.appendChild(el); })(); \u0026lt;/script\u0026gt; \u0026lt;!-- Tag Sequencing --\u0026gt; \u0026lt;script\u0026gt; (function() { var el = document.createElement(\u0026#39;script\u0026#39;); el.src = \u0026#39;https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\u0026#39;; el.async = true; el.addEventListener(\u0026#39;load\u0026#39;, function() { window[\u0026#39;google_tag_maanger\u0026#39;][{{Container ID}}].onHtmlSuccess({{HTML ID}}); }); document.head.appendChild(el); })(); \u0026lt;/script\u0026gt; It\u0026rsquo;s better to be safe than sorry, so when working with potential race conditions, make sure to always check if the resources you are trying to use actually exist.\nif (typeof jQuery !== \u0026#39;undefined\u0026#39;) { // Use jQuery } else { // Use something else }  FURTHER READING Here are some useful links on race conditions, callbacks, and such.\n  Callback Hell\n  MDN: Promise\n  Understanding Tag Sequencing in Google Tag Manager\n    Single-page web applications (SPA) utilize something called the History API to provide a multi-page experience with only one initial page load in the web server. Subsequent transitions from page-to-page are commonly done using the History API, by withdrawing content from the server without initiating a page refresh, and manually creating history entries in the web browser. Common frameworks for SPAs are e.g. React, AngularJS, and Backbone.js.\nSPAs have significant implications for web analytics. For example, an SPA can suffer from the rogue referrer problem, where URL query parameters that are used for campaign settings are overwritten by a subsequent page view hit. For Google Tag Manager, the problem of persistence rears its ugly head, where variables maintain the values you set earlier, leading to these values bleeding into subsequent tags.\nBrowser history is manipulated because when content is dynamically loaded, the site needs to tell the web browser that the user has entered a new page without the benefit of the page reload doing this automatically. Otherwise you wouldn\u0026rsquo;t be able to link to dynamic content, nor would the Back and Forward buttons of the browser find your dynamic content.\nThe two interfaces you\u0026rsquo;ll run into most often are pushState and replaceState. The former creates a new history entry in the web browser, and the latter replaces the current entry entirely. Here are examples of how to use them:\nwindow.history.pushState( {pageType: \u0026#39;formThankYou\u0026#39;}, \u0026#39;Form Success\u0026#39;, \u0026#39;/thank-you/\u0026#39; );  When you run this command, the URL changes to /thank-you/ without a page reload. The first two parameters add additional information about the state. If you now click the Back button of the browser, you are taken to the URL where you called pushState.\nwindow.history.replaceState( {pageType: \u0026#39;formThankyou\u0026#39;}, \u0026#39;Form Success\u0026#39;, \u0026#39;/thank-you/\u0026#39; );  When you execute this command, it actually replaces the current page in the browser history. In other words, when you click the browser\u0026rsquo;s Back button, you are taken to the page you were on before you entered the page from where you called replaceState.\nNote that if you simply add #someHash in the URL, it\u0026rsquo;s the equivalent of calling pushState without the benefit of the other parameters in the interface (e.g. the state object).\nIn Google Tag Manager, you have a really useful Trigger type called the History Change Trigger. It goes off whenever one of the following History events occur:\n pushState replaceState hashchange (when the #someHash in the URL appears/changes) popstate (when the active history entry changes)  For example, if you want to build pageviews around these history events, it\u0026rsquo;s quite simple to do with the History Change Trigger!\nFURTHER READING For more information on the Window History API, look no further than these articles:\n  MDN: Manipulating the browser history\n  w3schools: JavaScript Window History\n  Google Tag Manager History Listener\n    The web page is stateless. With every page load, the entire page is built from scratch. All the variables, JavaScript libraries, HTML templates, CSS files and other resources are loaded again.\nIn other words, if the web browser needs to know something, anything, about what happened at any time in the past, you need to manually persist this information.\nYou can persist the information in the web server, and this is often quite a robust way to do it. However, because of how the web is built, you still need a way of aligning any given web browser (and thus, user) with the requests that are sent to the web server.\nThe web browser provides a number of ways to persist this information. The most common methods are browser cookies, Web Storage, and IndexedDB.\n  Browser cookies Browser cookies are useful for simple storage. The browser cookie is essentially a string stored by the web browser, and its accessible only by pages that share the same parent domain to which the cookie was written. So a cookie written on www.simoahava.com would, by default, only be available to pages on www.simoahava.com, but you could also write the cookie on simoahava.com, where it would be available to all subdomains.\nA cookie written on simoahava.com would not be available for pages on derekahava.com, because they do not share the same domain.\nCookies also survive transitions from the http:// protocol to https://.\nWeb Storage Web Storage (or DOM Storage) encompasses both localStorage (no expiration) and sessionStorage (expires when the web browser is closed). Web Storage is much more flexible than using cookies, because the entries are stored as a hash table, and lookups can be done on a key-by-key basis. With cookies, you need to unravel the entire cookie string just to find the value want.\n// TO SET if (window[\u0026#39;Storage\u0026#39;]) { localStorage.setItem(\u0026#39;subscribe\u0026#39;, \u0026#39;true\u0026#39;); sessionStorage.setItem(\u0026#39;subscribe\u0026#39;, \u0026#39;true\u0026#39;); } else { setCookie(\u0026#39;subscribe\u0026#39;, \u0026#39;true\u0026#39;); } // TO FETCH localStorage.getItem(\u0026#39;subscribe\u0026#39;); sessionStorage.getItem(\u0026#39;subscribe\u0026#39;);  Note that Web Storage does not survive the transition from http:// to https://. Both protocols have their own storage.\nIndexedDB IndexedDB is much more complex than the aforementioned methods, and is thus more suited for application logic on a much larger scale. For example, if you\u0026rsquo;re using Service Workers to maintain offline browsing capabilities, IndexedDB is an excellent utility to maintain state while waiting for the internet connection to come back up.\nCheckout the link to my article on persisting data in Google Tag Manager for more information (and some tips) on how to setup cookie and Web Storage persistence in GTM!\nFURTHER READING Check out the following links for more information:\n  w3schools: JavaScript Cookies\n  MDN: Web Storage API\n  Two Ways To Persist Data Via Google Tag Manager\n    Locating elements on the page is one of the more obvious use cases for web analytics. Most commonly we want to know if some specific element was the target of a user action such as a click or form submission.\nIn Google Tag Manager, identifying an element is easy enough with CSS selectors, but what if we actually want to retrieve some element other than what GTM initially gave us?\n  There are many ways to retrieve an element relative to some other with JavaScript. A very clumsy way to do it would be to build a complete element chain all between the two elements. So if you wanted to retrieve the element at the top of the DOM in the image above, you could do something like this in a Custom JavaScript Variable:\nfunction() { // Please don\u0026#39;t do this:  return {{Click Element}} .parentElement .parentElement .parentElement .parentElement .parentElement; }  But please don\u0026rsquo;t do this. It\u0026rsquo;s clumsy and it has multiple points of failure. You\u0026rsquo;re basically expecting the DOM to have a certain structure without providing any redundancies or fallbacks. The longer the chain, the more sensitive it is to even minor changes to the page markup.A better way is to disregard the actual number of elements, and just climb up the DOM until you reach the very top.\nSo a more robust way of retrieving the element would be something like this:\nfunction() { var el = {{Click Element}}; while (el.className !== \u0026#39;content-sidebar-wrap\u0026#39; \u0026amp;\u0026amp; el.tagName !== \u0026#39;BODY\u0026#39;) { el = el.parentElement; } return el.tagName !== \u0026#39;BODY\u0026#39; ? el : undefined; }  This little script simply climbs up the DOM until it either reaches the element you wanted (.content-sidebar-wrap) or the \u0026lt;body\u0026gt; tag. If it reaches \u0026lt;body\u0026gt;, the assumption is that the element you were looking for was not found, and undefined is returned instead.\nNote that this script only works for direct ancestry. If you want to find sibling elements or elements which branch from the direct path between the original element and the root of the document, you might want to leverage frameworks like jQuery to make DOM traversal code more economic.\nFURTHER READING Follow these links to find more information on DOM traversal:\n  w3schools: JavaScript HTML DOM Navigation\n  DOM Enlightenment\n  Node Relationships And GTM\n    When Google Tag Manager introduced the matches CSS selector Trigger operator a while ago, it eliminated a lot of complexity from validating Triggers to only fire for certain HTML element interactions. In a similar manner, the querySelector and querySelectorAll JavaScript DOM methods were just as impactful when they were introduced some years ago into the web browser API.\nIf you\u0026rsquo;ve built websites or worked with CSS (Cascading Style Sheets) styles before, you\u0026rsquo;ll know what selectors do. They let you select any element or group of elements on the page, and then apply a style to them. For example, the following style declaration would remove the default underline from all links (\u0026lt;a\u0026gt;) with class internal:\na.internal { text-decoration: none; } Because it\u0026rsquo;s such an effective way of selecting HTML elements, it\u0026rsquo;s not a big surprise that JavaScript introduced methods for using CSS selectors to find and retrieve HTML elements on the page:\n// Select every link with attribute data-gtm on the page: var gtmLinks = document.querySelectorAll(\u0026#39;a[data-gtm]\u0026#39;); // Select the first \u0026#39;h2\u0026#39; element on the page, which is the direct child of \u0026#39;section\u0026#39; var firstH2 = document.querySelector(\u0026#39;section \u0026gt; h2\u0026#39;); // Select the \u0026#39;span\u0026#39; with class \u0026#39;title\u0026#39; that is a descendant of the aforementioned \u0026#39;h2\u0026#39; var spanTitle = firstH2.querySelector(\u0026#39;span.title\u0026#39;);  Similarly, if you already have an HTML element, you can use the matches API to check if it matches a specific CSS selector:\nvar isElementLink = function(el) { return el.matches(\u0026#39;a\u0026#39;); }  The function above would return true or false, depending on whether the element passed as a parameter is a link or not. Note that matches isn\u0026rsquo;t widely supported yet, but there\u0026rsquo;s a polyfill you can use to make it work across all browsers.\nIn Google Tag Manager, the matches CSS selector is ridiculously useful. You can use it to fire your Triggers only when a specific element is the target of the interaction. For example, I could configure a Click / All Elements Trigger like the following to make it only fire when the clicked element is a specific \u0026lt;span\u0026gt; (or any of its nested descendants) on the page!\n  CSS selectors shave a crazy amount of time off your JavaScript, as you don\u0026rsquo;t have to write clumsy comparison queries using DOM traversal when trying to identify a specific element. Take a look at the further reading links below to learn more about CSS selectors!\nFURTHER READING More stuff about CSS selectors:\n  w3schools: CSS Selectors Reference\n  Matches CSS Selector Operator In GTM Triggers\n    jQuery is undoubtedly the most used JavaScript library in the world, and its contributions to standardizing the web experience across browsers and devices should not and do not go unnoticed. The framework provides such a wealth of utilities that trivialize and abstract many of the woes a web developer might have when developing their site to be as accessible across the splintered browser landscape as possible.\nIn short, jQuery is a library which provides a bunch of simple APIs to perform otherwise complex tasks. For example, let\u0026rsquo;s take the DOM traversal example from chapter 7 and rewrite it with jQuery:\n// Vanilla JS function() { var el = {{Click Element}}; while (el.className !== \u0026#39;content-sidebar-wrap\u0026#39; \u0026amp;\u0026amp; el.tagName !== \u0026#39;BODY\u0026#39;) { el = el.parentElement; } return el.tagName !== \u0026#39;BODY\u0026#39; ? el : undefined; } // jQuery function() { return jQuery({{Click Element}}).closest(\u0026#39;.content-sidebar-wrap\u0026#39;)[0]; }  So a single line of jQuery abstracted the previous implementation.\nSimilarly, here\u0026rsquo;s the difference between building a custom HTTP POST request using regular JavaScript versus using jQuery:\n// Vanilla JS var xhr = new XMLHttpRequest(); var payload = \u0026#39;v=1\u0026amp;tid=UA-12345-1\u0026amp;t=pageview\u0026amp;dl=https://www.simoahava.com/\u0026amp;cid=12345.12345\u0026#39;; xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;https://www.google-analytics.com/collect\u0026#39;); xhr.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/x-www-form-urlencoded\u0026#39;); xhr.onload = function() { if (xhr.status === 200) { alert(\u0026#39;Yay!\u0026#39;); } else if (xhr.status !== 200) { alert(\u0026#39;Aww :(\u0026#39;); } }; xhr.send(encodeURI(payload)); // jQuery jQuery.post( \u0026#39;https://www.google-analytics.com/collect\u0026#39;, \u0026#39;v=1\u0026amp;tid=UA-12345-1\u0026amp;t=pageview\u0026amp;dl=https://www.simoahava.com/\u0026amp;cid=12345.12345\u0026#39; ) .done(function() { alert(\u0026#39;Yay!\u0026#39;); }) .fail(function() { alert(\u0026#39;Aww :(\u0026#39;); });  I hope you see the trend here. jQuery really makes things more elegant for both the developer as well as for anyone who wants to look at the code and quickly understand what it tries to do.\njQuery isn\u0026rsquo;t all-powerful, though, and relying too much on a framework can make your code weak. My suggestion is to always understand what a given jQuery method does before using it. That way you\u0026rsquo;ll know what the background process is, and you\u0026rsquo;ll learn to avoid nasty surprises such as how jQuery\u0026rsquo;s return false; in event handlers can break GTM\u0026rsquo;s listeners!\nRemember also to be wary of race conditions when loading jQuery.\n  If you load the library asynchronously, it might not be ready by the time you refer to it in your GTM Tags.\nFURTHER READING And here\u0026rsquo;s the scoop on jQuery:\n  jQuery API Documentation\n  Codecademy: jQuery\n    Finally, the Data Layer. If you\u0026rsquo;ve used GTM or any tag management solution, it\u0026rsquo;s probably you\u0026rsquo;ve had to really put some thought into understanding how tag management solutions leverage the Data Layer to communicate with the website or other digital property.\nIt\u0026rsquo;s not purely a JavaScript concept, so it\u0026rsquo;s kind of a black sheep in this mix. Nevertheless, the most typical implementation of a Data Layer, when it comes to tag management solutions, is an Array-type data structure in the global namespace.\nI\u0026rsquo;ve written about Data Layer quite a lot on this blog, so instead of repeating what I\u0026rsquo;ve already written far more extensively, I want to direct you to the links below and I strongly suggest you read through them to get a better idea of how the Data Layer works.\nFURTHER READING Be sure to check these links out for more information on the Data Layer:\n  The Data Layer\n  Google Tag Manager\u0026rsquo;s Data Model\n  GitHub: google/data-layer-helper\n  Summary The ten JavaScript concepts introduced in this article are, what I consider, the most useful tools that any web analyst would need to know about. Heck, I consider these vital to anyone working with the web, not just analysts!\nThe thing is, we\u0026rsquo;re working in an extremely technical medium. The web browser is tantalizingly complex in all its machinations, and it doesn\u0026rsquo;t really help that all web browsers act slightly differently when it comes to producing the web document into our screens.\nWhen working with web analytics, we are trying to decipher the abstract signals sent by users, web servers, the browser itself, and background processes, as we do our best to align these signals with their real-world counterparts.\nIt\u0026rsquo;s not easy!\nTrying to deconstruct the enormously difficult concepts of \u0026ldquo;intent\u0026rdquo;, \u0026ldquo;engagement\u0026rdquo;, and \u0026ldquo;interaction\u0026rdquo; using a handful of JavaScript handlers seems like a fool\u0026rsquo;s errand, but it\u0026rsquo;s the only thing we can do when creating an implementation framework for any given web application.\nBut the more we know about JavaScript and the numerous APIs that the web browser lets us tap into, the more we can uncover about our visitors and their actions on the site.\nI hope you found these tips useful. Do you have other JavaScript concepts that you consider are absolutely essential for anyone working with the web to acquaint themselves with?\n"
},
{
	"uri": "https://www.simoahava.com/tags/education/",
	"title": "education",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/revamped-user-interface-in-google-tag-manager/",
	"title": "Revamped User Interface In Google Tag Manager",
	"tags": ["Google Tag Manager", "user interface", "workspaces"],
	"description": "Reviewing the revamped user interface in Google Tag Manager.",
	"content": "It\u0026rsquo;s been a crazy week. Just crazy. Not only did Google Tag Manager introduce Workspaces, arguably one of its most important releases ever for GTM, but they also revamped the user interface! So very big changes have been underfoot, and I\u0026rsquo;m so happy to be writing about them, because in my completely biased opinion these changes are amazing and well worth the long wait.\nIn this article, I want to quickly walk you through what I think are the most meaningful changes in the interface. A lot has changed, but the underlying mechanisms of GTM are still the same. You\u0026rsquo;re still creating Tags, Triggers, and Variables, and you\u0026rsquo;re still debugging the container using Debug Mode. However, the new user interfaces enhances and streamlines the various creation processes in the container, and the new Abandon / View Changes feature allows you to debug and diff the changes you or your team has made.\n1. The overlays The new interface, starting with its flashy account selector menu, just reeks of Material Design. The added third dimension to navigation provides a completely novel experience for GTM veterans.\nA historical pet peeve in GTM had been that if you\u0026rsquo;re in the middle of the Tag creation process and you notice that you need to configure a new Trigger or Variable, you were taken away (or had to leave) the Tag view in order to configure the secondary asset in a separate part of the container. Even though this was largely fixed a while ago, it still leaves a clunky feeling with the page transitions and the staggered workflow.\nWith the overlays, you will never leave the location you are in. The overlays fly in so that they do not cover the whole page, so you\u0026rsquo;ll always know where you started off from.\n  Moving back in history is as easy as clicking the X in the top left corner of any overlay or just clicking the shaded area in the left-hand side of the screen.\nThis, in my opinion, will save time and grey hairs. These overlays will take time to get used to, but it\u0026rsquo;s well worth it. You can zip and zap through a Tag creation workflow in a fraction of the time it used to take.\n2. The Overview The Overview screen has been slightly revamped, especially if you\u0026rsquo;ve already started working with Workspaces.\n    It\u0026rsquo;s not \u0026ldquo;Container\u0026rdquo; anymore in the navigation bar, it\u0026rsquo;s \u0026ldquo;Workspace\u0026rdquo; now. Makes a lot of sense, since you will always be working in a Workspace.\n  You don\u0026rsquo;t add notes to Versions anymore. Instead, you modify the Description of a Workspace.\n  The center panel used to hold information about the current version. Now, it tells you what\u0026rsquo;s going on in the current Workspace.\n  The panel to the right used to be solely about the Published Version. Now it\u0026rsquo;s split into two, giving you the run-down on the Live Version (i.e. the Published Version) as well as the Latest Container Version. The latter is very important for Workspaces, as the Latest Container Version is what your workspaces will merge to/with. Clicking either half will take you to the respective version page.\n  The large center area is reserved for changes in your Workspace when compared to the Latest Container Version. Every change you make in Workspace is a divergence from the Latest Container Version.\n  The Activity History used to be for the whole container, but now it\u0026rsquo;s just for the current Workspace. To see activity history for the container, you will need to go through the Versions section of the container.\n  The PUBLISH menu is now red when you have pending changes, and next to it is a counter with the number of changes the current Workspace has introduced. If you click the Container ID (GTM-XXXXX), you\u0026rsquo;ll see an overlay with the container snippet and instructions for its installation.\n  Note also the \u0026ldquo;Current Workspace\u0026rdquo; selector in the left-hand navigation. There\u0026rsquo;s more details about this in my Workspaces guide.\n  3. Timestamps! This is minor but still major. You know how irritating it always was to just see relative timestamps instead of proper ones? \u0026ldquo;3 days ago\u0026rdquo; as a Last Modified time isn\u0026rsquo;t that informative, right? Well, we\u0026rsquo;ve been offered an olive branch. By hovering over any relative timestamp, you\u0026rsquo;ll see the actual timestamp in a tooltip.\n  You can argue that it would be better to only have the actual timestamp visible and none of this relative timestamp bullcrap, but this is a step in the right direction, definitely.\n4. Versions There\u0026rsquo;s just one important change to the Versions view itself. What used to be \u0026ldquo;Edit as New Version\u0026rdquo; is now \u0026ldquo;Set as the Latest Version\u0026rdquo;. This is, again, because of Workspaces.\n  When you set a version as the Latest Version, it has implications for all active Workspaces, as they will all need to be updated with the changes in this Latest Version. So don\u0026rsquo;t be cavalier when using this feature!\n  The biggest change to the Version page itself, apart from the new UI, is that there\u0026rsquo;s both \u0026ldquo;Version Changes\u0026rdquo; and \u0026ldquo;Activity History\u0026rdquo;. Version Changes describes the changes in this version compared the previous Latest Container Version. Activity History is a more detailed list of what things were done in the version and, importantly, by whom.\n  5. Tags So, now to the juicy part. As I said in the beginning, the workflow has been streamlined to the maximum. There\u0026rsquo;s lots of little changes here, so I\u0026rsquo;m sure I won\u0026rsquo;t be able to cover them all. These, however, are the ones I\u0026rsquo;ve found most meaningful.\n  Things have certainly changed! On top of the view, the underlined Undefined Tag is what you edit to modify the Tag name. Next to it is the Folder icon, which lets you assign the Tag to a folder.\nThe view is dominated by the two boxes. The first is where you create and configure the Tag itself, and the second is where you attach Triggers and Exceptions to the Tag.\nWhen you click \u0026ldquo;Choose a tag to being setup\u0026hellip;\u0026rdquo;, a new overlay pops out, offering you an impressive list of Tag templates to choose from.\n  After selecting the Tag type, the creation process itself is pretty much the same as it used to be.\n  As for Triggers, once you click the Trigger creation panel, a more complex overlay pops out. I\u0026rsquo;ll go over these steps in the next chapter.\nDo note that you will not be able to add an Exception until you have added at least one Trigger (makes sense).\n  Note that if you open any Tag, Trigger, or Variable for editing, and that asset has already been edited (or newly created) in this Workspace, you will see a new selection in the view, allowing you to Abandon and/or View the changes that have been committed to the asset.\n  I\u0026rsquo;ve covered Abandoning and Viewing changes in more detail in the Workspaces guide.\n  To save an asset, just click the blue SAVE button in the top-right corner of the overlay. Similarly, if you\u0026rsquo;ve opened a previously created asset, you can click the little dot menu next to the SAVE button to duplicate, delete, or view changes to the asset.\n6. Triggers Trigger selection is more robust now. The full table gives you all the information you need at a glance.\n  If you click anywhere on the Trigger row (except the little (I) icon to the right which opens the respective Trigger configuration), it will add the Trigger to the Tag.\nThe magnifying glass icon opens a search field which lets you do a filter search on the Trigger list.\nThe (+) button lets you create a new Trigger. When you click it, the process is very similar to creating a new Tag. You\u0026rsquo;ll see a new overlay with the same types of options you had when creating a new Tag.\n  When you choose the Trigger type, the iconography has been completely revamped.\n  I actually miss the color-coding GTM used to have for Triggers, but the icons are much clearer and easier to interpret.\nConfiguring a Trigger hasn\u0026rsquo;t had any real technical changes, but the user interface has changed for some of them, especially the ones which need a separate enabling condition (e.g. Just Links and Form Submit). Instead of the \u0026ldquo;Enable When\u0026rdquo; and \u0026ldquo;Fire On\u0026rdquo; steps, and actually instead of any steps, you just have a regular, straightforward form:\n  I\u0026rsquo;m not happy that they did away with the step numbers, as they made it easier to reference various parts of the Trigger configuration process. I also still wish they\u0026rsquo;d set the Page URL matches RegEx .* as the default for the \u0026ldquo;Enable When\u0026rdquo; step.\n7. Variables Variables have received a very similar treatment as Triggers and Tags. To start the Variable creation workflow, you can either go via the Variables page of the container (as you used to), or you can just click the little Variable icon wherever you see it.\n  Once you click that icon, you\u0026rsquo;re whisked to the Variable selection screen, which looks a lot like what you got with Triggers.\n  Note that the Built-In Variables are mixed together with the User-Defined Variables. The only way to tell them apart is to look for the (I) icon to the right, which opens the Variable configuration for User-Defined Variables. With Built-In Variables, you can\u0026rsquo;t do that.\nIf you want to add Built-In Variables to the list, you will need to enable them by clicking the little gear icon in the top-right corner. This opens a new overlay, where you can check the Built-In Variables you want included in the container.\n  Now, when you choose to create a new Variable, you get a list of the types you can create, with a new iconography (just as with Triggers).\n  There\u0026rsquo;s nothing new in this list, though I do like the fact that you see a short description after most of the Variable types.\nConfiguring a Variable is pretty much the same as it used to be, so I won\u0026rsquo;t dwell on that here.\nSo you are now able to complete the full workflow from the first time you create a Tag all the way to its Triggers and Variables without having to change your location in the container. Yes, the horizontal overlays might come as a shock to many, and it will take time to get used to the Material Design layout, but my personal opinion is that the changes are very welcome indeed. But that was predictable.\n8. Publish When you click the Publish button, it\u0026rsquo;s again, surprise surprise, delegated to an overlay.\n  The Version Configuration is more prominent than it used to be. Now you are recommended to give a proper name and description to the version. This has, in my mind, always been vital when creating versions in Google Tag Manager. You really want to make the version name as descriptive as possible.\nYou also have the option of publishing only to a specific Environment, as before.\nIn Workspace Changes, you can review the changes that have been done to the current Workspace. By clicking the menu at the end of each row of changes, you can choose to Abandon or View Changes one last time.\nFinally, the Activity History gives you a run-down of who did what in this current Workspace.\nBy clicking the big blue PUBLISH button you will consolidate the changes, create a new version, and make it the live version of the container, just as before.\nIf instead of Publish you chose Create Version, the screen will look almost the same, except you won\u0026rsquo;t have the Environment option, and instead of publishing the version as the Live version, you will only update the Latest Container Version.\n  Summary These were, in my mind, the biggest and most obvious changes to the new User Interface.\nAs I noted a number of times above, I\u0026rsquo;m certain this new UI will come as a shock to many. There\u0026rsquo;s always resilience to change, and even if the old UI was quite clunky, there was something consistent about the way you manoeuvred between different parts of the container.\nBut that\u0026rsquo;s all gone now. Instead, you have horizontal flyouts and overlays that keep in you in one location, allowing you to configure an entire workflow without ever leaving the asset you started from. This, I think, is very powerful and will definitely speed things up once you get the hang of the new UI.\nWorkspaces is very much present in the UI. You will be able to Abandon and View Changes at almost every turn, and the significance of creating a new version has increased. This, again, might lead to confusion in the early days, as the simple act of Publishing a container has implications to all the currently active Workspaces.\nThus, I want to repeat what I said at the end of my Workspaces guide:\nCommunication and good governance will always rule over any feature update.\nYou\u0026rsquo;ll still need a stable and healthy organization if you want to make the most out of Google Tag Manager.\nGood luck!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-workspaces/",
	"title": "Google Tag Manager Workspaces",
	"tags": ["Google Tag Manager", "user interface", "workspaces"],
	"description": "Guide to Google Tag Manager workspaces.",
	"content": "Well, well, well. Welcome to the Enterprise Game, Google Tag Manager! You know, if you took a look at all the feature requests and complaints that pass through the Google+ community or the Product Forums, you\u0026rsquo;d notice that a large portion of them revolve around lack of multi-user and multi-team support in the tool. Well GTM has taken a gigantic leap forward to soothe these concerns, with the release of its latest feature: WORKSPACES.\n  Now, not only did they roll out this new feature, but they completely revamped the Tag, Trigger and Variable creation and management process. This is so overwhelming that I\u0026rsquo;m not even going to complain how much work I have ahead of me, being forced to update all my current and future articles to align with the new interface. Anyway, I didn\u0026rsquo;t want to clog this post up with all the changes, so I wrote a short overview of the UI changes to GTM, where I walk you through some of the most meaningful changes to the user interface.\nGosh, I don\u0026rsquo;t even have a clue what Workspaces will do to my GTM Tools. Probably some sort of cosmic implosion. But it doesn\u0026rsquo;t matter. The new stuff is just so great that I can whine later.\nLet this article serve as an introduction and a guide to how Workspaces function. I\u0026rsquo;m sure there will be many other guides online soon, which is great as always, as I predict this to be a defining moment in Google Tag Manager\u0026rsquo;s release history.\nThere\u0026rsquo;s lots of things here, so let\u0026rsquo;s just jump in.\n1. What are Workspaces? To help wrap your mind around the new feature, think of GTM of the past having just one Workspace. Everybody working in GTM had to work on the same Container Draft, and when a Version was created and/or published, all changes were added to the Version at that moment, regardless of who worked on them or when.\nWith Workspaces, you\u0026rsquo;ll be working with multiple Container Drafts. In essence, when a Workspace is created, a new Container Draft is separated from the latest GTM container version, and this becomes your new Workspace. From that moment on, it will live a separate life as a draft, and you can edit it, Preview it, and Debug it to your heart\u0026rsquo;s content without interference from other Workspaces. There will still remain just one main branch of Versions, so you won\u0026rsquo;t be able to publish your Workspaces into a completely separate branch, only merging them at some distant point in the future (if even then).\nWork can be done in multiple Workspaces at the same time, as only once a Workspace is turned into a Version does it become part of the Google Tag Manager container. It follows that there\u0026rsquo;s no actual versioning of Workspaces themselves.\nWhen another Workspace is turned into a Version, all other Workspaces will get a notification that the Latest Container Version has changed. Any changes implemented in this new Container Version need to be synchronized with all other Workspaces before those can be turned into new Versions. You don\u0026rsquo;t have to do it immediately, but you will see the notification in the Container Overview reminding you that you need to update the current Workspace with changes in the Latest Container Version.\nPerhaps my amazing flow diagram will help you understand this better:\n    Container Version 1 (CV1) has three Workspaces created out of it: WS1, WS2, and WS3. All are being actively worked on.\n  WS2 is published, so its contents become the new Container Version 2 (CV2).\n  Since WS1 was based on CV1, which is now replaced with CV2, WS1 gets a notification that the Latest Container Version has changed and a sync is required. The same notification is received in WS3.\n  WS1 is ready to publish, so the sync is done, and the workspace is published. This becomes the latest container version CV3.\n  WS3 ignored the sync request for CV2, but now it, too, is ready to publish. Since the latest container version is CV3, this is synced with WS3 (thus bringing in the changes from CV2 as well), and the workspace is published into CV4.\n  Finally, a new workspace is created from CV4, with all the changes from the previous, now published, workspaces.\n  So it\u0026rsquo;s like an abstraction of a typical branching version control system. There\u0026rsquo;s just one version tree, but you can \u0026ldquo;branch\u0026rdquo; drafts out of it, merging changes in the version tree to these branches, and merging branches into new versions of the version tree.\nThe main difference to e.g. GitHub is that a workspace is deleted as soon as it is published or created into a version. Thus, workspaces are ephemeral. They exist solely to deliver an incremental update to the container.\n2. Managing (creating, saving, and deleting) a Workspace Creating a new Workspace is easy. Go to the Overview section of your Container, and click any one of the three options to \u0026ldquo;Manage Workspaces\u0026rdquo; you see in the image below.\n  Once you click one of those, you should enter the \u0026ldquo;Manage Workspaces\u0026rdquo; screen. To create a new Workspace, click the Plus icon in the top right corner to open the Workspace creation prompt.\n  By the way, notice the text saying 1 workspace left? In the free version of Google Tag Manager, you can have three Workspaces active at the same time. This might sound like a limitation, but it actually steers you into a more conservative approach to tag management. I\u0026rsquo;ll talk more about this in the last chapter of the post.\nOnce you click the plus icon, you can give your new Workspace a name and a description. I\u0026rsquo;m not one of those people who try to stick naming conventions down your throat, so I recommend you come up with a good schema all by yourself (or, preferably, within your team).\nEven though these screenshots don\u0026rsquo;t reflect it, I\u0026rsquo;m actually using the following naming convention:\n024 - AdWords Conversion Tracking - Ahava \u0026amp; Behava\nThe number would be the Version number the Workspace was created from, followed by the feature implemented in the Workspace, followed by the team working on the Workspace. A simple naming schema, with lots of information encoded within.\nIt\u0026rsquo;s a good idea to give a description to the Workspace, such as who\u0026rsquo;s working on it, what the goal is, when the expected publishing time is, etc. Once you\u0026rsquo;re done, just click Save in the top right corner.\n  You should see your fancy new Workspace in the list, and the Workspace you just created automatically becomes the draft that you\u0026rsquo;re currently editing.\nJust remember - the new Workspace is created off the Latest GTM Container Version, not the Workspace draft you had active when you created the new Workspace, nor the Published version (if different from the Latest Version)!\nNow, there\u0026rsquo;s no way to actually, explicitly save a Workspace. Since it\u0026rsquo;s a Container Draft, any Tags, Triggers, and Variables you save automatically become part of the draft. There are new, awesome ways to view and abandon any changes you\u0026rsquo;ve made in the current draft, and we\u0026rsquo;ll get to them shortly.\nTo delete a Workspace, you\u0026rsquo;ll need to open the Manage Workspaces overlay, and click the small info button next to the Workspace you want to delete.\n  In the view that appears, you can change the Workspace configuration (name, description), view its changes, and you can click the little menu in the top right corner to Delete the Workspace.\n  3. Switching between Workspaces, and Workspaces in Overview To switch between Workspaces is easy. In the left navigation is a new menu for Workspaces. Whenever you click that, an overlay flies out, which lets you switch between Workspaces simply by clicking the Workspace name in the list that appears.\n  The new menu item in the navigation serves a two-fold purpose: First, it lets you know which Workspace you\u0026rsquo;re currently working on. Second, it lets you quickly open the Workspace management overlay, where you can jump between Workspaces really smoothly.\nNow, head on over to Overview, and lets take a look at what\u0026rsquo;s changed.\n  There are some vital changes to the Overview dashboard, so let\u0026rsquo;s see what we find.\n  The Description field is now the Description you entered when creating the Workspace.\n  The whole central panel is reserved for Workspace-related information. The menu in the top-right corner of the box lets you either Update the Workspace or Manage Workspaces. The changes in this Workspace (compared to the Latest Container Version) are listed at the bottom.\n  If there\u0026rsquo;s a newer Container Version than the one that the Workspace was created from, you\u0026rsquo;ll see an alert here AND at the bottom of the screen, reminding you to update the Workspace.\n  Right next to the PUBLISH menu is another reminder of how many changes there are in the current Workspace when compared to the Latest Container Version.\n  The orange Conflict bar can pop up after you\u0026rsquo;ve updated your Workspace to merge the changes in the Latest Container Version. If, in the current workspace, you\u0026rsquo;ve made modifications to any of the Tags, Trigger, or Variables that were included in the update, you will need to resolve the conflicts either in favor of the Latest Container Version or your current Workspace.\n  A more detailed list of the changes in the current Workspace is here. By opening the menu aligned with each change, you can view what changes were made to each asset, and you can abandon the changes with the click of a button.\n  The usefulness of the Overview screen just multiplied by a zillion. I never used it before, now I find myself returning to it all the time.\n4. Viewing changes OK, get ready for pretty much the sweetest addition to the new user interface. In any of the places you can view changes to the Workspace, find the item you want to analyze and click open the menu next to the item. There are two options: Abandon Changes, which reverts any changes you made to that item back to their original settings (i.e. the values in the Latest Container Version), or View Changes, which lets you analyze all the changes you\u0026rsquo;ve made, and individually choose whether to keep or to abandon them!. Since the latter is more interesting, click \u0026ldquo;View Changes\u0026rdquo;.\n  What you see next can be pretty intimidating. Remember that \u0026ldquo;View Changes\u0026rdquo; is a completely optional operation. You don\u0026rsquo;t have to view any changes, nor do you have to accept any changes at any point when working with a Workspace. Indeed, once you\u0026rsquo;re happy with what you\u0026rsquo;ve done with the Workspace, you can just Create a new Version or Publish the Workspace without ever double-guessing your changes.\nBut if you DO want to check what you\u0026rsquo;ve (or someone else has) done, the overlay that opens is the perfect place to do so. Let\u0026rsquo;s first take a closer look at what we have here.\n  Here are the elements of this overlay:\n  On the left is the column which represents the item in the Latest Container Version. On the right is the column which represents the modified version in the current Workspace. This is, essentially, a diff of the two versions.\n  Up in the right you can see how many changes are yet to be resolved, and a disabled APPLY button, which you can click after you\u0026rsquo;ve resolved all the changes. Basically, any changes that you resolve in favor of the Workspace will remain as changes whenever you open this screen again, and any changes you resolve in favor of the Latest Container Version will be removed from the draft in your Workspace.\n  In Red are items you\u0026rsquo;ve REMOVED from the Latest Container Version. By clicking the arrow, you have two options: keep the change you\u0026rsquo;ve made to the Workspace (the IGNORE icon), or copy the original value from the Latest Container Version back to your Workspace (the COPY icon).\n  In green are NEW ADDITIONS to the current Workspace, which are absent from the Latest Container Version. You can click the arrow again to either abandon or keep the change.\n  In blue are actual MODIFICATIONS to existing properties. In this example, the value for a Custom Dimension has changed. You can use the arrow icon again to validate your change.\n  Anyway, don\u0026rsquo;t be too confused with the View Changes screen. Remember, you don\u0026rsquo;t have to accept or abandon any changes if you don\u0026rsquo;t want to. You can simply use this screen to check what changes have been made, and then close the overlay without applying or changing anything.\nIt\u0026rsquo;s simply an incredibly convenient way of verifying what changes have been made to the container. I love it!\n5. Viewing changes item-by-item Another way to observe changes in items is to open an asset itself (Tag, Variable or Trigger). If any changes have been made in this Workspace, or if it\u0026rsquo;s a newly created item, the UI will inform you of this, and you have the option of Abandoning the Changes, or Viewing the Changes (for modified assets only).\n  See all that other cool stuff in the new UI? Read on!\n6. Creating a Version or publishing a Workspace I want to remind you again: Workspaces are just Container Drafts. Just as before, a Container Draft becomes a Container Version in any of two ways: you either choose \u0026ldquo;Create Version\u0026rdquo; from a relevant menu (most common is the red menu in the top right corner of the container), or you choose \u0026ldquo;Publish\u0026rdquo;, which first creates the version and then publishes it as the new, live container.\n  When you click the Publish (or Create Version) button, there\u0026rsquo;s a new overlay:\n  This is very convenient, as you have the options to:\n  Set Version Name - very useful for making sense of the Versions view\n  Set Description - same comment as above\n  Choose Environment to Publish to\n  Last chance to View / Abandon Workspace changes\n  View the Activity History for the current Workspace\n  Note that if there are changes you need to sync from the Latest Container Version, you\u0026rsquo;ll see this warning in the Publish / Create Version overlay:\n  When you Create a Version or Publish the Workspace, it\u0026rsquo;s exactly as it was before Workspaces. The current draft becomes the Latest Container Version, and it\u0026rsquo;s now visible to all users in the Versions screen.\n  In this list, the Version name will match what you set when publishing the Workspace. You can, of course, rename it from the Actions menu as before. A feature update I\u0026rsquo;d like to see it the original Workspace name as a column of its own, so that I could make better sense of the publishing workflow (in the screenshot, the Version names are the respective Workspace names for clarity).\nRemember that when you create a new version from the Workspace, it automatically becomes the Latest Container Version, which means two things:\n  Any new Workspaces will be automatically created from this Latest Version\n  All other, existing Workspaces will need to sync the changes you made to the Latest Version, before they can create a version out of their Workspace\n  So even though Workspaces give you plenty of wiggle-room in terms of organization and management, it doesn\u0026rsquo;t and shouldn\u0026rsquo;t eliminate the need to periodically discuss your version plans, and it definitely doesn\u0026rsquo;t remove the need for proper governance. So make sure everybody knows when new versions are created, so that they aren\u0026rsquo;t blind-sided by a version update!\n7. Syncing a Workspace with an updated Latest Container Version Let\u0026rsquo;s say you\u0026rsquo;ve decided to start your work for the day, and you fire up GTM. In the Overview screen, you see an alert that an update is required!\n  There\u0026rsquo;s also the bar at the bottom of the page prompting you to update the Workspace, too.\nWhen you select to Update the Workspace (i.e. sync the changes with the Latest Container Version), a new overlay appears:\n  Here you\u0026rsquo;ll see which Versions need to be synced to the Workspace. So there might be multiple versions listed here, if there has been more than one update to the Latest Container Version since you\u0026rsquo;ve created the Workspace.\nRemember that updating the Workspace isn\u0026rsquo;t necessary right away. You can keep on working on your changes, only merging the changes from the Latest Container Version once you\u0026rsquo;re ready to create a version out of your Workspace. Nevertheless, it\u0026rsquo;s a good idea to do periodic merges, since the further away you diverge from the Latest Container Version, the more conflicts will arise once you do decide to go ahead with the merge.\nAnyway, when you select to Update the Workspace due to changes in the Latest Container Version, your Workspace files will be updated to reflect the modifications to the Latest Container Version. In other words, the very foundations of what you\u0026rsquo;ve been working on might have changed. It\u0026rsquo;s a good idea to closely research the Latest Version in the Versions view, so that you\u0026rsquo;ll know if any of the assets you have dependencies with have been changed.\n8. Resolving Conflicts in Workspaces Let\u0026rsquo;s say you did all the steps in the previous chapter, and happily clicked UPDATE to sync your Workspace with the Latest Container Version.\nBut something\u0026rsquo;s wrong, and a big, ominous, orange bar pops up in the screen, telling you that there was a conflict!\nA Conflict arises when a Tag, Trigger, or Variable was changed in the Latest Container Version, and you have also modified that same item in your current Workspace. For you to be able to synchronize your Workspace with the Latest Container Version, you will need to resolve each conflict either in favor of the Latest Container Version or your Workspace. You can see which items are in conflict by looking at the list of Workspace changes in the Overview screen. This is, again, a typical way to deal with branches in version control.\n  By clicking the RESOLVE button in the orange bar, you\u0026rsquo;re taken to the conflict resolution screen. So let\u0026rsquo;s take a closer look at that:\n  Conflict resolution is very similar to the View Changes interface I introduced earlier. Basically, each change is color-coded:\n  Blue for items you\u0026rsquo;ve modified in your Workspace\n  Green for items that you\u0026rsquo;ve added to the Workspace\n  Red for items that you\u0026rsquo;ve removed from your Workspace\n  You can click the arrow by each conflict to choose whether to IGNORE the conflict (resolve in favor of your Workspace) or to COPY the change (resolve in favor of the Latest Container Version). In other words, if you click the COPY icon, the state of the field in the Latest Container Version will overwrite anything you had modified in your Workspace.\n  You can change from item to item in the top left of the screen, where you can see something like \u0026lt; 1 / 4 \u0026gt;, which would signify that there are altogether four assets (Tag, Trigger or Variable) with conflicts, and you\u0026rsquo;re resolving the first one.\nWhen you click SAVE, any conflict resolutions you have made will be saved, and they will no longer appear as conflicts. So make sure you have it right before clicking the button! It\u0026rsquo;s always nasty if your hard work is overwritten due to misunderstanding what change is being overwritten.\nIn the top right corner, the number of Decisions marks the number of conflicts you still need to resolve in the currently open asset. The number goes down as your resolve the conflicts.\nBy clicking Resolve All, you resolve all remaining conflicts in the current asset in favor of your Workspace. You can always click the toggle again to reset your decisions, and you can click each individual conflict to roll back the clock, too. Just remember that clicking SAVE really saves your decisions.\nYes, the interface can be quite intimidating, and conflict resolution can be quite a chore, especially if there are many items. However, this is a vast improvement to the complete blind-groping we used to have with Google Tag Manager. Finally there\u0026rsquo;s transparency to the changes made in a container.\nAgain, remember that you don\u0026rsquo;t have to resolve conflicts or even update your Workspace the minute you see the alert. You can keep on making your changes as you wish. You\u0026rsquo;ll only need to resolve the conflicts before you want to Create a Version or Publish your Workspace. Nevertheless, it\u0026rsquo;s a good idea to get it out of the way as soon as possible, so that the changes don\u0026rsquo;t pile up.\n9. New Workspace-related user permissions If you browse over to Admin / Container / User Management or Admin / Account / User Management, you can see some changes to the Container user permission levels. The new levels are:\n  View - Read-only access to the container\n  Edit - Can create and edit Workspaces, but can\u0026rsquo;t Create Versions or Publish them\n  Approve - Can Create Versions of Workspaces\n  Publish - Can Publish Workspaces\n  Basically, the Approve level is what Edit used to be, and the Edit level is completely new, allowing the user only to edit items in the Workspace, but nothing else.\n  The reason for this extra level of user access is simple: Creating a Version has new significance in Workspaces. It updates the main \u0026ldquo;branch\u0026rdquo; of the Container, prompting all active Workspaces to update with the changes. In other words, Creating a Version from a Workspace with sloppy code, or without being aware of the requirements of other Workspaces, can, at worst, result in major conflicts once the other Workspaces are updated.\n10. Summary I really, really love the feature (big surprise). Especially being able to view changes you\u0026rsquo;ve made on a granular level, and being able to compare between the Latest Container Version and your current Workspace is huge in allowing multiple users and teams to interact in the same Container.\nIndeed, what Workspaces bring to the table will certainly alleviate friction in any organization using GTM. Just remember that you don\u0026rsquo;t have to use Workspaces. In fact, it might be best to restrict them to small, incremental changes only. The reason for this is that they still rely on the Latest Container Version, and the longer you work on a Workspace, the further it can deviate from the main version. Having to periodically sync the Workspace isn\u0026rsquo;t fun, either.\nA good idea would be to work on features small enough that you can Create a Version from the Workspace without having to worry about it creating havoc in other Workspaces. And this is where the restriction of \u0026ldquo;just\u0026rdquo; three Workspaces in free GTM comes into play. Having so few Workspaces available forces you to be extra efficient when modifying the container. I was never a fan of implementations, where you create a zillion new things, and only then Create a Version or Publish the Container. It always resulted in a difficult traceback, if there were issues with the implementation. With only a handful of Workspaces available, and with the conflict resolution step initiated by a Latest Container Version update, you will have to be efficient and aware of what\u0026rsquo;s going on in the whole Container, and not just your own little corner of it.\nA natural extension of this would be user-specific access to Workspaces themselves, but whether this is something that\u0026rsquo;s coming or not is known only by the GTM developer team.\nAnyway, I don\u0026rsquo;t want to dole out any best practices (not my thing), but my suggestion is to be careful with Workspaces, even if they do bring a lot of great stuff to the table. They still won\u0026rsquo;t patch up a sick organization, nor will they miraculously improve your work in large projects with multiple stakeholders all wanting a piece of Google Tag Manager.\nCommunication and good governance will always rule over any feature update. If anything, Workspaces just makes this ever so much more important.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/troubleshooting-cross-domain-tracking-in-google-analytics/",
	"title": "Troubleshooting Cross-Domain Tracking In Google Analytics",
	"tags": ["cross-domain tracking", "google analytics", "Google Tag Manager"],
	"description": "A troubleshooting checklist for Google Analytics cross-domain problems.",
	"content": "Cross-domain tracking, in Google Analytics, is the process of passing information stored in browser cookies from one domain to another. Due to web browsers\u0026rsquo; same-origin policy, a browser cookie is only available to the domain it is written on and all its subdomains (by default). Since Google Analytics uses cookies to persist the Client ID, once the user moves from domain to domain it\u0026rsquo;s important to somehow pass this Client ID, too.\nCross-domain tracking isn\u0026rsquo;t the easiest thing to implement, though. At its easiest, especially if you use Google Tag Manager, it\u0026rsquo;s a question of configuring a couple of fields in your Google Analytics trackers and snippets. However, issues typically arise when your website uses unconventional link redirects, or when you want to persist the Client ID in an iframe loaded from another domain.\n  In this article, I thought I\u0026rsquo;d provide a short checklist to go through when issues with cross-domain tracking arise.\nHow to test if cross-domain tracking is working If cross-domain tracking is working, all traffic from one domain to another should be part of the same session, and thus attributed to the same Source / Medium dimensions.\nYou should not use Google Analytics\u0026rsquo; Real Time Reports to analyze the cross-domain pattern! Real Time Reports show each hit only with the dimensions and parameters carried by the hit itself. This means that you won\u0026rsquo;t be able to debug session-scoped stuff like Source and Medium using only what you see in the Real Time Reports.\nIn other words, you\u0026rsquo;ll need to wait for the hits to populate in the standard reports.\nThe easiest way to verify if cross-domain tracking is working in Google Analytics, is to browse to the first domain using custom UTM parameters, for example:\nwww.domainA.com/?utm_source=xdom_test\u0026amp;utm_medium=xdom_test\u0026amp;utm_campaign=xdom_test\nThen, while still browsing the first domain, click a link or open a page with the iframe to the second domain.\nAfter this is done, in the Google Analytics View which shows data from both domains, you can apply a segment to only include your custom campaign traffic:\n  Once the data hits the Google Analytics reports, you should find your single session when applying the segment. After that, go to the Behavior \u0026gt; Site Content \u0026gt; All Pages report. If cross-domain tracking is working properly, you should see both the pageview(s) from the source domain and the pageview(s) from the target domain in the report.\nIf cross-domain tracking isn\u0026rsquo;t working, you\u0026rsquo;ll only see the pageview(s) from the source domain. The following checklist should help in this case.\nThe Checklist For cross-domain tracking to work on your website and Google Analytics Property, the following things need to all be in place.\n  All domains included in cross-domain tracking must collect data to the same Google Analytics Property - Jump to details\n  All domains that are the source of cross-domain traffic, i.e. the traffic departs from these domains, need to be in the Referral Exclusion List of the Google Analytics Property settings - Jump to details\n  When entering the target domain via a link in the source domain or an iframe, the URL of the page loaded in the web browser must have the _ga=1.234567.234567.234567 URL query parameter in place - Jump to details\n  Any Google Analytics trackers or tags firing on the target domain need to have the allowLinker field set to true - Jump to details\n  These are the four basic steps you need to make cross-domain tracking work on your site. Note that steps (3) and (4) have complicated workarounds for when query parameters or the linker plugin won\u0026rsquo;t work, but in the majority of cases these four steps are enough.\nIn the following chapters, I\u0026rsquo;ll examine each step in more detail.\n1. Collect all data to the same Google Analytics Property A Property in Google Analytics is a tracking configuration that collects data from your digital applications such as your website. Each Property in Google Analytics has a unique identifier, known as the Tracking ID:\n  Every Property has its own table of data, which comprises all the hits that are sent to that particular Property. Thus, each Property has its own users, sessions, and aggregation buckets, and these data sets are unique to each Property.\nBecause of this, it\u0026rsquo;s important that when you collect cross-domain traffic across two separate domains, both domains will need to collect data to the same Google Analytics Property. There is no such thing as cross-domain traffic across Google Analytics properties, and the only way to even approach something like that would be to utilize the Roll-up Reporting feature of Google Analytics Premium.\n2. Domains in the Referral Exclusion List Universal Analytics starts a new session whenever a new referral is detected as a traffic source. Thus, if you first enter a site via Google\u0026rsquo;s organic search (google / organic), and then follow a cross-domain link from the source domain to the target domain, the target domain hit would be recorded as having originated from sourceDomain.com / referral, marking the start of a new session. And this is even if you have cross-domain tracking otherwise in place!\nBy utilizing the Referral Exclusion List, you\u0026rsquo;re telling Google Analytics to disregard referral traffic from the source domain, and to treat it as Direct traffic instead. Universal Analytics relies on Direct traffic for campaign attribution as well as session stitching. Each hit in the session, after the initial acquisition, is actually a \u0026ldquo;Direct\u0026rdquo; hit, and this is how Google Analytics knows that the session should still be kept alive.\n  So, Referral Exclusion List keeps the traffic from the source domain to the target domain part of the same session, which is crucial for you to make sense of cross-domain traffic. Thus make sure that you have all the possible source domains, i.e. domains that send traffic to other domains, in the Referral Exclusion List of your Google Analytics property settings.\n3. Linker parameter in the URL As I said in the very beginning of this article, Google Analytics relies on the Client ID to assign hits to specific sessions and users. The Client ID is stored in a browser cookie named _ga, which is, by default, written on the highest possible domain name the website has access to. On my website, for example, the _ga cookie would be written on simoahava.com, and thus it is available to simoahava.com and all its possible subdomains.\nWhen you move from domain to domain, this Client ID needs to somehow travel with the user, but due to the restrictions of the web browsers\u0026rsquo; same-origin policy, the target domain cannot simply fetch the cookie written on the source domain.\nFor this reason, Google Analytics has introduced the linker plugin. When you invoke the plugin, it returns a URL query parameter which includes the Client ID as well as a signature which is valid for 2 minutes. So, if you visit a different domain URL with the query parameter within the two minute window, cross-domain traffic could be setup between the two domains.\nThe two minute window exists to prevent linker parameters from persisting in shared links and browser history entries. Otherwise every time someone would follow a link with the linker parameter in place, they would be considered the original user who created the link. This would lead into a horrible mess, as it would be almost impossible to distinguish users from each other.\nA typical way of loading the linker plugin is by using the autoLink feature of the plugin. When you use autoLink, you provide it with domain names that you want to automatically decorate with the linker parameters. Then when the user clicks a link or invokes a form redirection that has the given domain name as its target, autoLink automatically decorates the URL with the linker parameters.\nIn Google Tag Manager, you\u0026rsquo;d edit the Auto Link Domains field:\n  You\u0026rsquo;ll know its working when you see the _ga=1.234567.234567.234567 query parameter in the URL.\n  If you don\u0026rsquo;t see the parameter in the URL, it means that for some reason the autoLink plugin failed. Instead, you\u0026rsquo;ll need to manually decorate the URLs. This is particularly the case when working with iframes, as the iframe must be loaded with the linker parameters in its src attribute, if you want cross-domain traffic to work between the parent page and the framed document.\nTo manually decorate the URLs, you or your web developer needs to write a piece of code which takes the linker parameter and appends it to the URL of the link or the iframe just before the document is loaded.\nSo, remember that for a basic cross-domain setup to work, the URL of the target page, whether opened by a link, redirected by a form, or loaded in an iframe, needs to have the linker parameter _ga=1.234567.234567.234567 in the URL.\n4. allowLinker in the target domain Now you\u0026rsquo;ve got the URL query parameters in place, the Referral Exclusion List has all the necessary source domains, and you\u0026rsquo;re collecting data from both the source and target domain to the same Google Analytics Property.\nJust one thing missing.\nFor the trackers in the target domain to respect the _ga=1.234567.234567.234567 linker parameter in the URL, you need to tell the trackers to allow the linker parameter to reset the Client ID on the target domain.\nYou do this by configuring the allowLinker field in the tracker object. In GTM, you\u0026rsquo;d simply add a new field:\n  This setting tells the tracker to use the Client ID embedded in the linker parameter rather than the one created by the tracker on the target domain.\nYou can verify that it\u0026rsquo;s working by opening the Network Tab in your web browser\u0026rsquo;s developer tools, and loading a page both on the source domain as well as on the target domain after following a cross-domain link, for example. All requests to /collect should have the parameter \u0026amp;cid; use the same value. This tells you that the Client ID is the same across the domains.\n  Summary There are many ways in which cross-domain tracking can malfunction, but there are actually only four moving parts to it:\n  All domains need to collect data to the same Google Analytics Property\n  All source domains need to be listed in the Referral Exclusion List of the Google Analytics Property\n  The target domain URLs opened via the source domain need to be decorated with linker parameters\n  The target domain Google Analytics trackers need to have the allowLinker field configured\n  Once all four of these pass inspection, cross-domain tracking should work without a hitch.\n"
},
{
	"uri": "https://www.simoahava.com/tags/local/",
	"title": "local",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/run-google-tag-manager-google-analytics-local-files/",
	"title": "Run Google Tag Manager And Google Analytics In Local Files",
	"tags": ["google analytics", "Google Tag Manager", "local", "localstorage"],
	"description": "Use this guide to configure Google Analytics and Google Tag Manager to run on your local files, i.e. files that have not been published to the world wide web.",
	"content": "Last updated 2 March 2018.\nEvery now and then you might be urged to run Google Tag Manager and/or Google Analytics locally, meaning without the benefit of a web server serving your files. In other words, you\u0026rsquo;re loading an HTML file from your computer in the web browser. You can identify a locally run file by the file:/// protocol in the address bar.\n  Now, deploying Google Tag Manager onto that file with the hopes of running Google Analytics requests locally isn\u0026rsquo;t quite simple. Well, actually, the deployment is fairly simple, but customizing it so that it actually sends useful hits requires some tweaking.\n Note! You will not be able to run Preview mode with local files. GTM automatically uses relative protocol when downloading the preview library, and relative protocol on local files falls back to file:/// for the HTTP requests. If someone comes up with an elegant workaround, please let me know in the comments!\n 1. Modify the GTM Container Snippet First of all, you need to modify the Google Tag Manager container snippet itself. It uses relative protocol in the script loader URLs, and since local files use the file URI scheme, you need to explicitly tell Google Tag Manager from where to fetch the library.\nSo, if this is the container snippet:\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;iframe src=\u0026#34;//www.googletagmanager.com/ns.html?id=GTM-XXXXX\u0026#34; height=\u0026#34;0\u0026#34; width=\u0026#34;0\u0026#34; style=\u0026#34;display:none;visibility:hidden\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-XXXXX\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; You can see how the two URLs embedded within (//www.googletagmanager.com/ns.html... in the iframe and //www.googletagmanager.com/gtm.js... in the script loader itself) do not have an explicit protocol set. So, you need to change these two strings to become https://www.googletagmanager.com/ns.html... and https://www.googletagmanager.com/gtm.js..., respectively. That is the only change you need to make to load Google Tag Manager.\nSo, well done!\n2. Configure the Google Analytics Tag(s) The next step is to configure the Google Analytics Tags. GA places some restrictions on the web browser if loaded with its default settings:\n  The host making the requests to GA needs to have either http or https as the protocol.\n  The host must support storing the Client ID in browser cookies.\n  The host must be able to pass a proper Document Location value (the URL) to Google Analytics.\n  Each one of these three is violated by local files. First, local files use the file:/// protocol, as stated earlier. Second, web browsers disable cookies when browsing local files. Third, the URL sent by the client to Google Analytics in the Document Location field is the one with the file URI scheme, again, meaning it\u0026rsquo;s not parsed correctly by GA into a proper page path.\nLuckily, Google Analytics has fields that you can set to make it pass all these three checks.\nFirst, you\u0026rsquo;ll need to create a helper Variable. It\u0026rsquo;s a Custom JavaScript Variable with the name Empty function and the following code within:\nfunction() { return function() {} }  Next, in your Google Analytics Tags, browse down to Fields to Set, and add the following fields and values:\nField name: checkProtocolTask\nField value: {{Empty function}}\nField name: storage\nField value: none\nField name: page\nField value: {{Page Path}}\nThe first field tells GA not to check for valid protocol (http or https) when making the request to /collect.\nThe second field tells GA not to use browser cookies for persisting the Client ID.\nThe third field tells GA to override the faulty Document Location with the page\u0026rsquo;s pathname.\n  And that\u0026rsquo;s it! With these steps, you can track your Pageviews and Events in your local files, if you are so inclined.\n3. Persist Client ID However, the downside of setting storage : none is that you won\u0026rsquo;t have a persistent Client ID anymore. Thus, every single page load will reset the Client ID, resulting in a new User and new Session with every single page.\nWe don\u0026rsquo;t want that.\nInstead, we can hack around that using the browser\u0026rsquo;s localStorage API to store the Client ID and fetch it with each request.\nYou\u0026rsquo;ll need two new Variables.\nThe first one is a Custom JavaScript Variable with the name JS - Set _clientId, and the following code within:\nfunction() { return function() { if (window.Storage) { window.localStorage.setItem(\u0026#39;_clientId\u0026#39;, ga.getAll()[0].get(\u0026#39;clientId\u0026#39;)); } } }  The second is a Custom JavaScript Variable with the name JS - Get _clientId, and the following code within:\nfunction() { if (window.Storage) { return window.localStorage.getItem(\u0026#39;_clientId\u0026#39;) || undefined; } return; }  The first Variable stores the Client ID in the browser\u0026rsquo;s localStorage, and the second Variable fetches it from the same place (or returns undefined if Client ID isn\u0026rsquo;t stored).\nFinally, in your Google Analytics Tags, go to Fields to Set again, and set the following two fields:\nField name: hitCallback\nField value: {{JS - Set _clientId}}\nField name: clientId\nField value: {{JS - Get _clientId}}\nAnd there you go! Now, the first time a Google Analytics Tag fires, it stores the Client ID created in the process into the browser\u0026rsquo;s localStorage. For each subsequent Tag, until localStorage is manually purged, the Tags fetch the stored Client ID from localStorage and send it with the requests. This way sessions and users stay intact, and you\u0026rsquo;ll be able to derive proper insights from the data.\nSummary This hack probably has only marginal use, but it\u0026rsquo;s still a valid way of tracking Google Analytics in local files. You don\u0026rsquo;t always have the benefit of (or the skills to deploy) a web server, so running files locally is, if nothing else, a hat tip to the Notepad + FTP content management systems from the 1990s.\nAlso, you can use the Client ID hack in your regular trackers, too, if you have some reason to avoid browser cookies for persisting data.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/use-eventtimeout-eventcallback/",
	"title": "#GTMTips: Use eventTimeout With eventCallback",
	"tags": ["datalayer", "eventcallback", "google analytics", "Google Tag Manager", "gtmtips"],
	"description": "Introducing the eventCallback and eventTimeout keys in dataLayer when using Google Tag Manager.",
	"content": "There are times when I\u0026rsquo;m disappointed with Google\u0026rsquo;s developer documentation, especially for Google Tag Manager. Most of the time they get it right, and I\u0026rsquo;d say around 80% of questions being thrown around the forums can be answered just by reading through the documentation. But there are some cases where the documentation is misleading or even downright dangerous. One of these cases is Enhanced Ecommerce.\nThis isn\u0026rsquo;t going to be a thorough critique of said documentation, but the tip in this post has to do with one of the examples that the documentation gives for measuring Product Clicks. In the comment it says:\n...This function uses the eventCallback dataLayer variable to handle navigation after the ecommerce data has been sent to Google Analytics... Well, that\u0026rsquo;s an excellent use case for eventCallback. After all, it\u0026rsquo;s sole purpose is to execute code after all Tags have completed for the given event in the dataLayer.push payload.\nHowever, the pain point here is that you\u0026rsquo;re using Google Tag Manager to defer a crucial, potentially UX- and business-hurting action: the link redirect. The point of the example is that the link redirect is cancelled, and only once the eventCallback is invoked will the redirect continue.\nThat, my friends, is not a good pattern.\nIf any of the Tags that fire on this payload fail, timeout, or in some other way fail to inform GTM that they have completed, the redirect will never happen, and the user is left confused since the link they\u0026rsquo;re trying to click doesn\u0026rsquo;t work.\nIMPORTANT UPDATE, 5 Jan 2018: Firefox\u0026rsquo;s latest update to its Tracking Protection in Private Browsing now blocks Google Tag Manager, too. So even setting eventTimeout won\u0026rsquo;t help. My suggestion is to not cancel clicks outside Google Tag Manager with the intention of doing the redirect via some Google Tag Manager -related method, such as eventCallback. (Update over).\nThis is especially important in the recent versions of the Firefox web browser! Firefox introduced something called Tracking Protection in Private Browsing a while ago, and one of its features is that it blocks Google Analytics from loading on the page. However, it does not block Google Tag Manager. In other words, Google Tag Manager loads, the dataLayer.push() is processed and everything, but if there\u0026rsquo;s even a single Google Analytics Tag with a dependency on the push, the eventCallback function is never called.\nTo fix this, here\u0026rsquo;s a tip:\nTip 48: Always add eventTimeout when you use eventCallback   Always add the eventTimeout when using eventCallback. The former takes a numerical value as its parameter, representing the number of milliseconds to wait before calling eventCallback anyway. In other words, even if your Tags stall and never signal completion, after two seconds eventCallback is invoked.\nSo, let\u0026rsquo;s imagine you have this, problematic code:\nvar processLinkClick = function(e) { e.preventDefault(); var targetUrl = e.target.href; window.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;processLink\u0026#39;, \u0026#39;eventCallback\u0026#39; : function() { window.location = targetUrl } }); };  It\u0026rsquo;s a function which grabs the event object, prevents its default action (redirect), does the dataLayer.push(), and finally in the eventCallback finalizes the redirect. But there\u0026rsquo;s no safeguard a) for when Google Tag Manager isn\u0026rsquo;t loaded, and b) if the Tags stall.\nSo here\u0026rsquo;s the fixed code:\nvar processLinkClick = function(e) { var targetUrl; if (window[\u0026#39;google_tag_manager\u0026#39;]) { e.preventDefault(); targetUrl = e.target.href; window.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;processLink\u0026#39;, \u0026#39;eventCallback\u0026#39; : function() { window.location = targetUrl }, \u0026#39;eventTimeout\u0026#39; : 2000 }); } };  As you can see, we\u0026rsquo;re now also checking for the existence of google_tag_manager, which is the interface created by the gtm.js library when it is executed. So if the Google Tag Manager library isn\u0026rsquo;t loaded, the dataLayer.push() is never executed. Sure, there might be a race condition where the library is still in the process of being loaded, but there are ways to mitigate this, too.\nFinally, the eventTimeout introduces a timeout of two seconds, after which the redirect is done. Thus even if any Tags that fire on the \u0026ldquo;processLink\u0026rdquo; event stall, the redirect is not blocked.\nI hope this tip was useful to you! There are many instances where you really need to be careful that you Google Tag Manager setup isn\u0026rsquo;t killing the usability of your website. This is one of them.\n"
},
{
	"uri": "https://www.simoahava.com/tags/attribution/",
	"title": "attribution",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/attribution-enhanced-ecommerce-reports/",
	"title": "Attribution In Enhanced Ecommerce Reports",
	"tags": ["attribution", "enhanced ecommerce", "google analytics", "Google Tag Manager"],
	"description": "Guide to how attribution works in Google Analytics&#39; Enhanced Ecommerce data model.",
	"content": "Enhanced Ecommerce is a very useful set of reports in Google Analytics. They extend the standard Ecommerce funnel, which measures only purchases, and allow you to observe products from the very first impression, through various interactions, all the way to the purchase and even beyond, if the user wanted a refund. Google has some solid documentation on how to implement and interpret Enhanced Ecommerce, but if there\u0026rsquo;s one area that would deserve more illumination, it\u0026rsquo;s attribution.\n  In this case, I\u0026rsquo;m not talking about attribution in its profit-steering sense. Instead, I\u0026rsquo;m talking about how Enhanced Ecommerce treats the product in its nexus, and what parts of the Enhanced Ecommerce funnel persist and/or retroactively apply to all the other parts. This might sound cryptic, but I\u0026rsquo;ll try to make it clear in the subsequent chapters.\nUPDATE 25 July 2016: I originally completely overlooked the attribution associated with Internal Promotions. I updated this post to reflect this method of attribution, too. Thanks to Iain Duncumb for pointing this out in the comments!\n1. There is minimal attribution Wait, did I just spoil the entire article? No, I just wanted to catch your attention. Indeed, there is almost no attribution in the Enhanced Ecommerce funnel. If you send product details in, say, the Product Detail View step, you will need to send all the same details in the Add To Cart step, if you want to query any part of the payload across the two steps.\nAllow me to illustrate this. Let\u0026rsquo;s say the visitor views a pair of shoes in your online store. Upon loading the page, you send the following Product Detail View:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;ecommerce\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;s12345\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;Ahava Shoes\u0026#39;, \u0026#39;variant\u0026#39; : \u0026#39;Black\u0026#39; }] } } });  The user loves the shoes, so they add them to the cart. However, because you believe in attribution (why shouldn\u0026rsquo;t you?), you decide to cut some corners and only push the following Add To Cart action:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;ecommerce\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;s12345\u0026#39; }] } } });  Now, when querying this in the Google Analytics reports, you might expect to see an Add To Cart count for the product name \u0026ldquo;Ahava Shoes\u0026rdquo;, too, but sadly:\n  Because the primary dimension is Product (corresponds to name in the \u0026lsquo;products\u0026rsquo; object), you see only a Product Detail View for \u0026ldquo;Ahava Shoes\u0026rdquo;, because the \u0026ldquo;Ahava Shoes\u0026rdquo; product name was not included in the Add To Cart action!\nThis is crucial. If you want to query any product or any step of the funnel, you will need to add all the dimensions you want to query against in all the steps of the funnel.\nIn other words, there is no attribution in the Enhanced Ecommerce \u0026ldquo;Shopping Behavior\u0026rdquo; funnel. Sadly, that comprises most of the useful parts of Enhanced Ecommerce. However, I understand the design choice, since the data in Google Analytics is essentially one big table, where each row (i.e. hit) is self-contained. Adding attribution similar to how e.g. Session-Scoped Custom Dimensions work might be hazardous.\nAt the same time, this could be solved by adding more toggles to Enhanced Ecommerce options. Perhaps there could be a switch that would let you \u0026ldquo;freeze\u0026rdquo; payload attributes after sending them in a Product Detail View. So if you send a full payload of keys and values in the Product Detail View, it would be enough to just send the SKU in all the subsequent steps. But I\u0026rsquo;m still not sure how it would work.\n2. Product List and Internal Promotion Attribution However, there is attribution in Enhanced Ecommerce. Two kinds of it, in fact. The first one is Product List Attribution, and Google has recognized it in the support documentation:\nIn Enhanced Ecommerce, the Product List Performance report includes useful Product Attribution data. The report includes a \"last action\" attribute which gives product level credit to the last Product List (i.e. add to cart, checkout, or purchase) that the user interacted with prior to the conversion event. I want to show you how it works in practice.\nFirst, one detail that is missing from the documentation above. Product List Attribution only works against a Product SKU (\u0026lsquo;id\u0026rsquo;). It does NOT work with the Product Name (\u0026lsquo;name\u0026rsquo;). This is a very important distinction, as the developer documentation for Enhanced Ecommerce requires that each product payload have either \u0026lsquo;id\u0026rsquo; or \u0026lsquo;name\u0026rsquo; included, but if you want Product List Attribution to work, the payloads must share the \u0026lsquo;id\u0026rsquo; value.\nHere\u0026rsquo;s how Product List Attribution works in a nutshell:\n  As you can see, the list attribute is only added to the Add To Cart action. Nevertheless, all the subsequent actions against the same Product ID are attributed back to the list. That\u0026rsquo;s why you see 1 Product Checkout and 1 Unique Purchase for the Search Results list, even though that list attribute was not pushed in the checkout or purchase actions.\nActions, here, means the following Enhanced Ecommerce hit types:\n  Product Click\n  Product Detail View\n  Add To / Remove From Cart\n  Product Checkout\n  Product Purchase\n  In other words, if you send the list attribute with any of these payloads, it will persist through all the subsequent Enhanced Ecommerce actions in the same session. However, if you send another list attribute in a subsequent action, the current attribution chain will break, and the new list will receive all the glory for the later actions.\nHere\u0026rsquo;s an illustration with actual data:\n  In the report above, each row describes a different action which was the last one to receive the list attribute. So the addWithList Product List, for example, is a list which was sent with all the Ecommerce payloads up to the Add To Cart action (i.e. \u0026lsquo;impressions\u0026rsquo;, \u0026lsquo;click\u0026rsquo;, and \u0026lsquo;detail\u0026rsquo;), and with no list for the remaining payloads (i.e. \u0026lsquo;checkout\u0026rsquo; and \u0026lsquo;purchase\u0026rsquo;).\nAs you can see, clickWithList, detailWithList, addWithList and checkoutWithList all attribute the full Enhanced Ecommerce journey to the list. With impressionWithList there is no attribution, because Product Impressions is not an Enhanced Ecommerce action.\nThe last two rows show how the attribution breaks with a new list. The list firstAddThenClickWithList_add is a list sent with a specific Product SKU in the Add To Cart payload. Then, the same product SKU is suddenly the target of a Product Click on some other list named firstAddThenClickWithList_click. This breaks the attribution for the first list, which is why you see no data for actions after the Add To Cart. Instead, the user finishes the journey all the way up to the purchase, and all the credit for the actions is attributed to the last list that was interacted with.\nThis isn\u0026rsquo;t a very typical scenario, though, but it might occur if you\u0026rsquo;re promoting products which the user has already added to the cart. So if they see a product listing for a product already in the cart, and they click this listing, it will break the attribution chain which initially added the product to the cart.\nI hope that made sense. The logic is very clear: for each Product SKU, the last Product List included in an Enhanced Ecommerce Action gets all the credit for subsequent Enhanced Ecommerce actions.\nInternal promotions Attribution for internal promotions is a bit different and, if I might add, kinda awkward.\nIf there\u0026rsquo;s an Internal Promotion Click in the session with the transaction, then the last click sent before or with the transaction hit gets the full attribution of the purchase. In other words, if you send a promotion click in the transaction itself, then that promotion click gets the full transaction attribution. If there\u0026rsquo;s no promotion click in the transaction, then the last click sent in the session before the transaction gets the credit. Never will a promotion click after a transaction get credit for the purchase.\nIf there are no promotion clicks in the session before or during the transaction, but there are Internal Promotion Views in the transaction hit itself, then all those views in the transaction hit get full credit for the transaction. Never will a promotion view anywhere else except in the transaction hit itself get credit for the purchase.\n Thanks to Iain Duncumb and Christian Mochow for pointing out inconsistencies in this chapter.\n There are more details about this here.\nSummary That\u0026rsquo;s about the gist of it. The key things to remember are:\n  Outside Product Lists and Internal Promotions, there is not attribution of values across Enhanced Ecommerce payloads.\n  In Product Lists, attribution revolves around Product SKUs and Enhanced Ecommerce Actions.\n  For each Product SKU, the last Product List the user interacted with in an Enhanced Ecommerce Action is the one to which all subsequent actions within the same session are attributed.\n  In Internal Promotions, full credit for the transaction is attributed to the most recent Promotion Click in or before the transaction hit, or lacking that, any Promotion Views sent with the transaction hit itself.\n  In my own experience, especially product lists have received only a little attention in Enhanced Ecommerce, partly because not all online stores have what you would consider a consistent \u0026ldquo;list\u0026rdquo; setup. However, because of this powerful attribution feature, it might make sense to take a closer look at your online store to see if you could use it for something else, instead. Most of Google Analytics\u0026rsquo; power comes from taking a feature intended for A, and using it for B or maybe even C instead.\nThe same observation could be said for Internal Promotions, whose application might be difficult to figure out in a typical webstore. Nevertheless, the very fact that it has something resembling \u0026ldquo;post-view conversions\u0026rdquo; should already invite you to figure out ways of implementing Internal Promotions to your advantage.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtmtips-deploy-gtm-chrome-extension/",
	"title": "#GTMTips: Deploy GTM In Your Chrome Extension",
	"tags": ["chrome extension", "google analytics", "Google Tag Manager", "gtmtips"],
	"description": "Deploy Google Tag Manager in your Chrome extension.",
	"content": "Have you created a Chrome Extension, and now you\u0026rsquo;re dying to find out how users are interacting with it? Perhaps you want to see what features are (not) being utilized, or perhaps you\u0026rsquo;re just interested in knowing if people are actually using it.\nIn this article, I\u0026rsquo;ll show you how to configure Google Tag Manager, so that it works in the restricted sandbox of the Chrome Extension. You\u0026rsquo;ll need to make some tweaks, but it\u0026rsquo;s still perfectly doable.\nThis article was inspired by Mike Pantoliano\u0026rsquo;s question in the Google Tag Manager Google+ community. Thanks Mike!\nTip 47: How to make Google Tag Manager work in a Chrome Extension   This solution requires that your extension have a pop-up page. In other words, the extension needs to have actual HTML that the browser renders when you activate it.\n1. Deploy the Google Tag Manager container The first thing you\u0026rsquo;ll need to do is create a new JavaScript file. You can do it with any text editor. Within the file, add the following code:\n(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;https://www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-XXXXX\u0026#39;);  You probably recognize this as the JavaScript part of the regular Google Tag Manager container snippet.\nWe\u0026rsquo;re using a separate JavaScript file to load the container library, because the use of inline JavaScript is strongly discouraged in Chrome Extensions (well, almost universally in fact).\nThere are two things you\u0026rsquo;ll need to modify in this script.\nFirst, you need to explicitly load the gtm.js library using the HTTPS protocol. You can see this change in the line that has j.src='https://www.googletagmanager...'. So, the first thing to do is make sure you explicitly type https: in its rightful place, instead of the default relative protocol the snippet uses.\nThe second thing you need to do is change the GTM-XXXXX to match the container you will be using. Regular GTM stuff.\nOnce you\u0026rsquo;re done, save the file as gtm.js and store it in the extension folder.\n2. Load the file in your pop-up HTML Next, open the HTML file where your pop-up is loaded. Add the gtm.js file you just created as a reference, and place it in \u0026lt;head\u0026gt;. Why? Because it doesn\u0026rsquo;t make sense to load asynchronous libraries anywhere else but in \u0026lt;head\u0026gt; especially since you don\u0026rsquo;t need to worry about stuff like Search Console verification.\n\u0026lt;head\u0026gt; ... \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;popup.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;gtm.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;noscript\u0026gt; \u0026lt;iframe src=\u0026#34;//www.googletagmanager.com/ns.html?id=GTM-W92WQQ\u0026#34; height=\u0026#34;0\u0026#34; width=\u0026#34;0\u0026#34; style=\u0026#34;display:none;visibility:hidden\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/noscript\u0026gt; ... If you really want, you can keep the \u0026lt;noscript\u0026gt; block which loads GTM for JavaScript-less users, but again I\u0026rsquo;m struggling to figure out a use case where you\u0026rsquo;d want to fire some pixels when people are using your Chrome Extension.\nThis change in the markup simply executes the JavaScript in the file gtm.js, which means that when this HTML is rendered by the browser, the Google Tag Manager library is also loaded in the extension.\n3. Modify manifest.json Next, you need to add a Content Security Policy to your extension\u0026rsquo;s manifest.json file. Basically, you\u0026rsquo;re telling the extension that requests to certain endpoints are OK, and the extension doesn\u0026rsquo;t need to panic when those requests take place. A Chrome Extension has the potential to do all sorts of vile things, so this level of additional security is definitely warranted.\nIn the CSP, you\u0026rsquo;ll need to tell Chrome that requests to https://www.google-analytics.com and https://www.googletagmanager.com must be allowed. If you\u0026rsquo;re firing any Tags to other endpoints, you need to list their hostnames here, too. Just remember that all communications must happen across HTTPS, or your extension will report an error when you try to upload it to your browser.\n{ \u0026#34;manifest_version\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;content_security_policy\u0026#34;: \u0026#34;script-src \u0026#39;self\u0026#39; https://www.google-analytics.com https://www.googletagmanager.com; object-src \u0026#39;self\u0026#39;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;...\u0026#34;, ... }  4. Modify your GTM Tags Finally, you\u0026rsquo;ll need to make some necessary modifications to your Google Tag Manager Tags.\nFor every Google Analytics Tag firing in the container, you need to make the following change.\nAdd checkProtocolTask : false to Fields to Set\nScroll down to Fields to Set, and add a new field:\nField Name: checkProtocolTask\nValue: false\nNormally, Google Analytics requires that the request to GA originate from either HTTP or HTTPS. If the requests originate from anywhere else, the process is cancelled. By setting the task named checkProtocolTask to false, we can prevent this check from happening, since the extension uses the custom chrome-extension:// protocol.\nNext, in your Page View Tags, make the following change.\nAdd a custom page to Fields to Set\nScroll down to Fields to Set, and add a new field:\nField Name: page\nValue: /some-custom-page-path/\nBecause the extension page doesn\u0026rsquo;t have a proper URL (at least in the typical sense), GA can\u0026rsquo;t decrypt the Document Location parameter for the page path. Page path is a required dimension in all Google Analytics requests, so you\u0026rsquo;ll need to manually add it to each Tag that fires.\nThese two modifications need to be in place for tracking to work. For non-Google-Analytics Tags, you might have to make similar (or additional) modifications to enable them in Chrome Extensions.\nAnd that\u0026rsquo;s it!\nSummary I hope this guide was helpful to you! Understanding the full breadth of security that Chrome Extensions implement can be daunting, so perhaps this illuminates some of the peculiarities of working in such a sandboxed environment.\nBy following these steps, you should be able to accumulate data in Google Analytics on how users interact with your extension. Remember, though, that the HTML is only rendered when the pop-up is clicked open, and each time the user clicks the pop-up open again, a new pageview is sent. So you can actually use Pageviews as a pretty reliable proxy for extension open rates!\nOn an only slightly related note, it came to my mind that it\u0026rsquo;s impossible to use a Chrome Extension like Ghostery to block other extensions (due to the sandbox). So even if the user has blocked Google Analytics and Google Tag Manager from their browser, it wouldn\u0026rsquo;t apply to the extension. This is something you might want to make clear in the privacy statement of your Chrome Extension!\n"
},
{
	"uri": "https://www.simoahava.com/tags/chrome-extension/",
	"title": "chrome extension",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/content-dashboard-klipfolio-google-analytics/",
	"title": "Content Dashboard With Klipfolio And Google Analytics",
	"tags": ["dashboard", "google analytics", "klipfolio"],
	"description": "How to build a content dashboard using Klipfolio with Google Analytics data sources.",
	"content": "I have very little against Google Analytics\u0026rsquo; default dashboards. The reason I shy away from them is because they lack the type of customization I\u0026rsquo;ve come to expect from a dashboarding tool. On top of that, they only let you look at GA data, and I learned early on in my career that focusing on just one vertical is one of the cardinal sins you can make as an analyst.\nFurthermore, over the years I\u0026rsquo;ve become more and more disenchanted with GA\u0026rsquo;s clunky user interface, and the less time I spend in it the better for my serotonin levels. In fact, GA\u0026rsquo;s excellent Core Reporting API combined with a platform like Tableau or Klipfolio to manage and manipulate the data stream has become invaluable in my everyday work.\nWell, in this article I want to show you a glimpse of a typical process I follow when creating a dashboard for my clients. My platform of choice is Klipfolio. It\u0026rsquo;s a pretty sweet dashboarding tool, with the added benefit of functioning as a low-level data store, where you can combine data from different data sources (and there are practically limitless integrations in the tool) as the input in your visualizations.\n  I shouldn\u0026rsquo;t even have to say this, as I don\u0026rsquo;t do commercials, but this article is in no way solicited or requested or paid for by Klipfolio. I\u0026rsquo;m just a fan of the tool, and I want to show you how it works.\nIn this guide, I\u0026rsquo;ll walk you through creating a single Klipfolio widget (or Klip) which pulls its data from Google Analytics. I\u0026rsquo;ll let you figure out the rest yourself. The data in the final dashboard (seen above) is pulled from a number of customizations I\u0026rsquo;ve done over the years, mainly:\n  SERP Bounce Timing\n  Content Interaction Time\n  Content As Enhanced Ecommerce\n  I\u0026rsquo;m positive you don\u0026rsquo;t have all these customizations on your website, but perhaps the following steps will be useful to you when creating your own Klips and visualizations.\nHow Klipfolio works Klipfolio revolves around two data structures: the data sources themselves (which store the pulled-in data), and Klips (which are the widgets you add to your dashboards). This is a pretty self-sufficient system, as once you\u0026rsquo;ve created a data source, you can set it to refresh anywhere from never to every 15 minutes to every 24 hours. A refreshed data source shows up as a cool animation on the dashboard itself, so people looking at the tool will concretely see how the data set is being updated.\n  The Klips themselves offer you a handful of ready-made visualizations, with the option of writing your own HTML styling if you really want to customize the output. When you create a Klip, you add one or more data sources to it, so you can actually visualize data across data sets.\n  Creating a Klip is pretty much like working with Excel. You have flat tables of data (the data sources), and you transform and combine them with your typical spreadsheet functions. It\u0026rsquo;s pretty intuitive if you\u0026rsquo;ve ever worked with spreadsheet formulae, but if you haven\u0026rsquo;t, there\u0026rsquo;s definitely going to be a learning curve with creating new Klips.\n  Finally, the data sources themselves. Let me put it this way, you won\u0026rsquo;t run out of options! In addition to having integrations with some of the most popular SaaS platforms out there (e.g. Google Analytics, Dropbox, Facebook, Salesforce), you can integrate with any HTTP REST API, SQL database (with online access) and RSS feed. And if that isn\u0026rsquo;t enough, you can also send the data as email attachments, or upload it as CSV or Excel files, for example. Lots and lots of options!\n  In this article, we\u0026rsquo;ll only use the Google Analytics connector, and the Klip we\u0026rsquo;ll create is fairly simple and intuitive. But trust me, there are almost limitless possibilities for data transformations and visualizations with this tool!\nArticle performance table The Klip we\u0026rsquo;ll create is the easiest one. It\u0026rsquo;s a simple table with performance statistics on my articles over the last 30 days. The three things we\u0026rsquo;ll measure are:\n  Pageviews\n  Average Engagement Time\n  Average Time On Page\n    Pageviews and Average Time On Page are GA\u0026rsquo;s built-in metrics. Average Engagement Time is a customization which measures the time users actually interact (move the mouse, click around, scroll, press keys) on each page. The reason this interests me is because Google Analytics\u0026rsquo; Time On Page metric is next to useless. It measures the difference in time between two pageviews. But this is a completely arbitrary time measurement, as it tells you absolutely nothing about how much time the user ACTUALLY spent on the page. They could have just opened it in a browser tab and left it be. With Average Engagement Time, we get an idea of how much the users are actively viewing the page.\nStep 1: Create the Data source Anyway, the first thing we need to do is create a data source. Since we only have one dimension (page title) and three metrics, we\u0026rsquo;ll manage with a single data source. In Klipfolio, go to Library -\u0026gt; Data Sources, and then click Create a New Data source.\n  Next, choose the Google Analytics connector. You will enter a screen where you need to first authenticate with Google to allow Klipfolio read-only access to your Google Analytics data. So, click Sign in to Google, and follow the prompts to authenticate Klipfolio with the Google account that has access to your data.\n  Once you\u0026rsquo;ve authenticated, you should find yourself in a screen where you can Configure your Google Analytics query. You could just use this interface to build the query, but I\u0026rsquo;ve got a better idea!\nBrowse to the Google Analytics Query Explorer. It\u0026rsquo;s an amazing tool that lets you test and build Google Analytics API queries on your data. Note! You might need to authenticate again, so that this tool, too, can access your GA data.\nSo, what you need to do first is build the query in this tool. You don\u0026rsquo;t have to worry about the Date parameters, so choose any dates which give you data. The query I\u0026rsquo;m building for the current Klip looks like this:\n  I can\u0026rsquo;t choose Average Engagement Time, since it\u0026rsquo;s a calculated metric and the Query Explorer doesn\u0026rsquo;t seem to support them yet.\nOnce the query is built, you can click Run Query and make sure it returns data.\n  So now that we know the query works, you can just scroll all the way to the bottom of the page, and copy the whole URL string within the API Query URI text box.\n  Now, go back to Klipfolio, and paste the copied URL into the Query URI box. You might need to select \u0026ldquo;Advanced\u0026rdquo; in the Mode menu first.\n  You can click Get Data to make sure that it returns the same data set you got when using the Query Explorer.\nBut some things were missing. First of all, we need to modify the date parameters to get only the last 30 days, and we also need to add the calculated metric into the mix.\nTo add dynamic date values, Klipfolio has those as built-in variables. You can read more about them here. For example, to change the \u0026amp;start-date; parameter in the query automatically to 30 days before the query was made, you\u0026rsquo;d use:\n{date.add(-30).format()}\nFor \u0026amp;end-date; we\u0026rsquo;ll just use Google Analytics\u0026rsquo; own shorthand yesterday.\nTo add the Average Engagement calculated metric, you need to get the metric\u0026rsquo;s external name. You\u0026rsquo;ll find this in the Calculated Metric settings of Google Analytics, by clicking open the entry you want to fetch into the query:\n  To add this metric to the query, add it after ga:pageViews, separated by a comma.\nThe final, complete query URL looks like this:\nhttps://www.googleapis.com/analytics/v3/data/ga?ids=ga:XXXXXXXX\u0026amp;metrics=ga:pageViews,ga:calcMetric_AverageEngagement3,ga:avgTimeOnPage \u0026amp;dimensions=ga:pageTitle\u0026amp;start-date={date.add(-30).format()}\u0026amp;end-date=yesterday\u0026amp;sort=-ga:pageViews\u0026amp;include-empty-rows=false\nIn English, the query is:\n\u0026ldquo;In the Google Analytics profile with ID XXXXXXXX, fetch me Page Views and the calculated metric Average Engagement 3, together with Average Time On Page. Query these against the Page Title dimension, and set the start date of the query to 30 days ago. The end date should be yesterday. Sort them by Page Views, in descending order, and do not include rows which do not have any values.\u0026rdquo;\nClick Get Data to make sure the data it returns is valid.\n  If you\u0026rsquo;re satisfied with the results, click the big Continue button in the bottom of the screen.\nFinally, give your new data source a descriptive name and set its refresh rate. Since in this example the \u0026amp;end-date; parameter is yesterday, you don\u0026rsquo;t need to refresh the data source all the time, just once per day should suffice. Nevertheless, I always choose a shorter refresh rate in case the timezones of Klipfolio and Google Analytics don\u0026rsquo;t match.\n  When you\u0026rsquo;re done, click Save, and you\u0026rsquo;ll be transported to the data source view for this new data set.\nNext up, building the Klip!\nStep 2: Create the Klip So now we have the data source, and we\u0026rsquo;re ready to turn it into a widget, also known as a Klip. While still in the data source view, click the big orange button labelled Build a Klip with this Data Source:\n  The first thing you need to choose is the component type. You can combine multiple component types in a single widget, but in this case we\u0026rsquo;re only interested in a Table, so choose that.\nYou\u0026rsquo;re taken to the Klip Editor. The editor lets you turn a data table into a visual representation thereof. You can use formulae to modify the data as it\u0026rsquo;s visualized in the graphs, and you can add data from multiple data sources into a single widget! This is an amazing feature, and turns Klipfolio into a light-weight Business Intelligence reporting tool.\n  It\u0026rsquo;s going to take some time to get acquainted with all the features of the editor, but once you get the hang of it, there\u0026rsquo;s a very simple flow to it. Here\u0026rsquo;s a great tutorial on how to use the editor.\nAnyway, it\u0026rsquo;s easiest to learn by doing. Select the leftmost column of the table by clicking either the column in the Preview screen or the top-most column in the hierarchy view in the left side of the editor.\nThe first thing we need to do is select data for this column. Because we want to align page titles with their respective pageview counts and engagement times, we\u0026rsquo;ll need to add the page titles to this first column. Thus, while having the first column selected, click the header A in the data table to select all the data in this column.\n  When you click the column, the following things happen automatically:\n  The formula bar gets a single entry: A:A, which is spreadsheet language for \u0026ldquo;everything in the A column\u0026rdquo;\n  The first column in the table is automatically filled with data in the same order as it is in the data source\n  So now you\u0026rsquo;ve created your first formula, congratulations! However, there\u0026rsquo;s a slight hiccup. As you can see, the header row (ga:pageTitle) is the first row in the chart. We don\u0026rsquo;t want that! To get rid of it, we need to apply a function to the A:A selection.\nSo, while still having the A:A column selected, click the Wrap Function button in the toolbar.\n  This lets you wrap a function around any part of the formula you have selected. You\u0026rsquo;ll see the function selector appear, and it has quite an impressive list of functions. Many of them should be familiar from spreadsheet tools, but you might want to bookmark Klipfolio\u0026rsquo;s function reference.\nAnyway, to get rid of the first row of data, simply select SLICE from the list. It automatically removes the first row of data from the widget. Note that you can specify start and/or end as parameters, but we don\u0026rsquo;t need to do that as SLICE removes the first row by default.\n  And that\u0026rsquo;s our first data column!\nNext, we\u0026rsquo;ll repeat these exact steps for the column with the header ga:pageViews in the second column of the table. If you do it correctly, you should see the following:\n  And then we\u0026rsquo;ll do the same for the two remaining columns, adding ga:calcMetric_AverageEngagement3 to the third column, and ga:avgTimeOnPage to the fourth and final column. So now you should see this:\n  So now we have our data, but it\u0026rsquo;s not looking good at all! For one thing, the data is unformatted, and this means that the duration metrics (average engagement and average time on page) are pretty indecipherable. So, let\u0026rsquo;s start formatting! Select the first column (the one with the page titles) in the Preview, and click the Properties tab.\n  Let\u0026rsquo;s start adding some bling to the widget. First, type \u0026ldquo;Page Title\u0026rdquo; into the Column Header field. You should see the change in the Preview immediately.\nNext, we want to make sure the Page Title has the largest width in the table, since all the other columns only contain numbers. So click the Fix column to a specific width\u0026hellip; checkbox, and type 50% into the field that appears. Again, you should see the change immediately. If you want to stretch the Preview table to better see how the widget will look on a full-screen dashboard, you can resize the table by dragging from the right side.\n  That\u0026rsquo;s better!\nNext, choose the second column, and go to Properties again. First, set the Column Header to \u0026ldquo;Pageviews\u0026rdquo;. Then, select Number from the Format As menu. This formats the numbers with commas as the thousands separator.\n  Finally, since it\u0026rsquo;s a column of numbers, make sure they\u0026rsquo;re right-aligned. If you don\u0026rsquo;t like this practice, you can of course leave it as it is. Now the table should look like this:\n  Next, choose the third column and click Properties. Set the Column Header to \u0026ldquo;Avg. Engagement Time\u0026rdquo;, and choose Duration from the Format As drop-down. This will automatically convert the numerical value in the data set to minutes and seconds. So 78.135353 in the data set, for example, would become 1m18s after the formatting change. Finally choose Right-alignment again to make sure the alignment matches the previous column.\n  Repeat these properties in the final column, though remember to change the Column Header in the final column to \u0026ldquo;Avg. Time On Page\u0026rdquo;.\nNow the table should look like this:\n  So now all that remains is to give the widget a descriptive name. Click the Untitled Table header to open the Klip Properties view.\nGive it a descriptive name, such as \u0026ldquo;Article performance\u0026rdquo;.\nAnd you\u0026rsquo;re done! You\u0026rsquo;ve just created your first Klip! If you\u0026rsquo;re satisfied with what you\u0026rsquo;re looking at, click Save. You can give the Klip a new name here, though it defaults to the Klip title you just edited. This name is what the Klip will have in your library. The dashboard will show the title you chose in the editor.\nWhen you\u0026rsquo;re done, you\u0026rsquo;ll be taken to the View Klip screen. Here, you can click the orange button labelled Add to Dashboard to add the widget to a dashboard. Here\u0026rsquo;s what a sample placement might look like:\n  Looking good, if I may say so myself! Check out the Dashboard Tour on the Klipfolio website for more information on the dashboard view.\nSummary All the steps above might seem quite complex, and I\u0026rsquo;m not trying to cheat you into thinking otherwise. Klipfolio isn\u0026rsquo;t a one-click, turnkey, simple, easy or fast solution for dashboarding. Nope, it replaces all that vacuous marketing jargon with one word: efficiency. It has an impressive feature set for its price point, and most importantly it lets you transform the incoming data stream.\nOn top of that, you can create your own intermediary data stores, join data across data sets, poll an impressive array of data sources, and get pretty much any data you wish into your dashboards.\nKlipfolio\u0026rsquo;s main shortcomings at this point are its multi-user functionalities. A single user only has one \u0026ldquo;My Dashboards\u0026rdquo; view, so you can\u0026rsquo;t create a separate dashboard view for each of your projects, for example. You can add multiple dashboards to the single view, and Klipfolio will very smoothly transition between the dashboards at the time interval of your choice (if you click the Play button). But if you want one project to only see dashboard set X, and give dashboard set Y to another project, you\u0026rsquo;ll need to have multiple Klipfolio Users in the same project.\nAnother thing that sucks is that the Library views have no folders or any other hierarchical representation. This is why I have to use a convoluted naming convention for each set of related data sources.\nKlipfolio have updated their pricing models with affordable options for agencies and users with multiple dashboard needs, so I\u0026rsquo;m definitely going to take a look at those soon.\nKlipfolio has a learning curve, as do all good things in analytics and life. If you\u0026rsquo;ve followed my writings over the years, you\u0026rsquo;ll know how disenchanted I am with the marketing messages that try to make everything \u0026ldquo;simple\u0026rdquo; and \u0026ldquo;just drag and drop\u0026rdquo; (yes, Google, I\u0026rsquo;m looking at you) without exposing the actual customization power of the underlying tech. I\u0026rsquo;m sure Klipfolio will at some point focus on explosive growth by adopting the same marketing tact, but for now it\u0026rsquo;s refreshing to know that there\u0026rsquo;s such a powerful platform at its price point.\n"
},
{
	"uri": "https://www.simoahava.com/tags/dashboard/",
	"title": "dashboard",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/klipfolio/",
	"title": "klipfolio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/measure-google-tag-manager-event-duration/",
	"title": "Measure Google Tag Manager Event Duration",
	"tags": ["datalayer", "Google Tag Manager", "performance", "user timings"],
	"description": "How to measure how long it takes to complete each event you push into Google Tag Manager&#39;s dataLayer.",
	"content": "Google Tag Manager is a great tool. Yeah, you came all the way to this article to read that truism. It also performs really well, loading at a sweet, swift pace even on a slow connection, thanks to pretty decent response times from Google servers. On top of that, the library itself loads asynchronously, meaning the container download doesn\u0026rsquo;t interrupt the browser as it tries to make sense of your messy HTML.\n  However, after the container has downloaded, the rest is up to you. The Triggers you attach to Tags, the Variables you resolve in both Tags and Triggers, and all sorts of things like Tag Firing Priority can potentially bog down your site simply because they all represent complex JavaScript operations. Just because code is executed in the Google Tag Manager container doesn\u0026rsquo;t guarantee fast performance. The browser still needs to execute it line-by-line, using asynchronous requests where possible (such as when a call is made to the Google Analytics endpoint), but also relying on quite expensive operations such as DOM insertion.\nAnyway, being the experimenter that I aspire to be (how\u0026rsquo;s that for a circular statement), I wanted to measure just how much time it takes for GTM events to resolve. Here\u0026rsquo;s the result:\n  Interesting stuff! Out of the largest timing samples, it\u0026rsquo;s the most basic of events, gtm.js, which takes the longest time to complete! 2.5 seconds on average, phew! Here are the Tags on my site which fire on gtm.js:\n  There\u0026rsquo;s a tracker setup Tag, a little Konami code surprise, as well as some code to measure the SERP bounce time on my site. Each one of them has some dataLayer.push() methods executed, and the tracker setup does load the analytics.js library, so it\u0026rsquo;s not like it\u0026rsquo;s lightweight stuff. But still, 2.5 seconds? That\u0026rsquo;s a lifetime in Internet time!\nAnyway, this measurement has been quite eye-opening. It\u0026rsquo;s a good idea to be aware of what happens behind the scenes. Even asynchronous operations can be harmful to page performance, especially if there are many of them running one after the another. The reason for this is that the load event for the window object is delayed until the page and all its related resources (e.g. scripts and images) have completely loaded. This might lead to problems with frameworks like React, where many operations are executed only after the window.onload event has been dispatched in the browser.\nLet me quickly walk you through how I setup this measurement. It\u0026rsquo;s not a walk in the park, and if you want to try it yourself I urge you to test it in a staging environment first!\nLet\u0026rsquo;s hack the Data Layer To make it work, we need to hack the Data Layer.\nQueue ominous music.\nThat\u0026rsquo;s right, we need to do what Google Tag Manager has already done and hijack the dataLayer.push method to overwrite it with our own.\nSo, the following code needs to end up on your page template before the Google Tag Manager container snippet:\n\u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; var oldPush = window.dataLayer.push; window.dataLayer.push = function() { var e = [].slice.call(arguments, 0); if (e[0].event \u0026amp;\u0026amp; e[0].event !== \u0026#34;perfTime\u0026#34; \u0026amp;\u0026amp; [\u0026#34;gtm.linkClick\u0026#34;,\u0026#34;gtm.formSubmit\u0026#34;,\u0026#34;gtm.timer\u0026#34;].indexOf(e[0].event) === -1 \u0026amp;\u0026amp; document.readyState !== \u0026#34;complete\u0026#34;) { var t = (new Date).getTime(), a = e[0].event; e[0].eventStartTime = t, e[0].eventCallback = function(c){ window.dataLayer.push( { event : \u0026#34;perfTime\u0026#34;, \u0026#34;perfEvent.event\u0026#34; : a, \u0026#34;perfEvent.totalTime\u0026#34; : (new Date).getTime() - t, \u0026#34;perfEvent.containerId\u0026#34; : c } ) } } oldPush.apply(window.dataLayer, e); } \u0026lt;/script\u0026gt; Wow. That\u0026rsquo;s a beast. I\u0026rsquo;m not going to walk you through it line-by-line, but here\u0026rsquo;s how it works:\n  When a dataLayer.push() happens, this code is first executed.\n  It takes the object that was pushed, and if it contains an 'event' key and if it matches some other conditions, the following code is then run:\n  A timer is started (the time when the event execution began) and stored in the key eventStartTime\n  The [eventCallback](/gtm-tips/hitcallback-eventcallback/) of this object is set to a function, which basically pushes another dataLayer object, where a new timer calculates the difference between the event execution start and end times\n  Finally, whatever was processed by this script (whether it be the modified object or the new \u0026ldquo;perfTime\u0026rdquo; object) is returned to the original dataLayer.push method, so that GTM can pick it up\n    Ah, that IS a beast. Perhaps it\u0026rsquo;s easier if I show you an example. When this code is running (i.e. before the page has completely loaded), a simple dataLayer.push() like this:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;testing\u0026#39;, \u0026#39;someVar\u0026#39; : \u0026#39;someVal\u0026#39; });  \u0026hellip;is automatically turned into:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;testing\u0026#39;, \u0026#39;someVar\u0026#39; : \u0026#39;someVal\u0026#39;, \u0026#39;eventStartTime\u0026#39; : 1464890512708, \u0026#39;eventCallback\u0026#39; : function(e) {...} });  And when all Tags which fire on 'event' : 'testing' have completed their execution, the following is, again, automatically pushed into dataLayer:\nwindow.dataLayer.push({ event : \u0026#39;perfTime\u0026#39;, \u0026#39;perfEvent.event\u0026#39; : \u0026#39;testing\u0026#39;, \u0026#39;perfEvent.totalTime\u0026#39; : 421, \u0026#39;perfEvent.containerId\u0026#39; : \u0026#39;GTM-W92WQQ\u0026#39; });  So my little hack is automatically measuring the time any 'event' takes to complete, as long as it happens before the page has loaded (as that\u0026rsquo;s the only performance I\u0026rsquo;m interested in).\nBy the way, you don\u0026rsquo;t really need to add the eventStartTime into the object, as you already have a locally scoped variable t which stores the start time in a closure. However, I prefer to keep it there in case I want to do further debugging, since with the start time I can also build a waterfall model out of the performance times, to see how much execution of these different pre-load events overlap.\nNOTE!! The script automatically excludes gtm.linkClick, gtm.timer and gtm.formSubmit, as these have their own uses for eventCallback (the Wait for Tags option). Similarly, if you\u0026rsquo;re already using eventCallback with some events, be sure to add them to the Array of excluded events, or you might break your site!\nRemember to test first, please!\nThe setup in GTM requires the following:\n  A Data Layer Variable named DLV - perfEvent.event, pointing to variable name perfEvent.event\n  A Data Layer Variable named DLV - perfEvent.totalTime, pointing to variable name perfEvent.totalTime\n  A Custom Event Trigger named Event - perfTime, with Event name set to perfTime\n  A Universal Analytics Tag of type Timing (see below)\n  Here\u0026rsquo;s the Tag:\n  And this Tag will send your hits as User Timings, after which you\u0026rsquo;ll find the report in Google Analytics, under Behavior -\u0026gt; Site Speed -\u0026gt; User Timings.\nI\u0026rsquo;ve given User Timings a thrashing before, since they\u0026rsquo;re really useless for timing ALL the hits on the site. But if you\u0026rsquo;re fine with just a sample, such as in this case, they work really well.\nSummary Remember kids, good performance is good. Bad performance can kill\u0026hellip; your site\u0026rsquo;s user experience, and other stuff, too!\nHowever you want to do it, it\u0026rsquo;s a good idea to keep an eye on the processes that take place before the window object has completely loaded on your site. You can use User Timings to measure all sorts of important milestones, such as just how long it takes for that awesome font library you paid hundreds of bucks for to download. Or how long your favorite JavaScript framework (rhymes with \u0026ldquo;lay brewery\u0026rdquo;) hinders your page load times. Or how long it takes for you to reach the end of these articles of mine. Well, the last one might need some creativity, but I\u0026rsquo;ll leave it in your capable hands!\n"
},
{
	"uri": "https://www.simoahava.com/tags/performance/",
	"title": "performance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/user-timings/",
	"title": "user timings",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/plugins/",
	"title": "plugins",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/simple-tracker-duplication-universal-analytics/",
	"title": "Simple Tracker Duplication For Universal Analytics",
	"tags": ["JavaScript", "plugins", "tasks", "universal analytics"],
	"description": "How to duplicate all requests sent to Google Analytics using JavaScript and a Universal Analytics plugin.",
	"content": "First of all, I\u0026rsquo;m sorry about the title. I should really stop throwing the word \u0026ldquo;simple\u0026rdquo; around, since people always tell me that the stuff I claim to be easy and straightforward is rarely so. But since this is my blog, I reserve the right to use whatever stupid and misleading terminology I want. I maintain that what follows IS quite simple, especially when considering the amount of complexity it reduces in your Universal Analytics setup.\nNext, I want to direct your gaze to my latest Keynote / PowerPoint creation:\n  This piece of art is called \u0026ldquo;\u0026hellip;while waiting for the cosmos\u0026rdquo;. Note the enigmatic ellipsis in the beginning, the lack of capitalization, and the fact that the title has nothing to do with the image. Yes, friend. This is post-post-modernism at its best!\nSo, I guess my amazing art kind of gave away what I want to show to you in this article. In short, it\u0026rsquo;s a simple little plugin I\u0026rsquo;ve been using in projects I work on, which duplicates all hits sent to a Google Analytics property. These duplicated hits are sent to another Google Analytics property, whose property ID you specify when initiating the plugin.\nThis tutorial uses a Universal Analytics plugin (d\u0026rsquo;oh), which, in turn, utilizies the Tasks API.\nBefore I proceed, I want to direct you to David Vallejo\u0026rsquo;s blog, where he and a bunch of other people are working together on an open-source project which does similar stuff, but on a WAY more configurable level. The plugin I\u0026rsquo;m about to walk you through will simply make an exact duplicate of the hit you sent, without letting you modify the payload one bit.\nWhy, you may ask? Well, a surprisingly large number of projects I work with have a need for a \u0026ldquo;rollup\u0026rdquo; property, which collects data from all sites in the organization. The data sent to the rollup often mirrors whatever is collected in the local sites. It\u0026rsquo;s quite a chore to duplicate send commands across all trackers, so if the project is fine with simple duplication, I use this plugin.\nCAVEAT: This will best work with named trackers. Thus Google Tag Manager setting this up in Google Tag Manager is difficult, as you\u0026rsquo;ll have to mess with the Tracker name field. If you\u0026rsquo;re confident with your GTM implementation skills, feel free to do whatever you wish, of course. Nevertheless, in its current state, the plugin caters best to an on-page Universal Analytics implementation.\nModifying the tracking code For this to work, you will need to make a small change to your Universal Analytics tracking code. Let\u0026rsquo;s say the code looks like this now:\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;https://www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; You\u0026rsquo;ll need to add the following modification:\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;https://www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, \u0026#39;auto\u0026#39;); // ADD THIS LINE:  ga(\u0026#39;require\u0026#39;, \u0026#39;simolatorDuplicator\u0026#39;, {\u0026#39;newId\u0026#39; : \u0026#39;UA-12345-2\u0026#39;}); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; There\u0026rsquo;s a single addition: ga('require', 'simolatorDuplicator', {'newId' : 'UA-12345-2'});. This line invokes a plugin called simolatorDuplicator (I know! The awesomest plugin name in the UNIVERSE!), after which you pass an object with a single key-value pair: {newId : newTrackerId}. In place of newTrackerId, you place a String with the Google Analytics property ID to which you want to duplicate all the hits.\nThe plugin itself There are two quick and easy (sorry about those words again) ways to load the plugin. Either host it in its own JavaScript file which you then load with a \u0026lt;script\u0026gt;\u0026lt;/script\u0026gt; loader, or just run the code in the page template itself.\nWhat\u0026rsquo;s important is that the plugin code needs to be loaded AFTER the tracking code. The code looks like this:\n(function() { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]]; var GADuplicate = function(tracker, propertyId) { var o = tracker.get(\u0026#39;sendHitTask\u0026#39;); var temp; tracker.set(\u0026#39;sendHitTask\u0026#39;, function(model) { o(model); temp = model.get(\u0026#39;hitPayload\u0026#39;).replace(new RegExp(model.get(\u0026#39;trackingId\u0026#39;), \u0026#39;g\u0026#39;), propertyId.newId); if (temp) { model.set(\u0026#39;hitPayload\u0026#39;, temp, true); o(model); } }); }; ga(\u0026#39;provide\u0026#39;, \u0026#39;simolatorDuplicator\u0026#39;, GADuplicate); })();  So either write this in a file named something.js (feel free to replace something with something else), or just add the code in its own \u0026lt;script\u0026gt; block after the tracking snippet.\n\u0026lt;!-- METHOD 1 --\u0026gt; \u0026lt;script\u0026gt; // GA Tracking code here \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;something.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- METHOD 2 --\u0026gt; \u0026lt;script\u0026gt; // GA Tracking code here \u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; (function() { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]] || function() {}; var GADuplicate = function(tracker, config) { var o = tracker.get(\u0026#39;sendHitTask\u0026#39;); var temp; tracker.set(\u0026#39;sendHitTask\u0026#39;, function(model) { o(model); temp = model.get(\u0026#39;hitPayload\u0026#39;).replace(new RegExp(model.get(\u0026#39;trackingId\u0026#39;), \u0026#39;g\u0026#39;), config.newId); if (temp) { model.set(\u0026#39;hitPayload\u0026#39;, temp, true); o(model); } }); }; ga(\u0026#39;provide\u0026#39;, \u0026#39;simolatorDuplicator\u0026#39;, GADuplicate); })(); \u0026lt;/script\u0026gt; Both are equally fine, though to keep things nice and tidy I do recommend the first method.\nLet\u0026rsquo;s take a quick stroll through the code.\n(function() { var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;]] || function() {}; ... ga(\u0026#39;provide\u0026#39;, \u0026#39;simolatorDuplicator\u0026#39;, GADuplicate); })();  These lines wrap the plugin code in an immediately invoked function expression, which I use to protect the global namespace. No, you didn\u0026rsquo;t need to understand any of that.\nThe next line establishes the ga() interface locally, by scoping it to the global Google Analytics Object. This is so that the plugin still works even if you\u0026rsquo;ve renamed the global GA interface from ga to something else.\nAfter the plugin code (represented by the \u0026ldquo;\u0026hellip;\u0026quot;) we make the plugin available to the ga interface. This is a very important line, as it is the counterpart to the ga('require', 'simolatorDuplicator'...); command used in the tracking code. If you didn\u0026rsquo;t have this line, or if you had a typo in the plugin name, your GA code would not work at all! So remember to test, test, TEST.\nNext, the plugin constructor itself:\nvar GADuplicate = function(tracker, config) { var o = tracker.get(\u0026#39;sendHitTask\u0026#39;); var temp; tracker.set(\u0026#39;sendHitTask\u0026#39;, function(model) { o(model); temp = model.get(\u0026#39;hitPayload\u0026#39;).replace(new RegExp(model.get(\u0026#39;trackingId\u0026#39;), \u0026#39;g\u0026#39;), config.newId); if (temp) { model.set(\u0026#39;hitPayload\u0026#39;, temp, true); o(model); } }); };  Lots of things going on here.\nFirst, the function expression establishes a new function called GADuplicate, which takes two parameters: tracker and config. The tracker parameter is passed automatically by the plugin logic, and it contains a reference to the Universal Analytics tracker object which invoked the plugin. The config object is passed as a parameter in the modified tracking code. It\u0026rsquo;s the {'newId' : 'UA-12345-2'} which we covered earlier.\nNext, we make a copy of the original sendHitTask. This little method is actually the entire dispatch logic of analytics.js, so we\u0026rsquo;ll need it to send our data to Google Analytics.\nWe need it especially because on the very next line after the variable declarations we overwrite the sendHitTask of the tracker with a new one!\nFirst, the original sendHitTask, temporarily copied to the method o(), is used to send the regular hit to Universal Analytics. This is really important, as without this you\u0026rsquo;d just be sending your duplicate hit!\nThen, we take the hitPayload you just sent to GA, and we replace all instances of the current property ID with the new property ID that you configured in the tracking code! This is the main logic in this plugin. We take the payload, we send it first regularly, and then we modify it and send the modified version. By using model.set('hitPayload', temp, true); we\u0026rsquo;re rewriting the payload, which is then submitted with a new invocation of the o() method.\nI maintain that it\u0026rsquo;s really quite simple when you think about it, but naturally it does require some understanding of how the APIs work. So feel free to plunge into the documentation.\nAnd that\u0026rsquo;s it! That\u0026rsquo;s the code, the setup, and the implementation. Remember, you will need to modify the tracking code accordingly on all pages of your site where you want to use this plugin. You can use it with named trackers, too; just modify the plugin call to:\nga(\u0026#39;myNamedTracker:require\u0026#39;, \u0026#39;simolatorDuplicator\u0026#39;, {\u0026#39;newId\u0026#39; : \u0026#39;UA-12345-2\u0026#39;});  And yes, you can rename the plugin to something else than simolatorDuplicator, though I will love you slightly less if you do so.\nSummary Please remember, this is an exact hit duplication method. If you want to modify the payload, you\u0026rsquo;ll need to add some additional logic to the code, and I really recommend you check out the link to David Vallejo\u0026rsquo;s blog in the introductory chapter of this article.\nLet me know if you\u0026rsquo;re having problems with this, or if you have suggestions! Please do test this thoroughly before implementing it. You are messing with code that can potentially cripple your Google Analytics implementation.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/using-document-write-safely-gtm-tags/",
	"title": "#GTMTips: Using document.write Safely In GTM Tags",
	"tags": ["custom html", "dan wilkerson", "document.write", "Google Tag Manager", "gtmtips", "guest post"],
	"description": "Guest post by Dan Wilkerson on how the document.write setting in Google Tag Manager&#39;s Custom HTML tags works.",
	"content": "This article is a guest article by someone from the analytics community I really look up to. Dan Wilkerson is an analytics developer at Bounteous, a company I hold in high esteem. Dan is one of the smartest technical analytics experts out there, and a large bulk of the awesome scripts and hacks that Bounteous produces (almost on a daily basis) have been orchestrated by him. So I\u0026rsquo;m very pleased to give the floor to Dan, so that he can tell you all about using the pesky document.write() method in your Google Tag Manager Custom HTML Tags!\nTip 46: Safely using document.write in Google Tag Manager Tags   Often, 3rd party marketing tags make use of an archaic JavaScript method called document.write. This method has been the go-to for adding in dynamic tracking pixels and scripts because it is universally supported and predictable. However, document.write won\u0026rsquo;t play nicely with asynchronously loaded scripts. Since Google Tag Manager forces scripts to load asynchronously, and since many 3rd party marketing tags depend on document.write, there was a time when we couldn\u0026rsquo;t implement these tags in GTM. Fortunately, the engineers at Google added a clever solution which temporarily replaces document.write with a safe alternative while your tag executes.\nIn the past, document.write was meant to be used to add dynamic content using JavaScript in between when your browser first started to load the page and when the page completed loading. This content would execute synchronously by default, making the outcome predictable and implementation straightforward. Many ad tech vendors came to rely on this functionality to load in snippets and add in tracking pixels, instead of using more asynchronous-friendly methods. This made writing their tags simple and concise, but synchronously executing snippets would cause the browser to stop rendering and slow down the overall speed of the page on a site.\nUnfortunately, when document.write is called after the document is finished loading, the browser overwrites the existing document and replaces it with just the contents of the document.write call (read: your entire site becomes a blank page). In the past, this wasn\u0026rsquo;t a big deal; we\u0026rsquo;d hard code our handful of marketing tags straight into the HTML of the page.\nUnfortunately, modern front-end engineering best practices, the proliferation of tracking pixels, and tag management systems don\u0026rsquo;t play nicely with this paradigm. These days, it\u0026rsquo;s all about asynchronicity and speed. Thus, we see the crux of the issue: vendors want simplicity and reliability, and engineers want speed and efficiency.\nThe engineers at Google have implemented a solution to this problem for us in Google Tag Manager. When a Custom HTML Tag in Google Tag Manager requires document.write to run, Google Tag Manager temporarily replaces the default document.write function with a safe version of their own. When we want to use a tag that uses document.write, we simply need to check a box in the Custom HTML Tag creation interface. Enabling this feature will allow us to safely deploy any tags that use document.write, even after our page has loaded. Here\u0026rsquo;s where to do that in your tag:\n   Note! You should not check the box if the tag or any script loaded by the tag does not use document.write.\n Once the tag has finished firing, GTM then switches document.write back to the browser default. Wondering if you have a Tag that isn\u0026rsquo;t using this feature? If you see an error in your console like below:\n  you may have a tag in GTM that is firing asynchronously and attempting to use document.write. Try and track down this tag quickly and fix it. (Hint: if you click the line reference in the far right of the interface, Chrome will take you straight to the offending code):\n  Otherwise, your tracking pixels won\u0026rsquo;t work. GTM will even warn you proactively if you try and save a tag with document.write that doesn\u0026rsquo;t have this feature enabled:\n  This feature allows us to have our cake and eat it, too. Before, we\u0026rsquo;d have to deliver the tags synchronously. You deliver the same tags as before, and you can give your users a better experience.\nGTM\u0026rsquo;s replacement for document.write uses document.createElement and then appends the result into a hidden div at the bottom of the page. It negotiates the handoff by binding to the onload event or the onreadystatechange event of the element.\nAnd that\u0026rsquo;s all there is to it! Simply check the box, and your tag will load asynchronously and write to the document, as expected. Remember, if you see a warning in the Developer Tools, that means there may be a Custom HTML Tag with document.write that doesn\u0026rsquo;t have that box checked.\nSummary (by Simo) First of all, kudos to Dan for giving a thorough run-through of something most of us take for granted. The simple little checkbox in the Custom HTML Tag hides a wealth of complexity, engineered solely to make life easier for anyone who has to work with advertising pixels.\nIt\u0026rsquo;s kind of odd that something so crucial to businesses these days (ad revenue) hinges on a technology which is dubious at best. In my book, document.write is in the same bucket of nastiness as iframes, single-pixel image beacons, (third-party) browser cookies, and all other technologies designed to facilitate the spread of malware.\nBut this article wasn\u0026rsquo;t about advertising. This was about zooming in on yet another interesting technical manoeuvre that the Google Tag Manager engineers have done to make sure that the tool caters to all its target audiences (whatever they might be) with equal precision. So thank you, Dan!\nAs always, the comment section is here for you.\n"
},
{
	"uri": "https://www.simoahava.com/tags/document.write/",
	"title": "document.write",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/timer-trigger/",
	"title": "#GTMTips: The Timer Trigger",
	"tags": ["gtmtips", "Guide", "timer", "triggers"],
	"description": "Build a custom timer trigger in Google Tag Manager. You can set it to start and stop whenever you like.",
	"content": "Google Tag Manager has a Trigger type which fires after a certain duration of time has passed on the web page: the Timer Trigger. The most common uses for the Timer Trigger seem to be either to send an event to Google Analytics after X seconds of dwell time (to kill the Bounce), or to defer a Tag from firing until some asynchronous request has completed with certainty.\nIn the previous version of Google Tag Manager, the Timer was a separate listener Tag, which meant that you could start a timer based on a user interaction such as a click. In the current version of GTM, this is no longer possible, as the Timer Trigger will only start when GTM is first loaded on the page. So, one of the tips in this article will be how create your own Timer, in case you want to start one based on some other stimulus than the Page View.\nTip 45: The Timer Trigger   The Timer Trigger itself has a simple operation mechanism. You specify three different fields when creating the Trigger:\n  Event Name: This lets you customize the event name that GTM pushes to dataLayer. It\u0026rsquo;s most useful in the case where you want to fire multiple timers on the page.\n  Interval: Here you specify how many milliseconds should pass between each activation of this Trigger.\n  Limit: This option lets you choose how many times the Trigger fires before it is stopped.\n  Furthermore, you can delimit the Timer to only fire on specific pages using the Enable When option. You\u0026rsquo;ll need to provide some condition which is already present when GTM is first loaded, such as Page Path equals /home-page/, or something similar.\nIn Fire On you can fire a Tag based on some parameters of the Timer that fired. You might want to create a new Data Layer Variable for all of the following parameters:\n  gtm.timerCurrentTime - The timestamp of the most recent Timer activation.\n  gtm.timerElapsedTime - The time in milliseconds since the Timer started.\n  gtm.timerEventNumber - The number of Timers that have activated on the current page (whatever you have in the Limit option is the maximum).\n  gtm.timerId - A unique identifier number for the Timer. Each Timer you have on the page has a different ID.\n  gtm.timerInterval - The value you set in the Interval option when creating the Timer.\n  gtm.timerStartTime - The timestamp of when the Timer first started.\n  The gtm.timerId Data Layer Variable is definitely the most useful one. With that, you can stop a Timer even before it\u0026rsquo;s Limit is reached.\nTo do this, you need to run the following JavaScript command:\nvar timerId = {{DLV - gtm.timerId}}; window.clearInterval(timerId);  So executing window.clearInterval on the Data Layer Variable that points to gtm.timerId lets you halt the Timer. There\u0026rsquo;s no way to restart it once it\u0026rsquo;s halted on the page, though. The only way it starts again is with a page reload.\nThis is very useful on single-page apps, for example, where you might want to stop the Timer when a new page is loaded, but there\u0026rsquo;s no actual page refresh.\nNow, in case you DO want a timer to fire when the user does something (e.g. click), you\u0026rsquo;ll need to use a Custom HTML Tag with custom code. Here\u0026rsquo;s one way to do it:\n\u0026lt;script\u0026gt; (function() { // CHANGE THESE THREE:  var eventName = \u0026#39;custom.timer\u0026#39;; // The event name that is pushed into dataLayer  var interval = 5000; // The interval in milliseconds  var limit = 1; // The number of times the timer fires  // OTHER SETTINGS:  var timerNumber = 1; var startTime = new Date().getTime(); var fireTimer = function() { var timeNow = new Date().getTime(); window.dataLayer.push({ \u0026#39;event\u0026#39; : eventName, \u0026#39;custom.timerCurrentTime\u0026#39; : timeNow, \u0026#39;custom.timerElapsedTime\u0026#39; : timeNow - startTime, \u0026#39;custom.timerStartTime\u0026#39; : startTime, \u0026#39;custom.timerEventNumber\u0026#39; : timerNumber, \u0026#39;custom.timerId\u0026#39; : timerId, \u0026#39;custom.timerInterval\u0026#39; : interval, \u0026#39;custom.timerLimit\u0026#39; : limit }); timerNumber += 1; if (limit \u0026lt; timerNumber) { window.clearInterval(timerId); } }; var timerId = window.setInterval(fireTimer, interval); })(); \u0026lt;/script\u0026gt; It\u0026rsquo;s a very simple script, and it does pretty much exactly the same thing as the Timer Trigger. The difference is that you can use any Trigger you want to fire this custom Trigger. Also, the keys pushed into dataLayer differ in that the prefix is not gtm. but custom.. So gtm.timerId becomes custom.timerId and so forth.\nThat\u0026rsquo;s all for the Timer tips. I hope they were useful! Remember to test thoroughly whenever using custom scripts on your page. Accidentally setting the interval to 1 millisecond and the limit to 10000 can quickly destroy your page performance. Yes, this is something that happened to me\u0026hellip;sorry, my anonymous friend.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtmtips-10-blogs-10-articles-10-people-follow/",
	"title": "#GTMTips: 10 Blogs, 10 Articles, 10 People To Follow",
	"tags": ["google analytics", "Google Tag Manager", "gtmtips", "Guide", "resources"],
	"description": "10 blogs, 10 articles, and 10 people to follow if you&#39;re interested in Google Tag Manager.",
	"content": "Quite a while ago, I wrote an article on what I considered (then) to be my favorite Google Tag Manager resources. Many of them are still very valid, but I still wanted to write a follow-up. Times have changed, and GTM is very different from what it was two years ago when I wrote the post.\nSo in this article, I want to divert your attention to 10 blogs, 10 articles, and 10 people - all which are and/or share excellent Google Tag Manager content on a periodic basis.\nNow, I know I\u0026rsquo;m going to leave many amazing individuals and awesome resources out of this post. Please don\u0026rsquo;t read anything into it, and instead share these omissions in the comments section of this blog! These are not my \u0026ldquo;top 10\u0026rdquo; or \u0026ldquo;best of the best\u0026rdquo;, but rather individual works of art that I consider very relevant today.\nTip 44: My 10 x 10 x 10 Google Tag Manager resources today   Let\u0026rsquo;s start with blogs. There really aren\u0026rsquo;t any blogs devoted solely to Google Tag Manager, so I\u0026rsquo;m going to link to stuff that\u0026rsquo;s hugely useful for Google Tag Manager usage in general. This might include JavaScript tips or some excellent Google Analytics configuration stuff.\nAll the resources below are listed in alphabetical order.\n10 Blogs To Check Out There are many great analytics blogs out there, but only few produce content on a regular basis. In the following list, I\u0026rsquo;ve included blogs that I use or refer to almost daily. Not all of them have a regular publishing schedule, unfortunately, so hopefully if more people start reading them, the authors will pick up the slack!\n1. Analytics Ninja http://www.analytics-ninja.com/blog.html\nCheck it out: REAL Time On Page in Google Analytics\nYehoshua Coren publishes content quite rarely, but when he does, it\u0026rsquo;s just FULL of amazing information. He takes complicated, usually quite techy concepts, and translates them into a business context.\n2. Analytics Pros https://www.analyticspros.com/blog/\nCheck it out: Getting Started with Google Optimize\nAnalytics Pros are one of the better-known Google Partners, and they produce a lot of content especially around new product releases. They\u0026rsquo;ve done an excellent job at outreach, even if much of the content is geared towards the Google Analytics Premium audience.\n3. ConversionWorks http://www.conversionworks.co.uk/blog/\nCheck it out: Explain GTM - Google Tag Manager Containers in plain English\nConversionWorks do a great deal of outreach, especially in the small sphere of European Analytics events and conferences. They\u0026rsquo;ve written a bunch of free tools which are incredibly useful and value-adding in day-to-day GTM use. Their tall and extremely British leading man, Doug Hall, has also written an excellent book on GTM best practices and design patterns, which you can download (for free, of course) here.\n4. Digital Debrief http://www.kristaseiden.com/\nCheck it out: Step-by-Step: Adding a Second GA Property via Google Tag Manager\nKrista Seiden is an Analytics Advocate at Google, and certainly does her part in spreading the word of Google Analytics and Google Tag Manager around the globe. She occasionally writes extremely helpful guides and tip posts about GA and GTM in her blog, but she\u0026rsquo;s got a wealth of other, useful content for budding analysts, and women and men in tech.\n5. L3 Analytics http://www.l3analytics.com/blog/\nCheck it out: An alternative approach to GTM event tracking\nPeter O\u0026rsquo;Neill is one of the most industrious people in digital analytics, and his company L3 Analytics does a great deal of outreach in the form of blog posts and conference participation. The blog isn\u0026rsquo;t updated all that often (at least with cool GTM and/or GA articles), but when new content is published, it\u0026rsquo;s always very engaging. Usually, the articles offer alternative approaches or contrarian views to concepts we take for granted in the analytics world.\n6. Loves Data http://www.lovesdata.com/blog/\nCheck it out: A Beginners Guide to Google Tag Manager Environments\nLoves Data is a spectacular and wonderful company operating in Australia. Their outreach is impressive both in quantity and quality, and you\u0026rsquo;ll do well to follow them in social media channels as well. Many of their guides are geared towards beginners, but the comprehensiveness of these tutorials always manages to impress me.\n7. Bounteous https://www.bounteous.com\nCheck it out: Instantiating \u0026amp; Using The Google Tag Manager dataLayer\nHonestly, this whole article could have been just about Bounteous. I have a huge crush on the whole company. The amount of content they produce is staggering, and they target all audiences from beginner to creepy JavaScript nerd. They\u0026rsquo;re humble and down-to-earth about what they do, and whenever I read their content I never get the feeling that they\u0026rsquo;re trying to pitch or sell anything. They only want to educate GA and GTM users. Thus, they\u0026rsquo;ve set an example that I strive to follow in every single article I write in my blog.\n8. MeasureSchool http://measureschool.com/\nCheck it out: Google Tag Manager for Beginners\nI don\u0026rsquo;t really know of any free courses or trainings for Google Tag Manager (apart from Google\u0026rsquo;s own, excellent Google Tag Manager Fundamentals), so I\u0026rsquo;m ecstatic about the work that Julian Juenemann has put into his own MeasureSchool. The content is free and ready for you to download and make use of! Be sure to check it out. He also has a YouTube channel full of great GTM content, so take a look at that, too.\n9. Online Behavior http://online-behavior.com/\nCheck it out: Google Tag Manager Injector: Painless Tagging\nOnline Behavior is Googler Daniel Waisberg\u0026rsquo;s love-child, and incidentally my all-time favorite Google Analytics blog. The amount of content he and his impressive cast of guest writers produce is impressive, but the quality of the articles is what really makes my head spin. There\u0026rsquo;s a wealth of information for both beginners and for advanced users.\n10. Thyngster http://www.thyngster.com/\nCheck it out: Tips for working with Custom HTML and Custom JavaScript tags in Google Tag Manager\nDavid Vallejo is a GTM wizard, and he really knows his way around JavaScript. His posts are always VERY techy, but he approaches them with a clear goal in mind: helping others combat issues they might have with complicated setups. Usually his solutions are simpler than they seem, and always, always tackle the problem handily and without turning your setup into a complete mess.\n10 People To Follow The following 10 people are very active in social media, sharing excellent content on a regular basis, responding to user questions, and inviting conversation around hot analytics topics whenever and wherever possible. I can\u0026rsquo;t recommend enough that you follow them and join in on the discussion wherever appropriate.\n1. Damion Brown   LinkedIn: https://www.linkedin.com/in/damionbrown\nTwitter: @datarunsdeep\nGoogle+: +DamionBrown\n2. Julien Coquet   LinkedIn: https://www.linkedin.com/in/juliencoquet\nTwitter: @juliencoquet\nGoogle+: +JulienCoquet\n3. Mark Edmondson   LinkedIn: https://www.linkedin.com/in/markpeteredmondson\nTwitter: @holomarked\n4. Stéphane Hamel   LinkedIn: https://www.linkedin.com/in/shamel\nTwitter: @shamel67\n5. Phil Pearce   LinkedIn: https://www.linkedin.com/in/philpearce\nTwitter: @philpearce\n6. Lea Pica   LinkedIn: https://www.linkedin.com/in/leasynefakispica\nTwitter: @leapica\nGoogle+: +LeaSynefakisPica\n7. Zorin Radovancevic   LinkedIn: https://www.linkedin.com/in/zorinatesc\nTwitter: @zorinatesc\nGoogle+: +ZorinRadovancevic\n8. Jeff Sauer   LinkedIn: https://www.linkedin.com/in/jeffsauer\nTwitter: @jeffsauer\nGoogle+: +JeffSauer\n9. Krista Seiden   LinkedIn: https://www.linkedin.com/in/kristaseiden\nTwitter: @kristaseiden\nGoogle+: +KristaSeiden\n10. Daniel Waisberg   LinkedIn: https://www.linkedin.com/in/danielwaisberg\nTwitter: @danielwaisberg\nGoogle+: +DanielWaisberg\n10 Excellent GTM Articles The following 10 articles are, in my opinion, hallmarks in Google Tag Manager writing. They exemplify all the goodness that content creation can inspire, and they are quite simply brilliant resources for any skill level.\n1. 7 Steps to Pushing JSON Structured Data using Google Tag Manager (Mike Arnesen) http://www.6dglobal.com/blogs/7-steps-pushing-json-structured-data-using-google-tag-manager-2015-03-04\nMike Arnesen\u0026rsquo;s article on how to inject SERP refinements through Google Tag Manager is an example of how to write a great content piece: produce a hypothesis, test it opaquely, and present the results. The article nicely shows how important Google Tag Manager is also in the organic search world.\n2. Complete Guide to Google Tag Manager (iPullRank) http://ipullrank.com/google-tag-manager/\nA thorough guide through (almost) all facets of Google Tag Manager. For a \u0026ldquo;complete\u0026rdquo; guide, some of the parts are lacking (e.g. GTM for mobile), but the guide steers you through all the most critical concepts of GTM, which any users would do good to learn.\n3. Creative uses of Google Tag Manager: site hacking / ripping (Julien Coquet) http://juliencoquet.com/en/2015/03/24/creative-uses-of-google-tag-manager-site-hacking-ripping/\nOne of the funniest GTM articles out there. Learn how Julien Coquet took up the battle against idiots who had ripped his entire site content onto their own site. With GTM, Julien turned the tables against the hackers.\n4. Easier Enhanced Ecommerce Product \u0026amp; Promo Tracking (Eivind Savio) https://www.savio.no/analytics/easier-enhanced-ecommerce-product-promo-tracking\nExcellent article by Eivind on how to automatically split your Enhanced Ecommerce payload if it surpasses the payload size limit of 8KB that Google Analytics imposes on the HTTP requests. A techy but critical article on how to fix a potentially disastrous data quality issue in Google Analytics.\n5. Google Tag Manager: A Step-by-step Guide (Online Behavior) http://online-behavior.com/analytics/google-tag-manager\nDaniel Waisberg\u0026rsquo;s Google Tag Manager guide is still one of the best out there, and he keeps the content fresh and up-to-date.\n6. GTM for Mobile: Updating Your iOS Application Through Google Tag Manager (Analytics Pros) https://www.analyticspros.com/blog/tag-management/gtm-for-mobile/\nNot much has been written about Google Tag Manager for mobile. In this article, Luka Cempre from Analytics Pros walks your through one of the more powerful use cases speaking for the adoption of GTM for mobile.\n7. How to setup Content Grouping in Google Analytics for your WordPress site (Daniel Carlbom) http://dcarlbom.com/google-analytics/content-grouping-with-wordpress-and-google-analytics/\nDaniel\u0026rsquo;s blog has a lot of great information for WordPress users tackling with Google Analytics and Google Tag Manager. This article is a prime example of how to use the power of the WordPress engine to fuel your site tracking.\n8. Track Brightcove Player with Google Tag Manager (Isaac Abramowitz) http://iabramo.com/2016/03/08/track-brightcove-player-with-google-tag-manager/\nIsaac\u0026rsquo;s article on utilizing the Brightcove API with Google Tag Manager shows how sometimes the simplest solution is the best one.\n9. Tracking Form Submissions in Iframes (Bounteous) https://www.bounteous.com/insights/2015/10/21/google-analytics-iframes-form-submissions/\nA brilliant four-parter from Bounteous, tackling an extremely difficult topic: how to track interactions within Iframes.\n10. Understanding your Website Visitors: Prospects vs. Customers (L3 Analytics) http://www.l3analytics.com/2016/01/18/understanding-your-website-visitors-prospects-vs-customers/\nTypical to his style, Peter isn\u0026rsquo;t satisfied with just providing a nice, fun solution to improve your site\u0026rsquo;s data collection. Instead, he\u0026rsquo;s ambitious enough to show the business ramifications of the data collection method at hand. In this article, he walks you through a setup which allows you to segment your visitors in Google Tag Manager.\nSummary I hope you find these resources useful. I want to remind you that this is far from an exhaustive list. I left out many excellent blogs to subscribe to, people to follow, and articles to read. I wanted to keep things simple, and just give you a jumpstart on your journey to becoming a true GTM stalker.\nI don\u0026rsquo;t really need to even say this, but none of the references in this articles was paid for or solicited in any way, and no one coerced me to add them to the list. I genuinely think that these are excellent resources for you to keep up your GTM skills, and I truly hope you take a look at them!\nDon\u0026rsquo;t forget to follow the vibrant Google+ community, and if you have problems you can always share them in the Product Forums as well.\nPlease share YOUR favorite resources in the blog comments! Let\u0026rsquo;s spread the love.\n"
},
{
	"uri": "https://www.simoahava.com/tags/resources/",
	"title": "resources",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/blog-statistics/",
	"title": "Blog Statistics",
	"tags": [],
	"description": "",
	"content": "I started writing articles in May, 2013. The first couple of articles written during the first months of this blog\u0026rsquo;s life were not very good. Things picked up when I started focusing only on Google Analytics and Google Tag Manager articles.\nThe following statistics are powered by Google Data Studio.\n   "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-javascript-errors-events/",
	"title": "#GTMTips: Track JavaScript Errors As Events",
	"tags": ["errors", "Google Tag Manager", "gtmtips", "JavaScript"],
	"description": "Guide for tracking unchecked JavaScript errors to Google Analytics as Google Tag Manager events.",
	"content": "Back back to the friggin\u0026rsquo; basics. Almost two years ago, I wrote a two-parter on how to have fun with Google Tag Manager: Part 1, and Part 2. The first part had a nice exercise for tracking JavaScript Errors as Google Analytics Events, using the recently published \u0026ldquo;JavaScript Error Listener\u0026rdquo; Tag.\nWell, that was in GTM V1. Now we\u0026rsquo;ve been smoothly sailing with the wonderful new interface for well over a year, and it\u0026rsquo;s time to update some of these nifty tricks. In this #GTMTips post, I\u0026rsquo;ll show you how to track uncaught JavaScript errors in the new interface, plus give you some tips on the overall process.\nTip 43: Track JavaScript Errors As Events   The process is actually really simple. Much easier than it was in the previous Google Tag Manager interface.\nBy the way, uncaught refers to an error which hasn\u0026rsquo;t been captured and handled by any of the scripts on the page. The most common way to catch an error is to use try...catch blocks around your code, which capture any errors which bubble up to the context where you have the block. If an error makes its way through without having been caught in any way, it will trigger the JavaScript Error Trigger. Thus it\u0026rsquo;s a good way to audit your current error detection methods, too!\nAnyway, you start by making sure the Built-In Variables are activated.\n  If you\u0026rsquo;re curious about these Built-In Variables, check the relevant section of my Variable Guide For Google Tag Manager.\nNext, head on over to Triggers, and create a new one:\n  I\u0026rsquo;m going to let the Trigger fire on \u0026ldquo;All JavaScript Errors\u0026rdquo;, but be sure to read to the end of this post for tips regarding this selection.\nFinally, you need a Google Analytics Event Tag, which collects and concatenates all this information. Here\u0026rsquo;s what I use:\n  An example result using these settings would be something like:\nEvent Category: JavaScript Error\nEvent Action: Uncaught ReferenceError: appear is not defined\nEvent Label: 1255: https://www.simoahava.com/scripts/bundle.js\nI\u0026rsquo;m sending the event as Non-Interaction: True, because it\u0026rsquo;s not a user interaction, and I don\u0026rsquo;t want it to be counted as such.\nAnd that\u0026rsquo;s it for the implementation!\nNow, there are some things you might want to consider when implementing this on your own website. The next chapter will tackle these issues.\nTips Tracking JavaScript Errors can be quite the bipolar experience. On the one hand, it\u0026rsquo;s just a good, solid developer attitude to want to know about what\u0026rsquo;s breaking up or already broken on your site. On the other hand, it can cause a deluge of Events in your Google Analytics profile, leading to sessions reaching their hit limits and severe sampling issues. So here are a couple of tips to manage it all.\n1. Track errors as Google Analytics Goals \u0026ldquo;Track ERRORS as GOALS, what has he been smoking?!\u0026rdquo; I hear you exclaim. Yeah, it\u0026rsquo;s not a business goal, but goals can also be negative goals. The reason I\u0026rsquo;m interested in this particular negative goal, is because I want to see a Conversion Rate for sessions with JavaScript Errors. This will allow me to do all sorts of cool analyses on how the error rates are fluctuating over time. To set up the goal, you can use the following settings:\n  Next, you can create an Alert, which sends you an email every time the number of JavaScript Errors has gone up significantly on your site:\n  Now you\u0026rsquo;ll get an email every time there\u0026rsquo;s a significant increase in the number of JavaScript Errors recorded on your site.\n2. Use Environments or the Debug Mode Tracking every single uncaught JavaScript error can lead to a deluge of hits. The modern website is best known for the number of errors it dispatches, not the fancy UI or the awesome new framework it utilizes. To keep the number of errors down, you might want to delimit them to only fire in a specific environment, or when in the Debug Mode. You could, for example, allow the Error Trigger to work only in the staging environment.\n  This way only errors triggered by your testers and developers will be recorded. The downside, of course, is that it\u0026rsquo;s not an authentic setting, and no matter how much you test and debug, you will always miss some things.\n3. Add manual sampling Another way to prevent every single error from dispatching is to manually sample the hits that are sent to Google Analytics. You could, for example, only let the Trigger fire for 50% of recorded errors. You can use the Random Number Built-In Variable here:\n  The trick is to only fire the Trigger when the Random Number Variable returns a number that ends in 0, 1, 2, 3 or 4. That\u0026rsquo;s 50% of the possible numbers it can end in.\nNaturally, the downside here is that you might miss some outlier errors, which only pop up every now and then. But you should still catch most of the significant ones.\n4. Script Error If your site is using JavaScript files loaded from external content distribution networks, the messages and line numbers of the errors dispatched in these external scripts are not exposed in the browser. This is a reality you\u0026rsquo;ll have to work with. There\u0026rsquo;s no way to identify where and what error took place, as the browser will only dispatch the nondescript \u0026ldquo;Script Error.\u0026rdquo; message with a line number of 0. The URL of the script is still exposed, though, and you can use that to narrow down the possible errors.\n  It\u0026rsquo;s a good idea to host as many scripts as possible on your own website. Not only will you know more about the errors that are thrown, but also being subservient to an external CDN can introduce security issues, if the third party decides to add some malicious or dangerous code to the library you\u0026rsquo;re using (or if they\u0026rsquo;re hacked).\nSummary I hope this tip has been useful. Tracking errors is one of the ways to make sure that your website is catering an optimized experience for your visitors. Google Analytics provides a great tool for tracking these errors, because it also lets you create session- and user-scoped segments around error events. With those segments, you can start analyzing the actual business impact of errors thrown on the site.\nAn error in the eCommerce checkout funnel can be destructive to your business, and using Google Analytics for detection can help you get on top of things before you\u0026rsquo;ve lost too much money.\n"
},
{
	"uri": "https://www.simoahava.com/tags/errors/",
	"title": "errors",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/debug-google-analytics-on-your-mobile-browser/",
	"title": "Debug Google Analytics On Your Mobile Browser",
	"tags": ["debugging", "google analytics", "mobile"],
	"description": "Guide for debugging Google Analytics tracking when using your mobile phone&#39;s browser.",
	"content": "I\u0026rsquo;m currently at SMX München, which is still one of my favorite conferences in Europe. The quality of the talks is superb, and the organization is just perfect. So today, after my talk (joint session with the awesome Dave Sottimano), I was listening to the inimitable Mike King give an excellent presentation together with Ari Nahmani on technical skill prerequisites for all digital marketers today. Needless to say, I strongly agree with their view that digital marketing has always been a technical discipline, and the web is getting more and more complex each day that passes. The only way to address this flux is to hone your technical skills as much as you can.\nOne thing that Mike quickly visited was remote debugging for mobile devices. Now, debugging your website\u0026rsquo;s mobile experience is a pain, since the mobile browsers themselves give you only few tools to work with. For Google Analytics, for instance, it would be really interesting to know what requests your mobile browser is sending to GA, especially if you\u0026rsquo;re tracking stuff that\u0026rsquo;s idiosyncratic to the mobile experience. But you can\u0026rsquo;t just open a mobile browser and expect it to have the same awesome developer tools as your desktop browser.\nEnter remote debugging! You can simply plug in your iOS or Android device, and then open the web browser (Safari or Chrome, respectively) on your desktop, and in a few simple, easy-to-follow steps, you\u0026rsquo;ll be right there inspecting the DOM and analyzing the network requests! How awesome is that?\nThis isn\u0026rsquo;t a guide, just a \u0026ldquo;EUREKA\u0026rdquo; moment for myself, since it really makes it quite simple to debug your mobile browser. The steps I outline below are for the iPhone (since that\u0026rsquo;s what I use), but you can follow the steps in this guide to make it work on your Android device / Chrome combination as well.\niPhone instructions So, first of all, plug your iPhone into the USB port of your computer.\nNext, still in the iPhone, go to Settings -\u0026gt; Safari -\u0026gt; Advanced, and in the screen that opens, make sure Web Inspector is ON.\n  Open the Safari browser on the iPhone, and go to the website where you want to debug your mobile device on.\nNext thing to do is open your desktop Safari browser. In the Develop menu, you should now see your device, and when you open the device menu, you should see an entry for Safari, together with the URL of the site you\u0026rsquo;re viewing on the phone. Click that menu item.\n  TA-DA! You\u0026rsquo;re remotely debugging your iPhone, while it\u0026rsquo;s browsing the website! You can move your mouse button over the DOM elements in the Elements view to see how the Document Object Model is built on the phone.\n  You can open the Network Tab, find a request to collect (the Google Analytics endpoint), and then expand the Resource panel to view the parameters of the request and the HTTP headers and all that stuff!\n  Hell, you can even execute JavaScript in the mobile context! Want to check the state of dataLayer on your mobile browser? Easy! What about the User Agent string? Child\u0026rsquo;s play!\n  Honestly. Remote debugging. Where have you been all my life?\n"
},
{
	"uri": "https://www.simoahava.com/tags/debugging/",
	"title": "debugging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/calculated-metrics/",
	"title": "calculated metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/content-engagement/",
	"title": "content engagement",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/custom-metrics/",
	"title": "custom metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-content-engagement-part-2/",
	"title": "Track Content Engagement Part 2",
	"tags": ["calculated metrics", "content engagement", "custom metrics", "google analytics", "Google Tag Manager"],
	"description": "Second part of my content engagement tracking post for Google Analytics. This second part has some important clarifications to the lessons of the first one.",
	"content": "A couple of days ago, I wrote an article on tracking content engagement. Even though the solution itself works, and it\u0026rsquo;s a really neat trick if I can say so myself, it has its problems.\nAfter all the glory I showered on User Timings in Google Analytics, they have one serious flaw: they cap at 10,000 samples per day. What a ridiculous, arbitrary limit.\nIn any case, this means that if you have enough traffic to accumulate 10K user timing hits per day, it means that the solution I provided in the previous article will not work for you, as the Pageviews will not be capped, meaning the calculation of Total Engaged Time / Pageviews will be skewed.\nAfter awesome discussions with people like Al Wightman, Zorin Radovancevic, and Yehoshua Coren, this article is meant to replace the User Timings setup from the previous guide with a new solution, which utilizes Events and Custom Metrics. Since most of the article is still valid, please read the previous article all the way until you reach 4. User Timings. This is where this post will pick things up, and I\u0026rsquo;ll make sure to guide you all the way to the finish line, so you don\u0026rsquo;t have to refer back to the original article, unless you want to make sure you got all the initial steps right.\nBefore we get started Instead of User Timings, which measures milliseconds, this new solution will use a Custom Metric, which measures engagement time in seconds. This will be piggybacked on a non-interactive event hit.\nIn Google Analytics, we\u0026rsquo;ll create a Calculated Metric with the following formula:\nTotal Engaged Time / Pageviews\nSince each event is (or should) always be associated with a pageview, we can aggregate these nicely in a Custom Report we\u0026rsquo;ll build at the end of this tutorial.\nIn other words, this solution will use up a Custom Metric slot, and it will increase the event hit count for your site. If you\u0026rsquo;re already breaching the 500 hits per session limit, this might well take you over the top. But, let\u0026rsquo;s be honest, if you\u0026rsquo;re near the 500 hits per session limit, you\u0026rsquo;ve already fudged up something in your measurement plan, so don\u0026rsquo;t blame me for those mistakes!\n4. Custom JavaScript Variable Let\u0026rsquo;s pick up the steps of the previous guide with a new Custom JavaScript Variable. Name it JS - Get Engagement Time In Seconds (or something to that effect), and add the following code within:\nfunction() { return {{DLV - nonIdleTimeElapsed}} / 1000; }  This simply returns the nonIdleTimeElapsed key from the data layer divided by 1000, to get the number of seconds of engaged time collected by the script from the previous article.\n5. The Custom Metric (GA) Next, go to Google Analytics Admin. Open Custom Definitions under Property Settings, and click Custom Metrics.\nNext, create a new Custom Metric, by clicking the big red button.\n  Give the new Custom Metric a nice, descriptive name: Total Engaged Time. Make sure you\u0026rsquo;ve got Time selected as the Formatting Type, and you can keep all the other settings as they are.\n  Finally, click Save.\nMake note of the Index number assigned to the new metric. This is important!\n  That\u0026rsquo;s it for the metric!\n6. The Event Tag The Event Tag can have whatever values you want in the fields. For debugging reasons, I always include the value I\u0026rsquo;m piggy-backing on the hit (the Custom Metric in this case) as one of the fields.\n  To make this work, you will need the following:\n  Values in Event Category and Event Action, since these are required fields\n  A new field with name transport and value beacon to improve the accuracy of timing hits sent when the user is leaving the page\n  A Custom Metric with the index number from the previous step, and the Custom JavaScript Variable you created earlier as the value\n  Add the Custom Event Trigger you created in step 2 of the first guide to the Tag\n  The Custom Metric is naturally the most important part of this machine, but I strongly recommend adding the transport field as well.\n7. The Calculated Metric (GA) Almost done!\nNow, go to Google Analytics Admin, and under View Settings, click Calculated Metrics. Click the big red button with + New Calculated Metric, and make new metric look like this:\n  The important part to get right is the Formula. Instead of having to type in the brackets, you can just start typing \u0026ldquo;Total Engaged Time\u0026rdquo;, and the autocomplete should suggest the correct metric to you. Then, add the / to denote division, and finally start typing \u0026ldquo;Pageviews\u0026rdquo;, and again the tool should suggest the correct metric to you.\nGreat! Now we have our Calculated Metric, so we can build a nice, simple Custom Report to give us the low-down of our content engagement time.\n8. The Custom Report (GA) This is what my Custom Report setup looks like:\n  It\u0026rsquo;s very simple, but it yields a lot of interesting data. You can even juxtapose it with the default Average Time On Page to see the incredible discrepancy between just loading a page and actually interacting with it.\n  That\u0026rsquo;s a pretty big difference, wouldn\u0026rsquo;t you say?\nSummary Again, huge thanks to Zorin, Al, Yehoshua, and everyone else who pitched in. User Timings was a let-down, as it\u0026rsquo;s obviously not meant to be used as a data collection metric. However, I wish the sampling would be made more realistic. I perfectly understand why Page Timings are sampled, since you only need a sample to extrapolate a huge amount of useful information. But why not treat User Timings as regular hits, subject to the normal data collection limitations of the platform?\nOne of the caveats of using Custom Metrics and Events is the unpredictability of the session. For example, come midnight, the session will end, but the events will still continue collecting data in the next session, which might be without a landing page altogether! This is a problem on any site with a global audience (such as mine). However, the more data you collect, the more marginal stuff like this will be normalized out of the equation.\nIn any case, Custom Metrics lets you work around the problem of User Timings, since they can also be formatted for time. The combination of Calculated Metrics and Custom Metrics is a game-changer for Google Analytics, and has been instrumental in creating a huge amount of added value to all kinds of analytics setups.\nI hope this article has been helpful, and I\u0026rsquo;m sorry for making you pogo between two texts while trying to come up with the best way to measure content engagement on your website.\nLet me know if you have any comments or improvement suggestions!\n"
},
{
	"uri": "https://www.simoahava.com/tags/engagement/",
	"title": "engagement",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-content-engagement-via-gtm/",
	"title": "Track Content Engagement Via GTM",
	"tags": ["calculated metrics", "content", "engagement", "google analytics", "Google Tag Manager", "user timings"],
	"description": "How to track the actual time users are engaging with your content using Google Analytics. You can use Google Tag Manager to setup the tracking.",
	"content": "When looking at Google Analytics reports, you\u0026rsquo;d think you get a pretty good idea of how people are interacting with your site, right? I mean, you\u0026rsquo;re tracking events here, pageviews there, and user timings, custom dimensions, custom metrics, and calculated metrics are all part of your daily lingo. But you\u0026rsquo;re also probably aware of how futile this tracking is. After all, all you\u0026rsquo;re seeing are numbers that reflect certain outcomes the visitors have produced on the website, and how these outcomes match against your preconceived goals and objectives, right? It\u0026rsquo;s difficult to wrap your head around the individual use cases that constantly take place on the site, and to draw overarching conclusions or unearth business trends on the basis of these data sets can be very problematic indeed.\nIn anticipation of my upcoming talk at SMX München, I thought I\u0026rsquo;d show a pretty nifty way of adjusting what content engagement means on an article-by-article basis, and how to encode this using Google Tag Manager.\nSince this blog is pretty much the only digital property I have a stake in, it\u0026rsquo;s very important to me to uncover meaning in the content data I\u0026rsquo;m looking at. I\u0026rsquo;ve done this numerous times before in articles such as:\n  Track Content With Enhanced Ecommerce\n  Use Page Visibility API With GTM\n  Track Form Engagement With Google Tag Manager\n  And so on and so forth. But what I haven\u0026rsquo;t yet tackled, at least not really, is how much time users are actually spending engaged with my content. In other words, I want to see something like this:\n  I\u0026rsquo;m interested in knowing, among other things, out of all of the time users spend on the page, how much of it are they engaged with the page.\n!!! UPDATE !!! I\u0026rsquo;ve had to rewrite parts of this article due to a severe limitation in utilizing User Timings for data collection. Please read on until you reach step 4. User Timings. At the beginning of that step is another UPDATE, where I link you to the revised version of the article. The revised article picks up right after Step 3 and takes you all the way to the end, so you don\u0026rsquo;t have to jump back here unless you want to enjoy the comments. !!! END UPDATE !!!\nNow, this is a very difficult topic to breach, so I had to ground it somewhere. Enter Yleisradio (YLE; Finnish Broadcasting Company) and Chartbeat.\nYLE and Chartbeat A week ago I had the pleasure of visiting some friends at YLE with a group of like-minded people, all interested in analytics. What Jaakko Ojalehto, Leevi Kokko, and Sami Mattila, the guys from YLE, showed us, was how they\u0026rsquo;ve replicated Chartbeat\u0026rsquo;s research on engagement, and how they\u0026rsquo;re utilizing this data to feed their reporters and journalists with more actionable data about how their content is faring on this enormous digital property.\nChartbeat\u0026rsquo;s research showed, among other things, that the engagement window of a single user averages at around 5 seconds. So after interacting with the page with clicks, mouse movements, keyboard presses, and scrolls, the user would stay engaged for another 5 seconds, before starting to idle or changing to another window or browser tab:\nChartbeat’s JavaScript is constantly listening for acts of engagement on the in-focus webpage within an active browser (full list of events indicating engagement included below). These checks indicate when a user is actively engaged on the page and, based on a study conducted by Chartbeat, will likely remain so for five additional seconds. As this quote from their \u0026ldquo;Description of Methodology\u0026rdquo; page implies, engagement is something we can, and should, measure.\nSo I wanted to replicate what the YLE guys have been doing while replicating what Chartbeat have been doing. And I wanted to provide a simple means of doing so via Google Tag Manager. Thus the rest of this blog post will cover the steps required to send timing data to Google Analytics, where you\u0026rsquo;re measuring the cumulative time each visitor is engaged with any piece of content on your site.\nThis will give you a better understanding of active time spent on the page, as the Average Time On Page just doesn\u0026rsquo;t describe accurately the complexity of engagement. Time On Page is, after all, quite a complex and misleading metric, even if there are some excellent ways to improve it.\nWhat you\u0026rsquo;ll need To make it all work, you\u0026rsquo;ll need:\n  Custom HTML Tag (GTM) - To setup the script which measures engagement time\n  Custom Event Trigger (GTM) - To fire the User Timings Tag when the timings have been calculated\n  Data Layer Variable (GTM) - To pick up the time engaged from dataLayer\n  User Timings Universal Analytics Tag (GTM) - To send the timing data to Google Analytics\n  Calculated Metric (GA) - To calculate the average time engaged per page\n  Custom Report (GA) - To display the data correctly\n  Note that there are some caveats to what you can do in Google Analytics, which depend on whether you\u0026rsquo;re already using User Timings in your data collection. I\u0026rsquo;ll get back to those when discussing the calculated metric you need to create.\n1. Custom HTML Tag Create a new Custom HTML Tag, give it some fancy name, and copy-paste the following code within:\n\u0026lt;script\u0026gt; (function() { var startEngage = new Date().getTime(); var timeEngaged = 0; var idleTime = 0; var idle = true; var idleReport = false; var idleTimer, reportTimer; /* Set the user as idle, and calculate the time they were non-idle */ var setIdle = function() { idleTime = new Date().getTime(); timeEngaged += idleTime - startEngage; idle = true; }; /* Reset the 5 second idle timer. If the user was idle, start the non-idle timer */ var pulse = function(evt) { if (idle) { idle = false; startEngage = new Date().getTime(); idleReport = false; } window.clearTimeout(idleTimer); idleTimer = window.setTimeout(setIdle, 5000); }; // Utility function for attaching listeners to the window  var addListener = function(evt, cb) { if (window.addEventListener) { window.addEventListener(evt, cb); } else if (window.attachEvent) { window.attachEvent(\u0026#39;on\u0026#39; + evt, cb); } }; /* Push an event to dataLayer every 15 seconds unless the user is idle. Also, push an event when the user leaves the page */ var report = function(evt) { if (!idle) { timeEngaged += new Date().getTime() - startEngage; } // Push the payload to dataLayer, and only push valid time values  if (!idleReport \u0026amp;\u0026amp; timeEngaged \u0026gt; 0 \u0026amp;\u0026amp; timeEngaged \u0026lt; 3600000) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;nonIdle\u0026#39;, \u0026#39;nonIdleTimeElapsed\u0026#39; : timeEngaged }); } if (idle) { idleReport = true; } // Fix possible beforeunload duplication problem  if (evt \u0026amp;\u0026amp; evt.type === \u0026#39;beforeunload\u0026#39;) { window.removeEventListener(\u0026#39;beforeunload\u0026#39;, report); } timeEngaged = 0; startEngage = new Date().getTime(); reportTimer = window.setTimeout(report, 15000); }; addListener(\u0026#39;mousedown\u0026#39;, pulse); addListener(\u0026#39;keydown\u0026#39;, pulse); addListener(\u0026#39;scroll\u0026#39;, pulse); addListener(\u0026#39;mousemove\u0026#39;, pulse); addListener(\u0026#39;beforeunload\u0026#39;, report); idleTimer = window.setTimeout(setIdle, 5000); reportTimer = window.setTimeout(report, 15000); })(); \u0026lt;/script\u0026gt; Let\u0026rsquo;s take a look at the script, just briefly and superficially, so that you have an idea of what you\u0026rsquo;re doing.\nAt the very end, you have the following lines:\naddListener(\u0026#39;mousedown\u0026#39;, pulse); addListener(\u0026#39;keydown\u0026#39;, pulse); addListener(\u0026#39;scroll\u0026#39;, pulse); addListener(\u0026#39;mousemove\u0026#39;, pulse); addListener(\u0026#39;beforeunload\u0026#39;, report); idleTimer = window.setTimeout(setIdle, 5000); reportTimer = window.setTimeout(report, 15000);  Here, you\u0026rsquo;re binding the events mousedown (when a mouse button is depressed), keydown (when a key is depressed), scroll (when the page is scrolled), and mousemove (when the mouse is moved) to send a pulse that the user is engaged. If there\u0026rsquo;s a 5000 millisecond (5 second) delay between pulses, the user is set as idle.\nFinally, every 15 seconds the cumulative engaged time is sent to Google Analytics, resetting the time engaged for the next 15 second stretch.\nSo this is the heart of the script. The events are taken directly from Chartbeat\u0026rsquo;s study, and nicely cover pretty much all of the ways in which the user can tell the browser they haven\u0026rsquo;t fallen asleep. Whether or not they model engagement is another, more philosophical, question, but at least they model interaction.\nAs you can see, mobile-specific events such as touchstart and touchend are missing. This is because, first of all, the way you interact with a page while reading on your phone is different from the idle doodling you do while using a mouse. Secondly, following Chartbeat\u0026rsquo;s conclusions, creating arbitrary handlers for purely mobile browser events can be risky due to how browsers interpret touch interactions. So this is something to keep in mind when segmenting the data.\nThe three methods we use, setIdle, report, and pulse are described here:\n  setIdle sets the user as idle every 5 seconds of non-interaction. When the user is idle, the timer calculating engagement will be paused. Also, only the first \u0026ldquo;report\u0026rdquo; to dataLayer will be pushed. After that, nothing is pushed to dataLayer until the user starts engaging with the page again.\n  report pushes the current time engaged to dataLayer, and resets all necessary timers and variables.\n  pulse is what keeps the user \u0026ldquo;active\u0026rdquo;. If the user was idle, it sets their idle status to false, and the 5 second idling timer is also reset at this point.\n  It might seem complicated (I sure hope it doesn\u0026rsquo;t), but it\u0026rsquo;s actually a very simple set of scripts.\nSet this Custom HTML Tag to fire on a Page View / DOM Ready Trigger. You want to start engagement calculations only after the page content can be interacted with.\n2. Custom Event Trigger To fire your User Timings Tag (coming to that soon) when the engagement time is pushed into dataLayer every 15 seconds, you only need a Custom Event Trigger. It should look like this:\n  Hooray, that was easy.\n3. Data Layer Variable You\u0026rsquo;ll need to grab the time engaged from dataLayer, so create the following Data Layer Variable:\n  Quite simple as well, right?\n4. User Timings Tag !!! UPDATE !!! User Timings has one major flaw: it has a daily limit of 10,000 hits, which includes Page Timings. This means that if your site is even marginally at risk of collecting this many timings altogether, the rest of this article will not work for you. In fact, I strongly recommend you stop reading the rest of the steps here, and jump instead to the revised steps I wrote, which utilize an Event Tag and a Custom Metric Tag. I\u0026rsquo;m sorry for making you jump between texts, but the truth is that User Timings have proven to be a subpar data collection method, even if I think they\u0026rsquo;re excellent in so many other ways. So, read the rest of this article at your own peril, as the actual average engagement you\u0026rsquo;ll be collecting might well be very much off the mark. !!! END UPDATE !!!\nSo now you\u0026rsquo;ll need a Tag which pulls all this information together, and dispatches it to Google Analytics.\nWe\u0026rsquo;ll use the amazing User Timings Tag Type for this. User Timings, if you didn\u0026rsquo;t know, are just about the coolest feature of Google Analytics, and ridiculously useful for collecting arbitrary time measurements. I\u0026rsquo;ve written about them before a number of times:\n  Measure SERP Bounce Time With GTM\n  Form Field Timing With Google Tag Manager\n  Page Load Time In Universal Analytics\n  So, create the Tag and make it look something like this:\n  Remember to set the transport : beacon field in the Fields to Set. This is a very useful, albeit poorly supported, feature of Google Analytics, which attempts to preserve any hits that are sent while the user is leaving the page. I have, of course, written about this before, too.\nMake this User Timings Tag fire on the Custom Event Trigger you created earlier.\nAnd that\u0026rsquo;s it for data collection! Now Google Tag Manager will send a User Timings hit to Google Analytics every 15 seconds, where the timing value will be the time the user spent interacting with the page during that 15 second stretch.\nNow we\u0026rsquo;ll want to look at this data in Google Analytics, so that we can get an idea of how much people are interacting, by average, which each page on the site.\nCaveat There\u0026rsquo;s a very important caveat to discuss before proceeding. To calculate Average Engagement Time, you\u0026rsquo;d want to calculate the total milliseconds spent engaged with the Pageviews for each page. Unfortunately, User Timings are not bound to a Pageview, as they exist as their own hit type. In other words, it\u0026rsquo;s impossible to request only specific User timings per specific Pageviews in a calculated metric or by using a custom report with report filters.\nSo, the only way the following will work is if this is the only User Timing you\u0026rsquo;re measuring, because then you can create a calculated metric which calculates the average of all User Timings per all Pageviews, giving you what we\u0026rsquo;re after.\nIf this isn\u0026rsquo;t possible in your case, you might want to pull both Pageview and User Timings data out of Google Analytics, and join them together in Google Sheets or a tool like Klipfolio.\nIf you have a good idea on how to circumvent this problem in Google Analytics, I\u0026rsquo;m all ears. Using the default Avg. User Timing metric will not work, as that\u0026rsquo;s bound to the size of the User Timing Sample, and not Pageviews. Using Custom Metrics might work, but I\u0026rsquo;m having a hard time figuring out how to align them with Pageviews.\n5. The Calculated Metric Calculated Metrics are cool. No, they\u0026rsquo;re awesome. They let you do calculations! On metrics! COOL AND AWESOME!\nBefore you proceed, don\u0026rsquo;t forget to checkout some of the amazing articles my friends have written about Calculated Metrics:\n  Analytics Pros - 25 Calculated Metrics For Google Analytics\n  Bounteous - New Calculated Metrics In Google Analytics\n  Avinash Kaushik - Excellent Analytics Tip #27: Chase Smart Calculated Metrics!\n  Peter O\u0026rsquo;Neill - A Powerful Use Case for GA Calculated Metrics\n  Yehoshua Coren - Google Analytics Custom Metrics And Calculated Metrics\n  Honestly, seems like anybody who\u0026rsquo;s a somebody in web analytics has written about this feature, and that speaks heaps about how useful calculated metrics are!\nSo, the metric we\u0026rsquo;ll use will calculate the average user timing by the simple formula of total User Timing value / total Pageviews. Since the Time formatting type expects seconds as the input value, we\u0026rsquo;ll need to divide the total user timings value by 1000 to convert milliseconds to the required format. Anyway, the metric setup looks like this:\n  To find this screen, go to your View Settings in GA Administration, and choose Calculated Metrics (BETA). Next, click the ominous, big, red button labelled + New Calculated Metric, and make sure the settings match with the screenshot above.\n6. The Custom Report Finally, let\u0026rsquo;s pull it all together in a Custom Report. You can see a screenshot of the report itself at the beginning of this post, so here\u0026rsquo;s what the report settings would look like:\n  I\u0026rsquo;ve got Bounce Rate and Entrances there as well, so that I can get an idea of how sessions with these Page Views actually performed in terms of bounces and initial acquisition, but what\u0026rsquo;s important is the juxtaposition of Page Views, Average Engagement, and Average Time On Page.\nSummary Looking at Average Engagement versus Average Time On Page is illuminating. ATOP tells us how much time it took from one page view to another, but it doesn\u0026rsquo;t tell us anything about the quality of that time. With Average Engagement, we\u0026rsquo;re measuring the time user interacted with the page, i.e. kept it in focus and did something.\nDoes this tell us anything about whether the user read the page or how they digested the contents? No, of course not. We\u0026rsquo;d need a probe in their brain to uncover intent like that. But this data increases the significance and the meaning of the Time On Page metric, by describing actual, interactive time rather than just arbitrary time between two pageviews. It\u0026rsquo;s a far more powerful indicator than any of the out-of-the-box metrics in Google Analytics, and a crucial cog in the machine you\u0026rsquo;re building, whose job is to slowly, painfully open the lid on the can of worms, with the huge label of \u0026ldquo;USER INTENT\u0026rdquo; plastered on the side.\nAgain, huge thanks to the guys at YLE and to Chartbeat for the interesting and pioneering work they\u0026rsquo;ve done. I hope the steps within this tutorial will help you in uncovering yet another interesting aspect of the users\u0026rsquo; browsing behavior on your site!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-endpoint-debugger/",
	"title": "Google Analytics Endpoint Debugger",
	"tags": ["debug", "google analytics", "measurement protocol"],
	"description": "Guide to using the Google Analytics collection endpoint debugger.",
	"content": "This is a really cool feature for Google Analytics data collection, of which I\u0026rsquo;ve heard very, very little buzz. It\u0026rsquo;s a way to debug any and all hits sent to the Google Analytics endpoint at https://www.google-analytics.com/collect.\n  In all simplicity, you just need to copy the entire URL of the HTTP request to your clipboard, paste it into a web browser, and add /debug between the hostname and /collect.\nA few words on the Measurement Protocol Let\u0026rsquo;s take a quick step back and remind ourselves of one important thing. Every single time you\u0026rsquo;re sending a hit to Google Analytics, whether it be via mobile app SDKs, analytics.js on your website, or by using arbitrary HTTP requests, you\u0026rsquo;re using the \u0026ldquo;Measurement Protocol\u0026quot;. In other words, MP is not a discrete, isolated method of data transfer, but rather the underlying protocol that all the SDKs and libraries use as the method of dispatching payload data to GA.\nSo, when you hear a discussion along the lines of:\nJANE: Let\u0026rsquo;s just configure the ga('send'...) commands on the page to incorporate that feature. JACK: Nah, let\u0026rsquo;s use Measurement Protocol instead.\nThey\u0026rsquo;re actually talking about the same thing. However, what Jack is probably referring to is a manually built HTTP request to the /collect endpoint, using any of the zillions of different ways to do so. When you use the ga('send'...) syntax, you\u0026rsquo;re communicating with the analytics.js library, and abstracting the HTTP request build process with a simplified shorthand syntax.\nYou can verify this by browsing to a Universal-Analytics-enabled website, opening the browser\u0026rsquo;s debugging tools, and looking at the network requests the site is sending:\n  Why is this significant? Why am I quibbling on semantics? Well, for one, I love to nitpick. But also, especially with the whole phenomenon of referral spam, people have been condemning Measurement Protocol as it introduces an open, unauthenticated venue for spamming a Google Analytics reporting profile.\nWell, the reason it\u0026rsquo;s open and unauthenticated is because your website relies on the same protocol. If you were to add a layer of authentication there, it would need to happen server-side, as a spammer would be able to just visit your site, copy the payload request, and spam it until the authentication expires. Also, authentication would add latency, and that would affect Real Time reporting as well as the time for data to enter your reports.\nBut I\u0026rsquo;m sidetracking. Let\u0026rsquo;s get back on topic.\nUsing the debugger As I mentioned, this hasn\u0026rsquo;t really been advertised, for some odd reason, but it is there. In fact, there\u0026rsquo;s even a support page for this feature.\nSo let\u0026rsquo;s go back to the example of the browser\u0026rsquo;s developer tool, and inspecting the network request to /collect. Copy the entire request URL (in Chrome it\u0026rsquo;s right-click =\u0026gt; Copy Link Address on the request), and paste it into a new browser window.\nIf you now press Enter, your browser will simply send the request to GA. However, before you press Enter, add /debug into the URL, between the hostname and /collect, so it looks like this:\n  And now press Enter. You should see the response in your web browser:\n  Since you\u0026rsquo;re copying a request sent by your website, you should hope it\u0026rsquo;s valid. If it isn\u0026rsquo;t, start working on a fix!\nWhy it\u0026rsquo;s useful Well, for one, you can debug your hits and see if there\u0026rsquo;s something wrong with them. Common mistakes include when you\u0026rsquo;ve got a field value wrong in your code, or when you\u0026rsquo;ve forgotten to add a field which is required. For example, setting Event Value to a blank string or a decimal number would return an error, as if the field is in the request, it always needs to be an integer.\nAnother good use case is for when you\u0026rsquo;re actually using the Measurement Protocol in your custom setups. The debugger returns a response object, meaning you can debug your setup without actually sending data to Google Analytics! By parsing the response, you can easily identify if your custom payload is working or not.\nI know I\u0026rsquo;m in the geek camp with this, but this feature is definitely packing a kiloton worth of awesome in a small space. This is a perfect way to test your setups without sending actual data to Google Analytics, and the response object has lots of information, all missing from the actual request to /collect.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/check-if-dom-ready-or-window-loaded-have-fired/",
	"title": "#GTMTips: Check If DOM Ready Or Window Loaded Have Fired",
	"tags": ["Google Tag Manager", "gtmtips", "triggers"],
	"description": "When firing your tags, check if DOM Ready or Window Loaded events have already been triggered in Google Tag Manager.",
	"content": "Every now and then I run into a problem which needs some creativity to find a fix. When choosing a course of action, I tend to land first on an extremely complicated solution. However, if I\u0026rsquo;m patient enough, I manage to whittle it down to something far more manageable and efficient.\nIn this #GTMTips post, I\u0026rsquo;ll show you one of these extremely simple solutions to a problem which you might normally overcomplicate. The solution is a very well hidden feature of Google Tag Manager, buried deep in the bowels of the google_tag_manager interface.\nTip 42: Check if DOM Ready or Window Loaded have triggered   This tip is very, very simple. Basically, there are two flags in the google_tag_manager interface, and they are activated first when the \u0026ldquo;DOM Ready\u0026rdquo; event takes place, and then when the \u0026ldquo;Window Loaded\u0026rdquo; event takes place.\nIf you didn\u0026rsquo;t have access to these flags, your remaining options would be something horrible like going through the dataLayer Array, looking for the gtm.dom and gtm.load, or creating your own listeners for the DOM and window load events. Both have timing issues in Google Tag Manager, and both add extra complexity to a situation which could already be solved using something that is native to Google Tag Manager.\nAnyway, you can use these interface flags to make sure your Triggers don\u0026rsquo;t fire before either one of the page load Triggers have activated. A typical use case would be that you have a slowly loading site, and you don\u0026rsquo;t want a Trigger to go off before the DOM has completely loaded or all the linked resources (e.g. jQuery) have been downloaded.\nThis is how it works:\nwindow[\u0026#39;google_tag_manager\u0026#39;].dataLayer.gtmDom; // true when DOM Ready has fired window[\u0026#39;google_tag_manager\u0026#39;].dataLayer.gtmLoad; // true when Window Loaded has fired  These are the global flags. To get the data into GTM, create two JavaScript Variables:\n  Now you can check if these Variables return true, and use that in the Triggers to establish some order into the chaos of the page load.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/ga-snippet-and-ga-gtm-tag-on-the-same-page/",
	"title": "GA Snippet And GA (GTM) Tag On The Same Page",
	"tags": ["google analytics", "Google Tag Manager", "snippet", "tracker"],
	"description": "Guide to what happens if the Google Analytics JavaScript snippet and Google Tag Manager&#39;s container snippet coexist on a web page.",
	"content": "In this article, I\u0026rsquo;m going to tackle one of the most frequently asked questions out there:\nCan you run Google Analytics using the snippet AND using a Google Tag Manager Tag on the same page?\n  There are many facets to this query, so I\u0026rsquo;ll try to tackle as many of them as I possibly can.\nFirst, a terminology rant. You hear lots of talk about \u0026ldquo;on-page\u0026rdquo; and \u0026ldquo;inline\u0026rdquo; Google Analytics tracking, as that\u0026rsquo;s what\u0026rsquo;s used to describe the non-GTM way of tracking Google Analytics. Well, all the power to you, but I think they\u0026rsquo;re just adding to the confusion. GTM is just as \u0026ldquo;on-page\u0026rdquo; and \u0026ldquo;inline\u0026rdquo; as GA\u0026rsquo;s own snippet, as both are based on injecting an asynchronous HTTP request that downloads the respective library.\nI\u0026rsquo;ve chosen to use snippet-based tracking to describe Google Analytics tracking that\u0026rsquo;s outside GTM, and Tag-based tracking to describe Tags run through GTM. Note the capital \u0026lsquo;T\u0026rsquo;, as that\u0026rsquo;s the syntax I most commonly use in this blog to denote Google Tag Manager assets.\nI\u0026rsquo;m fully aware this distinction can cause confusion as well, but since I\u0026rsquo;d hesitate to ever call the ga('send',...) commands on the page \u0026ldquo;tags\u0026rdquo; of any kind, I think this makes a bit more sense.\nThe premise: what the problem really is Most often this question gets asked when migrating to Google Tag Manager. There was a crazy amount of questions about this in the Google Tag Manager Fundamentals course forums, where I was volunteering my help. There were so many questions about it that I felt the need to write the following intro to course participants:\n  I don\u0026rsquo;t know if it helped, but at least it let me jot some things down that would be helpful to anyone pondering about these issues.\nThe problem itself is usually a manifestation of the following, underlying queries:\n  Will just adding Google Tag Manager to the site break my existing Google Analytics tracking?\n  Will adding a Google Analytics Tag in GTM break my existing Google Analytics tracking?\n  Can I run the bulk of my tracking via the Google Analytics snippet, and use Google Tag Manager Tags for tracking certain specific things?\n  Can I run the bulk of my tracking via Google Tag Manager, and use ga('send',...) commands on the page to cover some, usually legacy, tracking needs?\n  I\u0026rsquo;m just curious, how does snippet-based tracking differ from GTM\u0026rsquo;s Tag-based tracking?\n  The last question is just the best, I can\u0026rsquo;t wait to get to it!\nOK, I\u0026rsquo;ll start with it! That\u0026rsquo;s how crazy and unpredictable I am.\n5. Snippet-based vs. Tag-based tracking When you create your Google Analytics tracker using the given snippet, you have the option of providing a tracker name vs. using the default name.\nFor example, if you want to name the tracker SuperTracker, you\u0026rsquo;d use the following syntax in the tracker creation command:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;SuperTracker\u0026#39;});  This would give the tracker a name, after which you interact with it by prefixing all commands sent to the ga() queue with the tracker name: ga('SuperTracker.send', 'pageview);.\nIf you don\u0026rsquo;t provide a tracker name, you can use the naked ga('send',...) syntax, where you don\u0026rsquo;t use a tracker name prefix. However, the tracker does have a name: t0 (t-zero). So the following two commands do the same thing:\nga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); ga(\u0026#39;t0.send\u0026#39;, \u0026#39;pageview\u0026#39;);  This is clear, yes? The reason you\u0026rsquo;d want to rename the tracker is if you\u0026rsquo;re running multiple trackers on the page, or due to some very special edge cases I might or might not touch upon later in the article (I\u0026rsquo;m just making this stuff up as I go along).\nNow, with Google Tag Manager it\u0026rsquo;s a bit different. In GTM, whenever a Tag is injected, i.e. whenever a Tag \u0026ldquo;fires\u0026rdquo; (hate that terminology as well) due to some Trigger \u0026ldquo;firing\u0026rdquo; as well, a new tracker is created with a random, unique name:\n  So here the GTM tracker was created with the name gtm1454402957304. For you fellow nerds, look at the numbers in that string. Don\u0026rsquo;t they look vaguely familiar? That\u0026rsquo;s right, it resembles Unix time! So, in fact, the tracker name isn\u0026rsquo;t random, it\u0026rsquo;s just the exact timestamp of when the tracker was created, down to the millisecond. So it\u0026rsquo;s not deterministically unique, either, but having two Tags fire at the exact same millisecond is, as far as I know it, next to impossible.\n  So, now the major difference is spelled out for you: the GA snippet requires that you either use a fixed, default tracker name when creating the tracker, or you give it a custom name. This tracker name is then used when communicating with the tracker object.\nGTM, on the other hand, creates a new, unique tracker name for every single Tag on the page - even when the same Tag is injected multiple times.\nThis is, in fact, how GTM manages to navigate through one of the biggest flaws of the platform. I\u0026rsquo;ve outlined this in a previous article on the dangers of messing with the tracker name setting in GTM\u0026rsquo;s GA Tags. In a nutshell, GTM doesn\u0026rsquo;t let you assign settings, fields, custom dimensions and the ilk on the hit itself, but they\u0026rsquo;re always set on the tracker object. This means that they are \u0026ldquo;hit-scoped\u0026rdquo; by default, since each Tag has its own tracker, but if you mess with the tracker name you\u0026rsquo;ll run into some unexpected side effects.\nNow that we have the basics sorted out, let\u0026rsquo;s jump into the queries themselves.\n1. Will Google Analytics break if just Google Tag Manager is installed on the page? This is easy to answer:\nNO, IT WILL NOT!\nRemember, Google Tag Manager has nothing to do with Google Analytics when you add the container on the page. It\u0026rsquo;s just a container, a chassis, a receptacle, a vehicle, a vessel (making the most out of my thesaurus here) for your Tags and other arbitrary JavaScript you want to execute on the page.\nWhen you add the Google Tag Manager container snippet to the page, all it does is load a library from Google servers. It doesn\u0026rsquo;t create any Google Analytics trackers, it doesn\u0026rsquo;t mess with your tracking code, it doesn\u0026rsquo;t interfere with other JavaScript running on the site, or anything like that.\nIn other words, feel free to add the container snippet to the page the minute you\u0026rsquo;ve created the container.\n2. Will adding a Google Analytics Tag in GTM break my existing tracking? This is also easy to answer:\nNO, IT WILL NOT!\nNow, choose your favorite conjunct: \u0026ldquo;However\u0026hellip;\u0026rdquo; / \u0026ldquo;But\u0026hellip;\u0026rdquo; / \u0026ldquo;Unless\u0026hellip;\u0026rdquo;. I\u0026rsquo;ll go with the first one.\nHowever, if you\u0026rsquo;ve manually set the Google Tag Manager tracker name to the GA tracker name, and you don\u0026rsquo;t know what you\u0026rsquo;re doing, problems can arise.\nI\u0026rsquo;ve covered many of these cases in my tracker name rant.\nThe most important thing to note is that if the tracker name is shared between the snippet-based tracker and GTM\u0026rsquo;s Tag, each time a new tracker is created on the page (usually via GTM) with that name, any settings it has will overwrite all the previous settings on any of the trackers.\nSo, for example, let\u0026rsquo;s say you\u0026rsquo;ve created the snippet-based tracker with:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;);  And then you set the Tracker name in GTM to blank:\n  Now, since you only create the tracker once when using snippet-based GA tracking, any GTM Tag that shares the tracker name and fires after the snippet-based tracker will overwrite all the settings that were in the snippet-based tracker.\nSo, let\u0026rsquo;s say you\u0026rsquo;re tracking to UA-12345-1 in the snippet, using the ga('create','UA-12345-1') command to create the tracker. Then, you have an Event Tag in GTM, collecting data to \u0026lsquo;UA-12345-2\u0026rsquo;, and firing after 15 seconds of dwell time on the page. If this Tag has the same tracker name as the snippet-based tracker, any ga('send',...) commands you run on the page after this GTM Tag has fired will now use the settings in the GTM Tag, including the new property ID.\nYou can see how this is potentially a huge problem.\nBut like I wrote in the beginning of this chapter, if you don\u0026rsquo;t mess with the tracker name setting, there will be no interference. You can freely track via Google Tag Manager to a Google Analytics property, and then keep the snippet-based tracker collecting data to some other property, if you wish.\nThis is a great segue to the next question.\n3. Run the bulk of tracking via the snippet, and use GTM for specific things In this case, we want to combine snippet-based GA tracking with Google Tag Manager Tags.\nInteresting!\nWell, it\u0026rsquo;s certainly doable. You can easily have a snippet-based tracker track to UA-12345-1, and then add the occasional Tag via GTM to also track to UA-12345-1, and you don\u0026rsquo;t have to make any changes if working with default settings. Google Analytics uses a combination of Client ID found in the _ga cookie together with the UA-XXXXX-Y code to make sure hits are attributed to the same session and the same user.\nThere are some cases where you will need to make changes:\n  If you\u0026rsquo;re using custom cookie settings in either, make sure they match between the snippet-based tracker and all the Google Analytics Tags you\u0026rsquo;re running via Google Tag Manager into the same GA property.\n  If you\u0026rsquo;re using custom traffic source tracking, make sure you respect these in all the Tags and snippet commands sending data to the same Google Analytics property. For example, if you GA snippet is sending the ?utm-parameters with the page URL, but in GTM you\u0026rsquo;re overwriting the page name with something customized, it\u0026rsquo;s possible the GTM Tag will use the referrer instead, creating a new session since the traffic source changed.\n  All in all, try to make all fields and settings match between the trackers. This way you won\u0026rsquo;t run into surprises when one creates a session and the other one creates a new session immediately after due to a discrepancy in the tracker settings.\n4. Using GTM to drive tracking, and occasionally resorting to snippet-based commands This is a tricky one. Usually it\u0026rsquo;s because there\u0026rsquo;s a huge, sprawling legacy library for snippet-based tracking, and the user wants to migrate one command at a time.\nThe easiest way to make it work is to remember to have the Google Analytics snippet and tracker creation commands on each page as before. If you do this, there won\u0026rsquo;t be any conflicts as long as you respect the tips in the previous chapter.\nHowever, if you\u0026rsquo;re in a jam where you only have a ga('send',...) command here or there, without any tracker creation via the snippet, things get much more complicated. To make it work, the following things need to happen:\n  The ga('send',...) cannot be run before the first GTM Tag has been added to the page. Before that, the ga() function doesn\u0026rsquo;t exist in the global namespace.\n  The Google Tag Manager Tag needs to have its Tracker Name setting set to blank (if you\u0026rsquo;re using ga('send',...) on the page), or to a custom name, in which case the snippet-based commands need to use that name as well: ga('customName.send',...).\n  The first one is very difficult to respect, since the Google Tag Manager is downloaded asynchronously. To mitigate this, you can add this line to the \u0026lt;head\u0026gt; of each page, before any ga() functions are called:\n\u0026lt;script\u0026gt; window[\u0026#39;GoogleAnalyticsObject\u0026#39;] = \u0026#39;ga\u0026#39;; window[\u0026#39;ga\u0026#39;] = window[\u0026#39;ga\u0026#39;] || function() { (window[\u0026#39;ga\u0026#39;].q = window[\u0026#39;ga\u0026#39;].q || []).push(arguments) }; \u0026lt;/script\u0026gt; This is taken from the Universal Analytics snippet, and it basically creates a temporary data store for all ga() commands that exist before the analytics.js library has been created.\nIf you take all the previous tips into account, then there should be minimal problems with combining tracking methods like this. However, it\u0026rsquo;s still suboptimal, and can lead to unexpected problems due to the complexity of the code you need to manage and maintain.\nSummary What started as a simple foray into tracker discrepancies blossomed into a sprawling, overly complex account of how tracking works across the Google Analytics snippet and Google Tag Manager\u0026rsquo;s Tag templates.\nBottom line is, if you do want to combine tracking across the two methods, you need to make sure that tracker settings match across the board.\nOn the other hand, if you want to track each tracker method to a different Google Analytics property, you need to make sure that certain tracker settings do NOT match across the trackers.\nAll in all, you\u0026rsquo;ll commonly hear people suggesting that you move to Google Tag Manager in all your tracking needs. I tend to agree in general, but there are many, many solid reasons for keeping the snippet-based tracker, at least for a little while.\nWhenever I do a migration from snippet-based to Google Tag Manager -based tracking, I run the two simultaneously (to different properties) until I\u0026rsquo;m satisfied the migration has been a success.\nSimilarly, it\u0026rsquo;s not an altogether bad idea to keep the snippet-based tracker on the page for a while, just so you can benchmark the two different tracker setups in your data, and perhaps use the information to improve overall data quality.\n"
},
{
	"uri": "https://www.simoahava.com/tags/snippet/",
	"title": "snippet",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/be-careful-with-the-tracker-name-setting/",
	"title": "#GTMTips: Be Careful With The Tracker Name Setting",
	"tags": ["Google Tag Manager", "gtmtips", "tracker", "universal analytics"],
	"description": "A word of caution if you want to modify the Tracker Name setting in your Google Tag Manager tags.",
	"content": "Hello friends! Today I want to direct your attention to a dangerous setting found within the bowels of the Universal Analytics Tag template in Google Tag Manager. In fact, GTM itself highly discourages you from meddling with it:\n  I actually agree with this warning. It should be highly discouraged, as modifying the tracker name introduces a potential hazard to your tracking plan, unless you know what you\u0026rsquo;re doing.\nBut I\u0026rsquo;m getting ahead of myself, let\u0026rsquo;s take a step back and look at just what we\u0026rsquo;re doing here.\nTip 41: One tracker name to rule them all   By setting the Tracker Name field, you can have multiple Google Tag Manager Tags use the same tracker name (for various purposes), or you can combine your on-page, traditional analytics.js tracking with tracking done via GTM\u0026rsquo;s Tag templates.\nDavid Vallejo\u0026rsquo;s written a great guide for tracking single-page sites. This solution relies on setting a Tracker name across your tag templates, and I consider it to be the only solid way of fixing the \u0026ldquo;rogue referral\u0026rdquo; problem that single-page sites have with GTM.\n(Just for reference, the problem with rogue referrals is that after the initial landing page hit with the UTM and/or GCLID parameters, any referrer information in the HTTP request will override these campaign settings due to how Universal Analytics processes referrals. In other words, after the landing page, any virtual pageview that does not have these UTM and/or GCLID parameters will, by default, be sent with the referrer as the campaign source, potentially starting a new session).\nHowever, before dashing to your GTM container (why would you do that anyway?), pause for a moment. Google Tag Manager, by default, uses a random, unique tracker name with each instance of a Tag, even if they\u0026rsquo;re injected by the same Tag template. This means that, by default, no two Tags fired via GTM ever share the tracker name.\nThis, in turn, means that settings you set in one GTM Tag will never be inherited by any other Tags. Thus you can\u0026rsquo;t make use of settings that would apply to multiple Tags simply by modifying the fields in one. You will need to replicate the settings across the Tags.\nTo solve the inheritance problem, all you need to do is set the multiple Tags to use the same Tracker name. This way, the settings you set in one Tag will affect the settings in all the subsequent Tags that share the Tracker name! How simple and beautiful. To fix the \u0026ldquo;rogue referral\u0026rdquo; problem in AJAX sites, you\u0026rsquo;d set the same Tracker name for each Tag, since now they\u0026rsquo;ll share the \u0026ldquo;Document Location\u0026rdquo; setting, meaning a possible referral will not override any UTM Tags or AdWords GCLID settings in the URL itself.\nWonderful.\nHowever, and this is a very emphatic \u0026ldquo;however\u0026rdquo;, setting the Tracker name will share all settings across the Tags! Yes, that\u0026rsquo;s right, even hit-scoped Custom Dimensions and any fields you set for one Tag alone. In GTM, at the moment, there\u0026rsquo;s no way to differentiate between fields you set for a hit and fields you set for the tracker. Only the latter exists in GTM.\nNaturally, if you do not touch the Tracker name setting, these fields are, in essence, \u0026ldquo;per hit\u0026rdquo;, since each Tag has a unique Tracker, right? But when you set the Tracker name across multiple Tags, they will share all the field settings whether you want them to or not.\nThis leads to some embarrassing situations, and it requires quite a hacky workaround. For example, if you set the Tracker name in Tag 1, and you also add a hit-scoped Custom Dimension that you want to apply to that Tag alone, in your next Tag with the same Tracker name, you\u0026rsquo;ll need to add a null-returning Custom JavaScript Variable to the same Custom Dimension field to ensure it isn\u0026rsquo;t inherited from the first Tag:\nfunction() { return null; }  That\u0026rsquo;s just clumsy.\nSo, my point is this. When you\u0026rsquo;re setting the Tracker name across your Tags, whether by design or because you\u0026rsquo;ve read the tip online somewhere, make sure you audit the fields you set in the Tags. If any of them should exist in one Tag alone, you will need to manually clear the those fields in all the other Tags that use the same Tracker name.\nI hope, no, I beg that Google Tag Manager would let you decide whether to set fields only for the Tag rather than for the Tracker name. This would solve a lot of issues.\nAlternatively, I wish analytics.js would take care of the \u0026ldquo;rogue referral\u0026rdquo; problem natively, making sure subsequent page views after the landing page (with the UTM parameters) would not utilize the HTTP referrer to override the currently active source/medium.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/data-layer-variable-versions-explained/",
	"title": "#GTMTips: Data Layer Variable Versions Explained",
	"tags": ["data layer", "data model", "Google Tag Manager", "gtmtips"],
	"description": "A guide to how the two versions (1 and 2) of Google Tag Manager&#39;s Data Layer variable work, and how they are different from each other.",
	"content": "Google Tag Manager\u0026rsquo;s Data Layer is something I\u0026rsquo;ve touched upon in pretty much all of my articles. It\u0026rsquo;s such an integral part of what makes a tag management solution great and applicable to a host of business scenarios. I\u0026rsquo;ve also talked at length about the internal data model of Google Tag Manager, and this #GTMTips post is very much related to this rather murky concept.\nIn this post, we\u0026rsquo;ll go over the Data Layer Variable Version selection, and I\u0026rsquo;ll try to explain just what this selector does.\nTip 40: The two faces of the Data Layer Variable   Let\u0026rsquo;s start by taking a step back. As a technical construct, the \u0026ldquo;Data Layer\u0026rdquo; in GTM is a JavaScript Array, which usually comprises a number of plain objects that have been pushed into its innards on a web page.\n  As you\u0026rsquo;ve learned from my data model article, GTM doesn\u0026rsquo;t actually access this Array when you create your Data Layer Variables. Instead, when an object is pushed into dataLayer, it\u0026rsquo;s made available to the data model of Google Tag Manager, and any Data Layer Variables you create will refer to these instances in the data model rather than directly accessing the Array itself.\nWhy? To decouple the Array from the internal mechanisms of the connected platform (GTM in this case). This is quite important, as the less GTM needs to rely on the abstract structure of dataLayer, which could be manipulated at any time by either sloppy code or another connected platform, the better.\nNow, when you select the Version of the Data Layer Variable, you\u0026rsquo;re instructing GTM to treat the values in the data model in two different ways.\nVersion 1 Version 1 of the Data Layer Variable has a very limited reach. Basically, the key-value pair that is pushed into dataLayer needs to be in the root of the object, as Version 1 doesn\u0026rsquo;t let you access nested keys.\nHere\u0026rsquo;s an example of what would happen with a Version 1 Data Layer Variable that points to key name 'product.price':\n// Works because the dots are in the key name, and the key is in the root: window.dataLayer.push({ \u0026#39;product.price\u0026#39; : \u0026#39;11.99\u0026#39; }); // Won\u0026#39;t work because \u0026#39;price\u0026#39; is nested in the \u0026#39;product\u0026#39; object: window.dataLayer.push({ \u0026#39;product\u0026#39; : { \u0026#39;price\u0026#39; : \u0026#39;11.99\u0026#39; } });  You could, though, access the nested object indirectly with a Version 1 Data Layer Variable by creating the variable for the key 'product', and then manipulating the returned object in a Custom HTML Tag or a Custom JavaScript Variable.\nAnd\u0026hellip;yeah. That\u0026rsquo;s all that Version 1 does. There\u0026rsquo;s no merging, no special data model functions, no nothing that you can use to further manipulate the retrieved object. So let\u0026rsquo;s take a look at what happens when I push two different values for the same key ('product'):\nwindow.dataLayer.push({ \u0026#39;product\u0026#39; : { \u0026#39;price\u0026#39; : \u0026#39;11.99\u0026#39; } }); window.dataLayer.push({ \u0026#39;product\u0026#39; : { \u0026#39;name\u0026#39; : \u0026#39;Tim Duncan Fan Club Membership\u0026#39; } });  In the example above, if I had my Version 1 Data Layer Variable pointing to 'product', it would return a plain object with {'price' : '11.99'} after the first push, and a plain object with {'name' : 'Tim Duncan Fan Club Membership'} after the second.\nIn essence, any information in the first push is completely overwritten (for this particular Data Layer Variable) by the second push with the same key name ('product').\nVersion 2 If you want more flexibility, use Version 2 of the Data Layer Variable. This treats objects more like you\u0026rsquo;d expect if you\u0026rsquo;re familiar with how JavaScript works.\nInstead of bulldozing over existing keys, Version 2 first checks whether a key with that name already exists in the data model. If it does, it recursively merges information in the new push with information in the existing object.\nLet\u0026rsquo;s see an example:\nwindow.dataLayer.push({ \u0026#39;product\u0026#39; : { \u0026#39;price\u0026#39; : \u0026#39;119.99\u0026#39;, \u0026#39;category\u0026#39; : \u0026#39;Membership\u0026#39; } }); window.dataLayer.push({ \u0026#39;product\u0026#39; : { \u0026#39;price\u0026#39; : \u0026#39;1199.99\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;Tim Duncan Fan Club Membership\u0026#39; } });  So, let\u0026rsquo;s say you\u0026rsquo;re using Version 2 of the Data Layer Variable, and you\u0026rsquo;re yet again pointing to the key with name 'product'.\nAfter the first dataLayer.push(), the variable would return a plain object with:\n{ \u0026#39;price\u0026#39; : \u0026#39;119.99\u0026#39;, \u0026#39;category\u0026#39; : \u0026#39;Membership\u0026#39; }  After the second push, the contents would be:\n{ \u0026#39;price\u0026#39; : \u0026#39;1199.99\u0026#39;, \u0026#39;category\u0026#39; : \u0026#39;Membership\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;Tim Duncan Fan Club Membership\u0026#39; }  Can you see what happened here? Another object with the same key name ('product') is pushed. Version 2 of the Data Layer Variable finds an instance of this key within the data model. Then, it recursively checks each key within this object to see if there are conflicts.\nFirst, it finds that 'price' is in both objects, so there\u0026rsquo;s a clash. Next, it checks if 'price' is also a plain object or an Array to see if it should perform yet another merge deeper in the structure. However, the value of 'price' is a String, which means that no recursive merge is performed, and the new value overwrites the old one.\nNext, it sees that 'category' does not have any conflicts, so it leaves it be in the object. Finally, it finds a completely new key in the second push, 'name', and it merges this key into the existing object in the data model.\nSo using Version 2 of the Data Layer Variable, resolution of conflicts in object key names is first attempted via a merge of the two objects, but if there are new \u0026ldquo;primitive\u0026rdquo; values for any pre-existing keys, their values are overwritten just as they would be with Version 1 of the Data Layer Variable.\nSo what? This information could lead to a number of conclusions, but I\u0026rsquo;m guessing many have the following two questions pop into their minds:\n  Why does the Version 1 still exist? Its numbering alone would mean that it\u0026rsquo;s inferior.\n  What on earth do I need recursive merge for?\n  One of the best use cases for Version 1 is with Enhanced Ecommerce. If you\u0026rsquo;re using the Custom JavaScript Variable method, which I seem to use almost exclusively, you probably want to use Version 1 of the Data Layer when interacting with the 'ecommerce' key. This way, GTM will only access the most recent push into dataLayer. And why is this significant? Take a look at the example below:\nwindow.dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;impressions\u0026#39; : [{ ... impressions ... }] } }); // Later on the same page window.dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { ... product detail view ... } } }); // And yet later on the same page window.dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { ... add to cart action ... } } });  If you used Version 2 of the Data Layer Variable to send the Enhanced Ecommerce payload using the Custom JavaScript Variable method, you\u0026rsquo;ll end up with three Impression payloads, two Product Detail Views, and one Add to Cart action. This is because of the recursive merge of the 'ecommerce' object! I\u0026rsquo;m guessing your intent is to only send each payload once.\nSo, if you instead use Version 1 of the Data Layer Variable to access the 'ecommerce' object, GTM will only access the payload that was most recently pushed into dataLayer. This way you\u0026rsquo;ll end up avoiding the payload multiplication problem.\nAs for the second point of what recursive merge is good for, there\u0026rsquo;s a whole universe of possibilities when it comes to merging of objects instead of overwriting previous instances. Examples include:\n  Updating an object as more information is revealed, e.g. collecting a history of interactions on a single page.\n  Updating an object as more information becomes available, e.g. updating a \u0026ldquo;user\u0026rdquo; object as certain API calls complete.\n  Making a valid merge of two Enhanced Ecommerce payloads, such as combining 'impressions' with an 'add' object.\n  There are a host of other things you can do with Version 2 of the Data Layer Variable, and those are all detailed in my data model article.\nI hope this has been illuminating, and I hope it helps you understand the rationale of having both versions available for the Data Layer Variable.\n"
},
{
	"uri": "https://www.simoahava.com/tags/data-layer/",
	"title": "data layer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/cookie-settings-and-subdomain-tracking-in-universal-analytics/",
	"title": "Cookie Settings And Subdomain Tracking In Universal Analytics",
	"tags": ["cookiedomain", "cookies", "google analytics", "Google Tag Manager", "universal analytics"],
	"description": "How the cookieDomain settings works in your Google Analytics tags, and what happens if you don&#39;t set it.",
	"content": "Welcome back my friends (to the show that never ends)! It\u0026rsquo;s been a couple of weeks since my last barrage of articles, and I think the time is ripe to do some testing!\nFirst things first, here\u0026rsquo;s a picture of me shovelling snow:\n  And now back to the topic at hand.\nOne of the things that seems to be a hot topic in Universal Analytics is cross-domain tracking. I\u0026rsquo;ve never really tackled the beast head-on, since there\u0026rsquo;s such a wealth of excellent articles about it out there. However, I have taken a plunge into the deep end with some specific stuff, such as iframe and subdomain tracking.\nI want to pick up where I left off in the subdomain article, and focus on something really cool but complicated: cookie settings.\nFirst, take a look at the official developer guide for Universal Analytics\u0026rsquo; cookie stuff. It\u0026rsquo;s an excellent resource to get started with.\nWhat I want to talk about is what are the implications of inconsistent field settings as well as leaving the cookieDomain field out entirely from your tracker settings.\n  I give you\u0026hellip;drumroll\u0026hellip;my epic, sugar-rush, pigeon-army, mudslide, Iron Man, lollapalooza, Han-shot-first, cookie-testing-extravaganza-MIND-EXPLOSION!\nSorry.\nWhat we know Let\u0026rsquo;s start at the beginning (always a good idea). This is what we, most likely, know about Universal Analytics\u0026rsquo; cookies.\n  Universal Analytics uses a single _ga cookie to establish the user dimension.\n  This cookie stores a unique Client ID in its last two segments (GA1.2.12345.23456).\n  By default, i.e. without any settings, the cookie is written on the current subdomain and all even more nested subdomains thereof (.sub.domain.com, .www.sub.domain.com, .simo.rules.ok.sub.domain.com would all share the cookie written on .sub.domain.com, but www.domain.com or domain.com would not).\n  By setting cookieDomain to auto, or manually setting it to the topmost possible domain name (i.e. .domain.com), all subdomains can use the cookie.\n  I hope that\u0026rsquo;s all clear.\nThis is why the \u0026ldquo;tracking snippet\u0026rdquo; for Universal Analytics always has the recommendation of including 'auto' as the third parameter of the 'create' command. That\u0026rsquo;s the equivalent of adding the field cookieDomain : 'auto' into the tracker object:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, \u0026#39;auto\u0026#39;); // is the same as ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;cookieDomain\u0026#39; : \u0026#39;auto\u0026#39;});  NOTE! The Universal Analytics Tag in Google Tag Manager does not add this field by default! The implications of leaving this field out are the focus of this test, so bear with me.\nThe test I want to test the following:\n  What happens if 'cookieDomain' : 'auto' is missing from your tracker, and you move across subdomains?\n  What happens if there are multiple _ga cookies, each written on different domains\n  What is the air-speed velocity of an unladen swallow?\n  Let\u0026rsquo;s start with the first one.\n1. No 'cookieDomain' : 'auto' set I\u0026rsquo;ll browse to test.simoahava.com, make sure there are no pre-existing _ga cookies, and type in the following command:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;test1\u0026#39;});  So, no cookieDomain anywhere. This results in a new _ga cookie, written on .test.simoahava.com:\n  You can also see where the cookie is written on:\n  That\u0026rsquo;s all clear, right?\nNext, I browse to www.simoahava.com, and run the same command:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;test1\u0026#39;});  And this is what I\u0026rsquo;ll see:\n  The cookie itself is set on the parent domain (without www.):\n  Now, let\u0026rsquo;s go back to test.simoahava.com and see what happens:\n  This is expected, of course. We see both the cookie written on the subdomain in the first step, as well as the cookie written on the main domain in the second step. The latter cookie is available here as well, as since the cookie was written on the parent domain, it can be used by all subdomains as well.\nThis has one very important implication. If you don\u0026rsquo;t have 'cookieDomain' : 'auto', and the traffic is from the parent domain to subdomains, there\u0026rsquo;s no issue here. Two different _ga cookies will still exist on the subdomain, as they have different domain settings, but they will share the same Client ID, thanks to one of the core features of the Universal Analytics library, which copies the Client ID from both pre-existing _ga cookies as well as legacy __utma cookies. Here\u0026rsquo;s an example where I first landed on the parent domain and then entered the subdomain:\n  However, if the traffic lands on the subdomain first, and then moves to the parent domain, you will end up with different _ga cookies and different Client IDs on both domains, and thus the user will be different as well!\nThis is quite significant. I\u0026rsquo;ll return to the implications later.\n2. What happens if there ARE multiple _ga cookies? Now, if you DO end up with multiple _ga cookies, two things can happen:\n  They share the same Client ID, i.e. the movement was from higher-level-domain to subdomain, and all is fine.\n  They have different Client IDs, i.e. the traffic was from the subdomain to a higher-level (or same-level) domain.\n  The first case is just fine. All your Tags will share the same Client ID, you won\u0026rsquo;t have problems with some Tags firing starting new sessions or creating new users, and you can sleep soundly at night.\nThe second case is problematic. Let\u0026rsquo;s say you DO end up with two different _ga cookies, and that it\u0026rsquo;s, at least you claim it is, intentional. How do you choose which cookie you\u0026rsquo;ll interact with your trackers?\nWell, actually, it\u0026rsquo;s pretty straight-forward. When you create a tracker, it will use which ever _ga cookie shares the cookieDomain setting of the tracker.\nAllow me to explain.\nLet\u0026rsquo;s say you now have two different _ga cookies, one written on .simoahava.com, and one written on .test.simoahava.com. You are now on test.simoahava.com, and you want to use the Client ID of the cookie written on this subdomain. You need to create a tracker, which would write the cookie on .test.simoahava.com if you want to interact with the cookie. In other words:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;test1\u0026#39;});  This would work, because without 'cookieDomain' : 'auto', the cookie would be written on .test.simoahava.com. Now, since a _ga cookie already exists on .test.simoahava.com, it is used in all interactions with tracker named test1.\nIf you want to interact with the cookie written on .simoahava.com, then you need to add the cookieDomain parameter, and make sure it would write the cookie on the top-most possible domain:\nga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;test2\u0026#39;, \u0026#39;cookieDomain\u0026#39; : \u0026#39;auto\u0026#39;}); // OR ga(\u0026#39;create\u0026#39;, \u0026#39;UA-12345-1\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;test2\u0026#39;, \u0026#39;cookieDomain\u0026#39; : \u0026#39;.simoahava.com\u0026#39;});  Both would do the same thing: attempt to write the _ga cookie on .simoahava.com. However, since that cookie already exists, the tracker named test2 can now be used to interact with that particular cookie.\nPhew! So much talk about cookies it makes my stomach growl.\nImplications Now, let\u0026rsquo;s talk about what this all means. I\u0026rsquo;ll split this up into a little Q\u0026amp;A, which might help interpret the results.\nQ: Should I set 'cookieDomain' : 'auto' on in all my Tags and trackers?\nA: I would say that, by default, yes. Having the setting on in all Tags ensures that the same Client ID is used everywhere across your subdomains. Even if you have Tags collecting data to different UA-XXXXX-Y properties, it doesn\u0026rsquo;t matter that they share the Client ID, since it\u0026rsquo;s just a label each property uses to assign hits to the same user.\nQ: What would happen if only one of my Tags has 'cookieDomain' : 'auto', but the others don\u0026rsquo;t?\nA: You should still be fine, as long as that one Tag fires on every single page. Basically, when a Tag fires on the page, it first checks for a pre-existing _ga cookie. If one is found, it then checks the domain on which the cookie is written, and whether or not this matches the tracker settings. If the settings match, then the Tag simply uses this cookie. If the settings don\u0026rsquo;t match, a new _ga is written on the current domain, but the Client ID is still copied from the first cookie.\nThus, as long as one Tag on the page is fired with 'cookieDomain' : 'auto', the Client ID copying mechanism ensures that the same Client ID is accessible across your domains.\nQ: What happens if I don\u0026rsquo;t have the 'cookieDomain' : 'auto' setting anywhere?\nA: In the best case scenario, nothing. This would require that traffic is always from a higher-level-domain to its subdomain. This way the subdomain can copy the Client ID from the higher-level-domain cookie.\nHowever, if traffic can be the other way around, so starting from deeper in the domain structure and moving up (or parallel), you\u0026rsquo;re in trouble. You\u0026rsquo;ll end up with different Client IDs since the cookies are not shared across the domains.\nI hate doling out \u0026ldquo;best practices\u0026rdquo; since I don\u0026rsquo;t believe they exist, but in this case we\u0026rsquo;re talking about a technological reality. I strongly recommend that you always add the 'cookieDomain' : 'auto' field to all your Tags and trackers. If that\u0026rsquo;s a stretch, then make sure that at least one Tag which is guaranteed to fire on all pages has that setting. This way the Client ID will be copied and passed across the domains.\nNext time I\u0026rsquo;ll write something lighter, I promise!\n"
},
{
	"uri": "https://www.simoahava.com/tags/cookiedomain/",
	"title": "cookiedomain",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/dynamic-content/",
	"title": "dynamic content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-dynamically-loaded-youtube-videos-in-google-tag-manager/",
	"title": "Track Dynamically Loaded YouTube Videos In Google Tag Manager",
	"tags": ["dynamic content", "Google Tag Manager", "videos", "youtube"],
	"description": "How to track embedded YouTube videos using Google Tag Manager, when the videos are loaded dynamically after initial page load.",
	"content": "Tracking YouTube videos in Google Tag Manager is one of the more useful things you can do in terms of tracking. YouTube has a wonderful API that you can tap into and convert the detected events into dataLayer messages.\nThere are some really good solutions out there for tracking YouTube videos in GTM:\n  The wonderful solution by Cardinal Path.\n  An even more thorough treatment by Bounteous.\n  Both do a great job of tracking videos that have been loaded with the page. However, both have difficulties with tracking dynamically loaded videos. That means videos which are loaded lazily, or in pop-ups, or when some content link is clicked.\n  In this article, I\u0026rsquo;m going to show how you can track dynamically loaded videos using Bounteous\u0026rsquo; solution with slight modifications. Bounteous is my favorite group of geeks in North America, and they bribed me with their awesome book just before Christmas, which would already be reason enough to use their code. However, their solution is also all kinds of robust, and it really lends itself to tracking dynamic stuff as well.\nWhat you\u0026rsquo;ll need The modifications will be done to the JavaScript code in the Custom HTML Tag. In other words, you can\u0026rsquo;t use Bounteous\u0026rsquo; CDN for this. You will have to create a Custom HTML Tag, and copy-paste the code in the next chapter within.\nAs for the rest of the stuff, just follow Bounteous\u0026rsquo; guide. All this does is add a method you can trigger when videos have been loaded dynamically. Don\u0026rsquo;t worry, I\u0026rsquo;ll explain the method later on as well.\nThe modified Custom HTML Tag So, follow the Bounteous guide, avoiding the CDN, and when you come to the chapter titled Google Tag Manager Installation, read the following instead.\nCreate a new Custom HTML Tag, and give it some cool name like Utility - Video Tracking Bomb Overload Christmas Awesome.\nSorry for that.\nNext, add the following code.\n\u0026lt;script\u0026gt; // Respectfully copied and modified from // https://github.com/lunametrics/youtube-google-analytics // // Original implementation by Bounteous var ytTracker = (function( document, window, config ) { \u0026#39;use strict\u0026#39;; window.onYouTubeIframeAPIReady = (function() { var cached = window.onYouTubeIframeAPIReady; return function() { if( cached ) { cached.apply(this, arguments); } // This script won\u0026#39;t work on IE 6 or 7, so we bail at this point if we detect that UA  if( !navigator.userAgent.match( /MSIE [67]\\./gi ) ) { init(); } }; })(); var _config = config || {}; var forceSyntax = _config.forceSyntax || 0; var dataLayerName = _config.dataLayerName || \u0026#39;dataLayer\u0026#39;; // Default configuration for events  var eventsFired = { \u0026#39;Play\u0026#39; : true, \u0026#39;Pause\u0026#39; : true, \u0026#39;Watch to End\u0026#39;: true }; // Overwrites defaults with customizations, if any  var key; for( key in _config.events ) { if( _config.events.hasOwnProperty( key ) ) { eventsFired[ key ] = _config.events[ key ]; } } //*****//  // DO NOT EDIT ANYTHING BELOW THIS LINE EXCEPT CONFIG AT THE BOTTOM  //*****//  // Invoked by the YouTube API when it\u0026#39;s ready  function init() { var iframes = document.getElementsByTagName( \u0026#39;iframe\u0026#39; ); var embeds = document.getElementsByTagName( \u0026#39;embed\u0026#39; ); digestPotentialVideos( iframes ); digestPotentialVideos( embeds ); } var tag = document.createElement( \u0026#39;script\u0026#39; ); tag.src = \u0026#39;//www.youtube.com/iframe_api\u0026#39;; var firstScriptTag = document.getElementsByTagName( \u0026#39;script\u0026#39; )[0]; firstScriptTag.parentNode.insertBefore( tag, firstScriptTag ); // Take our videos and turn them into trackable videos with events  function digestPotentialVideos( potentialVideos ) { var i; for( i = 0; i \u0026lt; potentialVideos.length; i++ ) { var isYouTubeVideo = checkIfYouTubeVideo( potentialVideos[ i ] ); if( isYouTubeVideo ) { var normalizedYouTubeIframe = normalizeYouTubeIframe( potentialVideos[ i ] ); addYouTubeEvents( normalizedYouTubeIframe ); } } } // Determine if the element is a YouTube video or not  function checkIfYouTubeVideo( potentialYouTubeVideo ) { // Exclude already decorated videos  if (potentialYouTubeVideo.getAttribute(\u0026#39;data-gtm-yt\u0026#39;)) { return false; } var potentialYouTubeVideoSrc = potentialYouTubeVideo.src || \u0026#39;\u0026#39;; if( potentialYouTubeVideoSrc.indexOf( \u0026#39;youtube.com/embed/\u0026#39; ) \u0026gt; -1 || potentialYouTubeVideoSrc.indexOf( \u0026#39;youtube.com/v/\u0026#39; ) \u0026gt; -1 ) { return true; } return false; } // Turn embed objects into iframe objects and ensure they have the right parameters  function normalizeYouTubeIframe( youTubeVideo ) { var a = document.createElement( \u0026#39;a\u0026#39; ); a.href = youTubeVideo.src; a.hostname = \u0026#39;www.youtube.com\u0026#39;; a.protocol = document.location.protocol; var tmpPathname = a.pathname.charAt( 0 ) === \u0026#39;/\u0026#39; ? a.pathname : \u0026#39;/\u0026#39; + a.pathname; // IE10 shim  // For security reasons, YouTube wants an origin parameter set that matches our hostname  var origin = window.location.protocol + \u0026#39;%2F%2F\u0026#39; + window.location.hostname + ( window.location.port ? \u0026#39;:\u0026#39; + window.location.port : \u0026#39;\u0026#39; ); if( a.search.indexOf( \u0026#39;enablejsapi\u0026#39; ) === -1 ) { a.search = ( a.search.length \u0026gt; 0 ? a.search + \u0026#39;\u0026amp;\u0026#39; : \u0026#39;\u0026#39; ) + \u0026#39;enablejsapi=1\u0026#39;; } // Don\u0026#39;t set if testing locally  if( a.search.indexOf( \u0026#39;origin\u0026#39; ) === -1 \u0026amp;\u0026amp; window.location.hostname.indexOf( \u0026#39;localhost\u0026#39; ) === -1 ) { a.search = a.search + \u0026#39;\u0026amp;origin=\u0026#39; + origin; } if( youTubeVideo.type === \u0026#39;application/x-shockwave-flash\u0026#39; ) { var newIframe = document.createElement( \u0026#39;iframe\u0026#39; ); newIframe.height = youTubeVideo.height; newIframe.width = youTubeVideo.width; tmpPathname = tmpPathname.replace(\u0026#39;/v/\u0026#39;, \u0026#39;/embed/\u0026#39;); youTubeVideo.parentNode.parentNode.replaceChild( newIframe, youTubeVideo.parentNode ); youTubeVideo = newIframe; } a.pathname = tmpPathname; if(youTubeVideo.src !== a.href + a.hash) { youTubeVideo.src = a.href + a.hash; } youTubeVideo.setAttribute(\u0026#39;data-gtm-yt\u0026#39;, \u0026#39;true\u0026#39;); return youTubeVideo; } // Add event handlers for events emitted by the YouTube API  function addYouTubeEvents( youTubeIframe ) { youTubeIframe.pauseFlag = false; new YT.Player( youTubeIframe, { events: { onStateChange: function( evt ) { onStateChangeHandler( evt, youTubeIframe ); } } } ); } // Returns key/value pairs of percentages: number of seconds to achieve  function getMarks(duration) { var marks = {}; // For full support, we\u0026#39;re handling Watch to End with percentage viewed  if (_config.events[ \u0026#39;Watch to End\u0026#39; ] ) { marks[ \u0026#39;Watch to End\u0026#39; ] = duration * 99 / 100; } if( _config.percentageTracking ) { var points = []; var i; if( _config.percentageTracking.each ) { points = points.concat( _config.percentageTracking.each ); } if( _config.percentageTracking.every ) { var every = parseInt( _config.percentageTracking.every, 10 ); var num = 100 / every; for( i = 1; i \u0026lt; num; i++ ) { points.push(i * every); } } for(i = 0; i \u0026lt; points.length; i++) { var _point = points[i]; var _mark = _point + \u0026#39;%\u0026#39;; var _time = duration * _point / 100; marks[_mark] = Math.floor( _time ); } } return marks; } function checkCompletion(player, marks, videoId) { var duration = player.getDuration(); var currentTime = player.getCurrentTime(); var playbackRate = player.getPlaybackRate(); player[videoId] = player[videoId] || {}; var key; for( key in marks ) { if( marks[key] \u0026lt;= currentTime \u0026amp;\u0026amp; !player[videoId][key] ) { player[videoId][key] = true; fireAnalyticsEvent( videoId, key ); } } } // Event handler for events emitted from the YouTube API  function onStateChangeHandler( evt, youTubeIframe ) { var stateIndex = evt.data; var player = evt.target; var targetVideoUrl = player.getVideoUrl(); var targetVideoId = targetVideoUrl.match( /[?\u0026amp;]v=([^\u0026amp;#]*)/ )[ 1 ]; // Extract the ID  var playerState = player.getPlayerState(); var duration = player.getDuration(); var marks = getMarks(duration); var playerStatesIndex = { \u0026#39;1\u0026#39; : \u0026#39;Play\u0026#39;, \u0026#39;2\u0026#39; : \u0026#39;Pause\u0026#39; }; var state = playerStatesIndex[ stateIndex ]; youTubeIframe.playTracker = youTubeIframe.playTracker || {}; if( playerState === 1 \u0026amp;\u0026amp; !youTubeIframe.timer ) { clearInterval(youTubeIframe.timer); youTubeIframe.timer = setInterval(function() { // Check every second to see if we\u0026#39;ve hit any of our percentage viewed marks  checkCompletion(player, marks, youTubeIframe.videoId); }, 1000); } else { clearInterval(youTubeIframe.timer); youTubeIframe.timer = false; } // Playlist edge-case handler  if( stateIndex === 1 ) { youTubeIframe.playTracker[ targetVideoId ] = true; youTubeIframe.videoId = targetVideoId; youTubeIframe.pauseFlag = false; } if( !youTubeIframe.playTracker[ youTubeIframe.videoId ] ) { // This video hasn\u0026#39;t started yet, so this is spam  return false; } if( stateIndex === 2 ) { if( !youTubeIframe.pauseFlag ) { youTubeIframe.pauseFlag = true; } else { // We don\u0026#39;t want to fire consecutive pause events  return false; } } // If we\u0026#39;re meant to track this event, fire it  if( eventsFired[ state ] ) { fireAnalyticsEvent( youTubeIframe.videoId, state ); } } // Fire an event to Google Analytics or Google Tag Manager  function fireAnalyticsEvent( videoId, state ) { var videoUrl = \u0026#39;https://www.youtube.com/watch?v=\u0026#39; + videoId; var _ga = window.GoogleAnalyticsObject; if( typeof window[ dataLayerName ] !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; !_config.forceSyntax ) { window[ dataLayerName ].push( { \u0026#39;event\u0026#39; : \u0026#39;youTubeTrack\u0026#39;, \u0026#39;attributes\u0026#39;: { \u0026#39;videoUrl\u0026#39;: videoUrl, \u0026#39;videoAction\u0026#39;: state } } ); } else if( typeof window[ _ga ] === \u0026#39;function\u0026#39; \u0026amp;\u0026amp; typeof window[ _ga ].getAll === \u0026#39;function\u0026#39; \u0026amp;\u0026amp; _config.forceSyntax !== 2 ) { window[ _ga ]( \u0026#39;send\u0026#39;, \u0026#39;event\u0026#39;, \u0026#39;Videos\u0026#39;, state, videoUrl ); } else if( typeof window._gaq !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; forceSyntax !== 1 ) { window._gaq.push( [ \u0026#39;_trackEvent\u0026#39;, \u0026#39;Videos\u0026#39;, state, videoUrl ] ); } } return { init : init, digestPotentialVideos : digestPotentialVideos } })(document, window, { \u0026#39;events\u0026#39;: { \u0026#39;Play\u0026#39;: true, \u0026#39;Pause\u0026#39;: true, \u0026#39;Watch to End\u0026#39;: true }, \u0026#39;percentageTracking\u0026#39;: { \u0026#39;every\u0026#39;: 25, \u0026#39;each\u0026#39;: [ 10, 90 ] } }); /* * Configuration Details * * @property events object * Defines which events emitted by YouTube API * will be turned into Google Analytics or GTM events * * @property percentageTracking object * Object with configurations for percentage viewed events * * @property each array * Fires an event once each percentage ahs been reached * * @property every number * Fires an event for every n% viewed * * @property forceSyntax int 0, 1, or 2 * Forces script to use Classic (2) or Universal(1) * * @property dataLayerName string * Tells script to use custom dataLayer name instead of default */ \u0026lt;/script\u0026gt; Yes, that is a LOT of stuff. Bear with me, though.\nThe modifications to Bounteous\u0026rsquo; solution are few but significant.\nFirst of all, instead of using an anonymous function, we\u0026rsquo;re actually reserving a slot in the global namespace, and exposing the function in a variable named ytTracker.\nWe do this because we want to invoke certain methods in the setup after the initial execution. If we didn\u0026rsquo;t expose a public method for it, we\u0026rsquo;d need to run the whole thing again and again, each time a video is dynamically loaded, and that\u0026rsquo;s just a huge strain on performance.\nNext thing we\u0026rsquo;re doing is adding a single line into the normalizeYouTubeIframe() method:\nfunction normalizeYouTubeIframe( youTubeVideo ) { ... youTubeVideo.setAttribute(\u0026#39;data-gtm-yt\u0026#39;, \u0026#39;true\u0026#39;); // \u0026lt;--- NEW  return youTubeVideo; }  The youTubeVideo.setAttribute() command is used to add a data attribute to all iframes which have already been treated by the script. This is because we want to avoid running the initialization methods again and again for videos which have already been configured for tracking. In my testing, if a video was tracked multiple times it led to runtime conflicts.\nNow that the data attribute is there, we need to check for its existence. In the checkIfYouTubeVideo() method, we\u0026rsquo;ll add the check like this:\nfunction checkIfYouTubeVideo( potentialYouTubeVideo ) { if (potentialYouTubeVideo.getAttribute(\u0026#39;data-gtm-yt\u0026#39;)) { return false; } ... }  This check just looks for the data-gtm-yt attribute we added in the previous step, and if it\u0026rsquo;s found, the initialization is skipped for this particular video.\nThis way only videos which have not been treated yet will be processed.\nFinally, we need to expose two methods in the ytTracker interface. These will let us handle dynamically added videos. Lets do that in the very end of the function expression.\nvar ytTracker = (function(...) { ... return { init : init, digestPotentialVideos : digestPotentialVideos } })(...);  We return an object with two members: init and digestPotentialVideos. So, when we call ytTracker.init(), the script is basically run again, and all newly added, yet untreated YouTube iframe embeds will be processed and decorated with event trackers.\nIf you want it to be a bit more robust, you can use ytTracker.digestPotentialVideos(iframe) instead. You pass an iframe object as its parameter, and the script will only treat the one video. This is better since it won\u0026rsquo;t loop through all the iframes on the page, and instead just decorates the one you want.\nFinally, set the Custom HTML Tag to fire on a Page View / DOM Ready Trigger.\nThat\u0026rsquo;s the changes right there, ladies and gentlemen.\nHow to use it As said, there\u0026rsquo;s two new methods in town:\nytTracker.init(); // \u0026lt;-- Decorate all new videos on the page ytTracker.digestPotentialVideos(iframe); // \u0026lt;-- Only decorate the iframe that was passed as a parameter  Making the solution work will still most likely require developer help. If your developers have added some complex, dynamic content loaders, which add the videos after some awesome jQuery fade-in has finished, you\u0026rsquo;ll need to cooperate with them to add the ytTracker.init() or ytTracker.digestPotentialVideos() command in the right place.\nHere\u0026rsquo;s an example of what a modified content loader might look like:\nfunction processVideoClick(ytEmbedSrc) { var video = document.createElement(\u0026#39;iframe\u0026#39;); video.src = ytEmbedSrc; var content = document.querySelector(\u0026#39;#content\u0026#39;); content.appendChild(video); window.ytTracker.digestPotentialVideos(video); }  After the video is injected, you call ytTracker.digestPotentialVideos(). You could use the init() method without any parameters, but since you already have the iframe element at hand, it\u0026rsquo;s better to use the more direct method.\nSummary Since loading content dynamically is notoriously standard-free, it\u0026rsquo;s difficult to give a generic solution for managing dynamically loaded content in your analytics or tag management platform. However, Bounteous\u0026rsquo; original solution has a very nice set of tools to tackle dynamically loaded YouTube videos as well, without having to make drastic modifications to the code.\nBounteous\u0026rsquo; solution is so flexible and useful. Poking some holes into its interface to let some light out just makes it even better, in my own, humble opinion.\nThe optimal way of working with it is to cooperate with your developers. I hope we\u0026rsquo;ve all matured past the notion of GTM being developer-independent. So, communicate with your developers, ask them to modify the JavaScript controlling the dynamic video injection, and request that they\u0026rsquo;ll add the simple ytTracker methods to the handlers they create.\n"
},
{
	"uri": "https://www.simoahava.com/tags/videos/",
	"title": "videos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtmtips-track-launch-campaigns-in-gtm-for-ios/",
	"title": "#GTMTips: Track Launch Campaigns In GTM For iOS",
	"tags": ["campaign tracking", "Google Tag Manager", "gtmtips", "ios", "swift"],
	"description": "How to track launch campaigns in Google Analytics, using Google Tag Manager for iOS (legacy SDK).",
	"content": "In Google Analytics for iOS, there are two types of campaign measurement. There\u0026rsquo;s install campaign measurement, which lets you track the channels which brought your visitors to the App Store, where they proceeded to download your app. There\u0026rsquo;s also launch campaign tracking, which lets you attribute app launches to specific campaigns.\nIn this tip post, we\u0026rsquo;ll tackle the latter. We\u0026rsquo;ll leverage a feature of the Google Analytics iOS SDK to build the parameters, and then push them to dataLayer so that they can be used in the Tag.\nWe\u0026rsquo;ll be using Swift as the language of choice. Check out my previous two posts on GTM for iOS if you need more information on how to make GTM work in your iOS / Swift app.\nTip 39: Launch campaign tracking in Google Tag Manager for iOS   To make it all work, you need two things.\n  Code which parses the parameters from the URL and pushes them to dataLayer.\n  Data Layer Variables to send the campaign parameters through your Tag.\n  1. Modifying the app code To parse the URL, we\u0026rsquo;ll leverage the setCampaignParametersFromUrl method in the GAIDictionaryBuilder class of the Google Analytics SDK. We won\u0026rsquo;t use it for anything else except for parsing the URL. So, in your AppDelegate.swift file, locate the overloaded constructor with the handleOpenURL method, or create it yourself:\nfunc application(application: UIApplication, handleOpenURL url: NSURL) -\u0026gt; Bool { ... } This is where the app parses any URL parameters used in the launch.\nSo, now we need to leverage the GA SDK\u0026rsquo;s built-in methods, and we need to push the parameters to dataLayer. The setup might look something like this:\nfunc application(application: UIApplication, handleOpenURL url: NSURL) -\u0026gt; Bool { let urlString = url.absoluteString let hitParams = GAIDictionaryBuilder.init() let dataLayer = TAGManager.instance().dataLayer hitParams.setCampaignParametersFromUrl(urlString) dataLayer.push([ \u0026#34;campaignMedium\u0026#34; : hitParams.get(kGAICampaignMedium) ?? \u0026#34;\u0026#34;, \u0026#34;campaignName\u0026#34; : hitParams.get(kGAICampaignName) ?? \u0026#34;\u0026#34;, \u0026#34;campaignSource\u0026#34; : hitParams.get(kGAICampaignSource) ?? \u0026#34;\u0026#34; ]) return true } Let\u0026rsquo;s just quickly walk through this. urlString is the variable where we\u0026rsquo;ll store the URL string itself (d\u0026rsquo;oh). hitParams is what we\u0026rsquo;ll use to parse the URL, and dataLayer is where we\u0026rsquo;ll push the information.\nThe setCampaignParametersFromUrl() method takes a URL string, and parses it for the regular UTM campaign parameters. It stores them in a bunch of constants the GA SDK leverages to pass information between the app and Google Analytics.\nSo, if your app is launched with something like:\nawesomeApp://?utm_source=newsletter\u0026amp;utm;_medium=email\u0026amp;utm;_campaign=December_2015\nThe values for source (newsletter), medium (email), and campaign (December_2015) would be stored in the constants kGAICampaignSource, kGAICampaignMedium, kGAICampaignName, respectively.\nThe dataLayer.push() pulls these from hitParams and stores them in the data structure. If a parameter is missing from the URL, then a blank string is stored for that particular parameter.\nNow we have the values in dataLayer. Next, we need our container to pick them up and send them with our Tag(s).\n2. Modifying the container To make it all work, you now need to create three Data Layer Variables (one for each parameter), and add them to your Tag.\nDLV - campaignSource\nVariable name: campaignSource\nDLV - campaignMedium\nVariable name: campaignMedium\nDLV - campaignName\nVariable name: campaignName\nNext, edit your Screen View Tag (or whichever Tag fires first on your app). Go to More Settings -\u0026gt; Fields to Set, and add the following three fields:\nField Name: \u0026amp;cs\nValue: {{DLV - campaignSource}}\nField Name: \u0026amp;cm\nValue: {{DLV - campaignMedium}}\nField Name: \u0026amp;cn\nValue: {{DLV - campaignName}}\nFinally, publish the container, download the binary, update your app, and go crazy with launch campaign tracking.\nSummary There\u0026rsquo;s one thing you need to pay heed to. We\u0026rsquo;re pushing the campaign parameters to dataLayer but we\u0026rsquo;re not sending an \u0026lsquo;event\u0026rsquo; key. The reason for this is that we want the app to send its regular Screen View, using these values only if they\u0026rsquo;re in the dataLayer when the Screen View is dispatched.\nYou probably see how this can be a problem. If your app sends a Screen View as soon as the initial view appears, it\u0026rsquo;s possible this dataLayer.push() happens after the Screen View hit is built, leading to a nasty race condition. The best way to ensure it works is to defer the initial hit to Google Analytics until the handleOpenURL method has completed.\nThe whole thing is slightly more complex than when using just the GA SDK, as many things seem to be with the Google Tag Manager SDK, but it\u0026rsquo;s still consistent with leveraging dataLayer for message passing, rather than communicating directly with the endpoint.\n"
},
{
	"uri": "https://www.simoahava.com/tags/campaign-tracking/",
	"title": "campaign tracking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/url-source-in-the-url-variable/",
	"title": "#GTMTips: URL Source In The URL Variable",
	"tags": ["Google Tag Manager", "gtmtips", "Guide", "variables"],
	"description": "You can use the URL Source setting in your URL variables to make your Google Tag Manager URL variables even more flexible.",
	"content": "One of the less-known features of Google Tag Manager, a hidden gem if you will, is the URL Source setting in the URL Variable type. It lets you parse any URL String for its components.\nTip 38: Parse URL strings with the URL Source setting   The setting itself is easy to find. Just edit an existing URL Variable, or create a new one. Then, scroll down to the More Settings divider, expand it, and you\u0026rsquo;ll see the URL Source drop-down list.\nThis list will list all the Variables in your container, Built-In Variables included, and you can pick the one you want to parse. Note that the Variable you choose needs to return a properly formatted URL string for parsing to work. A properly formatted URL has at least the protocol (e.g. http:// or https://) and hostname (e.g. domain.com).\nWon\u0026rsquo;t work:\nsimoahava.com\nwww.simoahava.com\nwww.simoahava.com/some-page\nWill work:\nhttp://careers.simoahava.com\nhttps://makemoney.simoahava.com/about-us/\nThe URL Source setting establishes a URL String that is then parsed according to the Component Type setting you chose. In the image above, I\u0026rsquo;m using the Referrer Built-In Variable, which returns the referrer URL of the page in question, and I\u0026rsquo;m parsing it just for its path. In other words, this particular URL Variable will return the path of the previous page.\nIf the referrer is https://www.simoahava.com/about-simo-ahava/, the Variable will return /about-simo-ahava/.\nQuite a useful feature, and often overlooked.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/multiple-gtm-containers-on-the-page/",
	"title": "#GTMTips: Multiple GTM Containers On The Page",
	"tags": ["container", "Google Tag Manager", "gtmtips"],
	"description": "Tip on how to add multiple Google Tag Manager containers to a single page. For whatever reason.",
	"content": "For some reason, you might run into a situation where you need to add multiple Google Tag Manager containers on the same page. Usually this is because of poor governance or an inflexible organization. My recommendation is to fix things in your projects first, and only resort to multiple containers if you can\u0026rsquo;t seem to resolve your governance issues using your vocal cords (or your fists).\nOfficially, Google Tag Manager introduced support for adding multiple containers earlier in 2015. It\u0026rsquo;s still not a recommended process, however, and rightfully so. Personally, I hope that development efforts in GTM would be directed towards making a single container more manageable by different parties, rather than adding support for hacks like this.\nIn any case, this tip post is to remind you of how multiple containers can be implemented on a single page.\nTip 37: Multiple containers use the same dataLayer   As you can see, the implementation is simple. Just add your containers as you would normally, but remember to use the same name for dataLayer across the container snippets. This is an important thing to note, because the unofficial recommendation before this support surfaced was to rename the dataLayer objects across the different containers.\nGTM takes care of the rest. Triggers should not have any conflicts, and even though the gtm.js event is pushed into dataLayer multiple times (by each container snippet), your All Pages and Page View Triggers should still just fire once per container.\nOne place where potential conflicts can surface is with Data Layer Variables. When you push a key-value pair into dataLayer, it\u0026rsquo;s registered just once, but each container will have access to this value, even if you pushed it \u0026ldquo;for one container\u0026rdquo; only.\nThis means that with stuff like Enhanced Ecommerce tracking you might run into a situation where Tags in container B are inadvertently sending Ecommerce data, when you only intended Tags in container A to send them.\nThe best way to mitigate this is to send an \u0026lsquo;event\u0026rsquo; key with each Enhanced Ecommerce push, and then purge the key from the data layer after the Tag has fired. You can use hitCallback or eventCallback for that.\nAdding multiple containers is definitely not optimal, and there are many things to take into account, but it\u0026rsquo;s doable if you devote some time to the implementation. Naturally, I would suggest you devote this time to making the organization work with just one container, but sometimes inflexibility takes precedence.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtm-container-snippet-in-the-head/",
	"title": "#GTMTips: GTM Container Snippet In The HEAD",
	"tags": ["container snippet", "Google Tag Manager", "gtmtips"],
	"description": "Quick tip that you can actually add the GTM container snippet to the head of the document.",
	"content": "(UPDATE 28 Sep 2016: The official recommendation has finally caught up with the times. Now the correct placement for the JavaScript methods of the container snippet is, indeed, in \u0026lt;head\u0026gt;.)\nI want to address something I\u0026rsquo;ve been confused about from the very first day since I started using Google Tag Manager. Why on earth would an asynchronously loading JavaScript library be recommended to place in the beginning of \u0026lt;body\u0026gt;, when the logical place to start loading dependencies is as early as possible in the document load process? Well, the main reason for this is that the GTM library makes modifications to the document object model of the page in such a way that might cause conflicts when using Internet Explorer 7 or older.\nAnother reason is that the \u0026lt;iframe\u0026gt; within the \u0026lt;noscript\u0026gt; block is not valid HTML, which is a legitimate concern.\nThe third reason, which isn\u0026rsquo;t really a reason but rather an offshoot of the original Google Tag Manager recommendation, is that Google\u0026rsquo;s Search Console verification requires that the container snippet be embedded in the beginning of \u0026lt;body\u0026gt;.\nIn this post, I\u0026rsquo;ll tackle these three \u0026ldquo;issues\u0026rdquo; and wrap up with a consolation note that yes, it\u0026rsquo;s perfectly fine to add Google Tag Manager to the \u0026lt;head\u0026gt; of the document, and it might even be preferable to do so.\nTip 36: Add the GTM Container Snippet to \u0026lt;head\u0026gt;   Now, the possible conflicts arising with Internet Explorer 7 and older are easy to dismiss. Just forget those users! Punish them for using an antiquated version of the web browser. Honestly, Internet Explorer 7 is being used by 0.1% of Internet users, and that ratio is only going to go down. Do not weep for the lazy. Make them suffer in the hopes of inspiring them to upgrade or, preferably, change to some other brand of web browser.\nNow, one thing you will have to do is remove the \u0026lt;iframe\u0026gt; element from the container snippet, since that\u0026rsquo;s just not valid HTML. An \u0026lt;iframe\u0026gt; is embedded content, and in HTML5, embedded content is never allowed in the \u0026lt;head\u0026gt; of the document. However, you don\u0026rsquo;t need to remove it entirely. Just copy-paste the entire \u0026lt;noscript\u0026gt; in the beginning of \u0026lt;body\u0026gt; if you\u0026rsquo;re concerned about your non-JavaScript users.\n\u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-XXXXXX\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;iframe src=\u0026#34;//www.googletagmanager.com/ns.html?id=GTM-XXXXXX\u0026#34; height=\u0026#34;0\u0026#34; width=\u0026#34;0\u0026#34; style=\u0026#34;display:none;visibility:hidden\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/noscript\u0026gt; Simple solution for dealing with the validation problem.\nFinally, the Search Console validation issue is a pain. There\u0026rsquo;s nothing you can do about that. If you choose to move the container snippet to the \u0026lt;head\u0026gt; of the document, you will need to use some other means of validating your site.\nWhy do it? Sometimes you don\u0026rsquo;t have an option. WordPress themes, for example, rarely have a hook for adding code to the beginning of \u0026lt;body\u0026gt;, and you\u0026rsquo;ll need to hack around that by modifying the child templates. In these cases, adding the library to the \u0026lt;head\u0026gt; can save a lot of time and effort.\nAlso, remember that Google Tag Manager is an asynchronously loading library. This means that it\u0026rsquo;s loading as the page renders, and it\u0026rsquo;s non-blocking. Thus, the earlier you start loading the library, the more chance your Tags will have to fire and complete before the user leaves the page or closes the browser. This might result in a slight increase in data collection accuracy and quality.\nAs soon as the Search Console validator relaxes a little, I see little reason why you shouldn\u0026rsquo;t always add the container snippet to the \u0026lt;head\u0026gt; of the page. In fact, if we ever get a synchronous version of the container (for running e.g. A/B tests), adding the snippet to the \u0026lt;head\u0026gt; becomes a necessity.\n"
},
{
	"uri": "https://www.simoahava.com/tags/container-snippet/",
	"title": "container snippet",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/firing-a-single-tag-multiple-times-in-gtm/",
	"title": "Firing A Single Tag Multiple Times In GTM",
	"tags": ["Google Tag Manager", "Guide", "triggers", "guest post"],
	"description": "How to fire a single Google Tag Manager tag multiple times, with a variable changing value dynamically. This is useful if you want to use a single GA tag to track data to multiple properties.",
	"content": "There might be many reasons you\u0026rsquo;d want to fire a single Tag multiple times in Google Tag Manager. The most common one is when you want to deploy multiples of a single tracking point on the web. Perhaps you have a roll-up account you want to send the hits to, in addition to the site-specific tracking property.\nQuite a while ago, I gave a solution for this with a specific focus on Google Analytics Tags. It leveraged the hitCallback feature of the Universal Analytics library by increasing a global counter each time a Tag had fired. This solution had a number of drawbacks: being GA-specific, polluting the global namespace, and requiring a unique setup for every single Tag you wanted to fire.\nNot long after this, I actually started doing the whole thing in a different way. A much more durable, sensible, robust way, and in this article I want to open up the method. In fact, it\u0026rsquo;s not just me who wants to introduce this. This article is a collaboration with Marco Wekking from Adwise. He approached me with almost exactly the same solution asking for a review. Instead of reviewing, I asked if he would like to contribute to a collaboration of sorts on this post, and he gracefully accepted. So, much of this post is from Marco\u0026rsquo;s pen, with my typical bad humour and some JavaScript shenanigans intertwined in the rhetoric.\nHow it works   The skilfully grafted diagram above shows how the solution works. Every single event that passes through dataLayer is duplicated by suffixing it with .first and .second. This way, all the Tags you want to fire twice per event just need to be modified to use the original event name plus the two suffixes. You can even use this in a Lookup Table to fire the hits to different Google Analytics tracking properties, for example.\nIt\u0026rsquo;s a simple, elegant solution to a pressing problem which, if you ask me, should be handled natively by the platform. This is such a common use case for so many people - it would be immensely helpful if you could just denote multiple cycles to each Tag using the Tag template settings instead of having to resort to hacks like this.\nHow to do it The solution comprises three steps:\n  Create a new Custom Event Trigger\n  Create the Custom HTML Tag which manages the duplication\n  Create new Triggers\n  We\u0026rsquo;ll wrap it up with an example of how to leverage this when firing to multiple Google Analytics properties! So stay tuned.\n1. Create the Custom Event Trigger To fire the Custom HTML Tag, we\u0026rsquo;ll need a Trigger which activates on every single event. Why? Because we want to duplicate every single event! There\u0026rsquo;s just one exception - we don\u0026rsquo;t want to fire the Trigger when the actual duplication events are pushed into dataLayer, or we might find ourselves in an infinite loop. Ontologically fascinating, but not cool when it comes to site performance.\nBefore you create the Trigger, make sure you have the Event Built-in Variable checked in the appropriate slot of the Variables page in your container.\n  This is required for the Trigger to work.\nNext, create a Trigger which matches all events except for the duplicated ones. In this example, we\u0026rsquo;re using the generic .first and .second as the suffixes, but nothing\u0026rsquo;s stopping you from using something more descriptive, for example .Country and .Rollup.\n  To reiterate: The exception is important! If you don\u0026rsquo;t add that Fire On condition, you will run into issues, as the Trigger would fire again and again with each duplication cycle. Yes, you could use the Tag Firing Options feature of the Tag templates, but it\u0026rsquo;s better to nip bad behavior in its bud.\n2. Create the Custom HTML Tag The Custom HTML Tag is the heart and soul of this solution. It\u0026rsquo;s very simple, but it does have some peculiarities that might surprise if you\u0026rsquo;re not familiar with how GTM works.\nActually, to ensure that one of these special features works as it should, you\u0026rsquo;ll need to activate yet another Built-In Variable: Container ID. This returns, surprise surprise, the public ID of the GTM container the Tag is firing in (GTM-XXXX).\n  Create a new Custom HTML Tag and add the following code within:\n\u0026lt;script\u0026gt; (function() { var event = {{Event}}; var gtm = google_tag_manager[{{Container ID}}]; window.dataLayer.push({ \u0026#39;event\u0026#39; : event + \u0026#39;.first\u0026#39; }); window.dataLayer.push({ \u0026#39;event\u0026#39; : event + \u0026#39;.second\u0026#39;, \u0026#39;eventCallback\u0026#39; : function() { gtm.onHtmlSuccess(); } }); })(); \u0026lt;/script\u0026gt; We\u0026rsquo;re wrapping the whole thing in an IIFE as we want to avoid polluting the global namespace. Next, we\u0026rsquo;re assigning a local variable event with whatever value is currently stored in the Built-in Event variable (i.e. the event that was pushed in the first place).\nFinally, we\u0026rsquo;re doing two consecutive dataLayer.push() commands, one for each iteration of the cycle. If you want to add more events there, be my guest. As far as we know, there is no limitation to the number of events you can push into dataLayer this way.\nThe latter push includes the special eventCallback key (read more about it here). The key holds a callback function which is invoked as soon as Tags which fire on this particular push have signalled their completion. Within this callback, we\u0026rsquo;re using the onHtmlSuccess() feature of GTM\u0026rsquo;s interface. This is something that was exposed for public use with the Tag sequencing feature. The only thing you need to know about it is that it\u0026rsquo;s our way of telling GTM that it can now proceed with whatever was going on before this loop of dataLayer.push() commands.\nIn other words, if you\u0026rsquo;re duplicating a Click / Just Links, and you\u0026rsquo;ve got \u0026ldquo;Wait for Tags\u0026rdquo; checked (meaning the Trigger will wait for all dependent Tags to fire before proceeding with the link default action), the process goes something like this:\n  The link click Trigger fires this Custom HTML Tag.\n  The Custom HTML Tag pushes the first duplicated event, and any Tags which use it start their execution.\n  Immediately after, the Custom HTML Tag pushes the second duplicated event, and any Tags which use it start their execution.\n  Once the last Tag firing on the second duplicated push signals its completion, the eventCallback callback is invoked, and the Custom HTML Tag tells the link click Trigger that everything is done, allowing it to proceed with the redirect.\n  Now, add the Trigger you created in the previous step to this Tag, and you\u0026rsquo;re ready to duplicate. You can preview this to see what happens in your dataLayer with each event.\n  As you can see, each event is duplicated. There\u0026rsquo;s the Pageview, followed by its duplicates: gtm.js.first and gtm.js.second. There\u0026rsquo;s DOM Ready, followed by its duplicates: gtm.dom.first and gtm.dom.second, and so on.\n(If you don\u0026rsquo;t understand what \u0026ldquo;Pageview\u0026rdquo; and \u0026ldquo;gtm.js\u0026rdquo; have to do with each other, the latter is the underlying event representation of the former. More information in my Trigger guide.)\n3. Create new Triggers The only housekeeping pain you\u0026rsquo;ll need with this solution is with the Triggers. Here\u0026rsquo;s how it should work:\n  To duplicate a Trigger, you need one \u0026ldquo;base\u0026rdquo; Trigger of the event type firing without any delimiting conditions. This is the event that is duplicated.\n  To fire your duplicate Tags, you\u0026rsquo;ll need to use Custom Event Triggers which use the original event name (e.g. gtm.linkClick) plus .first or .second, all wrapped in a simple regular expression.\n  In other words, we\u0026rsquo;re taking a step back to how GTM used to work before the auto-event triggers. We\u0026rsquo;re creating listeners using the generic event Triggers, and when these are duplicated, the Custom Event Triggers are used to delimit the Tags to fire only when specific conditions exist.\nNOTE! You don\u0026rsquo;t need to create a \u0026ldquo;base\u0026rdquo; Trigger for the Page View event type. This is because \u0026ldquo;Pageview\u0026rdquo;, \u0026ldquo;DOM Ready\u0026rdquo;, and \u0026ldquo;Window Loaded\u0026rdquo; are automatically pushed into dataLayer as the page and GTM load. In other words, they are automatically duplicated, and you just need to focus on creating the Custom Event Triggers.\nLook at the illustration below for clarity:\n  Let\u0026rsquo;s zoom in. If you have an \u0026ldquo;Outbound Links\u0026rdquo; Trigger, which fires when the Click URL is not your own hostname, the duplicated Trigger would look something like this:\n  Make note of the \u0026ldquo;Event name\u0026rdquo; field. That\u0026rsquo;s what it should look like for your Triggers. With custom event names it\u0026rsquo;s easy, as you\u0026rsquo;re the one pushing them into dataLayer in the first place. With the built-in Triggers it might be a bit more difficult, so here\u0026rsquo;s a cheat sheet for you:\n   Built-in Event Underlying event name     Page View / Page View (also All Pages) gtm.js   Page View / DOM Ready gtm.dom   Page View / Window Loaded gtm.load   Click / All Elements gtm.click   Click / Just Links gtm.linkClick   Form Submit gtm.formSubmit   History Change gtm.historyChange   JavaScript Error gtm.pageError   Timer gtm.timer    Make sure you\u0026rsquo;ve got the Fire On condition on the Custom Event Trigger, as you don\u0026rsquo;t need it on the generic event Trigger. If you miss it from the Custom Event Trigger, you\u0026rsquo;ll inadvertently fire the Tag whenever any such event is detected on the page. For example, if we\u0026rsquo;d left out the Click URL Hostname condition from the example above, the Outbound Links Tag would fire whenever the Link Click event is pushed into dataLayer.\nBONUS: Use variable Tracking ID Here\u0026rsquo;s a tip straight from Marco. To create a variable which sends a different Universal Analytics tracking ID depending on which cycle of the duplication loop is currently active, use the following Custom JavaScript Variable:\nfunction() { var event = {{Event}}; var regexFirst = /\\.first$/; var regexSecond = /\\.second$/; if (regexFirst.test(event)) { return \u0026#34;UA-XXXXXXXX-1\u0026#34;; } else if (regexSecond.test(event)) { return \u0026#34;UA-XXXXXXXX-2\u0026#34;; } // Do something in case neither matches  }  This returns the UA code \u0026ldquo;UA-XXXXXXXX-1\u0026rdquo; if the Tag is firing on the first loop of the duplication cycle, and \u0026ldquo;UA-XXXXXXXX-2\u0026rdquo; if on the second. You might want to setup some type of fallback or default return value in case neither matches.\nOverview and summary While the solution works for all kinds of tags and creates less redundancy in many setups, it has some drawbacks of its own.\nFirst, it inflates the number of dataLayer events significantly. A simple setup with just two events (Page View and Outbound Link Clicks) already triples the amount. Note that this doesn\u0026rsquo;t really have any impact on performance. dataLayer is just a message bus used by Google Tag Manager\u0026rsquo;s internal data model. Whenever GTM needs to access the \u0026ldquo;Data Layer\u0026rdquo;, it\u0026rsquo;s actually just performing a lookup in its own data model, so the size of dataLayer is inconsequential here.\nSecond, it requires a different and probably more difficult approach to triggers. All tags need to be linked to custom event triggers instead of the built-in Trigger types you might have become used to. If you were around during GTM\u0026rsquo;s previous version, you might be familiar with the setup, as it resembles how things were done with the old auto-event tracking setup. However, when you really chew it down, all you\u0026rsquo;re actually doing is creating one extra Trigger per event type, and moving from the built-in Trigger types to Custom Event Triggers.\nFinally, trigger conditions become more critical as they can cause infinite loops or Tags firing in wrong situations. While such accidents shouldn’t cause infinite regret (assuming you test before publish) it does remain another difficulty for you to deal with.\nOne thing you might be concerned about is whether or not all the Variables populated with the initial base Trigger are still available when the duplicate Custom Event Triggers fire. For example, you might need the Click URL Variable, and now you\u0026rsquo;re worried that it\u0026rsquo;s not available when the duplicate Triggers fire. Don\u0026rsquo;t worry! GTM persists Variable values until they\u0026rsquo;re overwritten or there\u0026rsquo;s a page unload/refresh. So, unless you\u0026rsquo;re manually overriding the Data Layer values populated by GTM\u0026rsquo;s auto-event Triggers, you should be fine.\nWell, we might have overstated the simplicity and elegance of the setup, but the idea of intercepting each event and duplicating it is far more approachable than the complex hitCallback setup used before.\nI want to thank Marco for putting the approach into writing, and most of the content in this article has come from his pen, edited to suit the devil-may-care style of this blog. Any errors, factual mistakes, or radicalist propaganda cleverly hidden in the whitespace is solely the fault of me, Simo, and I take full responsibility for all the uprisings that will inevitably follow.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/useful-gtm-for-ios-tips/",
	"title": "#GTMTips: Useful GTM For iOS Tips",
	"tags": ["Google Tag Manager", "gtmtips", "ios", "swift"],
	"description": "Useful tips for setting up Google Tag Manager for iOS using the legacy SDK.",
	"content": "A while ago I wrote a blog post about how to install Google Tag Manager for iOS using the Swift programming language (you can read the article here). I\u0026rsquo;ve been doing a lot of work with Swift lately, and I\u0026rsquo;m more and more convinced that GTM is actually a very powerful utility for running in your iOS app instead of the Google Analytics SDK.\nWhy? Because it abstracts a lot of the nitty-gritty you\u0026rsquo;d otherwise need to explicitly manage in your GA installation. Also, by using the TAGDataLayer interface, you\u0026rsquo;re abstracting stuff yourself, not forced to adopt any specific syntax or semantic structure in your tracking. Instead, you\u0026rsquo;ll be leveraging generic key-value pairs and an object structure, which you\u0026rsquo;re probably already familiar with if you\u0026rsquo;ve worked with GTM for the web.\nIn any case, here\u0026rsquo;s a bunch of tips I\u0026rsquo;ve discovered while working with the SDK. Some of them might be quite evident, but they\u0026rsquo;re useful nevertheless. Also, I\u0026rsquo;ve learned that nothing is 100% self-evident in the world of GTM.\nTip 35: Bunch of GTM4iOS tips!   The tips are:\n  Dispatch the Tag Manager / GA queue when the application is entering background / terminating\n  Log verbosely and set dispatch interval to 1 second only when debugging\n  Purge dataLayer in between pushes\n  Use the dryRun of the GAI.sharedInstance() to avoid sending data to GA\n  Get the Google Analytics Client ID from the tracker instance\n  Switch IDFA on in your Tags\n  1. Dispatch the Tag Manager / GA queue when the application is entering background / terminating One of the rather ugly default features of the Google Analytics and Google Tag Manager SDKs is that they have a 120 second batch interval. It makes a lot of sense when the app is running, as it preserves battery et cetera, but it\u0026rsquo;s nasty when the app is in the background. Basically, it forces the app to \u0026ldquo;wake up\u0026rdquo; after 120 seconds of down time, just for the queue to dispatch.\nIn AppDelegate, you have a bunch of useful functions, which you can use to run commands when the app is ready to do something. Also, the TAGManager implementation has the method dispatch(), which lets you send all the hits that are currently in the dispatch queue. So, by combining these two, you can empty the queue whenever the app is about to enter the background or terminate!\nfunc applicationDidEnterBackground(application: UIApplication) { tagManager.dispatch() } func applicationWillTerminate(application: UIApplication) { tagManager.dispatch() } 2. Log verbosely and set dispatch interval to 1 second only when debugging If you\u0026rsquo;ve set preprocessor macros (you should!), you can execute code only when in a certain environment, such as QA, staging, or production.\nGTM also provides verbose logging, and you can set the dispatch interval of the request queue (see the previous tip) manually. Now, combine all these in a bowl, stir well, and you\u0026rsquo;ll get some really powerful debugging tools.\n#if !ENV_PROD tagManager.logger.setLogLevel(kTAGLoggerLogLevelVerbose) tagManager.dispatchInterval = NSTimeInterval(1.0) #endif The first command sets the logging level to verbose, and the second one sets the dispatch interval to just one second. Both commands are only executed if the environment is not ENV_PROD, thus covering ENV_QA, ENV_TEST, ENV_DEBUG etc. These are, of course, just examples, and your setup probably uses some other names.\nYou can utilize the #if DEBUG further in the containerAvailable callback, which is invoked when a container is first opened. Now, if you\u0026rsquo;re debugging the GTM implementation, it\u0026rsquo;s possible you\u0026rsquo;re also updating the container in Google Tag Manager as well, and you might want to see changes immediately rather than having to wait the 12 hours for the container to go stale. So, modify the callback accordingly:\nfunc containerAvailable(container: TAGContainer) -\u0026gt; Void { dispatch_async(dispatch_get_main_queue(), { self.container = container; #if !ENV_PROD container.refresh() #endif }); } See that container.refresh() there? That fetches a container over the network every single time, as long as you\u0026rsquo;re not in an environment named ENV_PROD.\n3. Purge dataLayer in between pushes In my Analytics.swift module (see this article for more information), I call the following function just after dataLayer.push() where content or hit-scoped custom dimensions are passed. To keep things consistent, I\u0026rsquo;ve created a dataLayer syntax, where all content data is stored under the key contentData, and all eCommerce data is stored under the key ecommerce. I have other collections as well, such as metaData and userData, but these I want to persist, so the purge is not called after they are used.\nprivate static func purgeDataLayer() -\u0026gt; Void { dataLayer.push([ \u0026#34;contentData\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ecommerce\u0026#34; : \u0026#34;\u0026#34; ]) } Yes, pushing blank strings is a clumsy way to purge the structure but it works. Basically, every time after content data and ecommerce data have been pushed into dataLayer and subsequently processed by GTM, I call purgeDataLayer() to ensure that this hit-scoped information does not persist.\n4. Use the dryRun of the GAI.sharedInstance() to avoid sending data to GA I\u0026rsquo;m actually not sure why you\u0026rsquo;d want to do this, since you can just send your debug hits to a secondary Google Analytics property. But if you do want to avoid sending any data to Google Analytics, and just look at the verbose logs to see if things are working, you can set this flag in the Google Analytics tracker instance.\nYes, I\u0026rsquo;m aware this isn\u0026rsquo;t GTM-specific, but it works with GTM-created trackers as well!\nGAI.sharedInstance().dryRun = true A simple tip, no doubt.\n5. Get the Google Analytics Client ID from the tracker instance Again, this is more Google Analytics than Google Tag Manager, but it works in the latter as well. All trackers created via the GA or GTM SDKs have the same Client ID, which persists until you clear all settings and content in the app.\nTo get the Client ID, run the following commands:\nlet tracker = GAI.sharedInstance().trackerWithTrackingId(\u0026#34;UA-12345-1\u0026#34;) let clientId = tracker.get(kGAIClientId) Now clientId has the GA Client ID, and you can do whatever you wish with it. In other words, you can actually use the string \u0026ldquo;UA-12345-1\u0026rdquo; if you wish, since it would have the same Client ID as the actual tracker (with the real Universal Analytics property ID).\n6. Switch IDFA on in your Tags This is a very simple tip, as there\u0026rsquo;s really almost no customization required. All you need to make sure is that the following Pod is loaded in your Podfile:\npod 'GoogleIDFASupport'\nThis loads the IDFA library, and you won\u0026rsquo;t need to do anything else in your Swift app. The only other thing you\u0026rsquo;ll need to do is switch on the Enable Advertising ID Features in your GTM Tags.\n  Now your app will be sending the \u0026amp;idfa; parameter with your device\u0026rsquo;s IDFA identifier, and you can set on your demographics reports and your iTunes install campaign tracking and whatnot via GA.\nSummary That\u0026rsquo;s it for this group of tips! I have lots more to share around Google Tag Manager for iOS, but I can\u0026rsquo;t extinguish my entire tip collection in one single post.\nI hope you found these useful! Do you have other tricks up your sleeve?\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/referral-exclusion-on-receipt-page/",
	"title": "#GTMTips: Referral Exclusion On Receipt Page",
	"tags": ["ecommerce", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "Here&#39;s an easy way to block all those pesky payment portals from messing up your Google Analytics data. Set all referral hits to null on the receipt page of your site.",
	"content": "You might have noticed that the Referral Exclusion List in Google Analytics is difficult to maintain. You can\u0026rsquo;t copy a list from one property to another, the list is a wildcard match for domain names (so all subdomains are automatically excluded), and it\u0026rsquo;s just about the clunkiest user interface we\u0026rsquo;ve seen since ERP tools from the 1990s. A while ago, I wrote of a solution which lets you manage referral exclusions using Google Tag Manager, and it\u0026rsquo;s still a neat trick, as it\u0026rsquo;s way more flexible than said clunky UI.\nHowever, it\u0026rsquo;s still an a priori setup. You need to know what referrals are coming in. Also, if you have hundreds of referrals that you want to exclude, it\u0026rsquo;s still a headache. With Ecommerce payment gateway referrals, it\u0026rsquo;s very common to have a huge amount of domains you didn\u0026rsquo;t even know to exist polluting your session data.\nIn this tip, I\u0026rsquo;ll show you a simple and elegant way to make sure that nothing interferes with correct attribution on your Ecommerce receipt page.\nTip 34: Exclude all referrals on your receipt page   That\u0026rsquo;s right! We\u0026rsquo;re going to brutally exclude all referrals on the receipt page. No matter where you come from, you will not be counted as a referral, and thanks to Google Analytics\u0026rsquo; last non-direct source attribution, your pre-payment-gateway-session will stay intact.\n\u0026ldquo;Exclude all referrals? Regardless of where they come from?\u0026rdquo; I hear you whisper in fear. Speculations about me finally losing my mind along with my hair are flying to and fro like suicidal swans.\nI\u0026rsquo;m serious! Think of a scenario where a referral to your receipt page is significant. You\u0026rsquo;re not going to be sending campaign traffic to that page. You\u0026rsquo;re not going to want to see it in Google\u0026rsquo;s search index. Unless you\u0026rsquo;re the worst SEO manager in the world, you\u0026rsquo;re not going to want to build links to that page. So we don\u0026rsquo;t care about referrals to the receipt page. The hypothesis is that whenever someone lands on the receipt page, it\u0026rsquo;s always as part of your checkout funnel.\nI know, always is not something you should articulate outside of singing Bon Jovi at the local karaoke bar. But in this case it\u0026rsquo;s worth the risk. The only reason you might want to see referral data on the receipt page is to debug some issue with your checkout funnel. But, my friends, there are better tools than Google Analytics\u0026rsquo; session attribution to do that. If you\u0026rsquo;re really concerned, you can always send the referrer as a Custom Dimension.\nAnyway, to get things working, you need to do two things: create a Custom JavaScript Variable and modify the Tags which fire on the receipt page. Let\u0026rsquo;s start with the first one.\nCreate a new Custom JavaScript Variable, and name it {{JS - Null referrer on thank you page}} or whatever suits your fancy. Next, populate it with the following code:\nfunction() { return {{Page Path}}.indexOf(\u0026#39;/thank-you-page/\u0026#39;) \u0026gt; -1 ? null : document.referrer; }  I\u0026rsquo;m aware that this is different than the code in the image above. I\u0026rsquo;m dark, mysterious, and unpredictable that way! (Both work just fine).\nThe key thing is to change the string in the indexOf method to match the path (or part of path) of your receipt page. It\u0026rsquo;s case sensitive, so if you\u0026rsquo;re one of the 3 people who think that URLs that are something else than all lowercase are cool, make sure you include the case in the check.\nOnce you\u0026rsquo;ve created the Variable, head on over to your Universal Analytics Tags which fire on the receipt page. That\u0026rsquo;s right, you\u0026rsquo;ll need to edit all of them, because even a single Tag which gets through with incorrect referrer information will start a new session. In the Tag settings, browse down to Fields to Set, and add a new field with this setup:\nField Name: referrer\nValue: {{JS - Null referrer on thank you page}}\n  And now you\u0026rsquo;re all set! When you run this setup, whenever the user lands on /thank-you-page/, all hits to GA are sent without a referrer.\nThanks to Michael] for bringing up the point of editing all the Tags on the page, not just the Page View Tag.\nSimple, elegant, and quick.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/better-qa-with-google-tag-manager-environments/",
	"title": "Better QA With Google Tag Manager Environments",
	"tags": ["environments", "Google Tag Manager", "Guide"],
	"description": "Introducing Google Tag Manager&#39;s environments. This guide will walk you through the feature, and how you can use it to enhance quality assurance in your data organization.",
	"content": "Google Tag Manager, our favorite free tag management solution, has always struggled with its enterprise-worthiness. There are many features still lacking, most of which have to do with working in multi-user environments. Now, grab the last word of that sentence (see what I did there), and hug it tightly, for GTM just introduced a new, enterprise-friendly feature: Environments.\n  These Environments are actually browser cookies, which you use to link a Google Tag Manager container state with the browser of the user who needs to or wants to view that particular state. In other words, if you have a QA (quality assurance) process, as you should have, or if you do most of your testing on a staging server, as you should do, you can create an Environment in GTM, after which you can publish container versions (even the draft) into that particular Environment alone.\nThis is, actually, pretty significant compared to how GTM used to work. Before, you had to Share a Preview version, but that was always a container version. When the version changed, you had to retrieve a new authentication token. Now, GTM will distribute Environment tokens instead, meaning as long as the Environment is linked to Google Tag Manager and the authentication is not revoked, that particular token will have access to whatever versions are published into said Environment. Cool!\nThis makes GTM more manageable across multiple contexts. There are two ways to do this: by sharing a link, as before, or by adding a piece of code into the page templates of each Environment, in which case the link between GTM and the Environment works without you having to distribute authentication tokens via cumbersome links.\nOfficial support documents can be viewed here. Let\u0026rsquo;s get cracking!\nWhy this change? This development is necessary not only because it makes GTM function better across environments and teams, but also because the old Share Preview feature was not optimal. In this chapter, I\u0026rsquo;ll briefly walk you through how container version sharing used to work, so that you\u0026rsquo;ll understand why this development is necessary.\nMany thanks to Brian Kuhn, again, for his invaluable insight into the inner workings of Google Tag Manager.\n  Thus far, when you\u0026rsquo;ve clicked the Share Preview link either in the yellow preview bar (seen above), or via the Versions page, you get a URL which you can then distribute to whomever should be able to preview (or debug) this particular container version.\nThis URL, when executed in the browser, actually writes a secret authentication token into the user\u0026rsquo;s browser while on the googletagmanager.com domain. Now, when requesting the gtm.js library from googletagmanager.com on the website where the container is installed on, GTM will detect this authentication token in the cookies written on googletagmanager.com. Once it detects the authentication token, it will know to download the container version with which this particular token is associated. The following modern art masterpiece illustrates this relationship:\n  As you can see, the authentication token in the Share Preview link matches that of a specific cookie found on the googletagmanager.com domain.\nIn other words, since an authentication token links a specific version with the user\u0026rsquo;s browser, it means that that user is permanently forced to digest that particular container version in their browser.\nOK, well, I might be exaggerating a little. The cookie itself is a session cookie, meaning it will expire when you close the browser. However, the token is permanent. So if the user saves the link, they can revisit that version as much as you like, and there\u0026rsquo;s very little you can do about it (short of deleting the version).\nAlso, to get rid of Preview mode without having to shut down the browser is a bother as well. Basically, you need to browse again to the page you get when copy-pasting the Share Preview link, and then click the \u0026ldquo;Exit Preview\u0026rdquo; link you see. It\u0026rsquo;s easy to miss and easy to forget.\n  Not very streamlined.\nAnyway, this is how it\u0026rsquo;s worked thus far. The main problems with the current setup are:\n  Authentication tokens are permanent, so there\u0026rsquo;s no way for you to revoke authentication. Also, cookies are session cookies, and it\u0026rsquo;s difficult to delete them (unless you remember to click the \u0026ldquo;Exit Preview\u0026rdquo; link on the Share Preview page). The permanence of the authentication token is especially problematic with the Container Draft, as once a user has the authentication token for the draft, they\u0026rsquo;ll be authenticated against ALL future drafts as well!\n  Since the token is linked to a specific version, it makes the QA process cumbersome. Optimally, you\u0026rsquo;d want the cookie to be bound to the environment, not the version. Now QA needs a new authentication token whenever a new version of the container is ready for testing.\n  Sharing the container preview link is a hassle, looks suspicious, and is sensitive to errors. Also, it does not scream \u0026ldquo;FLEXIBILITY\u0026rdquo;, which is what I do a lot when working with GTM (my co-workers find it amusing).\n  These are the problems the Google Tag Manager team wanted to tackle with Environments, and they sure did take a step in the right direction with the new feature.\nIntroducing Environments The main benefit of Environments is that you no longer share a version cookie, but rather an Environment cookie. In other words, if you want to link a user\u0026rsquo;s browser with an Environment created in GTM, you don\u0026rsquo;t have to send them a new token each time the version updates. Instead, you give them the authentication token to that particular Environment, after which you simply publish to that Environment, and the user on the other end with their QA browser will have access to whichever version is currently published to that Environment.\nAnother thing that has been made simpler is revoking access. You can now nullify an authentication token via GTM, helping you manage who has access to which Environments.\nTo get started, you will need to enable Environments in your Google Tag Manager container. If you don\u0026rsquo;t do that, nothing will change in your Google Tag Manager user interface. So, head on over to the Admin section of your container, and click the new Environments link.\n  This is where you\u0026rsquo;ll create your own, long-lived Environments. I stress long-lived, since you really only want to create an Environment which lasts more than the lifespan of a single version. I mean, you CAN use Environments to substitute the old \u0026ldquo;Share Preview\u0026rdquo; functionality, but they truly come to life when actually creating an Environment for a permanent process in your organization, and then using it methodically to test new versions of the container, without having to share cookies every single time.\nManage Environments In the list of Environments, you\u0026rsquo;ll see three default Environments. These can\u0026rsquo;t be deleted or published to manually. They are:\n  Live - The currently live Container Version is automatically added to this Environment\n  Latest - The newest created version (not the Draft)\n  Now Editing - The current container draft\n    Now, these are all states that exist in GTM with or without creating custom Environments. When you edit a container, the version is maintained in the \u0026ldquo;Now Editing\u0026rdquo; Environment. When you save a draft as a container version, the version is maintained in the \u0026ldquo;Latest\u0026rdquo; Environment. And when you publish a version, it is published into the \u0026ldquo;Live\u0026rdquo; Environment.\nClick New to create your first, custom Environment. Give it a honest name and a good description. You can also choose to enable the Debug panel by default (if you don\u0026rsquo;t check this, then Environments will be Preview only by default, meaning users won\u0026rsquo;t see the debug panel).\nIf you want, you can set the hostname of the site where the Environment is. This way the Share Preview dialog will directly link the user to the website.\n  So, now you\u0026rsquo;ve created your first custom Environment, and you\u0026rsquo;ll see it in its own list in the Environments page.\n  As you can see, the Version ID field is empty, meaning you haven\u0026rsquo;t published a version into this Environment yet.\nLet\u0026rsquo;s take a look at the Actions menu.\n  NOTE! The default containers have the same menu, except they do not have the Delete or Publish to\u0026hellip; options available.\nThe options are:\n  Edit Settings - lets you edit the settings you first set when creating the Environment\n  Publish To\u0026hellip; - opens a publish dialog, where you can select which container version you want to publish to the selected Environment. After this, sharing a preview link to this Environment will allow users to preview the published container only.\n  Share Preview - shares a preview link to the Environment, allowing users with access to this particular Environment preview and debug it with their browsers.\n  Reset Link - removes authentication from the previously shared link AND from the new snippet (see below). This is very cool but also dangerous (see the next chapter).\n  Get Snippet - gives you the updated GTM Container Snippet which replaces the original snippet. All pages with this Environment will automatically be in preview mode for the Environment.\n  Delete - lets you delete the Environment.\n  Let\u0026rsquo;s move on to how the sharing works.\nSharing Environment access To share Environment access, you have two options.\nYou can distribute it as a link, as before, which binds the user\u0026rsquo;s browser to the Environment via an authentication token. This way, the user\u0026rsquo;s browser is linked to that particular Environment until the authentication token is revoked or the user manually deletes the token cookie using the Share Preview page.\n  An alternate, more robust way is to modify the container snippet on the website of the Environment (e.g. QA or staging site). The simple modification enables the authentication token to be accessed whenever that site is visited, meaning the site (read: the container snippet) is bound to the Environment instead of just the user\u0026rsquo;s browser. This method dispenses with the need to share the authentication link, and it allows the user to browse other Environments on the same domain (e.g. the live site) without interference.\nIn other words, the Environment will only be active on pages with the new container snippet. When you browse to a page or site (even under the same domain) that does not have the authentication token in the snippet, you will be privy to the live, published container as usual.\n  NOTE! When you reset the link to the Environment, it will also make the container snippet for that Environment change! In other words, only reset a link which is used in a container snippet if you are certain it\u0026rsquo;s necessary. Otherwise you\u0026rsquo;ll have hell to pay with your developers.\nOn that note, it\u0026rsquo;s probably best to avoid using the same Environment both with Share Preview links and distributing the authentication token via the container snippet. Make a note of what the Environment is used for in the Description field or, even better, in the Name field. That way you\u0026rsquo;ll know what you risk if you want to reset the link.\nThings you can do via the GTM UI Once you enable Environments, you can selectively publish to these Environments, you can use a new Built-In Variable to query which Environment the user is currently in, and you can view which version is currently live in which Environment.\nFirst, go to the Versions screen. As you can see, there\u0026rsquo;s a new column \u0026ldquo;Environments\u0026rdquo;. That column contains information about to which Environments a particular version has been published.\n  Next, if you click the Actions menu next to a version, you\u0026rsquo;ll find the Publish To\u0026hellip; link again. Here, you can publish the selected version to an Environment of your choice! So no need to go through the Environments page every single time you want to publish a version to an Environment.\n  Next, go to the Container overview and click the red Publish button. As you can see, you can now choose to which Environment you actually want to publish the container draft to! Live is selected by default, of course.\nNOTE! When you publish a container draft, it is first created into a version, and then automatically published to the Latest Environment in addition to whichever Environment you chose in the Publish dialog. Also, the version in the \u0026ldquo;Now Editing\u0026rdquo; Environment is updated to the NEW container draft. Phew!\nThe new Environment Name variable Yes, there\u0026rsquo;s a new Built-In Variable in town: Environment Name.\n  You might be surprised that this variable returns the name of the Environment the user\u0026rsquo;s browser is currently viewing (shocker!). My favorite use case for this is to create a Lookup Table Variable, which distributes hits to different Google Analytics properties depending on which Environment the user is in:\n  You can also use this Variable in a Trigger, creating a powerful Exception to avoid firing expensive Tags in certain Environments.\n  All in all, a very useful addition to your arsenal of variables.\nSummary Environments are really useful. They\u0026rsquo;re a huge leap in the right direction, again, turning Google Tag Manager into a more manageable mess, especially in multi-user projects.\nThe first thing you\u0026rsquo;ll want to do is establish some permanent Environments (e.g. QA and Staging), and update the web servers with the Environment container snippet. Then, make sure to write it down clearly that the link for these Environments should not be reset without good reason! If you reset the link, the container snippet needs to be rewritten.\nAfter that, you can start using other Environments for your own purposes as well. Perhaps you want to create an Environment only for a particular side project, where you need to allow preview and debug access to an agency or consultant. Or perhaps you want to start your own \u0026ldquo;branch\u0026rdquo; of versions, debugging and previewing them in your own, enclosed Environment, distributing links to anyone who might be able to help you in your work.\nThe fact that giving and revoking authentication is now completely in your administrator\u0026rsquo;s hands is also an excellent change. Now you don\u0026rsquo;t have to hunt down the person you shared the container draft link with, as you can just reset the link to the \u0026ldquo;Now Editing\u0026rdquo; Environment, revoking preview access from anyone who used to have it.\nOne thing that\u0026rsquo;s missing is the end user\u0026rsquo;s ability to revoke access by themselves. A simple \u0026ldquo;Close\u0026rdquo; link in the debug panel would be awesome. Now you have to revisit the \u0026ldquo;Share Preview\u0026rdquo; page to remove access. If the authentication is done via the container snippet, there\u0026rsquo;s no way to get rid of Preview mode while in that environment. It would be nice if you could temporarily (e.g. via a session cookie) revoke the authentication imposed by the container snippet while browsing the site.\nIt\u0026rsquo;s also a bit strange that you can see deleted versions in the Environment lists. For example, if you create a new version and then delete it, it still appears in the \u0026ldquo;Latest\u0026rdquo; Environment, and you can publish it to any Environment you want via the Actions dropdown.\nFinally, remember that tools don\u0026rsquo;t create processes, they facilitate them. If you don\u0026rsquo;t have a process the Environment is designed for, this new GTM feature will bring very little gratification. It will make some aspects of testing and previewing easier, but Google Tag Manager Environments truly shine when they reflect and facilitate existing processes.\n"
},
{
	"uri": "https://www.simoahava.com/tags/environments/",
	"title": "environments",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/from-project-to-process/",
	"title": "From Project To Process",
	"tags": ["agile", "analytics", "data layer", "process", "tag management solutions"],
	"description": "How to introduce agility to your analytics projects. A successful analytics endeavor is always a process, never a single project.",
	"content": "This year I had the opportunity to present at eMetrics London and Berlin on a topic that is very close to my heart. I\u0026rsquo;m psychotically neurotic about data quality. I\u0026rsquo;ve written about it many times before, and it\u0026rsquo;s pretty much why I want to keep on blogging and writing about analytics and tag management customizations. At eMetrics, I stepped out of my comfort zone of development and implementation, and chose to talk about organization practices.\n  For the past number of years, I\u0026rsquo;ve had the fortune of working with some amazing clients and colleagues, helping entire organizations understand the concept of meaningful data and how data is always an investment. In this article, I want to pull some of these threads together, and explain how I approach data quality and the optimization thereof within an organization.\nDisclaimer As always, I do not want to preach, nor do I have any authority to tell others how to conduct their business. Quoting my good friend Craig Sullivan, experts and best practices are \u0026ldquo;rather rare and mystical beasts\u0026rdquo;.\nAll we can rely on are the experiences of people we trust.\nIf we find these experiences useful, and we can trust that the person who shared them knows what they\u0026rsquo;re talking about, we can see if these experiences could be adopted by whatever environment or context we are working in. This, I think, is lost to many who\u0026rsquo;ve succumbed to the ease of finding answers instead of methods. Too often, people ask for Please do this for me instead of Please show me how it works.\nThe way I approach learning new things is to find experiences with which I can relate, after which I dissect these experiences, trying to recreate them in a way that is most beneficial to me and my goals.\nIn this article, I will be sharing my experiences of working in and with various types of organizations. If some of these thoughts resonate with you, then I\u0026rsquo;m glad, and I look forward to hearing what you think about what I\u0026rsquo;ve written. If this sounds alien to you, then I\u0026rsquo;m equally glad, and I look forward to explaining myself further in the comments.\n1. Don\u0026rsquo;t screw up data collection Surprise, surprise, I\u0026rsquo;m all about data collection. I don\u0026rsquo;t hesitate to say that it\u0026rsquo;s the most crucial part of the web analytics process. But what about analysis, insights, visualization, segmenting? I hear you say. Yeah, those are really important as well, but if you screw up data collection, you screw up all the subsequent stages as well.\n  That image above, lovingly adopted from online-behavior.com (my favorite analytics blog), illustrates this nicely. I used my Photoshop skillz to copy-paste the cute little diamond bug on top of the \u0026ldquo;Collect Data\u0026rdquo; node. This represents a virus that has infected data collection. It means you\u0026rsquo;ve either failed or forgotten to measure something on the site. As you might guess, it means that this data is either not available or is corrupt.\nA typical example is in Ecommerce tracking. Ecommerce comprises such a huge number of variables that successful comparison across date ranges would require that these variables be encoded consistently across different points in time. How often does that happen? I\u0026rsquo;m willing to say rarely, and the likelihood decreases with time. However, still we see people comparing Revenue from last quarter (which included shipping and tax) with Revenue from a year ago (which did not include shipping and tax). The very fact that they have different aggregation methods means that they can\u0026rsquo;t be compared unless you know how these methods differ. Unfortunately, this information is often misplaced when teams and organizations change.\nIn other words, data collection is an investment. It requires labor, love, and a crystal ball. It\u0026rsquo;s one of the most difficult things to get right, and requires a lot of experience. But there\u0026rsquo;s nothing, nothing as frustrating as data that\u0026rsquo;s missing or incorrectly collected.\n2. Analytics as a PROJECT is the root of all evil Here we go: diving into the controversial end of the pool. Let me juice it up with another claim.\nData quality is destroyed by ineffectiveness and non-involvement.\nLet\u0026rsquo;s see if we agree on this. Are you, or have you been, affected by the following phenomena in your organization?\n  Monthly reports, which lack relevance, are rife with generic suggestions that lack research in the context of your business, reiteration of previous month\u0026rsquo;s points, even if there are solid reasons why they weren\u0026rsquo;t addressed\n  Ridiculously ugly and ineffective JavaScript hacks for measurement points, which should be tackled in the Data Layer\n  Clueless managers who want you to fix metrics rather than the underlying reasons that output the data into those metrics (\u0026ldquo;Fix our Bounce Rate, please!\u0026quot;)\n  Analytics features are deprioritized, and deployed extremely infrequently\n  These are all symptoms of data being treated as a project outcome! They smack of non-involvement. They are deliverables and off-shoots of mail-order projects, where an entire analytics process is treated as something that can be time-boxed, outsourced, and ignored.\nIn the agency world, these assignments were quite typical. It\u0026rsquo;s not because the agencies are lazy (well, sometimes it\u0026rsquo;s that too), but it\u0026rsquo;s because organizations have not matured enough to turn analytics into a business-driver, and treat it as a diagnostics tool instead.\nWhat I\u0026rsquo;ve learned is that the only way to make analytics truly work in an organization is to implement it as a process. As consultants, we can be \u0026ldquo;purchased\u0026rdquo; to help get things moving. This might mean an implementation project, or a consultancy project, but the key deliverable of these projects is a shift in the momentum of the organization that we are trying to help. If the organization chooses to externalize analytics to an \u0026ldquo;extra pair of hands\u0026rdquo;, which happens ever so often, they will lack the incentive to actually integrate an understanding and respect of data into their processes.\n3. Tag management solutions help us fight Conway\u0026rsquo;s law One of the reasons analytics is so difficult to integrate into organizations is because it\u0026rsquo;s cross-disciplinary. I wrote about this in a recent article in my company\u0026rsquo;s blog (10 Truths About Data):\nData is the lifeblood of the organization. It flows through all departments, across job titles, permeating the very fabric of the organization, reinforcing its foundations for growth. It cannot and should not be contained in one vector (a dedicated analyst) alone. Now, try to think of anything in a modern organization that works like that. It\u0026rsquo;s difficult, right? A typical organization is riddled with departments and silos, and there\u0026rsquo;s just so much friction in the seams.\nIn 1968, Melvin Conway said something pretty insightful, which was then coined into a law bearing his name. The law is approximated as (quoting Wikipedia):\norganizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations In other words, if the departments within your organization (e.g. IT and marketing) are unable to communicate with each other, any systems they manage or want to create (e.g. the CMS and an analytics platform) will be unable to function together as well.\nThis is one of the reasons we resort to projects so eagerly. By requesting a project, we are allocating our department\u0026rsquo;s budget to said project, and we\u0026rsquo;ll receive something that\u0026rsquo;s optimized for our department, and our department alone. For example, the marketing department might want to implement Google Analytics on some new landing pages, and they request the help of an agency to write the implementation plan. The result is an implementation plan that only tackles the problems the marketing department outlined, instead of treating data collection as something owned by the entire organization.\nThe agency might also utter the most ridiculous words in the history of language:\n\u0026ldquo;We recommend you install Google Tag Manager, because then we won\u0026rsquo;t need IT anymore to implement analytics measurement.\u0026rdquo;\nI\u0026rsquo;m not a violent man, but when I hear or see that argument it makes my blood boil. I feel very strongly about this, and for a good reason. Nothing is as detrimental to an organization as an outside influence (e.g. agency) that communicates only with a single point of contact (e.g. marketing), but inadvertently gets mixed up in something that spans the entire organization (the IT infrastructure).\nFriends, compatriots, fellow analytics aficionados: data and analytics belong to the entire organization, not just a subset thereof. It\u0026rsquo;s unnerving, and quite honestly impossible, to try to implement something as significant as a bloody JavaScript injector (which a tag management solution often is) without involving the developers or IT!\nIn fact, I want to make the following bold claim:\nA tag management solution empowers DEVELOPERS more than others.\nWe implement a tag management solution because we want to harness a very developer-centric tool to facilitate communication within our organization. This necessarily involves multiple disciplines in the decision-making process. If you entrust a TMS solely in the hands of a department or individual that already has prejudices against working with developers, it will only inflame this relationship further.\nThe way I see it, an optimal implementation of a tag management solution is perfect for mitigating the effects of Conway\u0026rsquo;s law. Similarly, a cooperative IT or developer process is paramount in an optimal implementation of a tag management solution.\n4. Data Layer is the universal translator You see, a tag management solution is built on top of a Data Layer. Typically, we have a number of definitions for a Data Layer, but here are the three I most often fall back to:\n  A set of business requirements, encoded as key-value pairs against each hit, visit, and visitor that your digital property might collect\n  A uniformly encoded, global data structure that collects and distributes data on the digital property\n  An internal data model in the tag management solution with which you communicate through the global data structure\n  OK, let\u0026rsquo;s try again. That was confusing.\nThe purpose of a Data Layer is to provide a **bilateral** layer on the digital asset, which **decouples**, **normalizes**, and **uniformly encodes** semantic information passed through and stored within.   Hmm, I\u0026rsquo;m not sure that made any sense either.\nLook, the Data Layer is a translator. It forces you and your buddies in IT to communicate using a common syntax. It\u0026rsquo;s a joint venture, where people and systems communicate across silos. You can keep on hating the gremlins in the IT lair if you want, but you\u0026rsquo;d better respect and love the Data Layer. If you don\u0026rsquo;t get this right, then you can just chuck the TMS out of the window, as it won\u0026rsquo;t be worth the migration.\nPerhaps unsurprisingly, I\u0026rsquo;ve written about the Data Layer before, and the more I work with organizations, the more I place my faith in its transformational power.\nWhen crafting an implementation or measurement plan, involve all the relevant departments and individuals in the process, and speak only Datalayerian. Instead of writing vague descriptions of what you might want to measure, write down key-value-pairs. Start and end with the Data Layer, in all parts of the implementation and data collection process. Let she or he who knows the constraints of this structure best lead the discussion. Translate all requirements you hear into variables and values, associated with their respective measurement points.\n5. Empower and facilitate the developers My past year at Reaktor has only strengthened something I already knew: agility is extremely helpful in the proper implementation of an analytics process.\nIt\u0026rsquo;s not easy in a world still dominated by the pitfalls of the waterfall, where projects comprised distinctive stages, each with entry and exit conditions.\nThe handovers between the discrete stages of the waterfall are where data is most often corrupted! When a deliverable is passed from one stage to another, it\u0026rsquo;s always a release rather than a continuum. Whoever passed the information is absolved of responsibility outside that of the deliverable, and whoever received the setup is forced to survive the possible conflicts, quality issues, and misgivings of the previous stages.\nThe void between the project milestones is where good data turns bad.\nInstead, what if we treat the process not as a collection of milestones but rather something that\u0026rsquo;s on-going and part of everything we do in our front-end or app development?\nThe first thing I want to implement in an analytics process is a proper consideration of measurement when developing features.\n  Since the agile process is, by nature, quite chaotic, we need something to facilitate quality assurance. Often, this comes in the shape of a Definition of Done. It\u0026rsquo;s a set of conditions that determine what is considered a success in the development process.\nIf we add analytics and measurement requirements into the Definition of Done, we are actually mitigating data quality issues even before they crop up. By making sure that developers consider measurement when developing or updating features, we are cultivating a perfect breeding ground for successful data collection.\nThis requires, of course, that the developers are aware of what measurement entails. I could refer you to read the previous chapters again, but I can just reiterate myself here:\nA tag management solution empowers DEVELOPERS more than others.\nWhenever I start working with a client, I educate the developers first. This might include:\n  Helping them design modules and interfaces for communicating with the Data Layer\n  Getting them up-to-speed in front-end development and the requirements that tag management solutions place on the document object model\n  Inspire them to create a sandbox for tag management solution testing (this WASP playground is pretty much what I have in mind)\n  Only hire to educate, not to delegate\n  Oh, that last point is so important. I respect the naïveté and hypocrisy (as I\u0026rsquo;m also a consultant) of the statement, but I truly believe in it.\nA good consultant should work in a manner that inevitably makes them redundant.\nTake that statement with a grain of salt. What I mean is that if you hire consultants for outsourcing tasks that should be handled within your organization, you not only create an unfavorable dependency, but you also lose the opportunity to improve your own processes.\nData collection and analytics are prime examples of this. I have most success when working in the client\u0026rsquo;s premises, being extremely transparent about everything I do, and working together with their key players so that they can eventually do everything I\u0026rsquo;ve done themselves. This builds trust and helps the consultant find new areas to improve and focus on, rather than things that are trivial to the consultant but super important to the client.\nSummary This text is a summary of what I\u0026rsquo;ve been talking about this past year in conferences. I\u0026rsquo;m crazy passionate about data quality, data collection, and facilitating growth in organizations.\nI believe in this stuff with every fibre of my being.\nI believe that successful projects morph into processes. They rejuvenate organizations, and build trust across silos that are engaged in ridiculous trench warfare, propagated by inflexibility and complicated hierarchies.\nI believe that an empowered developer is at the heart of a successful analytics project. However, the entire team works together to build bridges from business requirements, through the Data Layer, all the way to reports.\nI also believe that data is difficult. I\u0026rsquo;ll end this article with another one of my \u0026ldquo;10 Truths About Data\u0026quot;:\nData is difficult. It has to be. There are no short-cuts, no omniscient tools, no out-of-the-box solutions that give you perpetual satisfaction. The entire life cycle of a single data point, from collection to reports, requires knowledge and expertise to manage. Turning arbitrary numbers into actionable insights is not, and never will be, a walk in the park. Do you agree?\n"
},
{
	"uri": "https://www.simoahava.com/tags/process/",
	"title": "process",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/tag-management-solutions/",
	"title": "tag management solutions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/iframe-cross-domain-tracking-in-google-tag-manager/",
	"title": "Iframe Cross-Domain Tracking In Google Tag Manager",
	"tags": ["google analytics", "Google Tag Manager", "Guide", "iframe"],
	"description": "How to track iframes with cross-domain parameters using Google Tag Manager and Google Analytics.",
	"content": " NOTE! This solution has been upgraded, and the new approach can be found here.\n If you\u0026rsquo;re unfamiliar with the lingo, cross-domain tracking is a hack used by Google Analytics to circumvent the web browser\u0026rsquo;s same-origin policy. Essentially, the policy dictates that browser cookies can only be shared with a parent domain and all its sub-domains. In other words, domainA.com and domainB.com do not share cookies.\nSince Google Analytics calculates sessions and users by using a cookie, this is problematic. If the user navigates from domainA.com to domainB.com, they will, by default, browse domainB.com in a different session (often a different user as well) from that on domainA.com.\nThere are some great guides out there for implementing cross-domain tracking, and I want to highlight the Bounteous article as well as the Knewledge tutorial. Both have excellent coverage of this difficult topic, and Knewledge have devoted a lot of words for iframe tracking as well.\n  In this article, I want to go over iframe tracking once again, and update the method to GTM V2. We\u0026rsquo;ll be using the hitCallback method to decorate the iframe properly with cross-domain tracking parameters, to ensure that any tracking that takes place within is attributed to the original session (and user).\nQuick note on SUB-domain tracking Remember, cross-domain tracking applies only to separate parent domains. If you want to track traffic across sub-domains, you don\u0026rsquo;t need cross-domain tracking. Instead, you just need the cookieDomain field set in your Tags and you\u0026rsquo;re good to go. Luckily some visionary has already written a simple guide about this.\nCustom JavaScript Variable To make it all work, all you\u0026rsquo;ll need is a Custom JavaScript Variable. Well, and a simple tweak to your Page View Tag. And some testing. And good luck. But anyway, the Custom JavaScript Variable is what patches everything together.\nWe\u0026rsquo;ll use the Variable in the hitCallback field of the very first Tag that fires on your site (should be the Page View Tag). This callback function will reload any iframe of your choice with linker parameters, meaning the Tags in the iframe will run nicely as part of the ongoing session.\nLinker parameter is basically your browser\u0026rsquo;s unique client ID (with some other stuff), attached as a query parameter to the iframe src value. This is then picked up by the tracker in the iframe and used to recreate the _ga cookie in the iframe. That\u0026rsquo;s how Google Analytics\u0026rsquo; cross-domain hack works!\nSo, to get started, create a new Custom JavaScript Variable, and name it intelligently. I\u0026rsquo;ve used {{JS - hitCallback for X-Dom iframe}}, but you can use something less poetic if you wish. Here\u0026rsquo;s what it looks like:\nfunction() { return function() { try { var gobj = window[window.GoogleAnalyticsObject]; var iframe = document.querySelector(\u0026#39;#myIframe\u0026#39;); var tracker, linker; if (gobj) { tracker = gobj.getAll()[0]; linker = new window.gaplugins.Linker(tracker); iframe.src = linker.decorate(iframe.src); } } catch(e) {} } }  So, time for a line-by-line walkthrough.\nfunction() { ... }  On the first line, you\u0026rsquo;re declaring a basic GTM JavaScript function, which needs to be an anonymous function. Nothing special here, moving on.\nreturn function() { ... }  Next, you\u0026rsquo;re returning another function, as hitCallback requires a function as a parameter for it to work correctly. Also, by returning a function, we\u0026rsquo;re avoiding unsolicited side effects which would result if you just executed global DOM methods in the outer function. Since we\u0026rsquo;re operating in a closure, we\u0026rsquo;re restricting the execution of this function to only the single time it\u0026rsquo;s called, which is when the hitCallback is executed at the end of the Google Analytics payload dispatch process.\ntry { ... } catch(e) {}  We\u0026rsquo;re wrapping the code with a try...catch because that\u0026rsquo;s simply a good practice. You can add some debugging code into the catch(e) {} block if you wish, but I usually just let it sizzle down silently.\nvar gobj = window[window.GoogleAnalyticsObject]; var iframe = document.querySelector(\u0026#39;#myIframe\u0026#39;); var tracker, linker;  The following three lines setup some variables. First, we\u0026rsquo;re setting gobj to the global Google Analytics function name. This is again just a good practice, since there are cases where you want to change the function name from ga to something else. With this one-liner, we\u0026rsquo;re making sure we\u0026rsquo;re always referring to the correct object.\nThe next line is important. This is where you\u0026rsquo;ll retrieve the iframe element you want to modify. I\u0026rsquo;m using a simple CSS selector to find my iframe (an element with the ID myIframe), so remember to change it to reflect whatever you want to target. You can run this code on multiple iframes if you wish. You just need to loop through each iframe, and run the linker.decorate() method on each.\nWe\u0026rsquo;re also declaring tracker and linker as utility variables, which we\u0026rsquo;ll use in a bit.\nif (gobj) { tracker = gobj.getAll()[0]; linker = new window.gaplugins.Linker(tracker); iframe.src = linker.decorate(iframe.src); }  The final part of the code is where the magic happens. Checking for the existence of the global Google Analytics object is just another good practice you might want to employ.\nNext, we initialize tracker with the first Google Analytics tracker created on the page. As I mentioned before, this should optimally be the tracker created by your Page View Tag. We use the getAll() function call to retrieve all the trackers created on the page, and then pick the first one with [0].\nOn the following line we use the Google Analytics Linker plugin, initializing it with the retrieved tracker. This plugin is what we\u0026rsquo;ll use to decorate the iframe src with the correct cross-domain parameters. The plugin has a utility method decorate, which takes a URL string as its parameter.\nIn other words, you\u0026rsquo;re updating the iframe\u0026rsquo;s src value with a decorated URL, and this decoration actually adds the correct cross-domain parameters to the URL.\nOnce you\u0026rsquo;ve written all this code, remember to save the Variable.\nFinally, you need to add this new Variable into the hitCallback field of your Page View Tag.\nSo, open the Tag, browse to More Settings -\u0026gt; Fields to set, and add a new field:\n  There we go! We have now successfully set the stage for one of nature\u0026rsquo;s most spectacular ev\u0026hellip; Sorry, been watching too much Planet Earth.\nWhat will happen Here\u0026rsquo;s the process of what happens when you next browse the site:\n  The Page View Tag fires, and after it has completed, its hitCallback field is executed\n  Since this field had our custom function returned, the code within is run\n  This code takes the tracker that was just created, and decorates the iframe(s) of your choice with cross-domain tracking parameters\n  The iframe will reload (because its src value changed) with the cross-domain tracking parameters\n  What else you\u0026rsquo;ll need Now, any Tags firing in the iframe should have the allowLinker : true field set. You can find instructions for this in the Bounteous guide, for example. This simple little field will ensure that the Universal Analytics library looks for cross-domain tracking parameters in the URL, and uses them to set up the tracker in the iframe.\nIn other words, we\u0026rsquo;ve circumvented the crippling effect of same-origin policy on cookie sharing by, well, sharing a cookie value as a URL parameter. The wonderfully robust Universal Analytics library takes care of the rest.\nAlso, you will want to defer the Page View Tag from firing on the landing page of the iframe. Why? Because if you didn\u0026rsquo;t, the iframe might send a Page View when it\u0026rsquo;s first loaded, and then again when the iframe is reloaded with the hack. By preventing the iframe from firing on any page whose referrer is not the iframe URL and when the page is in an iframe, you\u0026rsquo;ll prevent double- or triple-counting your page views in the iframe. For subsequent page views you\u0026rsquo;d want to fire the Tag, because it reflects actual browsing behavior.\nAlso, if you look at the Knewledge tutorial, they have a nice solution for actually not loading the iframe in the first place, and only loading it when the hitCallback is executed (or if there\u0026rsquo;s some problem with loading the JavaScript).\nIt\u0026rsquo;s all up to you, of course. Double- or triple-counting isn\u0026rsquo;t always a problem.\nIf you have a situation where the iframe isn\u0026rsquo;t in the DOM in time, you might also need to add a setTimeout in the hitCallback to allow time for the DOM to complete. However, this is very rare in my experience.\nAnyway, working with iframes is quite difficult, as you can probably guess. Lots of things to pay attention to.\nThat was a perfect segue to the final chapter and a well-deserved rant.\nSummary This was a rather simple guide for setting up iframe tracking with cross-domain parameters. Let me be clear and on-the-record about something:\nIFRAMES SUCK!\nThey are horrible, Lovecraftian subterranean beings of horror that crawl from the nether pits of darkness after the tolling of the midnight bell. They are playground bullies; entitled little brats that wreak havoc on innocent markup. They are artefacts of human laziness, true examples of Conway\u0026rsquo;s Law in motion. They are, in essence, untrackable little shit-monsters that exist in the void between websites, accountable to none and hazardous to all.\nI can personally attribute some 45% of my hair loss to working with iframes, and they have ruined my day far too many times to call it coincidence.\nIf you are forced to work with iframes, cross-domain tracking might be the least of your worries.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/setup-google-tag-manager-ios-with-swift/",
	"title": "Setup Google Tag Manager iOS With Swift",
	"tags": ["apps", "google analytics", "Google Tag Manager", "Guide", "ios", "mobile", "swift"],
	"description": "Guide for setting up Google Tag Manager tracking in your iOS app using Swift and the legacy SDK.",
	"content": "I\u0026rsquo;ve been meaning to write about Google Tag Manager for mobile apps for such a long time. Finally, I have some great use cases to share, as well as some useful examples for implementing GTM for iOS. That\u0026rsquo;s right, this is an iOS guide, and, to be more specific, geared towards a successful Swift implementation.\n  If you didn\u0026rsquo;t know, Swift is a relatively new programming language, developed by Apple for iOS and OS X programming. It\u0026rsquo;s got some nice synergies with existing Objective-C implementations, which means you can either gradually migrate to Swift, or just start writing new apps with it instantly.\nIn this article, we\u0026rsquo;ll walk through a simple Swift implementation of Google Tag Manager and Google Analytics. I\u0026rsquo;ll very briefly review the GTM SDK in the Comments chapter, but this is a how-to article, not a why-should-I writeup. I\u0026rsquo;m going to be expanding on this theme of mobile GTM in future texts, so this will stand out as a starting point for understanding how the SDK plays together with your application.\nMany thanks to my colleague, Sampsa Kuronen, for exposing me to the dark arts of Xcode and Swift.\nGetting started This guide is written with the assumption that you\u0026rsquo;re using the latest version of Xcode as your IDE of choice. It\u0026rsquo;s not the best IDE (by far) out there, but it has been created with iOS and OS X programming in mind, meaning there are certain benefits from using it.\nAnother, quite obvious, assumption is that you have access to your project source. You will be adding new files, importing headers and modifying project source code. It\u0026rsquo;s a good idea to run everything you\u0026rsquo;re doing by your team, as you don\u0026rsquo;t want to merge unverified source code into your project.\nHere\u0026rsquo;s something you might not yet fully realize:\nGTM for mobile apps is not the same as GTM for the web!\nThe level of freedom you have when working with the latter simply does not exist in the mobile app world. We\u0026rsquo;re working with projects and applications, so you need to have a solid understanding of how to do software development in a proper and consistent manner, abiding to any design patterns you have found successful.\n1. Google Tag Manager setup To get things rolling, let\u0026rsquo;s setup Google Tag Manager first. In this tutorial, we\u0026rsquo;ll create a simple Screen View Tag to test the implementation with. In later chapters, we\u0026rsquo;ll go over the event payload as well, but it\u0026rsquo;s up to you to create the Tag for it.\nIn GTM, create a new Container under some Account. Choose iOS as the container type, and give it a descriptive name.\n  Next, you\u0026rsquo;ll be whisked to the Container Overview, and you\u0026rsquo;ll see a small popup saying your container is ready to use. You can close that.\nAs you can see, it looks pretty much like GTM for web.\nNext, go to Tags, and click New.\n  Wow, that\u0026rsquo;s way fewer Tags than in GTM for web. Let\u0026rsquo;s hope we\u0026rsquo;ll get more soon! Also, make note that there is no Custom HTML Tag. It makes perfect sense, since you\u0026rsquo;re not working with HTML, but I thought I should point it out nevertheless.\nWe\u0026rsquo;ll be creating a simple Google Analytics Tag , which sends an App View (also known as a Screen View) to Google Analytics when the first View of the app loads (don\u0026rsquo;t worry, it will all be clear soon). So, create the Google Analytics Tag, and add the Tracking ID.\nMake sure that App View is selected as the Track Type, and expand More Settings -\u0026gt; Fields to Set.\nClick + Add Field, and add screenName as the Field Name. In Value, click the \u0026ldquo;Variable\u0026rdquo; button and choose New Variable\u0026hellip;.\n  We need to create a Variable, which picks up the desired Screen Name from the Data Layer (Screen Name is to an app what Page is to a website). So, after marvelling the list of Variables that\u0026rsquo;s completely different from GTM for web, choose Data Layer Variable.\nGive it a descriptive name, e.g. DLV - screenName, and set the Variable Name field to screenName. Save the new Variable.\n  Next, in the Tag screen, click Continue and proceed to the Fire On step.\nIn this step, click Custom, as you want to create a new Trigger for the Tag. So, in the popup that appears, click New.\n  Give the Trigger a descriptive name, e.g. Event - screenView, and set the condition as in the screenshot below.\n  Save the Trigger, and then click \u0026ldquo;Create Tag\u0026rdquo; to, well, create the Tag.\nPhew, almost done here! We\u0026rsquo;ll go with just this one Tag, as this is a simple setup guide.\nNext, go to the Versions page of your GTM container, and click open the Actions menu for the only container version you\u0026rsquo;ll see (the initial draft). Click Publish and follow the prompts to publish your container.\n  There, GTM is almost ready, hungry to process the Data Layer commands you dispatch from your app.\nJust one more thing to do. You\u0026rsquo;ll need to download the container version binary, as GTM for apps is a bit different in how it processes containers. Basically, you can\u0026rsquo;t trust that the app will have access to a container downloaded over the network, so you\u0026rsquo;ll always want to provide a local container binary file for the app as well. Read more about this in chapter 5.1.\nSo, click open the Actions menu for the Live container version, and choose Download. Make sure you download it somewhere you can easily find it from.\n  That\u0026rsquo;s it! We\u0026rsquo;re done with the GTM setup. All that\u0026rsquo;s left is the minor task of upgrading your App to use the Google Tag Manager (and Google Analytics) SDK!\nCongratulate yourself with a power bar and some coffee.\nI\u0026rsquo;ll wait. Maybe get some coffee myself, and boogie silently to your profound success in setting up GTM!\n(2. Create a new Xcode workspace) I won\u0026rsquo;t linger here, as I\u0026rsquo;m actually assuming you already have an Xcode project and workspace to work with. If you need something to get started with, take a look at this tutorial by Ray Wenderlich. If you pay close attention to the screenshots in the article you\u0026rsquo;re reading right now, you might notice that I\u0026rsquo;m actually using Ray\u0026rsquo;s Tip Calculator as the basis for this article.\nIf you only have a project setup and no workspace, keep reading. When you add the required dependencies to your project using Cocoapods (see below), a workspace is automatically generated for you.\n3. Download the SDKs using Cocoapods In order for Google Tag Manager to work with your app, you will need to download the SDKs for both Google Tag Manager and Google Analytics. To do that we\u0026rsquo;ll use Cocoapods, which is an open-source dependency manager for your iOS and OS X projects. As you might have guessed, both Google Analytics and Google Tag Manager SDKs are also distributed as Pods.\n  After you\u0026rsquo;ve installed Cocoapods (if you need help, check e.g. this guide), you need to go to your workspace root directory (the one with your .xcodeproj and .xcworkspace files), and type:\npod init\nThis command initializes a simple Podfile for your project.\nOpen Podfile with your favorite editor.\nBetween the lines target 'YourProject' do and end, you need to add the Pod references you want to download and install for your project, so add the following construct into the file:\ntarget \u0026#39;YourProject\u0026#39; do pod \u0026#39;GoogleTagManager\u0026#39; pod \u0026#39;Google/Analytics\u0026#39; end Once you\u0026rsquo;ve done this, you can run the command pod install, which will automatically install and import all the files to your Xcode project. Neat, huh?\n  Now that you\u0026rsquo;ve installed the Pods, the next thing you need to do is import them into your project.\nIt\u0026rsquo;s not exactly straightforward, as Google Tag Manager source code is in Objective-C, which means you can\u0026rsquo;t just directly import it to your Swift project. Instead, you need to use something called a Bridging Header.\n4. Create and edit the Bridging Header file A bridging header is key to creating projects where the codebase includes code from multiple languages - Objective-C and Swift in this case. Basically, you use a bridging header file to expose Objective-C structures in your Swift files, so that you can, for example, work with the GTM SDK (which is Objective-C at the time of writing) without having to worry about clashes and conflicts that occur when working across languages.\nFirst of all, if you installed the Pods correctly, you should see them in your Project navigator.\n  Now, to create the bridging header file, go to File -\u0026gt; New -\u0026gt; File. Under iOS / Source, you\u0026rsquo;ll find Header File, so select that and click Next.\n  Give the file a name: yourProjectName-Bridging-Header.h, and save it in your project root. Remember to choose a target for the file, so that the header file is properly associated with your project.\n  You should see the file open in your editor. Add the following import statements to the file, and then save it:\n#import \u0026#34;TAGManager.h\u0026#34; #import \u0026#34;TAGContainer.h\u0026#34; #import \u0026#34;TAGContainerOpener.h\u0026#34; #import \u0026#34;TAGDataLayer.h\u0026#34; #import \u0026#34;TAGLogger.h\u0026#34; #import \u0026lt;Google/Analytics.h\u0026gt; These headers include all the necessary Google Tag Manager headers, as well as all the Google Analytics SDK headers.\nFinally, you need to include the bridging header file in your build settings.\nClick your project (the root level node) in your Project navigator. In the settings screen that opens, choose the tag labelled Build Settings, and scroll all the way down to Swift Compiler - Code Generation.\nEdit the field next to Objective-C Bridging Header, and type in the file name of the bridging header file you just created.\n  Now, build your project and make sure everything checks out right.\nNext up, setting up Google Tag Manager in the project!\n5. Load Google Tag Manager in the project My, how far we\u0026rsquo;ve come!\nIf you look at how the developer guide kicks off, you\u0026rsquo;ll notice the rather limited capabilities of Google Tag Manager for mobile. For now, at least. Basically, you can dispatch Google Analytics hits and run custom functions. Also, you can change configuration values.\nIn this tutorial, we\u0026rsquo;ll focus on the first use case, so we\u0026rsquo;ll setup a simple interface for tracking screen views and events to Google Analytics.\n5.1. Add container binary to project Before we do anything, however, we\u0026rsquo;ll need to add the container version binary to the project. This is rather important, as even though GTM can load a \u0026ldquo;fresh\u0026rdquo; container over the network, some of the interactions on the site might actually need to be tracked before the asynchronous operation of loading the container is complete. In this case, GTM can fall back to the local container binary, also referred to as the default container.\nNOTE! This also means that you should try to keep the binary in the project as up-to-date as possible, so that batches which use the binary would be as similar as possible to those that use the fresh container. So make it a habit that every time you publish a new container version, you download the version binary, and add it to the project (delete the old binary first).\nWe\u0026rsquo;ll cover all this very soon, don\u0026rsquo;t worry.\nIn Xcode, simply drag-and-drop the container binary file to your project. A window should open, where you need to make sure that you add the file to your target (so check the relevant box).\n  If you don\u0026rsquo;t have a Supporting Files group under your project, now\u0026rsquo;s a good time to create one. Right-click on the container binary, and choose New Group from Selection. Name the group Supporting Files. Your navigator should look like this:\n  If you already do have a Supporting Files group, just move the binary into that folder.\nNext up, source code and stuff.\n5.2. Modify AppDelegate The AppDelegate.swift file is where you control states of your app. For example, when the app loads, AppDelegate, well, delegates operations to different parts of your application. Similarly, when your phone rings and interrupts the application, you\u0026rsquo;d control this process through AppDelegate.\nSince Google Tag Manager is something you want to load as the application loads, you\u0026rsquo;ll need to make changes to AppDelegate.swift. So, proceed to open the file in your editor.\nWe want to modify the function which is called once the application is loaded. Aptly, there\u0026rsquo;s one that looks like this:\nfunc application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -\u0026gt; Bool { // Override point for customization after application launch. // ... return true } That\u0026rsquo;s the callback we\u0026rsquo;ll open the container in. Here, \u0026ldquo;open the container\u0026rdquo; means we\u0026rsquo;ll make it available for dispatching hits using the available interfaces. On top of that, having a container available is a prerequisite for interacting with the TAGDataLayer interface. This is a slight departure, again, for how things work with GTM for web, where you could create a dataLayer object before the container loading began.\nAdd the following lines into the function:\nlet tagManager = TAGManager.instance() TAGContainerOpener.openContainerWithId( \u0026#34;GTM-XXXXXX\u0026#34;, tagManager: tagManager, openType: kTAGOpenTypePreferFresh, timeout: nil, notifier: self) // if DEBUG tagManager.logger.setLogLevel(kTAGLoggerLogLevelVerbose) Line-by-line, here\u0026rsquo;s what happens. First, you initialize an immutable variable named tagManager as a singleton instance of the TAGManager class.\nNext, you use the TAGContainerOpener interface to signal that you want to fetch a container to work with. Remember to change the \u0026ldquo;GTM-XXXXXX\u0026rdquo; to whatever the container ID you\u0026rsquo;re using is.\nThis next bit is a bit more complicated.\nWhen you download the container binary into the Supporting Files of your project, it\u0026rsquo;s called a default container. This is what GTM uses until a saved container is found, or if this saved container is not fresh (\u0026gt; 12 hours old), a new, fresh one is loaded over the network. This is why it\u0026rsquo;s imperative that the saved binary is usable. The first screen view of the app might well be ready to dispatch before a non-default container has loaded, which means you want the default container to be as accurate as possible.\nWhen choosing the openType value, I suggest you use either kTAGOpenTypePreferNonDefault or kTAGOpenTypePreferFresh.\nkTAGOpenTypePreferNonDefault attempts to load a saved container (i.e. one that was previously loaded over the network), regardless of whether it\u0026rsquo;s stale or not. Stale means it\u0026rsquo;s over 12 hours old. This means that you\u0026rsquo;ll have a container to work with sooner. If the container is stale, Google Tag Manager starts an asynchronous request for a fresh container over the network.\nkTAGOpenTypePreferFresh only loads a saved container if it\u0026rsquo;s fresh. If it isn\u0026rsquo;t, it loads a new, fresh container over the network.\nBoth revert to the default container if a saved container couldn\u0026rsquo;t be loaded or if there was a network error.\nNext, you can enable logging. I\u0026rsquo;ve commented if DEBUG because it might be a good idea to verbosely log only if you\u0026rsquo;re debugging the app, since it produces a lot of console output, which is unnecessary overhead in the live app.\nYou might have noticed that I skipped over the notifier: self key-value pair in the TAGContainerOpener call. That\u0026rsquo;s because it requires a bit more attention.\nThe notifier parameter is basically a setting, where I instruct GTM to notify my app when a container becomes available. Once a container is available, our methods and functions can create an instance of the AppDelegate class to access the loaded container. You can use container to update your app\u0026rsquo;s configuration values through GTM, for example.\nBecause you\u0026rsquo;re invoking self as the parameter value, it means that the class you\u0026rsquo;re currently in (AppDelegate) needs to implement the TAGContainerNotifier protocol, as self is bound to an instance of the current class.\nScroll up to the beginning of AppDelegate, and change the class declaration to:\nclass AppDelegate: UIResponder, UIApplicationDelegate, TAGContainerOpenerNotifier { ... var container: TAGContainer? In other words, you add the TAGContainerOpenerNotifier to the declaration line, and in the beginning of the class (so not in a func), you create a new variable container, which refers to a TAGContainer optional.\nPhew! Almost there.\nIn the end of the class, after all the other func declarations, add a new one:\nfunc containerAvailable(container: TAGContainer) -\u0026gt; Void { dispatch_async(dispatch_get_main_queue(), { self.container = container; }); } And now things should start aligning in your mind.\ncontainerAvailable is the callback that TAGContainerOpener invokes once a container becomes available. The container is passed as a parameter to this function.\nNext, we initialize self.container with the retrieved container. This way your app can use the loaded container through an instance of the AppDelegate class.\nBy the way, here\u0026rsquo;s a nifty trick. If you want to always load a fresh container over the network, you can add the following line to the dispatch_async callback:\ncontainer.refresh() This forces a fresh container load over the network.\nHow you want to load the container is completely up to you. Some methods, such as container.refresh(), impose more network requests than others, so it\u0026rsquo;s always a compromise between freshness vs. availability vs. network load.\nMuch of the container load process is explained here, though I have to say it\u0026rsquo;s not very clearly documented at all (and it\u0026rsquo;s in Objective-C).\nNow, build the project and make sure no errors pop up. You can run the project, and you should already see some verbose logging:\n  You\u0026rsquo;re ready to push stuff to the Data Layer to fire your Tags. Awesome!\n6. Push stuff to Data Layer This is where Google Tag Manager for apps is very similar to GTM for web. By invoking the push method of the Data Layer instance, GTM will react to the dictionary you push and fire any relevant Tags.\nTo try it out, open a view controller file in your project. If it\u0026rsquo;s a default project setup, there should be one named ViewController.swift.\nNow, to send the screen view, it makes sense to only send it once the view has loaded. So, find the method with the following signature, or create one if it doesn\u0026rsquo;t exist:\noverride func viewWillAppear(animated: Bool) { super.viewWillAppear(animated) } Let\u0026rsquo;s create an instance of dataLayer and push the payload to it. So modify the function to look like this:\noverride func viewWillAppear(animated: Bool) { super.viewWillAppear(animated) let dataLayer = TAGManager.instance().dataLayer dataLayer.push([\u0026#34;event\u0026#34; : \u0026#34;screenView\u0026#34;, \u0026#34;screenName\u0026#34; : \u0026#34;Home\u0026#34;]) } First you create an immutable variable dataLayer, which is a reference pointing to the dataLayer interface of a TAGManager instance.\nNext, you do the dataLayer.push(), sending a dictionary with key-value pairs. As with GTM for web, the 'event' is what makes things tick, and the other variables can (and will) be utilized if that\u0026rsquo;s how you\u0026rsquo;ve set the Tags up.\nSince we already have a Tag which uses the 'screenName' variable, this code should work, sending a screen view with Home as the screen name.\nGo ahead, build the project and run your app. You should see something like this in the debugger:\n  As you can see, the \u0026amp;cd; (for screen name) and \u0026amp;t; parameters are right there, meaning the payload is OK. Then, there\u0026rsquo;s the GET request to the Google Analytics endpoint, signalling that the payload has been dispatched.\nAnd that\u0026rsquo;s it! You\u0026rsquo;ve got a functioning Google Tag Manager installation in your hands.\nBONUS: Simple interface for interacting with the Data Layer Since we want to be slightly more ambitious, we can actually create an interface (or module) for interacting with the Data Layer. This way you can create elegant code in your app, simplifying some of the complex interactions and mitigating the risk of human error (e.g. typos).\nCreate a new file called Analytics.swift. Add the following code within:\nimport Foundation class Analytics { private static let dataLayer = TAGManager.instance().dataLayer private struct DataLayerMessage { let event: String let ScreenName: String func getPayload() -\u0026gt; [String:AnyObject] { return [\u0026#34;event\u0026#34; : self.event, \u0026#34;screenName\u0026#34; : self.screenName] } } static func pushScreen(screenName: String) { let screenViewData = DataLayerMessage(event: \u0026#34;screenView\u0026#34;, screenName: screenName) dataLayer.push(screenViewData.getPayload()) } } Here, you create a new class called Analytics. Within, you define some variables, structs and methods that can be used by other parts of your app.\nYou can expand the DataLayerMessage struct and the pushScreen method to include more extensive payloads, such as event objects (with category, action, label, and value), custom definitions, content groups and so forth.\nTo send a screen view with this interface, all you need is the following command:\nAnalytics.pushScreen(\u0026#34;Some screen name\u0026#34;) The class will take care of the rest.\nIt might seem complex, but it actually reduces a lot of overhead, since everything is neatly encapsulated in this single Analytics module.\nIn later articles we\u0026rsquo;ll use a similar structure in all the GTM interactions, so that things will stay nice and functional.\nComments on Google Tag Manager for iOS It\u0026rsquo;s not perfect, I\u0026rsquo;ll give you that. You might wonder what the actual benefit is to just using the Google Analytics SDK, and you\u0026rsquo;re right to feel a bit confused.\nHowever, as with GTM for web, the main benefit is with the Data Layer. Using the Data Layer interface lets you collect arbitrary semantic information from the app, and push it into the message queue without having to consider platform-specific implications. Instead, you can trust that whatever app uses the Data Layer will be able to transform the data to the form required by whatever endpoint it communicates with.\nAnd that\u0026rsquo;s exactly why we spent some time to create a custom interface for interacting with Google Tag Manager. We want to decouple semantic information from actual app logic, only sending generic strings and dictionaries through the interface.\nThis way, when GTM for apps hopefully expands and becomes richer and larger, you\u0026rsquo;ll have the general infrastructure in place already, and implementing expansions and additions will happen organically and in a controlled, fluid manner, which leaves your app none the worse for wear.\nSummary This has been a rather extensive tutorial for something as \u0026ldquo;simple\u0026rdquo; as setting up Google Tag Manager in your mobile app. Well, I hope that bubble has burst. It\u0026rsquo;s not simple. It\u0026rsquo;s not meant to be. You\u0026rsquo;re messing with a software project for goodness sake. You don\u0026rsquo;t want to mess up your chances with App Store review, or the hundreds of thousands of eager users that cringe if even the smallest thing is amiss in your beautiful app.\nGTM for mobile is not perfect. Far from it. There\u0026rsquo;s still a lot to be done, and I hope the developers come up with some solid selling points for the platform, before people dismiss it entirely in favor of the pretty robust Google Analytics SDKs.\nTo me, one of the biggest issues is, funnily enough, the terminology used. GTM for mobile is so far removed from GTM for web that it\u0026rsquo;s weird they even share the same name. There are no \u0026ldquo;tags\u0026rdquo; in mobile development. It\u0026rsquo;s just variables, classes, structs, dictionaries, functions, objects, etc.\nIt\u0026rsquo;s not \u0026ldquo;tag management\u0026rdquo;, it\u0026rsquo;s app management, and as such it carries with it far greater risks than anything you could unleash on your website.\nAnd the risks are the main reason I want to stay far away from the value collection methods and the whole idea of updating configuration values of the app through GTM. A mobile app, being a software project, should be governed by the code base, and not some outside dependency.\nI don\u0026rsquo;t understand why you\u0026rsquo;d want to delegate app operations to a web-based control panel, when all it takes is a single failed network request and a slightly outdated default container to screw up the experience for the user. And I\u0026rsquo;m sure that app users are less forgiving than web visitors, especially if the app costs money.\nThings like automatic screen tracking or automatic exception handling are not things I particularly miss, as creating the Analytics.swift module pretty much gives you a one-liner to track all your screenviews with (and events, if you extend it a little). Thus, ultimately it\u0026rsquo;s the same effort as setting a self.screenName property or something.\nOther than the above, setting GTM for apps up is a breeze once you understand the logic. I love how I can use the Data Layer to decouple semantic information stored in the app from the app logic itself. I also love how I don\u0026rsquo;t have to write analytics-specific syntax, and I can just use a generic Data Layer syntax instead. Perfection!\nI\u0026rsquo;m looking forward to writing more about GTM for mobile soon. There are many ways you can improve the analytics tracking, and the SDK still has many tricks up its sleeve that you can use to facilitate your app tracking even further.\n"
},
{
	"uri": "https://www.simoahava.com/tags/adblocker/",
	"title": "adblocker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/content-blocker/",
	"title": "content blocker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/measure-ad-content-blocker-impact-on-traffic/",
	"title": "Measure Ad And Content Blocker Impact",
	"tags": ["adblocker", "content blocker", "google analytics", "Google Tag Manager", "Guide"],
	"description": "How to measure ad and content blocker impact on your Google Analytics tracking using Google Tag Manager.",
	"content": "So, looks like iOS 9 has built-in support for integrating \u0026ldquo;content blocking\u0026rdquo; extensions on your iPhone. Now, blocking ads and other intrusive content is nothing new, nor do I want to get into a debate about whether it\u0026rsquo;s a good thing or not. But as a data geek I\u0026rsquo;m very interested in knowing just what share of my site tracking has some content blocker enabled. In this post, I\u0026rsquo;ll show two tricks (easy and advanced) to expose these content blockers. It\u0026rsquo;s always a good idea to keep tabs on the numbers, especially if you\u0026rsquo;re concerned about them blocking your analytics tools as well (as you should be!).\n  The two solutions I\u0026rsquo;m going to introduce differ in their accuracy. Both require that you upload a small file to your web server. After that, the accuracy depends on whether or not you\u0026rsquo;re interested in knowing how many of these content blockers end up blocking Google Tag Manager as well! As it turns out, there are some that do, and you might want to measure that traffic as well.\nNOTE! Just to dispel any confusion, the following solutions will not work if Google Analytics tracking is blocked by preventing the HTTP request from ever leaving the site. The hack using an XMLHttpRequest() will circumvent those blockers which simply prevent analytics.js from loading, but it will not help if the HTTP request to Google Analytics is blocked. To measure THAT share of traffic in GA, you\u0026rsquo;ll need to relay the hits via a local web server endpoint, and send the Measurement Protocol hit to Google Analytics from your web server, where ad and content blockers can\u0026rsquo;t reach it.\nCreate the JavaScript file and upload it First thing you need to do is create a simple JavaScript file called advertisement.js and upload it to the web server. This file has just one single line:\nwindow[\u0026#39;noBlocker\u0026#39;] = true;  This file functions as bait. It has the most blatant name you could think to give to an ad library, so most of the blockers should latch onto it with their greedy, publishers\u0026rsquo;-livelihood-decimating-tendrils.\nYou need to upload this file to some location on your web server where you can link to it from your page template (or from GTM). Because I\u0026rsquo;m using WordPress, I uploaded it to the directory /wp-content/.\nOnce you\u0026rsquo;ve done this, you can follow either the easy solution (coming up soon), or the more intricate one (coming up later in the article). But start by reading the generic GTM configuration you\u0026rsquo;ll need for either approach.\nGoogle Tag Manager configuration First, you\u0026rsquo;ll need a Custom JavaScript Variable, which we\u0026rsquo;ll call {{JS - noBlocker}}. This Variable has the following code within:\nfunction() { return window[\u0026#39;noBlocker\u0026#39;] ? undefined : \u0026#39;true\u0026#39;; }  This will return nothing if the advertisement.js file loaded and created the global variable, and \u0026lsquo;true\u0026rsquo; if the file was not loaded. In other words, if a content blocker blocked advertisement.js, this Variable will return \u0026lsquo;true\u0026rsquo;.\nNext, you\u0026rsquo;ll need to create a Custom Dimension in Google Analytics. I\u0026rsquo;ve chosen Session as the scope, as Hit-level might be too granular, and User-level might be too broad. But it\u0026rsquo;s up to you. When you create the Custom Dimension, make note of the Index number that GA assigns to it.\n  Next, edit your generic Page View Tag. You need to add a new Custom Dimension row to it, with the Index number derived from Google Analytics\u0026rsquo; settings, where you just created the new dimension. The value of this dimension needs to be the Custom JavaScript Variable you created earlier. So it would look like this:\n  Let\u0026rsquo;s look at what\u0026rsquo;s going on here. When this Tag fires, it tries to populate Custom Dimension with Index number 4 with whatever the variable {{JS - noBlocker}} returns. If the site did not load the file advertisement.js, this dimension gets the value \u0026lsquo;true\u0026rsquo;, which is what\u0026rsquo;s dispatched to Google Analytics. If the file did load, the variable returns undefined, and the dimension does not get sent.\nAnd that\u0026rsquo;s the generic setup! That\u0026rsquo;s how we\u0026rsquo;ll know if a content blocker is running on the site or not.\nNext, we\u0026rsquo;ll need to choose how to link the file to the site, and whether or not we want to account for blockers that actually block GTM as well.\nEasy: link the file via GTM This is the \u0026ldquo;easy\u0026rdquo; setup, as we\u0026rsquo;ll be only using Google Tag Manager to upload advertisement.js.\nStart by creating a new Custom HTML Tag, named SETUP - Link to advertisement.js. This has the following code within:\n\u0026lt;script\u0026gt; (function() { var d = document.createElement(\u0026#39;script\u0026#39;); d.src = \u0026#39;/wp-content/advertisement.js\u0026#39;; // Modify this!  document.head.appendChild(d); })(); \u0026lt;/script\u0026gt; This creates a new element in the DOM (a script element), adds a link to the file you\u0026rsquo;ve uploaded earlier, and then appends it to the head of the document. Remember to modify the line with d.src = ... to reflect the actual location where you\u0026rsquo;ve uploaded the file to!\nSave the Tag. Note! Do NOT add any Triggers to it. Just save it. Good.\nNext, open your Page View Tag (the one where you just added the Custom Dimension to), and open Advanced Settings -\u0026gt; Tag Sequencing.\nUnder Tag Sequencing, check the box for \u0026ldquo;Fire a tag before Page View Tag fires\u0026rdquo;, and choose the SETUP - Link to advertisement.js from the drop-down list.\n  Save the Page View Tag.\nHere\u0026rsquo;s what\u0026rsquo;s going to happen. When it\u0026rsquo;s time for the Page View Tag to fire, the Custom HTML Tag is first executed. This Tag links to and executes the advertisement.js file you\u0026rsquo;ve uploaded to your web server. Next, the Page View Tag fires, and the Custom JavaScript Variable either detects a content blocker (if one is activated) or doesn\u0026rsquo;t (if one isn\u0026rsquo;t activated), and sends the Custom Dimension accordingly.\nStill with me? That\u0026rsquo;s all you\u0026rsquo;ll need. After this, data will start flowing in with your Page View Tags, annotating sessions which have a content blocker with the value \u0026lsquo;true\u0026rsquo; in the new Custom Dimension field.\nAdvanced: Page Template magic This is more advanced, as it requires you to modify the page template, but it will also account for traffic which also blocks Google Tag Manager (the b*stards!). It\u0026rsquo;s not a large share by any count, but it\u0026rsquo;s still something you might want to be wary of.\nNOTE! This solution uses a XMLHttpRequest to dispatch the data to GA, and many blockers filter block these requests as well. So you might be left with no other option than to send the data to an endpoint you own and are certain is not blocked by these tools. That\u0026rsquo;s left up to you and your developers, of course.\nLet\u0026rsquo;s start with the technical stuff first.\nYou\u0026rsquo;ll need to add the following tag to the page template before the Google Tag Manager container snippet. The logical place for it is in the \u0026lt;head\u0026gt; of the document.\n\u0026lt;script src=\u0026#34;/wp-content/advertisement.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Remember to change the value of the src attribute to reflect the actual location of the file.\nNext, add the following minified JavaScript code to the template as well. You can add it anywhere you want in the template, though adding it to the very end is logical as it\u0026rsquo;s just synchronous JavaScript.\n\u0026lt;script\u0026gt;!function(d,e){window.addEventListener(\u0026#39;load\u0026#39;,function(){if(!window.noBlocker\u0026amp;\u0026amp;!window.google_tag_manager){for(var t=new XMLHttpRequest,n=(new Date).getTime()+Math.floor(1e8*Math.random()),o=\u0026#39;ab_gc=\u0026#39;,a=document.cookie.split(\u0026#39;;\u0026#39;),i=0;i\u0026lt;a.length;i++){for(var c=a[i];\u0026#39; \u0026#39;==c.charAt(0);)c=c.substring(1);0==c.indexOf(o)\u0026amp;\u0026amp;(n=c.substring(o.length,c.length))}t.open(\u0026#39;POST\u0026#39;,\u0026#39;https://www.google-analytics.com/collect\u0026#39;);var r=\u0026#39;tid=\u0026#39;+e+\u0026#39;\u0026amp;cd\u0026#39;+d+\u0026#39;=true\u0026amp;t=pageview\u0026amp;dp=\u0026#39;+document.location.pathname+\u0026#39;\u0026amp;v=1\u0026amp;cid=\u0026#39;+n;t.send(r);var g=new Date;g.setTime(g.getTime()+63072e6);var d=\u0026#39;expires=\u0026#39;+g.toUTCString();document.cookie=\u0026#39;ab_gc=\u0026#39;+n+\u0026#39;; \u0026#39;+d}})}(\u0026#39;1\u0026#39;,\u0026#39;UA-1234567-1\u0026#39;);\u0026lt;/script\u0026gt; It looks nasty, I know! Read through the following bit carefully.\nThis script adds a window.onload listener, which fires after the entire window object has loaded. In other words, it waits for the page load and all scripts, images, and external assets to load as well.\nNext, it checks for the existence of the noBlocker global variable, which, if you remember, is created in the advertisement.js file. It also checks for the existence of the google_tag_manager object, which is created by Google Tag Manager.\nIf both of these are missing, it means that a content blocker has blocked both advertisement.js and GTM!. How rude!\nIf these are blocked, the script proceeds with the following:\n  Create a new randomized clientId, using the current timestamp and a random number.\n  If a cookie named ab_gc exists, use its value as the clientId instead.\n  Fire a Measurement Protocol hit to Google Analytics, using the clientId established in either step (1) or (2), and send the hit as a Page View, using the current page as the location, and setting the value \u0026lsquo;true\u0026rsquo; to a custom dimension.\n  Write/update the cookie ab_gc with the clientId.\n  One important thing to note here. See the very last parentheses in the script: ('1','UA-1234567-1')? You need to update those yourself.\nThe first value in the parentheses is the index number of the Custom Dimension you want to send details about the ad blocker to. You can, for example, just use the Custom Dimension you created earlier, or you can do what I\u0026rsquo;ve done.\nI created a new Google Analytics property to collect these hits, and the Custom Dimension is a hit-scoped dimension, used in an Include Only filter on the reporting view. In other words, this new property will only accept hits that have this Custom Dimension in them, meaning it will only collect data from users that block both advertisement.js and GTM (grrr!).\nSo that\u0026rsquo;s what the second value in the parentheses is. It\u0026rsquo;s the property ID where you want to dispatch the data to. Personally, I wouldn\u0026rsquo;t use my main property, as this data simply isn\u0026rsquo;t comparable. It\u0026rsquo;s programmatically created, and is very limited as to what information is sent. You can, of course, populate all the dimension fields with some clever JavaScript magic, but I\u0026rsquo;m not that interested in all those gimmicks.\nI\u0026rsquo;m just interested in knowing how many sessions are blocking both ads and GTM (stop it!).\nSummary What you do next is up to you. You can create a custom report with session shares for those with a ad blocker vs. those without. With this data, you can optimize your site, making sure no critical information is behind potentially blocked sources.\nThere are other solutions out there as well that you might want to check out.\nUploading the advertisement.js might not be the most robust solution out there, as it doesn\u0026rsquo;t require too much imagination from content blockers to disregard this file in the future. Some methods of blocker detection include creating elements in the DOM which resemble slots for advertisements. These will probably work nicely as well.\nWith this solution, you\u0026rsquo;ll get some useful information about content blocker shares on your site, and you can immediately act on this information.\nI really, really, wish Google Tag Manager wouldn\u0026rsquo;t be blocked, as it\u0026rsquo;s not an advertisement or content distribution platform by itself. It\u0026rsquo;s just a JavaScript injector that can be used for benevolent, cool stuff as well.\nDo note that one thing that\u0026rsquo;s missing is checking whether or not Google Analytics was blocked but Google Tag Manager wasn\u0026rsquo;t. That\u0026rsquo;s an interesting use case as well, and it\u0026rsquo;s simple enough to do with a check for window['GoogleAnalyticsObject'] upon page load. You can do this via GTM or in the page template as well - it just requires some customization.\nDo you have other tips for detecting ad and content blockers? What results are you seeing? On my blog, content blockers account for around 25 % of the sessions, and those who block GTM as well are very few (but they do exist).\n"
},
{
	"uri": "https://www.simoahava.com/analytics/measure-serp-bounce-time-with-gtm/",
	"title": "Measure SERP Bounce Time With GTM",
	"tags": ["google analytics", "Google Tag Manager", "Guide", "SEO", "serp", "timings"],
	"description": "How to measure the dwell time of users who visit your site via SERP before bouncing back to the SERP page. Using Google Tag Manager, of course.",
	"content": "Here\u0026rsquo;s an interesting and hacky use case for you. It\u0026rsquo;s all about uncovering bounce metrics for visits which originate from organic Google search results. In particular, the metric we\u0026rsquo;re interested in is how long user dwelled on the landing page after arriving from organic Google search AND returned to the search engine results page (SERP) using the browser\u0026rsquo;s back button.\n  The inspiration for this post came from an audience question at the Best Internet Conference in Lithuania, which I recently attended as a speaker. They were concerned that Google is using Bounce Rate as a search ranking signal, and I was fairly strongly opinionated that it\u0026rsquo;s simply not possible, as all the \u0026ldquo;native\u0026rdquo; GA metrics are really easy to manipulate. However, Dr. Pete from Moz wrote about dwell time in 2012, and it makes a lot of sense. Google should be very interested how long the visitor stays out of the SERP when following a link. If users tend to immediately return to the SERP, it\u0026rsquo;s very likely the result was not relevant for them.\nSo, inspired by this question, I wanted to see if I can get some metrics from how long people dwell on these landing pages before returning to the SERP. I got my results, but it\u0026rsquo;s definitely not an easy problem to solve. As usual, we\u0026rsquo;re using a combination of Google Tag Manager and Google Analytics to perform the operation.\n(UPDATE 17 April 2016 I updated this article, as I had to make some modifications to the code. It\u0026rsquo;s slightly more robust now, and it lends itself better to e.g. Custom Metrics, if you prefer to use those instead of User Timings.)\nThe result is a list of User Timings, where each landing page can be scrutinized against the time users spent on there before clicking the browser\u0026rsquo;s back button.\nThe mystery of the history The difficulty is that when the user clicks the browser\u0026rsquo;s back button, we have no knowledge of where the user is taken to. That\u0026rsquo;s browser security for you. It would be highly questionable if the website had access to the history of the web browser. Makes sense, right?\nAnother difficulty is the actual browser back button. It\u0026rsquo;s not part of the browser object model, so we can\u0026rsquo;t actually measure clicks on it. Instead, we can infer a back button click from how people navigate with URL hashes! When a hash change is recorded, we can infer that it was due to browser back if we implement a little hack, where the hash is unique to organic Google search.\nYou see, by creating a new browser history entry for people landing from organic Google search, the back button click doesn\u0026rsquo;t take them to the SERP, but rather to the original landing page that existed before we redirected the user to the new history state. Using that as an indicator, we can extrapolate that the hash change occurred due to a browser back button click (or the backspace).\nThe process The process is as follows:\n  If the user lands on the site through organic Google search, create a new browser history entry with the hash #gref\n  Later, if a hash change is registered, and the user is still on the landing page, and the hash change is from #gref to a blank string, fire a Google Analytics timing event, after which programmatically invoke the \u0026ldquo;Back\u0026rdquo; event in the browser history\n  Looks simple (actually, doesn\u0026rsquo;t look simple at all), but it\u0026rsquo;s very hacky indeed. You see, we\u0026rsquo;re manipulating browser history by creating a new, fictional entry called #gref. Then, when the user clicks browser back, instead of taking them back to Google search, it actually takes them to the previous state, which is the URL without the #gref.\nTHAT\u0026rsquo;S how we know both that the user clicked browser back AND that they were trying to return to the SERP. All we have to do is send the Google Analytics timing hit, and then move the user manually to the previous entry in the history (i.e. the SERP).\nWhy is it hacky? Well, you\u0026rsquo;re manipulating browser history, for one. You\u0026rsquo;re creating a custom state, and you\u0026rsquo;re forcing the user to adopt that state if they land from organic search. Next, you\u0026rsquo;re intercepting a legitimate browser back event, and instead of letting the user directly leave the site, you\u0026rsquo;re forcing them to send the GA timing hit first, before manually whisking them back to the SERP.\nWhew! Lots of things that can go wrong. Luckily the JavaScript is solid and beautiful, but don\u0026rsquo;t forget to test thoroughly!\nWait, let me repeat that: test thoroughly. Also, if you\u0026rsquo;re running a single-page website, I can almost promise you that this won\u0026rsquo;t work out-of-the-box.\nThe Custom HTML Tag At the heart of this solution is a single Custom HTML Tag. It\u0026rsquo;s fired by two different events, to which we\u0026rsquo;ll return shortly.\nFirst of all, here\u0026rsquo;s the code:\n\u0026lt;script\u0026gt; (function() { var s = document.location.search; var h = document.location.hash; var e = {{Event}}; var n = {{New History Fragment}}; var o = {{Old History Fragment}}; // Only run if the History API is supported  if (window.history) { // Create a new history state if the user lands from Google\u0026#39;s SERP  if (e === \u0026#39;gtm.js\u0026#39; \u0026amp;\u0026amp; document.referrer.indexOf(\u0026#39;www.google.\u0026#39;) \u0026gt; -1 \u0026amp;\u0026amp; s.indexOf(\u0026#39;gclid\u0026#39;) === -1 \u0026amp;\u0026amp; s.indexOf(\u0026#39;utm_\u0026#39;) === -1 \u0026amp;\u0026amp; h !== \u0026#39;#gref\u0026#39;) { window.oldFragment = false; window.history.pushState(null,null,\u0026#39;#gref\u0026#39;); } else if (e === \u0026#39;gtm.js\u0026#39;) { window.oldFragment = true; } // When the user tries to return to the SERP using browser back, fire the  // Google Analytics timing event, and after it\u0026#39;s dispatched, manually  // navigate to the previous history entry, i.e. the SERP  if (e === \u0026#39;gtm.historyChange\u0026#39; \u0026amp;\u0026amp; n === \u0026#39;\u0026#39; \u0026amp;\u0026amp; o === \u0026#39;gref\u0026#39;) { var time = new Date().getTime() - {{DLV - gtm.start}}; if (!window.oldFragment) { dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;returnToSerp\u0026#39;, \u0026#39;timeToSerp\u0026#39; : time, \u0026#39;eventCallback\u0026#39; : function() { window.history.go(-1); } }); } else { window.history.go(-1); } } } })(); \u0026lt;/script\u0026gt;  Let\u0026rsquo;s quickly walk through this code. First of all, the whole block is encased in an immediately invoked function expression (IIFE) (function() {...})();, which protects the global namespace. Also, the whole solution only works if the user\u0026rsquo;s browser supports the History API: if (window.history) {...}.\nThe first important code block is this:\nif (e === \u0026#39;gtm.js\u0026#39; \u0026amp;\u0026amp; document.referrer.indexOf(\u0026#39;www.google.\u0026#39;) \u0026gt; -1 \u0026amp;\u0026amp; s.indexOf(\u0026#39;gclid\u0026#39;) === -1 \u0026amp;\u0026amp; s.indexOf(\u0026#39;utm_\u0026#39;) === -1 \u0026amp;\u0026amp; h !== \u0026#39;#gref\u0026#39;) { window.oldFragment = false; window.history.pushState(null,null,\u0026#39;#gref\u0026#39;); }  This code checks the following:\n  Was the Tag fired due to the Page View Trigger (i.e. a page load)?\n  Did the user land from a Google site (referrer contains www.google.)?\n  If they did, make sure it\u0026rsquo;s not from an AdWords ad (check that the URL does not have ?gclid) or custom campaign.\n  Make sure also that the URL does not already contain #gref, which would imply the user followed a link with that hash or that the user already had a history entry with #gref, meaning they\u0026rsquo;ve navigated somewhere else within or outside the site after landing on it in the first place from the SERP.\n  If these checks pass, then a new global variable oldFragment is initialized with the value false. This means simply that this is a brand new landing on the site through organic Google search, and we check against this when we push the payload to dataLayer. We only want to send the Timing hit for landing page bounces.\nFinally, a new browser history state is created, where the URL is appended with #gref to show that the user landed from organic Google search.\nThe next code block is:\nelse if (e === \u0026#39;gtm.js\u0026#39;) { window.oldFragment = true; }  Here, we check if the event is a page load again, but the URL already has #gref. In this case, we set the global variable to true, since obviously this entry is not a direct landing from the SERP but something else. This way, we\u0026rsquo;ll block the Timing hit from happening, as we only want to measure true landing page bounces.\nThe final code block is:\nif (e === \u0026#39;gtm.historyChange\u0026#39; \u0026amp;\u0026amp; n === \u0026#39;\u0026#39; \u0026amp;\u0026amp; o === \u0026#39;gref\u0026#39;) { var time = new Date().getTime() - {{DLV - gtm.start}}; if (!window.oldFragment) { dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;returnToSerp\u0026#39;, \u0026#39;timeToSerp\u0026#39; : time, \u0026#39;eventCallback\u0026#39; : function() { window.history.go(-1); } }); } else { window.history.go(-1); } }  Again, there\u0026rsquo;s a checklist of things:\n  Is the event a browser history event?\n  Is the old hash #gref and the new hash a blank string?\n  If all these checks pass, it means that the user tried to go back in browser history to the SERP. Due to our manually imposed history state, they\u0026rsquo;re actually taken to the #gref-less landing page.\nNext, we check the dwell time on the page, using the difference between the current time and the time when the GTM container snippet was first loaded. This is a reasonable description of dwell time, but you can use something else for start time if you wish.\nFinally, we check if oldFragment is false, meaning we\u0026rsquo;re still on the landing page. If it is, the payload is pushed into dataLayer. The \u0026lsquo;eventCallback\u0026rsquo; key has the actual return to SERP command, and it will only be executed after any Tags that use the returnToSerp event have fired.\nHacky-dy-hack-hack! I love it!\nAll the other stuff you\u0026rsquo;ll need Here\u0026rsquo;s a list of the assets you need to create for this to work. Feel free to improvise, if you wish!\n1. Built-in Variables First, make sure the following Built-in Variables are checked in your container\u0026rsquo;s Variables settings.\n  So that\u0026rsquo;s Page URL, Event, History Source, New / Old History Fragment.\n2. The Triggers for the Custom HTML Tag The Custom HTML Tag runs on two Triggers.\nThe first one is the default All Pages Trigger.\nThe second one is a History Change Trigger which looks like this:\n  This Trigger only launches when a browser history event is detected which is also a popstate, and the new history fragment is empty.\n3. The Variables Create the following Data Layer Variables:\n  This one stores the time when the GTM container snippet was executed.\n  This is where the time spent on the landing page is stored.\n4. The Trigger for the Timing Tag The Trigger you\u0026rsquo;ll attach to the Timing Tag (created next) looks like this:\n  This Trigger fires when the returnToSerp dataLayer event is pushed via the Custom HTML Tag. It also checks that the dwell time pushed into dataLayer is less than 30 minutes. This way a new session won\u0026rsquo;t be started with the event, if the visitor stays on the page for an exceptionally long time.\n5. The Timing Tag And here\u0026rsquo;s the Universal Analytics Timing Tag you\u0026rsquo;ll need:\n  We\u0026rsquo;re sending SERP Bounce as the Timing Category, the full Page URL as the Timing Variable, and the time spent on the landing page as the value. As mentioned above, this Tag is fired by the Trigger you created in step (4) above.\nViewing the results Once you implement this, you\u0026rsquo;ll find the results under Site Content \u0026gt; Site Speed \u0026gt; User Timings under the Timing Category labelled SERP Bounce.\n  To drill into the data, it\u0026rsquo;s useful to view the Timing Variable as the primary dimension, as that\u0026rsquo;s the one where the landing pages are. Try to find landing pages with an abnormally small average dwell time:\n  These pages have a very short dwell time compared to site average. Just make sure that the timing sample is large enough for you to draw conclusions from. Pages with an abnormally short dwell time from the SERP MIGHT indicate a poor experience or lack or relevant content.\nAlso, the Distribution view is useful if you want to drill down to individual page performance:\n  This report lets you view the timing buckets. As you can see, there are some anomalies here, though the sample is quite small.\nSummary I hope I\u0026rsquo;ve convinced you by now that this is a very hacky solution. Remember to test thoroughly. I can also promise that you will have problems if you try to implement this on a single-page application (e.g. AJAX site), which rely on the browser history API for navigation.\nNevertheless, it\u0026rsquo;s an interesting way of uncovering potential issues with your landing pages. It would be interesting to align this data with keyword data, but that\u0026rsquo;s up to you to solve, since that data is difficult to come by these days, and User Timings don\u0026rsquo;t align well with acquisition dimensions like Keyword.\n"
},
{
	"uri": "https://www.simoahava.com/tags/seo/",
	"title": "SEO",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/serp/",
	"title": "serp",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/timings/",
	"title": "timings",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/check-for-new-user/",
	"title": "#GTMTips: Check For New User",
	"tags": ["cookies", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "How to check if a user is &#34;New&#34; in terms of Universal Analytics tracking. This uses Google Tag Manager to collect and send the information to Google Analytics.",
	"content": "Every now and then we want to create a bridge between the stateful machines we send data to (e.g. Google Analytics), and the stateless environment where we collect the data itself (e.g. Google Tag Manager). This is not easy. There is no synergy between Google Analytics and Google Tag Manager which would let the latter understand anything about things like sessions or landing pages or Bounce Rates.\nOne thing we can reliably measure, however, is whether or not the visitor is a New User in Google Analytics. This is done by checking whether or not the Universal Analytics cookie _ga exists in the user\u0026rsquo;s browser when they enter the site. Naturally, this check needs to happen before the Universal Analytics Tag has time to create the cookie, which is the only difficult thing to solve here.\nTip 32: Check for a new Google Analytics user   The process is quite simple. When the user lands on the page, check if they have the _ga cookie in their browser. If not, then they\u0026rsquo;re a \u0026ldquo;New User\u0026rdquo; in Google Analytics. Once this check is made, you can do whatever you want with this information (with one caveat, see below).\nIn any case, the first thing you need is a 1st Party Cookie Variable, which checks for the existence of the _ga cookie:\n  This would return undefined if the cookie does not exist, so we can check against this in a Custom HTML Tag.\nThe only thing to solve here is when to fire the Custom HTML Tag. Basically, you have two options. Either upon an event that takes place before the first Universal Analytics Tag on the page fires, or upon the same event which fires the first Universal Analytics Tag.\nIf you make the Custom HTML Tag fire upon an event which happens after the Universal Analytics Tag is fired, it\u0026rsquo;s possible that a race condition emerges, and the UA Tag has time to create the cookie before the check is made.\nStill following?\nThe Custom HTML Tag could look like this:\n\u0026lt;script\u0026gt; if (!{{Cookie - _ga}}) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;newUser\u0026#39;, \u0026#39;ga_newUser\u0026#39; : \u0026#39;true\u0026#39; }); } \u0026lt;/script\u0026gt; This Tag would push a dataLayer object which has an event and an arbitrary key (ga_newUser in the example). You can use these to fire some Tag or run some functions that you only want to run for Google Analytics \u0026ldquo;New Users\u0026rdquo;.\nUPDATE As pointed out by both dusoft and Stephen Harris in the comments (thanks guys!), you can use tag sequencing to send user status information to the main Tag without having to build a clumsy event chain. You can follow the guide linked to in the previous sentence, and simply update GTM\u0026rsquo;s data model with google_tag_manager[{{Container ID}}].dataLayer.set('ga_newUser', 'true') in the Custom HTML tag.\nThis way the Custom HTML tag, being the setup tag of the sequence, updates the value of ga_newUser in GTM\u0026rsquo;s data model, and it is then available to the main tag to use in a Data Layer Variable.\nCaveats So, there are some caveats here. First, it\u0026rsquo;s most likely that your Universal Analytics Tag fires on the \u0026ldquo;All Pages\u0026rdquo; Trigger. That\u0026rsquo;s the most common setup. If you set the Custom HTML Tag to fire upon the All Pages Trigger as well, any information you push into dataLayer in the Custom HTML Tag will not be available to the Universal Analytics Tag. Google Tag Manager has a fixed Data Layer for the duration of an event, so any changes will not be available until the next Data Layer event.\nAlso, it\u0026rsquo;s possible that the user is a \u0026ldquo;New User\u0026rdquo; even if they have the _ga in the browser. The cookie lifetime is 2 years, so it\u0026rsquo;s entirely possible that the cookie exists due to an older installation or something. Also, the page can collect to a number of Universal Analytics trackers, each with different cookie configurations. The cookie name could be different, or the cookie domain could vary. In this case, just looking for _ga might not be robust enough to identify a New User. You\u0026rsquo;ll simply need to adapt to how your site\u0026rsquo;s Universal Analytics configuration is setup.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/enhanced-ecommerce-tips-and-learnings/",
	"title": "Enhanced Ecommerce Tips And Learnings",
	"tags": ["enhanced ecommerce", "google analytics", "Google Tag Manager", "Guide", "Tips"],
	"description": "Tips and learnings from a multitude of Enhanced Ecommerce implementations. Most of these tips relate to Google Tag Manager, but there are some Google Analytics tricks mixed in as well.",
	"content": "Enhanced Ecommerce is undoubtedly an excellent feature of Google Analytics. It provides us with a set of reports that truly extend the capabilities of funnel-based website analysis. As I\u0026rsquo;ve shown before, it\u0026rsquo;s also very useful for tracking other transactional events on your site, such as content engagement.\n  However, here\u0026rsquo;s the thing. It\u0026rsquo;s not very easy to implement. Even if you get everything right according to the documentation, there are still quite a number of pitfalls, and many of the learnings emerge only through experience. This is where I want to help out, so I composed this post with some of my key findings about Enhanced Ecommerce implementation and use.\nIt\u0026rsquo;s well worth learning this stuff, as a fully functional Enhanced Ecommerce implementation might well be the key to finally coming up with a useful report to plaster across your vanity dashboards.\n1. Implementation via Google Tag Manager As a self-proclaimed Google Tag Manager fanatic number 1, I do all my implementations with GTM. There are some quirks you need to consider, though, when implementing Enhanced Ecommerce through GTM. Here they are, in no particular order.\nThe fleeting ecommerce object If you use the Enable Enhanced Ecommerce / Use Data Layer option in your Tags, there\u0026rsquo;s one very important thing to understand.\nOnly the most recent ecommerce object is included in the hit!\nIn other words, if you do two consecutive dataLayer.push() commands with their own \u0026lsquo;ecommerce\u0026rsquo; objects, any Tag which fires after the latter push will only have access to the latter \u0026lsquo;ecommerce\u0026rsquo; object.\n  This is because the \u0026ldquo;Use Data Layer\u0026rdquo; option uses version 1 of the Data Layer, which doesn\u0026rsquo;t have fancy things like recursive merge of objects. In practice, this means that an \u0026lsquo;ecommerce\u0026rsquo; object will always overwrite the previous \u0026lsquo;ecommerce\u0026rsquo; object in the data layer, if the version 1 interface is used.\n  If you do want to recursively merge \u0026lsquo;ecommerce\u0026rsquo; objects, you will need to use the \u0026ldquo;Read data from variable\u0026rdquo; option which becomes available when you uncheck the \u0026ldquo;Use Data Layer\u0026rdquo; option. Read on\u0026hellip;\nCustom JavaScript Variable to the rescue I\u0026rsquo;ve written about this before, and these days I actually only use this option to send the payloads. When you use the Custom JavaScript option (see the developer guide for further information), you can create, parse, and delete parts of the \u0026lsquo;ecommerce\u0026rsquo; object until it\u0026rsquo;s to your liking. This is an incredibly powerful tool, letting you pull in data from multiple sources to compile the object, or allowing you to access the full \u0026lsquo;ecommerce\u0026rsquo; object stored in the data model, and not just the stunted version 1 object available if you use the \u0026ldquo;Use Data Layer\u0026rdquo; option in your Tags.\nfunction() { var ecom = {\u0026#39;ecommerce\u0026#39; : { \u0026#39;impressions\u0026#39; : pageData.productImpressions, \u0026#39;detail\u0026#39; : { \u0026#39;actionField\u0026#39; : {\u0026#39;list\u0026#39; : \u0026#39;Related Products\u0026#39;}, \u0026#39;products\u0026#39; : pageData.productDetailView } }; return ecom; }  The example above pulls together both product impressions and product detail view in a single object, sending the complete payload to Google Analytics with the Tag.\nTo access a recursively merged ecommerce object, you will need to create a new Data Layer Variable which accesses the \u0026lsquo;ecommerce\u0026rsquo; object pushed within. Instructions for this can be found in this article.\nYou can combine multiple data types in a single payload This is pretty vaguely covered in the developer guide, but combining data types in a single payload can be very useful if you want to keep the number of requests to Google Analytics down. So, if you want to send multiple \u0026lsquo;ecommerce\u0026rsquo; objects in one single payload, you can, but only if you send one of each object type (promoView, promoClick, impressions, action). An action is any Enhanced Ecommerce data type which has the products Array. In other words, these are the available action objects:\nclick, detail, add, remove, checkout, purchase, checkout_option, refund\nAlso, you can\u0026rsquo;t combine a promoClick with a promoView or an action object. These are, thus, the valid combinations:\n  impressions with one of promoView, promoClick or action\n  impressions with promoView and action\n  promoView with one of impressions or action\n  promoView with impressions and action\n  promoClick with impressions\n  action with one of impressions or promoView\n  action with impressions and promoView\n  In any case, combining objects into a single payload can truly save a lot of time and make the whole thing more efficient, but don\u0026rsquo;t forget that Google Analytics has a character limit in the payload!\nThere is an 8KB limit in the payload The analytics.js library refuses to send a hit if the full payload size is larger than 8192 bytes. This does require quite a lot of data to be stuffed within, but it only takes some dozens of product impressions to clog up the channel.\nEivind Savio has written an excellent post about how to manage this overflow, so I suggest you head on over to take a look!\n2. Consistency is key Enhanced Ecommerce is comprised almost entirely of hit-level interactions. This means that every Enhanced Ecommerce payload sent to Google Analytics is unique, and does not persists its information across subsequent hits.\nThis, in turn, basically means that the products Array needs to be consistent throughout the funnel. For example, if you have a product which has the category \u0026lsquo;T-Shirts\u0026rsquo;, you will need to have this information in all the payloads the product is sent in if you want to query for it across the entire funnel. GTM or GA will not persist this information.\nYou\u0026rsquo;ll want to be very careful when designing and developing the Data Layer for your Enhanced Ecommerce payloads. The developers need to understand how important it is to have the product details be identical throughout the funnel process.\nThis has implications for queries as well. For example, let\u0026rsquo;s say you send the following two payloads to Google Analytics:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;A12345\u0026#39;, }] } }, \u0026#39;event\u0026#39; : \u0026#39;ecommerce\u0026#39; }); dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;A12345\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;My T-Shirt\u0026#39; }] } }, \u0026#39;event\u0026#39; : \u0026#39;ecommerce\u0026#39; });  In this case, querying for product SKU \u0026lsquo;A12345\u0026rsquo; will return a result when queried against product detail views and product adds to cart. However, product name \u0026lsquo;My T-Shirt\u0026rsquo; will only return a result when queried against product adds to cart. Since this product name was missing from the \u0026lsquo;detail\u0026rsquo; payload, you won\u0026rsquo;t be able to query for this information.\nMany times when working with an Enhanced Ecommerce implementation which uses client-side methods to populate some of the data (e.g. scraping from the page), this consistency requirement has become a serious issue. I\u0026rsquo;ve been forced to persist full product information throughout the funnel using solutions like HTML5 Storage and cookies.\nThis is not the recommended approach.\nThe best way is to render product details in dataLayer as the page loads, making sure your developers add them consistently in all steps of the funnel. Once the products are in dataLayer, you can use the Custom JavaScript Variable method to pull them out and parse them into a valid ecommerce object.\nNote that checkout is a bit exceptional in terms of consistency. It\u0026rsquo;s enough to send the \u0026lsquo;products\u0026rsquo; Array with just the first step. This is because Enhanced Ecommerce only has an aggregate productCheckouts metric, which increases by one when a product is sent with the first checkout step. The checkout option and the rest of the checkout steps need not, and thus perhaps should not include product information.\n3. Product-scoped custom dimensions and metrics Product-scoped custom dimensions and metrics are a wonderful way to extend the rather limited set of information you can send with each product.\nTo send a product-scoped dimension or metric in Google Tag Manager, you need to include the dimensionX and/or metricX key in the respective product in the payload you want to send the dimension/metric in. Again, as in the previous chapter, product-scoped dimensions and metrics do not persist, so you will need to include them consistently in the payloads.\nFor example, to send a dimension with details about the T-shirt size, and a metric with the tax-free value of the shirt, the dataLayer.push() would look like this:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;A12345\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;My T-Shirt\u0026#39;, \u0026#39;dimension3\u0026#39; : \u0026#39;Large\u0026#39;, \u0026#39;metric2\u0026#39; : 11.99 }] } }, \u0026#39;event\u0026#39; : \u0026#39;ecommerce\u0026#39; });  Naturally, a payload like this will only let you query for this information with product detail view queries, and for this particular product only. These dimensions do not persist across funnel payloads, nor do they cover multiple products. They\u0026rsquo;re per product, per hit.\nI\u0026rsquo;ve written about product-scoped custom dimensions and metrics before, so remember to check out this article as well.\n4. Product categories The \u0026lsquo;category\u0026rsquo; field has been available in \u0026ldquo;traditional\u0026rdquo; Ecommerce as well, but it\u0026rsquo;s been slightly revamped in Enhanced Ecommerce.\nFirst of all, the consistency requirement applies here as well. If you want to make full-funnel queries against a product category, you will need to send the category with every single product in all steps of the funnel you want to query against. The field does not persist.\nThe other thing about categories is that you can send five levels of categories, resulting in some sweet segmentation in your reports. These levels are sent by using the slash (/) between levels, where level 1 is the first item in the string, level 2 the second, etc. So, a full five-level product category string would look like this in a payload:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;A12345\u0026#39;, \u0026#39;category\u0026#39; : \u0026#39;Clothes/T-Shirts/Men/Sleeveless/Used\u0026#39; }] } } });  This way, you can query for individual category levels with the following Google Analytics dimensions:\nProduct Category Level 1: Clothes\nProduct Category Level 2: T-Shirts\nProduct Category Level 3: Men\nProduct Category Level 4: Sleeveless\nProduct Category Level 5: Used\nThese aren\u0026rsquo;t available in the default reports, but you can create custom reports easily, or use them as secondary dimensions.\n  If your category names contain the character \u0026lsquo;/\u0026rsquo;, you will need to write this in a different way, as there\u0026rsquo;s no way to encode the slash without it being interpreted as a category delimiter.\n5. Product list attribution Not everything about Enhanced Ecommerce is hit-level, though. Product lists and promotions have an attribution mechanism, where the last list or promotion that the user interacted with before a purchase within the same session is the one which gets full credit for the purchase. In other words, you don\u0026rsquo;t need to persist list information throughout the funnel. It\u0026rsquo;s enough to send it only where the list interaction takes place, and the attribution will take care of the rest.\nRemember to check the developer guide for a description of this attribution mechanism.\n6. Summary This short list of tips includes a number of things that are either vaguely described in the developer guides, or are difficult to grasp without concrete examples.\nIf there\u0026rsquo;s one thing that stands out, it\u0026rsquo;s the consistency requirement. It\u0026rsquo;s very important to keep product details consistent across the funnel. A single change in the product name, for example, can make reporting extremely difficult, as you\u0026rsquo;ll need to consolidate multiple product names under a single SKU in your reports.\nDid I miss some important tip / learning? Do you have something to add? Sound off in the comments!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/prevent-tag-from-firing-in-iframe/",
	"title": "#GTMTips: Prevent Tag From Firing In iFrame",
	"tags": ["google analytics", "Google Tag Manager", "gtmtips", "JavaScript"],
	"description": "Use Google Tag Manager to block a tag from firing in an iframe. This is useful if you want to avoid double-counting measurements to Google Analytics, for example.",
	"content": "Unfortunately, iFrames still exist. They are used to embed content from one page into another. Frames are horrible, nasty things, very often riddled with cross-domain problems, performance issues, responsive design obstructions and other crap from the nether pits of hell. Regardless, if you\u0026rsquo;re stuck with an iFrame which also collects data to your Google Analytics property, for example, you probably want to prevent at least the first Page View from firing, since otherwise you\u0026rsquo;ll be double-counting Page Views: once on the main page and once in the iFrame. In this tip, I\u0026rsquo;ll show you how to prevent a Tag from firing if it\u0026rsquo;s executed in a document that is in an iFrame.\nTip 31: Prevent Tag from firing in an iFrame   For easy copying, here\u0026rsquo;s the code:\nfunction() { try { return window.top !== window.self; } catch(e) { return false; } }  As you can see, it\u0026rsquo;s a simple solution. The Custom JavaScript Variable simply checks if the window object on the page is different than the window object on the outermost frame. This script thus returns true if there is a difference, meaning the page is not loaded in the outermost frame (and is thus in an iFrame), and false if there is no difference, meaning the page is the \u0026ldquo;main\u0026rdquo; document. The Trigger would thus look like this:\n  Add this Trigger as an Exception to a Tag which fires upon the Page View Event, and you will effectively block the iFrame from sending the Page View.\nIf you want to create a \u0026ldquo;global\u0026rdquo; Exception, which blocks all Tags from firing when in an iFrame, use the Custom Event Trigger:\n  Sometimes you only want to block the first Page View in the iFrame, but then allow the user to navigate from page to page in the frame, sending Page Views for subsequent pages. In that case, you need a bit more creativity, and you\u0026rsquo;ll need to check for the Referrer, blocking the Tag if the Referrer is the \u0026ldquo;main\u0026rdquo; document:\n  This solution should work cross-domain, since simply checking for the window object does not violate same-origin policy. There have been scattered reports about unreliability in some earlier versions of Internet Explorer (surprise, surprise), so it\u0026rsquo;s good to have a fallback if the script fails, which is why it sends the false upon an error.\n"
},
{
	"uri": "https://www.simoahava.com/tags/form-tracking/",
	"title": "form tracking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-form-engagement-with-google-tag-manager/",
	"title": "Track Form Engagement With Google Tag Manager",
	"tags": ["form tracking", "google analytics", "Google Tag Manager", "Guide"],
	"description": "A guide to tracking forms with Google Tag Manager.",
	"content": "(Updated 13 August 2017)\nA little over a year ago, in April 2014, I wrote the post \u0026ldquo;Advanced Form Tracking In Google Tag Manager\u0026rdquo;, and it\u0026rsquo;s been at the top of my best seller list ever since. Turns out that many people are rightfully passionate about making the web forms on their websites as fluid and intuitive as possible, since a web form is often the only thing that stands between a prospect and their transformation into clienthood.\nI thought it was time to update the article to be Google Tag Manager V2 compliant. It\u0026rsquo;s not a major difference, but I have remodelled some of the JavaScript as well. Furthermore, I\u0026rsquo;ve pulled together some new ideas explored in other articles, so now it\u0026rsquo;s all conveniently packaged in this one big blog post.\nAt the heart of this article is still the methodology of picking up values from form fields. Perhaps you want to send some selection the user made to Google Analytics with the purpose of segmenting the data, or perhaps you want to identify values which most often block a conversion. Whatever the case, remember this: Google Analytics is very harsh towards PII (personally identifiable information) collection, and you need to be very vigilant in sanitizing any values you\u0026rsquo;re collecting. You don\u0026rsquo;t want to send anything to Google Analytics that can be used to identify the person who filled the form. The most typical value is an email address, which you should categorically avoid recording.\nWithout further ado, let\u0026rsquo;s get rolling. First, the basics\u0026hellip;\n1. Form tracking in Google Tag Manager In the new version (V2) of Google Tag Manager, form tracking is no longer isolated as its own \u0026ldquo;Form Listener\u0026rdquo; Tag type. Instead, auto-event tracking has shifted to handlers embedded in the Triggers you use to fire Tags.\nWhen you create a Form Trigger, it will activate on all pages where the Enable this trigger when\u0026hellip; setting is valid. Remember, you only see this setting if either \u0026ldquo;Wait For Tags\u0026rdquo; or \u0026ldquo;Check Validation\u0026rdquo; is checked in the Trigger settings.\n  If the Trigger is active on a page, it will actively listen for a form submission. The settings in the Trigger are:\n  Wait For Tags - GTM will pause the form submission for as long as all the Tags that use the Trigger have fired or until the timeout (default 2000 milliseconds) expires.\n  Check Validation - If this is checked, GTM will not fire the Trigger if the default action of the form (submit and redirect) is prevented. If left unchecked, the Trigger will go off whenever a submit event is registered, whether or not the default action is prevented.\n  Enable this trigger when\u0026hellip; - As mentioned above, this condition is for establishing on which pages GTM should listen for form submissions. There\u0026rsquo;s really no performance penalty in having the listener active on all pages, but there are cases where Wait For Tags interferes with other JavaScript on the page, so you might want to only have the listener enabled on pages you\u0026rsquo;ve tested it on.\n  Fire this trigger when\u0026hellip; - If you select \u0026ldquo;Some Forms\u0026rdquo; as the This trigger fires on setting, these conditions that govern when the Trigger makes any attached Tags go off. If you have just one form on the website, using \u0026ldquo;All Forms\u0026rdquo; here is justified. But if you want to specify you only want to fire the Tag when a form with id=\u0026quot;contactForm\u0026quot; should fire the Tag, you should add that as a condition here.\n    If you don\u0026rsquo;t see Variables like \u0026ldquo;Form ID\u0026rdquo; or \u0026ldquo;Form Classes\u0026rdquo; in the Variables drop-down menu, you will need to activate them, as they are Built-In Variables, introduced in Google Tag Manager V2.\nHowever, there might be many reasons why a properly created Form Trigger doesn\u0026rsquo;t work. So, read on\u0026hellip;\n2. Why the Form Trigger doesn\u0026rsquo;t always work For the Form Trigger to work, the form must dispatch a valid form submit event which also bubbles all the way to the document node. To de-gobbledegookify this statement, it means that:\n  The form must dispatch a valid submit browser event\n  This event must not be prevented from propagating to the document node\n  Those are the conditions. They seem simple enough, but you might be surprised how many forms out there violate either one.\nThe first means simply that GTM listens for a standard submit browser event, which is most often dispatched when an input or button element is clicked, where element type is submit.\n  The second means that the submit must be allowed to climb all the way up to the document node, where GTM\u0026rsquo;s listener is waiting for the event. You see, GTM uses something called event delegation to listen for form submissions. This is much more economical than adding a listener to all the forms on the page individually.\nThe first condition is most commonly violated when the submit event is either cancelled (see below) or never dispatched in the first place. Many forms send their data with custom-built requests (e.g. jQuery\u0026rsquo;s $.ajax or the XMLHttpRequest API), and these prevent the submit event from working, since it\u0026rsquo;s replaced with a custom dispatcher. Since the submit event is never dispatched, Google Tag Manager never records a form submission.\nThe second condition is violated when event propagation is stopped with return false; in a jQuery handler. Another common way to prevent propagation is to use stopPropagation() instead of preventDefault() on the event object.\nI\u0026rsquo;ve written about this phenomenon a number of times before:\n  Why Don\u0026rsquo;t My GTM Listeners Work?\n  #GTMTips: Fix Problems With GTM\u0026rsquo;s Listeners\n  The best way to fix the issue, however, is to open a line of communication with your developers, and tell them that GTM requires a standard submit browser event to propagate all the way to the document node to work.\nIf this can\u0026rsquo;t be done, the next best thing is to ask the developer to implement a custom dataLayer.push() into the callback function which is invoked upon a successful submission. The piece of code could be something like:\nfunction onFormSuccess(event) { window.dataLayer = window.dataLayer || []; window.dataLayer.push({ event: \u0026#39;formSubmissionSuccess\u0026#39;, formId: \u0026#39;contactForm\u0026#39; }); // Rest of the success callback code }  This would push formSubmissionSuccess into the dataLayer as the value of the event key. Then, you can create a Custom Event Trigger, which waits for an event named formSubmissionSuccess. Using this Trigger would then fire a Tag when the form is successfully submitted.\nThere are other workarounds which involve e.g. polling the page until a thank you message is identified, or perhaps using a custom-built DOM Listener. Unfortunately, all workarounds are far less robust than either fixing the form to respect GTM\u0026rsquo;s Form Trigger requirements or the custom dataLayer.push() method described above.\n3. Built-In Variables Google Tag Manager introduces a number of Built-In Variables you can use to simplify form tracking. Make sure all the \u0026ldquo;Form\u0026rdquo; Variables are checked before continuing.\n    Form Element - returns an HTML Object which contains the form element that was submitted. You can use this Variable to dig deep into the object properties of the form itself.\n  Form Classes - returns a string of values in the class attribute of the form that was submitted.\n  Form ID - returns a string with the value stored in the id attribute of the form that was submitted.\n  Form Target - returns a string with the value stored in the target attribute of the form that was submitted.\n  Form URL - returns a string with the value stored in the action attribute of the form that was submitted.\n  Form Text - returns the entire text content of the form that was submitted (NOT very useful).\n  Many of the following solutions rely on these Built-In Variables.\n4. Note about scope All the Variables described below are Custom JavaScript Variables. They are all scoped, by default, to the form that was submitted with {{Form Element}}.querySelector.... If you want to access any form on the page, submitted or not, you need to use the following syntax:\nvar form = document.querySelector(\u0026#39;#someform\u0026#39;); var field = form.querySelector...  Keep this in mind when applying the following solutions to your own measurement plan.\n5. Capture field value This is a simple Custom JavaScript Variable which captures the value user has input into a form field. Do note that items like checkboxes, radio buttons, and drop-down lists work a little differently, which is why they are covered in their own chapters.\nName: {{Field value}}\nfunction() { var field = {{Form Element}}.querySelector(\u0026#39;#inputFieldId\u0026#39;); return field ? field.value : undefined; }  Description: This solution accesses the value attribute of the form field with ID inputFieldId, as long as its found within the form that was submitted. If no such field is found, then undefined is returned instead. Remember, do not collect personally identifiable information!\n6. Capture selected radio button value This solution is for capturing the value of the checked radio button. The default solution is for capturing just one radio button value, i.e. accessing just one group. See chapter 9 for an example of how to access multiple values from similar elements.\nName: {{Checked radio button}}\nfunction() { var radioName = \u0026#34;radioName\u0026#34;; var checked = {{Form Element}}.querySelector(\u0026#39;[name=\u0026#34;\u0026#39; + radioName + \u0026#39;\u0026#34;]:checked\u0026#39;); return checked ? checked.value : undefined; }  Description: Returns the value of the checked radio button in the group with the name radioName. This group must exist within the form that was submitted. If no checked button is found, undefined is returned.\n7. Capture selected checkbox value Returning the value of the selected checkbox within a group is done exactly the same way as how you\u0026rsquo;d capture a radio button value. Both use the checked property to identify if an element is checked.\nSo use the solution from the previous chapter to capture checkbox values as well. Just remember that it only returns the value of the first checkbox in a group that was selected. If the user can check multiple checkboxes (as is usually the case), you might want to check chapter 9 for tips on how to do this.\n8. Capture selected drop-down list item value To capture the selected item value in a drop-down list, checking for the value of the list itself will not work, as you\u0026rsquo;d intuitively expect. Instead, you need to access the option in the list that was selected, and then capture its value.\nName: {{Selected list item}}\nfunction() { var selectList = {{Form Element}}.querySelector(\u0026#39;#selectListId\u0026#39;); return selectList ? selectList.options[selectList.selectedIndex].value : undefined; }  Description: First, the script retrieves the drop-down list with ID selectListId. Next, it returns the value of the selected option in the list. If the list does not exist, the script returns undefined.\n9. Capture multiple values Sometimes you\u0026rsquo;ll want to capture multiple values of some specified group. A prime example is the checkbox, where you can have multiple checked boxes in a single group. Here are some ideas for how to capture these values.\nReturn concatenated string of checked item values\nfunction() { var groupName = \u0026#34;groupName\u0026#34;; var elems = {{Form Element}}.querySelectorAll(\u0026#39;[name=\u0026#34;\u0026#39; + groupName + \u0026#39;\u0026#34;]:checked\u0026#39;); var vals = []; var i, len; for (i = 0, len = elems.length; i \u0026lt; len; i++) { vals.push(elems[i].value); } return vals.length ? vals.join(\u0026#39; \u0026#39;) : undefined; }  Description: This returns a concatenated string of all the checked item values within the group with name groupName. An example would be e.g. \u0026ldquo;breakfast lunch\u0026rdquo;, where the user chooses which meals they always eat out of \u0026ldquo;breakfast\u0026rdquo;, \u0026ldquo;lunch\u0026rdquo;, and \u0026ldquo;dinner\u0026rdquo;.\nTo return just the plain Array, so that you can process it further in the Tag which calls this Variable, change the return statement to:\nreturn vals.length ? vals : undefined;  If you want to get the values in a multiple selection list, use the following script. It returns the results in a concatenated string again, but by following the tip in the previous paragraph you can return an Array instead.\nReturn selected values in a multiple selection list\nfunction() { var selectList = {{Form Element}}.querySelector(\u0026#39;#selectListId\u0026#39;); var options = selectList ? selectList.options : []; var vals = []; var i, len; for (i = 0, len = options.length; i \u0026lt; len; i++) { if (options[i].selected) { vals.push(options[i].value); } } return vals.length ? vals.join(\u0026#39; \u0026#39;) : undefined; }  Description: This returns a concatenated string of all the selected options in a multiple selection drop-down list, which has the ID selectListId and which is within the submitted form.\n10. Track form abandonment For ideas on how to track form abandonment, check out this article I wrote a short while ago:\nTrack Form Abandonment With Google Tag Manager\nBounteous also has a sweet guide on how to achieve the same thing, albeit with a somewhat different approach.\nMy colleague at Reaktor, Lauri Piispanen, has also improved upon this solution, and you can check out his GitHub repo here.\n11. Track form field timings If you want to track how much time a user spends on your form, segmented by individual fields, check out the following article:\nForm Field Timing With Google Tag Manager\nIt utilizes User Timings in Google Analytics to collect data on how much time users spend in the fields of your form by average.\n12. One form element - multiple forms (ASP.NET) I decided not to dwell on this topic too much, since it\u0026rsquo;s so hacky and unreliable. The issue is that on e.g. ASP.NET sites, a single master form control wraps the entire page, and individual forms are just groups of elements under this one control.\nThe method I suggested in the previous guide is still somewhat valid, however. The way it works is that:\n  Fire a Custom HTML Tag when the submit button of the form is clicked\n  In this Custom HTML Tag, push the ID (or other relevant data) of the clicked element into the data layer\n  Create a Form Submit Trigger which looks for this value in the data layer\n  This way the Form Submit Trigger should only fire when the form is submitted due to a click on the correct submit button.\nI\u0026rsquo;ve noticed this doesn\u0026rsquo;t always work due to race conditions or just the general fragility of the event chain. Nevertheless, it\u0026rsquo;s something you might want to try if you can\u0026rsquo;t get developers to assist you.\nIf you do have an established line of communications with your developers, the solution is to ask them to create / edit the success callback of each form to push a custom dataLayer object when a form is submitted. This object should contain information about which form was submitted, and you can then create a Custom Event Trigger based on this information. Go back to chapter 2 for an example of such a push.\n13. Summary This guide is a bit leaner than the first edition (at least it feels like so). That\u0026rsquo;s because the new Form Submit Trigger has made many things easier, and I\u0026rsquo;ve also rewritten the JavaScript to utilise methods like querySelector and properties like selectedIndex to get the desired result with far less hassle.\nOnce IE8 and older disappear entirely from the face of this earth (can\u0026rsquo;t wait!), we can use even simpler language, with e.g. CSS3 pseudo-classes like :checked.\nDo you think something relevant is missing from this guide? Let me know. I\u0026rsquo;m always happy to update this with new solutions. Don\u0026rsquo;t forget to read the previous version of the guide as well, and check the comments section, too! Lots of great questions and answers there.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/understanding-tag-sequencing-in-google-tag-manager/",
	"title": "Understanding Tag Sequencing In Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "tag sequencing"],
	"description": "Guide to tag sequencing in Google Tag Manager. You can use this feature to set dependencies in the firing order of related tags.",
	"content": "A recent update to Google Tag Manager introduced a feature which has been on the wishlist of many users for a long time. It\u0026rsquo;s called Tag sequencing, and its purpose is to facilitate the sequential firing of Tags. The idea is that you can specify a setup and a cleanup for each Tag in your container.\n  This article is intended to function as a quick tour of the feature. It\u0026rsquo;s not the simplest feature to understand, as Tag sequencing runs parallel but separate from the normal flow of your container. Don\u0026rsquo;t worry, I\u0026rsquo;ll get back to this soon.\n1.Sequence in a nutshell A sequence comprises three phases:\nThe setup, which is a Tag which must successfully complete before the main Tag fires.\nThe main tag, which is the Tag with the dependencies. It\u0026rsquo;s either dependent on the setup, or it establishes a dependency with the cleanup.\nThe cleanup, which is a Tag which fires after the main tag has completed successfully.\n  The sequence is created by editing the Advanced Settings of the main tag. You will see two new settings here:\n  Fire a tag before\u0026hellip;\n  Fire a tag after\u0026hellip;\n  The first setting is for choosing the setup for the main Tag. The latter is for the cleanup. If you choose a setup, it will need to complete before the main tag itself fires. If you choose a cleanup, the main tag will need to complete before the teardown is fired. For both, you can choose whether or not a failure in either the setup or the main tag will abort the sequence.\nOK. That was confusing. Allow me to visualize it with my superior PowerPoint skills.\n  In the sequence above, the Page View Tag (setup) must successfully complete before the Event Tag (main tag) fires. If the Page View Tag fails, the sequence is aborted. Similarly, the Event Tag must successfully complete before the final Event Tag (cleanup) fires. If the main Tag fails, the final Event Tag will not fire.\nYou can, of course, specify that the Tags fire even if some member in the sequence fails.\nThe key thing to understand about the sequence is that it\u0026rsquo;s isolated. Using Tag sequencing lets you specify a sequence of hits that fire when the main tag is triggered. This means that the setup and cleanup Tags will act completely oblivious to any Triggers you might have added to them.\nIndeed, no matter what Triggers you have or have not equipped on them, they are completely ignored if the Tag is used as a setup or cleanup. For this reason, it might be prudent to not use setup and cleanup Tags for anything except sequencing, but there is a way to make it work both ways (I\u0026rsquo;ll get back to this soon).\nI emphasize again that this is a difficult concept to grasp. I\u0026rsquo;m sure the UX / UI could be made more intuitive, but the reason it\u0026rsquo;s difficult is that it relies on callbacks, which are invisible in actual GTM usage (unless you start debugging on a really granular level).\n2. The callbacks We still have some theory to cover before delving deeper into the practical stuff.\nIn GTM, each Tag has a callback. With Tag templates, these callbacks depend on how the Tag works. For example, the Google Analytics Tag uses the hitCallback feature as its callback.\nSo, when you establish a setup, for example, the main tag will not fire until the onSuccess callback of setup is invoked. With the Google Analytics Tag template, the tag will be fired only after the hitCallback of the setup is executed successfully. Similarly, the cleanup will not be fired until the onSuccess callback of the main tag is called.\nPhew!\nThe other callback is onFailure. This is invoked in case the Tag does not successfully complete. In the settings for Tag sequencing, you can specify that the main tag must not fire if setup fails, or you can choose to ignore the failure. Similarly, you can specify that the cleanup must not fire if main tag fails, or you can, again, choose to ignore the failure.\nYou don\u0026rsquo;t have to worry about these callbacks when using Tag templates. GTM takes care of calling onSuccess and onFailure for you, and all you need to do is make sure that you\u0026rsquo;ve checked the correct boxes in the settings (I promise, we\u0026rsquo;re going to get to the settings very soon!).\n3. The Custom HTML Tag The exception is the Custom HTML Tag. Since you might want to tell Google Tag Manager not to proceed to the next tag until something has happened (e.g. an asynchronous operation has completed), a workaround is required. If you go to the Built-in Variables, you should see a new one: HTML ID:\n  This is used to invoke either the onSuccess or onFailure callback for the Custom HTML Tag.\nSo, if you want to use a Custom HTML Tag as either the setup or the main tag, and you want to establish a sequence with success and/or failure conditions, you will need the following:\n  The HTML ID Built-In Variable\n  The Container ID Built-In Variable\n  The Custom HTML Tag itself\n  In the following code example, I\u0026rsquo;m running some custom JavaScript, and once it\u0026rsquo;s done, I\u0026rsquo;m letting the sequence know that onSuccess has been called.\n\u0026lt;script\u0026gt; (function() { var gtm = window.google_tag_manager[{{Container ID}}]; var el = document.getElementById(\u0026#39;content-head\u0026#39;); try { el.innerHTML = \u0026#39;We have taken over your website!\u0026#39;; gtm.onHtmlSuccess({{HTML ID}}); } catch(e) { gtm.onHtmlFailure({{HTML ID}}); } })(); \u0026lt;/script\u0026gt; In the first line of the script, you copy a reference to the GTM container interface into the gtm variable. This is a very specific interface into GTM\u0026rsquo;s mechanisms, so don\u0026rsquo;t worry about it too much. Suffice to say that this reference allows you to pass the onSuccess and onFailure callbacks, which is why it\u0026rsquo;s important here.\nNext, there\u0026rsquo;s a try...catch block, where a DOM method is executed. Once it\u0026rsquo;s completed, the onHtmlSuccess() method of the interface is invoked. The parameter to this method is the new HTML ID Built-In Variable. This way GTM will be able to communicate to the next member of the sequence that it was precisely this particular Custom HTML Tag that just completed.\nIf there\u0026rsquo;s an error, then the onHtmlFailure() method is invoked, and information is passed to the sequence that this Tag did not complete successfully.\nBy now it shouldn\u0026rsquo;t be too complicated anymore to understand what\u0026rsquo;s happening. Each Tag template has an onSuccess and onFailure callback, except for the Custom HTML Tag, where you can to provide them yourself.\n UPDATE 23 July 2018: Be sure to read my article on Custom HTML tags with tag sequencing for more details on how Custom HTML tags and tag sequencing work together.\n 4. How to create the sequence Finally! Some practical information!\nUnder the Advanced Settings of each Tag template, you will find three new options:\n  Tag firing options - Lets you delimit the Tag to fire either just Once per event, Once per page, or just let it fire Unlimited times.\n  Tag Sequencing -\u0026gt; SETUP - Lets you choose the Tag which must complete before the Tag whose settings you are currently editing. You can specify whether the setup must complete successfully, or if failures should still make this Tag fire.\n  Tag Sequencing -\u0026gt; CLEANUP - Lets you choose the Tag which must fire after the current Tag has completed execution. Again, you can specify whether the current Tag must complete successfully, or if its failure should be ignored, and the cleanup should fire regardless.\n    That\u0026rsquo;s how you set up a Tag Sequence.\nNow, in the very beginning I mentioned that the sequence works in isolation. This means that the setup you establish for any particular Tag will run the sequence regardless of what other Triggers you might have attached to the setup or cleanup. This also means that a sequence is always bound to the event of the main tag. Thus, if the main tag fires upon the \u0026ldquo;All Pages\u0026rdquo; Trigger, for example, then the entire sequence will be completed during this event.\nBecause you can use a Tag both independently and as part of a sequence, the new Tag firing options are very useful, indeed. For example, if you have Tag A as the setup for some Tag, as well as firing on its own, individual Trigger, you can specify that Tag A must only fire Once per page to prevent multiple hits from being sent. This means simply that after it has fired once, either independently or as part of a sequence, it will not fire again while the user is on the page.\nYou can also specify Once per event, which means that the Tag will only fire once for any given data layer event. So, you might have Tag A firing on the \u0026ldquo;All Pages\u0026rdquo; Trigger, and you might also have it as the cleanup of some Tag which also fires on the \u0026ldquo;All Pages\u0026rdquo; Trigger. By using the Once per event setting, Tag A will only fire once during the \u0026ldquo;All Pages\u0026rdquo; event.\n5. Note about using dataLayer.push() in the sequence One of the main use cases for using tag sequencing would be to have the setup Tag push something into dataLayer for the main Tag to use.\nHold your horses!\nIf you know your Google Tag Manager, you\u0026rsquo;ll know that the Data Layer available to any Tag is fixed for the duration of the Data Layer Event! So any Tags which fire on the initial dataLayer.push() will only be able to access the values in this push. If some Tag tries to modify the values in the Data Layer, these values will not be registered until the next dataLayer.push() with an event.\n(UPDATE: Some time after this article was written, GTM\u0026rsquo;s behavior changed so that variables are resolved separately for every tag in a sequence. This means that you don\u0026rsquo;t need to use google_tag_manager[...].dataLayer.get() if you want to retrieve a variable that changed from one tag in the sequence to the next. Instead, you can just use regular Google Tag Manager Variables, as their values will update for each tag in the sequence!)\nThis hampers development a little, since it sounds like dataLayer can\u0026rsquo;t be used as a messaging medium within a sequence, as the entire sequence operates in the span of a single Data Layer Event (the one which fired the main Tag in the first place).\nThe developer guide actually has a note about this:\n  However, at the time of writing this, dataLayer.set() does not exist. I asked the developers, and this seems to be a mistake in the guide. Instead, you need to use the following command to set values in the Data Layer even during the tag sequence:\n// To set a value, use set() with the key as the first parameter, and value as the second google_tag_manager[{{Container ID}}].dataLayer.set(\u0026#39;someKey\u0026#39;, \u0026#39;someValue\u0026#39;);  Once you use this, any Tag that fires after this one (in the same sequence), will have its Data Layer Variable reference to someKey updated to return someValue.\nIf you want to retrieve this changed value in the same Tag it was changed in, you can\u0026rsquo;t use a Data Layer Variable, since it\u0026rsquo;s resolved before the Tag is executed (and the set() command is run). You\u0026rsquo;ll need to use this syntax instead:\n// To get a value, use get() with the key as the parameter google_tag_manager[{{Container ID}}].dataLayer.get(\u0026#39;someKey\u0026#39;);  Remember to enable the Container ID Built-in Variable!\nThese are fairly advanced use cases, but it\u0026rsquo;s very good to keep this in mind when working with Tag sequences that are scoped to a single Data Layer event.\n6. Why use tag sequencing? The asynchronous loading of JavaScript is taken for granted these days. It\u0026rsquo;s a nice way for browsers to ensure a decent browsing experience even with large, bulky linked assets.\nHowever, asynchronous JavaScript makes it difficult to establish order, as you can\u0026rsquo;t predict when some script or library finishes loading. Order is introduced with callbacks or promises, but these are difficult concepts to manoeuvre around, and the Google Tag Manager UI doesn\u0026rsquo;t necessarily let you take advantage of them, especially if using templates.\nSequencing lets you tap into callbacks that GTM establishes by default (the tag templates), or that you create manually (Custom HTML Tags). It lets you chain Tags together, which is particularly useful if you want to establish dependencies.\n"
},
{
	"uri": "https://www.simoahava.com/tags/persistence/",
	"title": "persistence",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/storage/",
	"title": "storage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/two-ways-to-persist-data-via-google-tag-manager/",
	"title": "Two Ways To Persist Data Via Google Tag Manager",
	"tags": ["cookies", "Google Tag Manager", "Guide", "persistence", "storage"],
	"description": "Introducing browser cookies and HTML5 storage as options for data persistence when using Google Tag Manager.",
	"content": "The web is stateless. It\u0026rsquo;s basically blind to your past, and it does a poor job of predicting what you might do in the future. When you browse to a website, the browser requests the page from the web server, and then proceeds to render it for you. This is a detached, clinical process, and any personalized or stateful data transfer is left to the sophistication of your web server.\n  Statelessness has some implications to us concerned about web analytics and measuring our visitors. In Google Tag Manager, this is an ever-present elephant in the room. Indeed, many suffer over not being able to fire Tags for e.g. returning visitors, or after five page views, or if the original source of the Session was AdWords. GTM is (for now) blind to this type of thinking, as it\u0026rsquo;s simply a JavaScript machine operating in a stateless environment. Any notion of state needs to be injected manually into the machine.\nAnd how is this done? Cookies and the Web Storage API.\nBrowser cookies Cookies in the browser are the de facto way of persisting information in the web. They\u0026rsquo;ve been around for a long time (over 20 years!), and they\u0026rsquo;re supported by pretty much all browsers - legacy and modern. For a refresher on what cookies are and how they work, check out this Wikipedia article. The most relevant features of browser cookies are:\n  4KB size limitation (per cookie)\n  Unencrypted in a single string, stored in document.cookie\n  Sent to the server with every single HTTP request\n  Can be set with an expiration date\n  The size limitation quickly becomes an issue with large payloads, and the fact that they are sent to the server unencrypted with each request can be a turn-off for those with security concerns.\nWeb Storage API The Web Storage API is a more recent innovation, and this is represented by lack of support from Internet Explorer 7 and older (who cares, though!). The API has two interfaces: localStorage and sessionStorage. The main difference is that the latter persists for the duration of the browser session (i.e. is flushed when the browser instance is shut down), and the former persists indefinitely.\n  Features of the Web Storage API are:\n  Can only be read client-side in the browser. No data is sent to the server.\n  Stores a maximum of 5MB per domain\n  Stores data in a hash table, meaning you can make exact queries without having to parse the data\n  Basically, the Web Storage API is a logical evolution of persistence, taking the best of cookies and adding better data access and management features. The big drawback is the lack of server-side communication, so if you rely on cookies to pass information to the web server, migrating to the Web Storage API wouldn\u0026rsquo;t be logical unless you manually send the data to the server (e.g. via POST request).\nCookies and GTM Google Tag Manager, thankfully, already provides us an interface to query for cookie values: the 1st Party Cookie Variable. When you set the Variable to a cookie name, it will always return whatever value is stored in that particular cookie. This is a huge help, since you don\u0026rsquo;t need to go through the rather tedious process of parsing the entire cookie string for the name-value pair you\u0026rsquo;re looking for.\nSetting a cookie, however, still needs to be done manually. The easiest way to set a cookie is to add the following code in a Custom HTML Tag:\ndocument.cookie = \u0026#39;cookieName=cookieValue\u0026#39;;  This would set a cookie with the name cookieName and the value cookieValue. However, using this code alone would set the cookie to expire at the end of the browser session (i.e. when the browser is shut down), and the cookie would be set on the current path and the current domain only. The path and domain combination is especially important, as the cookie would only be accessible via the page it was set on.\nThis is why it\u0026rsquo;s customary to create a helper function, with which you can provide some important parameters. To create this type of helper in GTM\u0026rsquo;s context, I suggest using a Custom JavaScript Variable named {{JS - setCookie}}:\nfunction() { return function(name, value, ms, path, domain) { if (!name || !value) { return; } var d; var cpath = path ? \u0026#39;; path=\u0026#39; + path : \u0026#39;\u0026#39;; var cdomain = domain ? \u0026#39;; domain=\u0026#39; + domain : \u0026#39;\u0026#39;; var expires = \u0026#39;\u0026#39;; if (ms) { d = new Date(); d.setTime(d.getTime() + ms); expires = \u0026#39;; expires=\u0026#39; + d.toUTCString(); } document.cookie = name + \u0026#34;=\u0026#34; + value + expires + cpath + cdomain; } }  This handy little function actually returns another function, which takes a number of parameters:\n  name: Required. The cookie name.\n  value: Required. Value for the cookie.\n  ms: Expiration time of the cookie in milliseconds. If not set, defaults to Session.\n  path: Path of the cookie. If not set, defaults to the current path.\n  domain: Domain of the cookie. If not set, defaults to the current domain.\n  To use this Variable in a script, you need to use the following syntax:\n{{JS - setCookie}}(\u0026#39;session\u0026#39;, \u0026#39;true\u0026#39;, 1800000, \u0026#39;/\u0026#39;, \u0026#39;simoahava.com\u0026#39;);  This would set a cookie with the name session and value true, which persists for 30 minutes, and is written on the root page of the root domain.\nIf you want to leave a parameter out, you\u0026rsquo;ll need to replace it with undefined, if you still want to send one of the latter parameters. For example:\n{{JS - setCookie}}(\u0026#39;session\u0026#39;, \u0026#39;true\u0026#39;, undefined, undefined, \u0026#39;simoahava.com\u0026#39;);  Would set a cookie with name session, value true, which would be a Session cookie, written on the current page of the root domain.\nPhew! That\u0026rsquo;s a mouthful of code. But it does its job nicely. Combine this with the excellent 1st Party Cookie Variable, and you\u0026rsquo;ve managed to create a nice and handy cookie setter/getter.\nWeb Storage API and GTM You could create a fancy set of Custom JavaScript Variables for localStorage and sessionStorage as well, but why bother? They\u0026rsquo;re dead simple to interact with.\nTo set a key-value pair in storage, you can use the following syntax:\nif (window[\u0026#39;Storage\u0026#39;]) { // localStorage persists indefinitely  localStorage.setItem(\u0026#39;session\u0026#39;, \u0026#39;true\u0026#39;); // sessionStorage persists until the browser is closed  sessionStorage.setItem(\u0026#39;session\u0026#39;, \u0026#39;true\u0026#39;); } else { // Fallback for when the browser doesn\u0026#39;t support Web Storage API  {{JS - setCookie}}(\u0026#39;session\u0026#39;, \u0026#39;true\u0026#39;); }  First of all, you need to check if the browser supports the Web Storage API. That\u0026rsquo;s just a simple if...else block. See what I did with the fallback? That\u0026rsquo;s right! I used the {{JS - setCookie}} method we just created! What a beautiful segue.\nWeb Storage only accepts strings as values, so you need to make sure whatever you\u0026rsquo;re storing can be safely converted into a String. Don\u0026rsquo;t worry, the Storage API does the casting for you, if you fail to explicitly save a String type.\nUnfortunately, as of yet GTM does not have a handy Variable you can use to fetch items from storage (I hope this changes soon!). But to retrieve a value is just as simple as setting one:\nif (window[\u0026#39;Storage\u0026#39;]) { var localSession = localStorage.getItem(\u0026#39;session\u0026#39;); var sessionSession = sessionStorage.getItem(\u0026#39;session\u0026#39;); }  That\u0026rsquo;s right, it\u0026rsquo;s the getItem() method of the API that lets you access stored keys. It returns null if the item isn\u0026rsquo;t found.\nAnd that\u0026rsquo;s how you roll with the Web Storage API. You get by with a LOT less code, and the value lookup is way more intuitive than with cookies, though GTM does take a lot of the burden away from you with its awesome 1st Party Cookie Variable.\nSummary For many interactions, persisting data in the browser is very important. However, not all persistence should be left for the client, since servers do a far better and more robust job of storing user- and usage-based data. That\u0026rsquo;s why Google Analytics shifted from client-side calculations (ga.js and older) to server-side algorithms (Universal Analytics). And that\u0026rsquo;s why if you really have pressing and business-critical needs to persist information, you might want to talk to your developers about storing this in a proper data store or database, exposing the information in the dataLayer when and where relevant.\nBut for the simple, session-scoped persistence, the two methods outlined in this guide should be quite useful. I strongly recommend using the Web Storage API, but unfortunately you might need to introduce a fallback with browser cookies, since there are still many fools out there using outdated browsers.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/product-scoped-custom-dimensions-and-metrics/",
	"title": "#GTMTips: Product-Scoped Custom Dimensions And Metrics",
	"tags": ["enhanced ecommerce", "Google Tag Manager", "gtmtips"],
	"description": "How to use product-scoped custom dimensions and metrics in your Enhanced Ecommerce tracking. This guide is focused especially on Google Tag Manager.",
	"content": "With the advent of Enhanced Ecommerce for Universal Analytics, a new scope was introduced for Custom Dimensions and Metrics. Product scope can be used to send information about each product that is sent through Enhanced Ecommerce, but it\u0026rsquo;s not exactly the most logical or intuitive thing to wrap your head around.\nIn this #GTMTips post, we\u0026rsquo;ll take a look at how to implement Product-Scoped Custom definitions via Google Tag Manager, and I\u0026rsquo;ll quickly explain how they work in relation to queries and reports you might want to build on top of them.\nTip 29: Add product-scoped Custom Dimensions and Metrics to Enhanced Ecommerce   Here\u0026rsquo;s the key piece of information about the product scope: it\u0026rsquo;s bound to the Enhanced Ecommerce event and product it\u0026rsquo;s sent with. So, if you send the dimension or metric with a product in the Add to cart payload, it will only exist in that event. It won\u0026rsquo;t persist through the entire Enhanced Ecommerce funnel!\nThis is crucial information, as it reminds us that Enhanced Ecommerce is very clinical by nature. Barring some exceptions (e.g. product list click attribution), a well-oiled Enhanced Ecommerce funnel requires consistency. It\u0026rsquo;s critical to maintain a uniform product object throughout the funnel, adding relevant dimensions when they become available, as product variant (e.g. size) might be something you select only after the detail view, when you add the product to cart.\nProduct-scoped custom dimensions and metrics give you more dimensions and metrics to play with, but they are bound by the same consistency requirement as the regular dimensions and metrics. So keep this in mind when implementing them. In the picture above, dimension12 is used to maintain product size, which is \u0026ldquo;N/A\u0026rdquo; until the Add To Cart step. So any queries for \u0026ldquo;Show product abc123 with size Large\u0026rdquo; would only return hits for Add To Cart, Checkout and Purchase - not Impressions or Detail.\nExample uses for these custom dimensions and metrics would be:\n  In stock vs. out of stock (dimension)\n  Stock status (metric)\n  Profit margin (dimension if percentage, metric if value)\n  VAT-less price (metric)\n  Host product or bundle add-on (dimension)\n  To implement product-scoped custom definitions, you basically add the dimension or metric key directly into the relevant products Array object in dataLayer:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;eecDetail\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;abc123\u0026#39;, \u0026#39;dimension3\u0026#39; : \u0026#39;In Stock\u0026#39;, \u0026#39;metric5\u0026#39; : \u0026#39;11.99\u0026#39; },{ \u0026#39;id\u0026#39; : \u0026#39;abc234\u0026#39;, \u0026#39;dimension3\u0026#39; : \u0026#39;Out of Stock\u0026#39;, \u0026#39;metric5\u0026#39; : \u0026#39;13.00\u0026#39; }] } } });  After this, any Enhanced Ecommerce / Data Layer -enabled Tag which fires on Custom Event eecDetail will also send these custom dimension and metric values to GA along with their respective products.\nTo be more generic with dataLayer, a simple way of incorporating Google Analytics -specific dimensions (such as these product-scoped definitions) is to utilize the Custom JavaScript Variable method for Enhanced Ecommerce implementation.\nIf you want to see a nice example of how to use a product-scoped custom metric, check out my post on measuring cart value with Google Tag Manager.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/block-your-tags-with-trigger-exceptions/",
	"title": "#GTMTips: Block Your Tags With Trigger Exceptions",
	"tags": ["exceptions", "Google Tag Manager", "gtmtips", "Guide", "triggers"],
	"description": "Use trigger exceptions to block Google Tag Manager tags from firing under certain conditions and circumstances.",
	"content": "To prevent a Tag from firing in Google Tag Manager, you can:\n  Delete the Tag\n  Remove all Triggers from the Tag\n  Add an Exception Trigger to the Tag\n  The third option is usually the best if the blocking is just temporary. Exceptions are what used to be called blocking rules in the first version of GTM. To add them is easy enough. In the Fire On step of Tag creation, you can click Create Exceptions, and choose the Trigger that will block this Tag from firing.\n  However, creating an Exception can lead to confusion, as you have to pick a Trigger type for the exception! But you just wanted to block a Tag from firing; what\u0026rsquo;s with this Trigger type nonsense?!\nTip 29: Event of an Exception must match Event of the Tag   Every single Tag needs an Event to fire. An Event, in the GTM world, is a special event key that is pushed into dataLayer, either automatically by Google Tag Manager, or manually by the website developer:\nwindow.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;triggerSomeTag\u0026#39;, \u0026#39;money\u0026#39; : \u0026#39;lots\u0026#39; });  When an event key is pushed into dataLayer, GTM goes through all Triggers attached to Tags, and looks for Triggers which respond to this Event.\nIf such a Trigger is found, and if any other Trigger conditions pass, then the Tag can happily fire.\nIf you want to block a Tag from firing, you will essentially need to create an Exception for every single Event you want to prevent the Tag from firing on - individually. So let\u0026rsquo;s say you have a Tag which fires on Page View, Click, and Form, and you want to block it from firing on Page View and Click, but you still want it to respond to a Form submission. The Trigger you\u0026rsquo;d need to create as an Exception looks like this:\n  Now, when you add this Trigger as an Exception to a Tag, the following will happen:\n  Upon Page View (i.e. when the container snippet loads), GTM finds the Event name gtm.js (Page View Event) in the Exception Trigger and prevents the Tag from firing.\n  Upon a Click Event, GTM finds the Event name gtm.click in the Exception Trigger and prevents the Tag from firing.\n  Upon a Form Submit Event, GTM does not find the Event name gtm.formSubmit in the Exception Trigger, and the Tag fires\n  So as you can see, an Exception always wins against the corresponding Trigger. The Custom Event Trigger type combined with RegEx matching is a very powerful way of blocking multiple Events at once.\nIf you want to create a comprehensive, universal, block-all Trigger Exception, all you need to do is set the Event name field to .* in the Exception Trigger. This means that whenever _any_ Event is registered by GTM, the Exception will block every single one of these, as they are all covered by the wildcard pattern in the regular expression.\nKey takeaway here is this: the Exception Event must match the Trigger Event. For example, if you want to block a Click Trigger, you need to have a Click Trigger blocking it. Indeed, an easy way to block a single Trigger is to add the very same Trigger as the Exception!\n"
},
{
	"uri": "https://www.simoahava.com/tags/exceptions/",
	"title": "exceptions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/form-field-timing-with-google-tag-manager/",
	"title": "Form Field Timing With Google Tag Manager",
	"tags": ["form", "Google Tag Manager", "Guide", "user timings"],
	"description": "Measure the time users dwell in individual form fields using Google Tag Manager to send the data to Google Analytics.",
	"content": "The inimitable Craig Sullivan gave me an idea for a continuation to my latest post on form abandonment tracking. In this short tutorial, I\u0026rsquo;ll show you how to track the time users spend on your form fields. We\u0026rsquo;re going to use the User Timings hit type, and we\u0026rsquo;ll send the data for just one form. With small modifications, you can expand the script to cover multiple forms on a page.\n  This simple solution tracks the time the user spends on each form field by measuring the distance between the focus event and the blur or change event. The first one occurs when a form field is entered, and the latter depends on if a value changed (change) or no change was registered (blur).\nThe timing object that is sent to GA looks like this:\nTiming Category - Comment Form Field Timing\nTiming Variable - Form field name attribute value\nTiming Label - \u0026lsquo;blur\u0026rsquo; or \u0026lsquo;change\u0026rsquo; depending on how the field was exited\nTiming Value - Time spent in the field in milliseconds\nNote! Timings are counted against the 500 hits / session limit in Google Analytics. That\u0026rsquo;s why you have to be careful when implementing this. I suggest you only implement it on one form and for a short time, and then see if users\u0026rsquo; sessions are getting miscalculated thanks to them surpassing the 500 hit limit. There are some additional limitations per property as well, but they only count towards the timing hits and not all hits in the session.\nThe Custom HTML Tag The Custom HTML Tag looks like this:\n\u0026lt;script\u0026gt; (function() { var form = document.querySelector(\u0026#39;#commentform\u0026#39;); var fields = {}; var enterField = function(name) { fields[name] = new Date().getTime(); } var leaveField = function(e) { var timeSpent; var fieldName = e.target.name; var leaveType = e.type; if (fields.hasOwnProperty(fieldName)) { timeSpent = new Date().getTime() - fields[fieldName]; if (timeSpent \u0026gt; 0 \u0026amp;\u0026amp; timeSpent \u0026lt; 1800000) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;fieldTiming\u0026#39;, \u0026#39;timingCategory\u0026#39; : \u0026#39;Comment Form Field Timing\u0026#39;, \u0026#39;timingVariable\u0026#39; : fieldName, \u0026#39;timingLabel\u0026#39; : leaveType, \u0026#39;timingValue\u0026#39; : timeSpent }); } delete fields[fieldName]; } } if (form) { form.addEventListener(\u0026#39;focus\u0026#39;, function(e) { enterField(e.target.name); }, true); form.addEventListener(\u0026#39;blur\u0026#39;, function(e) { leaveField(e); }, true); form.addEventListener(\u0026#39;change\u0026#39;, function(e) { leaveField(e); }, true); } })(); \u0026lt;/script\u0026gt; To fire this Tag, I suggest you create a Page View Trigger which only fires this Tag on pages where your form exists.\nNext, change the very first variable var form = document.querySelector('#commentform'); to select the form you want to track. On my site this happens to be a form with ID #commentform.\nWhenever a focus event is detected, a hash table is updated with the form field name and the time when the field was entered.\nThen, when the field is exited, and either a blur or change is recorded, the time spent in the field is calculated, and then the field name is deleted from the hash table. If you don\u0026rsquo;t delete the field, a change event would send the timing twice, because when the user edits the value of a field and leaves it, a change is dispatched first, and then a blur.\nAnyway, feel free to change the value of the \u0026lsquo;timingCategory\u0026rsquo; variable in the dataLayer.push(). I use \u0026lsquo;Comment Form Field Timing\u0026rsquo;, as that\u0026rsquo;s what I\u0026rsquo;m doing here.\nFinally, if the form selector you use in the very first variable declaration of the script works, the three listeners are attached to the form with their respective callbacks.\nThis is the script itself. We\u0026rsquo;ll wrap up the setup in the next chapter.\nTiming Tag, Trigger, and Data Layer Variables To make the setup work, we need a Timing Tag, a Custom Event Trigger and four Data Layer Variables.\nData Layer Variables Create the Data Layer Variables first. You need four Variables with the following settings:\nName: {{DLV - timingCategory}}\nData Layer Variable Name: timingCategory\nName: {{DLV - timingVariable}}\nData Layer Variable Name: timingVariable\nName: {{DLV - timingLabel}}\nData Layer Variable Name: timingLabel\nName: {{DLV - timingValue}}\nData Layer Variable Name: timingValue\nCustom Event Trigger The Trigger is a Custom Event Trigger with the following settings:\nName: Event - fieldTiming\nFire On / Event name: fieldTiming\nTiming Tag Finally, you need to create the Timing Tag. Create a new Tag of type Google Analytics, and set the following settings:\nName: UA - Timing - Form Field Timing\nTracking ID: UA-XXXXXX-Y (substitute with your property ID)\nTrack Type: Timing\nVar: {{DLV - timingVariable}}\nCategory: {{DLV - timingCategory}}\nValue: {{DLV - timingValue}}\nLabel: {{DLV - timingLabel}}\nAttach the Trigger you created in the previous section to this Tag.\nAnd now you\u0026rsquo;re set to go! Each interaction with the fields in your form should now send the Timing hit. You can find the results in Google Analytics, by going to Behavior -\u0026gt; Site Speed -\u0026gt; User Timings.\nSummary Like I mentioned, this is a very simple solution. It just measures the time spent on each form field, and reports this as a User Timing hit to Google Analytics. You might want to tweak it a little to be more robust, and you have to be mindful of the 500 hits / session limitation that GA imposes on sessions.\nYou can do some cool stuff with the data, such as building a histogram where it\u0026rsquo;s easier to visualize the problematic fields. It would be also interesting to see if long time spent on a field correlates with form abandonment.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/track-form-abandonment-with-google-tag-manager/",
	"title": "Track Form Abandonment With Google Tag Manager",
	"tags": ["custom html", "form", "Google Tag Manager", "Guide"],
	"description": "Track how, where, and when users are abandoning your forms. Use Google Tag Manager to send the data to Google Analytics.",
	"content": "Form abandonment isn\u0026rsquo;t always easy to define. Most often, it refers to when someone starts to fill in an HTML form, but leaves the page without submitting it. As a definition, this works nicely. However, with multi-page forms it naturally refers only to the last page of the form. Also, especially with government institutions, forms can be saved to be submitted later. Here, again, form abandonment must be reconsidered.\n  In this article, I\u0026rsquo;ll go over four different ways to track form abandonment in Google Analytics, using Google Tag Manager to setup the tracking. The definition I\u0026rsquo;ll use is the first one: abandonment occurs when someone enters a form page, but leaves it without submitting the form. Do note that GA might not be the right tool to actually track issues with your form. There are dedicated resources for that, such as ClickTale or Hotjar. But if you want to measure form engagement as part of a more coherent visitor journey, then it might make sense to track this engagement (or lack thereof) in Google Analytics.\nThe different methods I\u0026rsquo;ll go over, in order of difficulty (as perceived by myself):\n  Advanced Segment in Google Analytics\n  Event sent if form interacted with\n  Event sent with the last field interacted with\n  Event sent with full interaction history\n  Well, the first one can be done without Google Tag Manager at all. But the others require some setting up.\nThe technology, if you will, is based on the beautiful combination of beforeUnload, a browser event that is dispatched when the user leaves or closes the web page, and the transport field in Universal Analytics. With these in place, GA will receive an event that contains information about form interaction when the user leaves the form page. You can then use an Advanced Segment to only view interaction data for sessions where the form was not submitted.\nI\u0026rsquo;ll even show you a simple way of sending the event only if the user didn\u0026rsquo;t submit the form.\nNote that this guide is created around a very simple, basic form, which has been created according to Best Practices; capital B, capital P. If you have some Ajax-postback-dynamic-validation-jQuery-horror-ultra-modern-form, this might not work for you, and you\u0026rsquo;ll have to put your JavaScript skills to the test as you apply these lessons to the abomination you\u0026rsquo;re cursed to work with.\nDon\u0026rsquo;t say I didn\u0026rsquo;t warn you!\n(UPDATE. My colleague, Lauri Piispanen, wrote a very, very nice upgrade to the Custom HTML Tag, and it\u0026rsquo;s freely downloadable from his GitHub repo. It automatically tracks multiple forms on the page!)\nAdvanced Segment in Google Analytics In Google Analytics, the simplest way to track form abandonment is to create an Advanced Segment, which includes Sessions where the form page was visited, but the session itself did not include a conversion for the form submission:\n  This is OK. It\u0026rsquo;s not polished, as it doesn\u0026rsquo;t really tell us anything about why the form was abandoned. Also, a simple page view of the form page is simply not a good way to extrapolate intent. But for all intents and purposes, this exposes possible flaws in a crucial funnel on your website.\nAs you create the Events in the chapters that follow, you can substitute the Page rule in the segment condition with an Event rule that matches the Event you\u0026rsquo;ve created.\nThe base Custom HTML Tag This Custom HTML Tag is what we\u0026rsquo;ll use to setup the beforeUnload listener as well as the interaction handlers created incrementally in the following chapters.\nSo, beforeUnload is a browser event that is dispatched when the page is unloaded from the browser. Most typical ways for a page to unload is if the user is redirected to some other page (via a link or form submission, for example), or if the user decides to close the browser. Naturally, if the user\u0026rsquo;s computer crashes, no controlled beforeUnload event is dispatched.\nWe want to use this browser event to invoke a dataLayer.push() when the user leaves the page. This push() will include a custom event key as well as values for the fields in the Event Tag we\u0026rsquo;ll create in the next chapter.\nCreate a Custom HTML Tag, give it some illustrious name, and set it to fire only on pages with forms. Here\u0026rsquo;s the code we\u0026rsquo;ll work on. Just copy-paste it into the Tag.\n\u0026lt;script\u0026gt; (function() { var eventAction; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;formAbandonment\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Abandonment\u0026#39;, \u0026#39;eventAction\u0026#39; : eventAction }); }); // Add actual logic here...  })(); \u0026lt;/script\u0026gt; That\u0026rsquo;s pretty simple, right? All you do is attach a listener to the page, which waits for the beforeunload event. When that takes place, a dataLayer.push() is invoked, which is then used to fire a Google Analytics Event Tag with all your abandonment issues within. The whole thing is wrapped in an immediately-invoked function expression to protect the global namespace from being polluted with your variables.\nThe Event Tag Before we go on, let\u0026rsquo;s create the Event Tag itself. And before we do that, you\u0026rsquo;ll need to create a Custom Event Trigger and two Data Layer Variables.\nThe Custom Event Trigger looks like this:\n  And the two Data Layer Variables look like this:\n    Next, onto the Event Tag itself. It\u0026rsquo;s your basic affair, with a simple Event Category and Event Action. I\u0026rsquo;ve set the Non-Interaction field to True, since this isn\u0026rsquo;t really a user interaction but more like metadata about the page view, but you can do as you wish.\nRemember to add your new Custom Event Trigger you just created to this Tag.\nThe Tag itself looks like this:\n  You\u0026rsquo;ll need one more thing that\u0026rsquo;s not visible in the screenshot. Go to More Settings -\u0026gt; Fields to Set, and add the following:\nField Name: transport\nValue: beacon\nThis will utilize a beautiful feature of the Universal Analytics library, where any HTTP request that is initiated as the page is unloaded will be allowed to complete before the page or browser is run down. It doesn\u0026rsquo;t have stellar browser support, which means that horrible browsers such as Internet Explorer might simply exit the page before the request to GA completes. But we can live with that.\nNow that we\u0026rsquo;ve taken care of the foundations, we can get to the juicy stuff. Are you ready? I didn\u0026rsquo;t hear you? ARE YOU READY?\nSorry for that.\nTrack simple interaction \u0026ldquo;Simple interaction\u0026rdquo; here translates to: the user changed the value of any form field. That\u0026rsquo;s all we care about. It doesn\u0026rsquo;t matter which field, and it doesn\u0026rsquo;t matter how many form fields. We want to draw a line between two types of people: those who didn\u0026rsquo;t interact with the form at all, and those who did.\nTo make this work, you\u0026rsquo;ll need to add another custom listener into the Custom HTML Tag. Here\u0026rsquo;s what we\u0026rsquo;ll go with in the Custom HTML Tag you\u0026rsquo;ve already created:\n\u0026lt;script\u0026gt; (function() { var eventAction; var formSelector = \u0026#39;form\u0026#39;; // Modify this CSS selector to match your form. Default is first form on the page.  window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if (eventAction) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;formAbandonment\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Abandonment\u0026#39;, \u0026#39;eventAction\u0026#39; : eventAction }); } }); document.querySelector(formSelector).addEventListener(\u0026#39;change\u0026#39;, function() { eventAction = \u0026#39;True\u0026#39;; }); })(); \u0026lt;/script\u0026gt; In the beginning, you define a formSelector. This is a basic CSS selector that you use to target a single form, e.g. \u0026lsquo;#formId\u0026rsquo; or \u0026lsquo;.formClass\u0026rsquo;. If you choose a selector that can have potentially multiple hits on the page, this script will only listen to the first instance.\nIn the end, we add a change listener to this particular form. If a change event is registered in any of the form fields, i.e. the form field value changes, then the eventAction variable is given a value \u0026lsquo;true\u0026rsquo;.\nFinally, the dataLayer.push() that sends the payload to dataLayer which triggers the Event Tag is set to execute only if the eventAction variable is set. Thus, you\u0026rsquo;ll only report on potential form abandonments if the user has interacted with the form.\nThe Event Tag will have Event Category Form Abandonment and Event Action True.\nTo get the most out of this solution, you\u0026rsquo;d create an Advanced Segment in Google Analytics, where you\n  Include only sessions where form goal completions is 0\n  AND where Event Category equals Form Abandonment\n  You need the \u0026ldquo;goal completions is 0\u0026rdquo; because this Event will be dispatched when a form is submitted as well. In order to only send the Event when the user has interacted with the form but did not submit it, jump to chapter 7.\nLike I said, this is a very simple, rudimentary way of analyzing traffic that interacted but did not submit.\nTrack last field interacted with Let\u0026rsquo;s improve the previous solution a little. Instead of just blindly looking at who interacted and who didn\u0026rsquo;t, let\u0026rsquo;s send the last field the user interacted with to GA. Maybe this will give us some clue about what made the user abandon the form.\nSo, modify the Custom HTML Tag to look like this:\n\u0026lt;script\u0026gt; (function() { var eventAction; var formSelector = \u0026#39;form\u0026#39;; // Modify this CSS selector to match your form. Default is first form on the page.  var attribute = \u0026#39;name\u0026#39;; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if (eventAction) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;formAbandonment\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Abandonment\u0026#39;, \u0026#39;eventAction\u0026#39; : eventAction }); } }); document.querySelector(formSelector).addEventListener(\u0026#39;change\u0026#39;, function(e) { eventAction = e[\u0026#39;target\u0026#39;].getAttribute(attribute); }); })(); \u0026lt;/script\u0026gt; We\u0026rsquo;ve added a new variable, attribute. In this, you specify the attribute name that you want to collect to describe which form field was the last one that was interacted with. I would recommend against choosing value or anything that the user can add content to. This is to protect your GA account from inadvertently collecting personally identifiable information such as e-mail addresses and such.\nThe other additions are the e parameter in the callback function of the \u0026lsquo;change\u0026rsquo; listener, and the attribute value of the e['target'] object as the value that eventAction retrieves. The target parameter of the event object contains the HTML element that was the target of the event action, which in this case would be the form field whose value changed.\nI would recommend you use the name attribute, since it\u0026rsquo;s usually the most descriptive one (especially with well-formed forms).\nSo, an input field like \u0026lt;input name=\u0026quot;email\u0026quot; type=\u0026quot;text\u0026quot;\u0026gt; would send \u0026lsquo;email\u0026rsquo; as the Event Action of the Event Tag.\nNow, again, this would send an Event every time someone leaves the form page, so you\u0026rsquo;ll get some false positives as well. Remember to apply the Advanced Segment, where you look for sessions with this event (using the Event Category filter in the segment settings), and sessions without any form submissions (using a goal completion filter or something similar).\nGet full interaction history Here we\u0026rsquo;ll extend the Event Action value that is sent to Google Analytics to include a complete \u0026ldquo;history\u0026rdquo; of fields that were interacted with this. This is done as a trail that might look like this:\nfirstName \u0026gt; lastName \u0026gt; address \u0026gt; creditCardNumber \u0026gt; expirationDate \u0026gt; creditCardNumber\nAs you can see, it will record multiple interactions as well. The history example above would indicate that the Credit Card Number field was the one that caused the user to disdainfully abandon your form.\nTo make this work, here\u0026rsquo;s the revamped version of the Custom HTML Tag:\n\u0026lt;script\u0026gt; (function() { var formSelector = \u0026#39;form\u0026#39;; // Modify this CSS selector to match your form. Default is first form on the page.  var attribute = \u0026#39;name\u0026#39;; var history = []; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if (history.length) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;formAbandonment\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Abandonment\u0026#39;, \u0026#39;eventAction\u0026#39; : history.join(\u0026#39; \u0026gt; \u0026#39;) }); } }); document.querySelector(formSelector).addEventListener(\u0026#39;change\u0026#39;, function(e) { history.push(e[\u0026#39;target\u0026#39;].getAttribute(attribute)); }); })(); \u0026lt;/script\u0026gt; Let\u0026rsquo;s see. We\u0026rsquo;ve removed the variable eventAction, and opted for a new Array named history. Instead of checking for whether the eventAction variable has a value, we now check if the history Array has any contents. If it does, we join() the history Array as a string delimited with whitespace-\u0026gt;-whitespace. So, an Array that looks like ['firstName', 'lastName', 'phone'] becomes 'firstName \u0026gt; lastName \u0026gt; phone'.\nFinally, in the change listener we use the history.push() method to insert the specified attribute value of the changed form field into the history Array.\nThus, each time the user changes a value of a form field, the specified attribute (\u0026lsquo;name\u0026rsquo; is again recommended) value will be pushed into the history Array, and this is how we get our neatly formatted interaction history to GA as the Event Action value of the hit.\nThis is, as you might have guessed, a very simple way of mapping form abandonment. Nevertheless, it does give you some idea about which fields are interacted with and how much. In fact, you can easily convert this from a form abandonment survey into an engagement analysis. Using some simple data analysis, you can map the most common paths in your form.\nOnly send event when a form isn\u0026rsquo;t submitted The major caveat with all the previous examples, apart from the browser support, is that the event is sent every time the page is unloaded and a field has changed, so for successful form submissions as well. This means you\u0026rsquo;ll need to create an Advanced Segment in GA to make the most of your form abandonment analysis.\nHowever, you can tweak the Custom HTML Tag slightly to only send the form abandonment event when the page unloads for some other reason than a form submission. To do this, the revised Custom HTML Tag should look like this:\n\u0026lt;script\u0026gt; (function() { var eventAction, i; var checkSubmit = function() { i = window.dataLayer.length - 1; while (i \u0026gt; -1) { if (window.dataLayer[i][\u0026#39;event\u0026#39;] === \u0026#39;gtm.formSubmit\u0026#39;) { return true; } i--; } }; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { if (!checkSubmit()) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;formAbandonment\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Abandonment\u0026#39;, \u0026#39;eventAction\u0026#39; : eventAction }); } }); // Add actual logic here...  })(); \u0026lt;/script\u0026gt; The first change is that you define a new counter variable i. Next, there\u0026rsquo;s a new mini-function in town called checkSubmit(). This loops through dataLayer, starting from the end, and returns the Boolean value true if an \u0026lsquo;event\u0026rsquo; key with the value \u0026lsquo;gtm.formSubmit\u0026rsquo; is found. This would indicate that the unload was due to a form submission.\nFinally, the window.dataLayer.push() is wrapped in an if block, which is executed only if the checkSubmit() function does not return a truthy value. This is because we want to send the Event only if a form submission didn\u0026rsquo;t cause the page unload.\nYou might be wondering why not just check against the value of the {{Event}} key. Well, GTM has some internal logic running which prevents the value of a Data Layer Variable from being modified during the lifetime of a Tag. Thus, {{Event}} will always return gtm.js or whatever event name you\u0026rsquo;re using to fire the Custom HTML Tag in the first place.\nIf you already have the if block there to check for either eventAction or history.length, you can simply add the checkSubmit() check with Boolean logic:\nif (eventAction \u0026amp;\u0026amp; !checkSubmit()) { ... } if (history.length \u0026amp;\u0026amp; !checkSubmit()) { ... }  This solution might not work on single-page apps or web pages which have forms that do not redirect. In these cases, you\u0026rsquo;ll need to make additional checks to see if the form that was submitted was the one you\u0026rsquo;re actually tracking. It\u0026rsquo;s relatively simple, but I won\u0026rsquo;t explain it here as it\u0026rsquo;s an edge case.\nWith JavaScript and GTM, it\u0026rsquo;s always a little bit of give-and-take, so you\u0026rsquo;ll need to modify these solutions to fit whatever frameworks and setups you have running on your site. With a generic, Best Practices -infused HTML form, though, these setups should work beautifully.\nSummary Tracking form abandonment is an interesting way to see what types of interactions (or lack thereof) result in your forms not being submitted. Fixing this funnel can be key to opening up your website to exponential goal conversions.\nHowever, abandonment is very difficult to define, and you need to ask the correct business questions first before you can start looking for a solution. The four different methods outlined here serve a very simple hypothesis that correlates the fields of a form with the user\u0026rsquo;s tendency to leave the form unfinished. This correlation might not work on many forms.\nLike I mention in the guide, it would be very simple to convert this from a form abandonment analysis to a form interaction analysis, where you inspect the \u0026ldquo;history\u0026rdquo; of the filled fields to see if users are bouncing around the form and not following the chronology or layout that you have established.\nIf none of this resonates with you, at least you\u0026rsquo;ll have some cool new tricks up your sleeve with the transport : beacon field as well as the beforeunload browser event!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/access-array-members-in-the-data-layer/",
	"title": "#GTMTips: Access Array Members In The Data Layer",
	"tags": ["Google Tag Manager", "gtmtips"],
	"description": "How to access individual array indices or members in Google Tag Manager Data Layer variable model.",
	"content": "In JavaScript, if you want to access an Array member, you use square bracket notation to retrieve the value stored at a specific index. Indices are numbered in order, with the first index always being at location zero (0). This means that to get the first value stored in Array simo, you\u0026rsquo;d use something like:\nvar firstValue = simo[0];  In Google Tag Manager, you can push Arrays into the Data Layer. However, the Data Layer Variable type, which you use to retrieve values stored in the Data Layer, does not support the aforementioned square bracket notation in its fields. Instead, you need to use a special, proprietary format to access Array members.\nTip 28: Use dot notation to access Array members in the Data Layer   As usual, this is a simple tip. Just substitute the square brackets you\u0026rsquo;d use in vanilla JavaScript with dots. So classic JavaScript Array[0].name becomes Array.0.name in Google Tag Manager.\nDo note that this also means that you can\u0026rsquo;t use square bracket notation to access object literal properties either. You need to use dot notation all the way through.\nA practical example is in Ecommerce tracking. To get the SKU of the first product in an ecommerce.purchase object (Enhanced Ecommerce), you\u0026rsquo;d use this in a Data Layer Variable:\necommerce.purchase.products.0.id\nThat retrieves the value stored in the id key of the first product in the products Array of the Enhanced Ecommerce purchase object.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/implement-referral-exclusions-via-gtm/",
	"title": "#GTMTips: Implement Referral Exclusions Via GTM",
	"tags": ["google analytics", "Google Tag Manager", "gtmtips"],
	"description": "Manage and update your referral exclusion list using Google Tag Manager. This is much more flexible than using the Google Analytics user interface.",
	"content": "Maintaining the list of Referral Exclusions in Google Analytics admin is a pain. Especially if you have a webstore, the number of referral sources you need to exclude to avoid sessions being split can grow really fast. Also, it\u0026rsquo;s not like the list is has the most intuitive UI. Instead of a handy text area where you could just copy-paste stuff, you\u0026rsquo;re left with a horrible line-by-line list, and there\u0026rsquo;s no way of copying lists across properties or anything useful like that.\nSo, in this tip, I\u0026rsquo;ll show you how you can modify the data at its source (the website), so that it becomes easier to manage the list of referrals you want to exclude.\nTip 27: Exclude referrals with a Custom JavaScript Variable   First, let\u0026rsquo;s get one thing straight. The referral exclude list excludes referral traffic, it doesn\u0026rsquo;t block it. \u0026lsquo;Exclude\u0026rsquo; here means that traffic that comes in from a referral you\u0026rsquo;ve excluded gets converted into the Direct / (none) source/medium bucket. If you know your Google Analytics, you know that GA attributes all hits to the last non-direct acquisition source. In plain English this means that all hits that have no referral information get attributed to whatever non-direct source you previously had active. If you don\u0026rsquo;t have a previous campaign, or if the Campaign Timeout setting has expired, then it gets attributed to Direct / (none).\nThat means also that Referral Exclusion Lists should not be used to fight referral spam. So many people have recommended this method, and all these people are thus guilty of giving horrible, horrible advice.\nNow that we\u0026rsquo;ve got that cleared, let\u0026rsquo;s get on with the solution. For this to work, you need to create a new Custom JavaScript Variable. Give it a descriptive name such as {{JS - Exclude Referrals}}. Copy-paste the following code within:\nfunction() { var referrals = [ \u0026#39;referrer1.com\u0026#39;, \u0026#39;referrer2.com\u0026#39;, \u0026#39;referrer3.com\u0026#39;, \u0026#39;referrer4.com\u0026#39; ]; var hname = new RegExp(\u0026#39;https?://([^/:]+)\u0026#39;).exec({{Referrer}}); if (hname) { for (var i = referrals.length; i--;) { if (new RegExp(referrals[i] + \u0026#39;$\u0026#39;).test(hname[1])) { return null; } } } return {{Referrer}}; }  The only thing you need to edit is the referrals Array. Each referral source you want to exclude is on its own row, enclosed in single quotes, and all lines end with a comma except the last one. That\u0026rsquo;s the syntax.\nThe list gets turned into a regular expression, against which the referrer of the page (i.e. the URL of the page that brought the visitor to the current page) is tested. The regular expression is open on the left, and closed on the right, meaning that an entry such as simoahava.co will match the following:\n  simoahava.co\n  www.simoahava.co\n  super.genious.simoahava.co\n  But not the following:\n  simoahava.com\n  www.simoahava.co.uk\n  www.google.fi\n  I hope you get the drift. If you\u0026rsquo;re handy with regular expressions, feel free to modify the strings in the referrals Array. For example, to only exclude simoahava.com but not, for example store.simoahava.com, you\u0026rsquo;d create the entry like:\nvar referrals = [ \u0026#39;^simoahava.com\u0026#39; ];  The $ which closes the expression at the end is added automatically to each line later in the script.\nSo, the script tests each entry you\u0026rsquo;ve added into the referrals Array against the hostname of the current referrer, and if there is a match, null is returned. This means that any GA Tag that uses this Variable will not send the Document Referrer key with the payload, which is what GA uses to establish referral traffic.\nTo add this to your Google Analytics Tag, browse to More Settings -\u0026gt; Fields to Set, and add the following details:\nField Name: referrer\nValue: {{JS - Exclude Referrals}}\n  Like so.\nNote! Try this at your own risk. Remember to test the solution carefully before going ahead with a full-scale implementation. Referrer information is crucial in Google Analytics, as it can make or break your traffic attribution reports.\n"
},
{
	"uri": "https://www.simoahava.com/tags/folders/",
	"title": "folders",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/folders-and-syntax-highlighting-in-google-tag-manager/",
	"title": "Folders And Syntax Highlighting In Google Tag Manager",
	"tags": ["folders", "Google Tag Manager", "release", "syntax highlight"],
	"description": "Introducing the new Folders feature in Google Tag Manager, and also showing how syntax highlighting works in the code editor.",
	"content": "What a nice way to wake up to a new day, when brand-spanking new features have been released for Google Tag Manager.\nThe two features I want to introduce here are Folders in the UI, and code syntax highlighting in Custom HTML Tags and Custom JavaScript Variables.\nFolders   Folders is one of those features that has been requested for over and over again since day one. The UI clutter in GTM is a serious problem, especially when dealing with dozens and dozens of items in a single view. With folders, you can logically categorize individual items, so it should dramatically reduce clutter. That is, once the feature is more refined.\n  To create a new folder, browse to the new navigation item Folders in the left-hand panel, after which you\u0026rsquo;ll be treated to a new view. In this view, it is possible to create a New Folder, and select multiple items to Move into this folder.\nHere\u0026rsquo;s the thing. For now, this is just a nice UI feature. In essence, it\u0026rsquo;s a new sortable column, nothing more. It\u0026rsquo;s a huge step in the right direction, but it requires a lot more refinement:\n  Ability to have one item in multiple folders\n  Restrict user access to only certain folders, preferably on all user levels, but at least so that they can only View the contents\n  Selective publish only folder X, or leave folder Y out of publish workflow\n  Only show items from folder X in the view (Tags / Triggers / Variables)\n  Include folders in the quick search on top of the left-hand panel\n  Only after refinements like these can we truly celebrate. Just kidding, we can do a little GTM jiggle right now.\nCode syntax highlighting This got me way more excited. I and many others have been using the excellent Code Editor for GTM, created by the awesome people at fifty-five.\nNo matter how great that extension is, it\u0026rsquo;s weird that GTM didn\u0026rsquo;t have syntax highlighting enabled natively. Well, now it does!\n  It looks great, and has features you\u0026rsquo;d expect from a syntax highlighter, such as automatic indentation. It does lack some stuff, like highlighting both brackets around a code block when the cursor passes over one of them, but I\u0026rsquo;m sure these are refinements that can be added to the tool later.\nBoth of these features are very welcome to Google Tag Manager, so good job devs, again! Now let\u0026rsquo;s make them even better, right?\n"
},
{
	"uri": "https://www.simoahava.com/tags/release/",
	"title": "release",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/syntax-highlight/",
	"title": "syntax highlight",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/measure-cart-value-in-enhanced-ecommerce/",
	"title": "Measure Cart Value In Enhanced Ecommerce",
	"tags": ["enhanced ecommerce", "google analytics", "Google Tag Manager"],
	"description": "How to collect and measure shopping cart value in your Google Analytics Enhanced Ecommerce funnel. Use Google Tag Manager to setup the measurement.",
	"content": "One of the glaring omissions in the Enhanced Ecommerce reports of Universal Analytics is the ability to calculate cart value for products. Cart value, here, is the value that has been added to the cart.\nThis value can be used to query for products that have the highest discrepancy between cart value and generated revenue. These are missed opportunities of the highest caliber.\nWith some Custom Metrics magic, we can, however, get cart value into our reports, and we can find our most and least \u0026ldquo;effective\u0026rdquo; products with just a glance:\n  As you can see, my most revenue-generating product, The Ahava Machine 2.0, also has the most value left in the shopping cart. This is more interesting to me than comparing the buy-to-detail and cart-to-detail ratios, as now I actually have a currency attached to the gap between purchases and cart interactions.\nIt\u0026rsquo;s very easy to setup. We will, of course, be using Google Tag Manager, here, but with a little tinkering you should be able to set it up with the on-page tracking code as well, if you are archaic enough to still use that. I\u0026rsquo;m assuming here that you already have Enhanced Ecommerce setup through Google Tag Manager, and you have a valid ecommerce.add (and ecommerce.remove) object in dataLayer or returned with a Custom JavaScript Variable.\nTo get things rolling, you need to create the Custom Metric itself. A product-scoped Custom Metric will be scoped to a single product in a single Enhanced Ecommerce interaction. So, if you send a product-scoped Custom Metric with a product object in the ecommerce.add.products Array, the metric can only be queried against that particular product. The metric itself will only apply to the Add To Cart interaction, but you can of course combine valid metrics for the same product as I have done in the screenshot above.\nCreate the Custom Metric In the Property settings in your Google Analytics Admin, open Custom Definitions \u0026gt; Custom Metrics, and click + New Custom Metric.\n  In the screen that opens, give the Custom Metric a name (I chose Cart Value), select Product as the scope and Currency (Decimal) as the type.\n  Remember to make sure the Metric has Active checked, and then click Save.\nYou should be back in the Custom Metrics screen. Make note of the Index the Custom Metric receives. You\u0026rsquo;ll need this in a short while.\n  Next we\u0026rsquo;ll head on over to Google Tag Manager, where we\u0026rsquo;ll modify the ecommerce.add object to include the new Custom Metric with the value that was added to cart.\nSend the Custom Metric with the product object Let\u0026rsquo;s assume that your ecommerce.add object looks like this in dataLayer:\ndataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;addToCart\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;42\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;The Ahava Machine 2.0\u0026#39;, \u0026#39;brand\u0026#39; : \u0026#39;AHAVA\u0026#39;, \u0026#39;price\u0026#39; : \u0026#39;3999.90\u0026#39;, \u0026#39;quantity\u0026#39; : \u0026#39;2\u0026#39;, \u0026#39;variant\u0026#39; : \u0026#39;Awesome|Kick-ass\u0026#39; }] } } });  So it\u0026rsquo;s a fairly basic affair. With an object like this, you\u0026rsquo;ll be able to track adds-to-cart no problem through Enhanced Ecommerce.\nBut if we want to have the product-scoped Custom Metric there, we\u0026rsquo;ll need to add metricX into the product object in the products Array like this:\n... \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;42\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;The Ahava Machine 2.0\u0026#39;, \u0026#39;brand\u0026#39; : \u0026#39;Ahava\u0026#39;, \u0026#39;price\u0026#39; : \u0026#39;3999.90\u0026#39;, \u0026#39;quantity\u0026#39; : \u0026#39;2\u0026#39;, \u0026#39;variant\u0026#39; : \u0026#39;Awesome|Kick-ass\u0026#39;, \u0026#39;metric1\u0026#39; : \u0026#39;7999.80\u0026#39; }] ...  That would solve it. You\u0026rsquo;d push the total value of the cart addition for that particular product into the Custom Metric at index 1 (which we got from previous chapter), after which you\u0026rsquo;ll accumulate cart value for that particular product.\nOn top of that, you\u0026rsquo;d need to push a negative value for cart removals, right? So the ecommerce.remove object for when you want to remove one of the Ahava Machines (why would you, seriously?) looks like this:\ndataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;removeFromCart\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;remove\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;42\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;The Ahava Machine 2.0\u0026#39;, \u0026#39;price\u0026#39; : \u0026#39;3999.90\u0026#39;, \u0026#39;quantity\u0026#39; : \u0026#39;1\u0026#39;, \u0026#39;metric1\u0026#39; : \u0026#39;-3999.90\u0026#39; }] } } });  That way you\u0026rsquo;ll have the correct balance in your Cart Value metric at all times.\nBut this sucks! It sucks because metric1 is purely for Google Analytics. It\u0026rsquo;s not generic! dataLayer should be as generic as possible.\nSo let\u0026rsquo;s do what I did in my previous article about modifying the ecommerce object using a Custom JavaScript Variable. We\u0026rsquo;ll add the Custom Metric dynamically using a Custom JavaScript Variable.\nHow cool is that? I know, very cool.\nModify the \u0026lsquo;ecommerce\u0026rsquo; object in the Data Layer First, you\u0026rsquo;ll need to create a Data Layer Variable for the ecommerce object, just as I did in the article linked to a paragraph or two ago. It looks like this:\n  This will return the most recent ecommerce object stored in the Data Layer. Since we\u0026rsquo;re working with the Add To Cart or Remove From Cart events, it will return the ecommerce.add or ecommerce.remove object, respectively. This makes sense, right?\nNext, we\u0026rsquo;ll need a Custom JavaScript Variable, which adds the Custom Metric into the payload, and returns the modified ecommerce object to your Tag. You can name it whatever you want, e.g. {{JS - Modified EEC Cart Object}}. The Custom JavaScript would look like this:\nfunction() { try { var index = \u0026#39;1\u0026#39;; // Change to reflect the Custom Metric Index  var ecom = {{DLV - ecommerce}}; var i, len, prefix, obj; if (\u0026#39;add\u0026#39; in ecom) { prefix = \u0026#39;\u0026#39;; obj = \u0026#39;add\u0026#39;; } else if (\u0026#39;remove\u0026#39; in ecom) { prefix = \u0026#39;-\u0026#39;; obj = \u0026#39;remove\u0026#39;; } if (typeof prefix != \u0026#39;undefined\u0026#39;) { for (i = 0, len = ecom[obj][\u0026#39;products\u0026#39;].length; i \u0026lt; len; i += 1) { ecom[obj][\u0026#39;products\u0026#39;][i][\u0026#39;metric\u0026#39; + index] = prefix + (ecom[obj][\u0026#39;products\u0026#39;][i][\u0026#39;price\u0026#39;] * ecom[obj][\u0026#39;products\u0026#39;][i][\u0026#39;quantity\u0026#39;]); } } return {\u0026#39;ecommerce\u0026#39; : ecom}; } catch(e) { return {\u0026#39;ecommerce\u0026#39; : {{DLV - ecommerce}}}; } }  This piece of JavaScript takes the existing ecommerce.add or ecommerce.remove object, and cycles through all the products stored within. For each product, it calculates the product\u0026rsquo;s price multiplied by its quantity. This gives the total value for that product in the cart interaction. This value is then stored in the metricX, and X is defined in the beginning using the index variable. The value is positive if it was an Add To Cart action, and negative if it was a Remove From Cart action.\nThe Variable finally returns the modified ecommerce object, which is then used by your Tag as instructed in this article. If you can\u0026rsquo;t bother to read through the linked article, you will need to change your Tags\u0026rsquo; Enhanced Ecommerce settings to not use the Data Layer, but rather retrieve the data from the Custom JavaScript Variable you just created.\n  Summary The core of this article is really simple. Let\u0026rsquo;s use a Custom Metric to fix a flaw in Enhanced Ecommerce reports of Universal Analytics. The flaw is that Cart Value isn\u0026rsquo;t automatically calculated, as it would be very simple to do so with the existing dimensions and metrics.\nCart Value lets us see how much value is left into the shopping cart when people are browsing on your site. When someone adds a product to the cart, the value added is automatically saved into the Custom Metric for that particular product in the Add To Cart action. When products are removed from the cart, their value is deducted from the Custom Metric.\nUsing the product-scoped Custom Metric lets you then query the Custom Metric with the Product SKU or Name, for example, and you can combine total Product Revenue in the custom report as well, as in the screenshot at the very beginning of this article.\nThe Custom JavaScript Variable method lets you add a lot of platform-specific flexibility to your Enhanced Ecommerce measurement. Let\u0026rsquo;s just hope this is one of those articles that becomes obsolete soon, as Google Analytics hopefully introduces Cart Value as a default metric in the Enhanced Ecommerce reports!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/enhanced-ecommerce-with-a-custom-javascript-variable/",
	"title": "Enhanced Ecommerce With A Custom JavaScript Variable",
	"tags": ["enhanced ecommerce", "Google Tag Manager", "Guide", "universal analytics"],
	"description": "Measure Enhanced Ecommerce with a Custom JavaScript variable in Google Tag Manager, rather than directly via the dataLayer. This improves the flexibility of the setup.",
	"content": "Enhanced Ecommerce is a very nice improvement to the pretty lame, transaction-based Ecommerce tracking in Universal Analytics. Instead of staring blindly at what happens on a receipt page, Enhanced Ecommerce expands your entire webstore into one large funnel labelled \u0026ldquo;Shopping Behavior\u0026rdquo;, and you\u0026rsquo;re able to zoom in on the Checkout funnel as well. Also, the addition of product-scoped tracking is incredibly useful, and it\u0026rsquo;s enabled us to think of any asset (our content, for example) on our site as something we could track through the Enhanced Ecommerce reports.\n  Tracking Enhanced Ecommerce through Google Tag Manager is pretty straight-forward. It boils down to a properly formatted Data Layer, which GTM will then use to send the hits to Google Analytics.\nNow, the thing with the Data Layer is that you want to try to make it as platform-agnostic as possible. I mean, it doesn\u0026rsquo;t make sense to encode objects in the Data Layer for one platform\u0026rsquo;s needs alone, as a complex object syntax will make it quite difficult to reuse the same information for platforms which might have a completely different syntax.\nEnhanced Ecommerce requires a very specific syntax in the Data Layer object. In addition to this, there are some very ill-designed, Google Analytics -specific, customizations you will need to implement if you want to utilize the Data Layer for Enhanced Ecommerce. An example of these customizations would be adding product-scoped Custom Dimensions and/or Metrics to the payload.\nProduct-scoped Custom Definitions the GTM way Here\u0026rsquo;s an example. Let\u0026rsquo;s send a product-scoped Custom Dimension with our \u0026ldquo;Add To Cart\u0026rdquo; action. The Custom Dimension will tell whether the product is a promotion or a regular item. The code you\u0026rsquo;d need in your Data Layer would look like this:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;addToCart\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;12345\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;ACME Flame Thrower\u0026#39;, \u0026#39;dimension2\u0026#39; : \u0026#39;Promotion\u0026#39; }] } } });  This hit would prepare the Data Layer for an \u0026ldquo;Add To Cart\u0026rdquo; action, where the value \u0026lsquo;Promotion\u0026rsquo; is sent to GA to Custom Dimension index 2.\nNow, this would be fine in a perfect world, where you have an incredibly agile cooperation model with your own developers and with the developers of third-party platforms you have on your site. If the Custom Dimension should change, you can just quickly ask them to push an update to the Data Layer, where dimension2 is replaced with the new index number.\nHowever, this is rarely the reality. Also, all other parts of the Data Layer object above can be easily reused in other platforms, and the key names (e.g. \u0026lsquo;id\u0026rsquo;, \u0026lsquo;name\u0026rsquo;) are self-explanatory, but \u0026lsquo;dimension2\u0026rsquo; just won\u0026rsquo;t ring a bell with platforms that do not think in terms of dimensions. So we\u0026rsquo;ve taken an agnostic Data Layer and turned it very proprietary, respecting the needs of one single platform (Google Analytics) alone.\nUse a Custom JavaScript Variable instead Turns out that you can use a Custom JavaScript Variable to push Enhanced Ecommerce payloads to Google Analytics as well:\n  We can thus use a Custom JavaScript Variable to do all the platform-specific customizations, and we can leave our Data Layer as generic as possible.\nSo, let\u0026rsquo;s assume we still have the Data Layer in its normal state, but without the \u0026lsquo;dimension2\u0026rsquo; key in it:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;addToCart\u0026#39;, \u0026#39;ecommerce\u0026#39; : { \u0026#39;add\u0026#39; : { \u0026#39;products\u0026#39; : [{ \u0026#39;id\u0026#39; : \u0026#39;12345\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;ACME Flame Thrower\u0026#39; }] } } });  We want to fire the GA Tag, which sends the Add To Cart action to Google Analytics, when this payload is pushed into Data Layer, but we want to also add the Custom Dimension to the payload without having to modify the dataLayer.push() method.\nTo make this happen, you will first need to create a new Data Layer Variable, which returns the \u0026lsquo;ecommerce\u0026rsquo; payload:\n  This Variable returns the value of the \u0026lsquo;ecommerce\u0026rsquo; key in the data model. Note, I\u0026rsquo;m using Version 1 of the Data Layer, because that\u0026rsquo;s what GTM uses for Enhanced Ecommerce. It protects your Enhanced Ecommerce objects from something called a recursive merge, which would result in each \u0026lsquo;ecommerce\u0026rsquo; payload persisting all the previous objects, e.g. \u0026lsquo;promotions\u0026rsquo; and \u0026lsquo;impressions\u0026rsquo; in each hit, resulting in a lot of extra, invalid information being sent to Google Analytics.\nOnce you have that Variable in place, you will need the actual Custom JavaScript Variable, which I\u0026rsquo;ve named {{EEC - AddToCart With Dimension}}.\nfunction() { var dIndx = \u0026#39;2\u0026#39;; // This is the Custom Dimension Index  // Make a shallow copy of the ecommerce object  var ecom = JSON.parse(JSON.stringify({{DLV - ecommerce}})); var ecomAddProducts = ecom[\u0026#39;add\u0026#39;][\u0026#39;products\u0026#39;]; ecomAddProducts[0][\u0026#39;dimension\u0026#39; + dIndx] = ecomAddProducts[0][\u0026#39;id\u0026#39;] === \u0026#39;12345\u0026#39; ? \u0026#39;Promotion\u0026#39; : \u0026#39;Regular\u0026#39;; return {\u0026#39;ecommerce\u0026#39; : {\u0026#39;add\u0026#39; : {\u0026#39;products\u0026#39; : ecomAddProducts}}}; }  On the first line, you define the Custom Dimension index. Next, you create a little placeholder variable for the \u0026lsquo;products\u0026rsquo; key in the {'ecommerce' : {'add' : {}} } object.\nNext, you\u0026rsquo;ll make a copy of the original {{DLV - ecommerce}} variable by using the JSON.parse(JSON.stringify(obj)) trick. This is to prevent multiple iterations of the Custom JS variable from inadvertently running whatever modifications you do over and over again on the source variable, resulting in unexpected results.\nThen, you basically check if the first (only) product in the \u0026lsquo;products\u0026rsquo; Array has ID \u0026lsquo;12345\u0026rsquo;. If it does, you add the Custom Dimension key \u0026lsquo;dimension2\u0026rsquo; into the product object with the value \u0026lsquo;Promotion\u0026rsquo;. If the ID is something else, the dimension value is set to \u0026lsquo;Regular\u0026rsquo;.\nYou can be very creative here, fetching valid IDs from a Lookup Table, for example. The key thing is to return a complete \u0026lsquo;ecommerce\u0026rsquo; object in the Custom JavaScript Variable. That\u0026rsquo;s how GTM works. If you don\u0026rsquo;t return a syntactically valid \u0026lsquo;ecommerce\u0026rsquo; object, Enhanced Ecommerce will not work.\nJust to clarify: a valid \u0026lsquo;ecommerce\u0026rsquo; object means that the payload needs to be crafted according to how you would create the Data Layer object. A good place to find more information is in the developer guide for the Enhanced Ecommerce Data Layer.\nThings to keep in mind To make the Custom JavaScript Variable method work with GTM, you will need to remember the following, easy steps:\n  Use a Custom JavaScript Variable which returns a valid {'ecommerce' : {}} object\n  Uncheck Use data layer in your GA Tag settings\n  Select the variable you created in (1) in the drop-down menu labelled Read data from variable\n  That\u0026rsquo;s all you need to change. Your Tag should still fire on the Custom Event Trigger for Event Name: addToCart. This time, however, instead of using the Data Layer \u0026lsquo;ecommerce\u0026rsquo; object, it uses the \u0026lsquo;ecommerce\u0026rsquo; object returned by your Custom JavaScript Variable.\nUse cases The best way to use the Custom JavaScript Variable has already been mentioned a few times in this article. Use the Custom JS Variable to turn a generic Data Layer object into a Google Analytics -specific payload. This means that you can ask your developers to implement a very generic Ecommerce payload with the \u0026ldquo;Add To Cart\u0026rdquo; action. This makes it easier for other platforms to tap into the data, as you don\u0026rsquo;t need to know how Google Analytics works to be able to understand how the object is built.\nAnother good use case is for product-specific Custom Dimensions and Metrics, which I\u0026rsquo;ve showed an example of in this article. It doesn\u0026rsquo;t really make sense to add those to the Data Layer, as dimension and metrics can change, and adding keys that are only used by one platform is a bit counter-intuitive.\nI\u0026rsquo;ve found myself using the Custom JavaScript Variable method in almost all the implementations I work with, even if the Data Layer \u0026lsquo;ecommerce\u0026rsquo; object is well-formed to begin with. I like the idea that I can flexibly change the payload on a whim, without having to update the Data Layer.\nNaturally, if the changes are fundamental, such as product names changing or IDs receiving a new format, you will want to update the on-page Data Layer instead of encoding these changes in your Custom JavaScript Variable. But the Custom JavaScript Variable is perfect for implementing platform-specific payload syntax in your Ecommerce tracking.\nHave you got some other use cases for the Custom JavaScript Variable method? Please, do share!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/cross-domain-tracking-across-subdomains/",
	"title": "Cross-domain Tracking Across Subdomains",
	"tags": [],
	"description": "Quick tip how cross-domain tracking to Google Analytics works across subdomains.",
	"content": "Wait. What? Why write an article about something that should work by default in Universal Analytics? I mean, here\u0026rsquo;s a screenshot from the guide I just linked to in the previous sentence:\n  There it is. Clear as day: \u0026ldquo;Tracking users across subdomains does not require any additional configuration.\u0026rdquo; Also, some of the recent, excellent guides to cross-domain tracking, written by E-Nor and Bounteous enforce the same: you just need a default Universal Analytics Tag in Google Tag Manager.\nWell, unfortunately, that\u0026rsquo;s not true. You do need a configuration setting, but it\u0026rsquo;s thankfully very simple.\nTL;DR: In the Fields to Set of your Universal Analytics (Page View) Tag, add the following setting:\n  But since you\u0026rsquo;re awesome, you didn\u0026rsquo;t come here for the dirty truth, you want to know why this is necessary. So read on!\nHow Universal Analytics writes cookies To track the same user and session across any two pages, these pages require the following:\n  A tracker object that tracks to the same Google Analytics Property ID (UA-XXXXXX-Y)\n  A _ga cookie that has the same Client ID\n    Client ID is what is used to stitch the hits in the session together, and also to stitch sessions the same user has had into a common level of aggregation (the GA concept of a user).\nNow, if you don\u0026rsquo;t have any customizations in your Universal Analytics Tag, here\u0026rsquo;s the logic with which cookies are written:\n  If the hostname of the page (http://this is the hostname/home/) starts with www., the \u0026ldquo;www\u0026rdquo; is stripped, and the cookie is written on what remains. So, in the case of www.simoahava.com, the cookie is written on .simoahava.com.\n  On all other hostnames, the cookie is written on the hostname itself, prefixed with a period. So, test.simoahava.com would retrieve a _ga cookie written on .test.simoahava.com\n  If prefixed with a period, like the _ga cookie is, the cookie can be used on all hostnames that contain this string. So the .simoahava.com cookie can be used by test.simoahava.com, www.simoahava.com, simoahava.com, and immortal.genius.simoahava.com, for example.\nThe .test.simoahava.com cookie, on the other hand, can be used by www.test.simoahava.com, but not by www.simoahava.com, as the latter does not contain the string \u0026ldquo;test.simoahava.com\u0026rdquo;.\nCan you see where I\u0026rsquo;m going with this?\nIf traffic is from www.simoahava.com to test.simoahava.com, the _ga cookie is shared, and all is well. Right?\nHowever, if the visitor first visits test.simoahava.com, and then moves to www.simoahava.com, these two domains will have different _ga cookies, and thus different Client IDs, and thus the user will be a different user with a new session!\nSo clearly, in many, many cases, this will lead to problems. We need to somehow ensure that the _ga cookie is always written on .simoahava.com, so that it can be used by all subdomains, regardless of if they have \u0026ldquo;www.\u0026rdquo; or something else as a prefix.\ncookieDomain : auto The answer is in the cookieDomain setting. When you set cookieDomain to auto, the following will happen with a (fictional) domain like www.simoahava.co.uk:\n  GA tries to write the cookie on .co.uk, which is the first possible root domain candidate. This fails because the browser is not authorized to write a cookie on a top-level domain like that.\n  Next, GA tries to write the cookie on .simoahava.co.uk, which is the next possible root domain candidate. This works because that\u0026rsquo;s a valid domain to write the cookie on.\n  So there\u0026rsquo;s a recursive algorithm, which tries to write the cookie, starting from the most generic domain-level (the top-level domain), and stopping once it succeeds. What should be left is the root domain, and thus the cookie will be available to all subdomains.\nYay!\nThe new and improved recommendation Here\u0026rsquo;s the improved recommendation for cross-domain tracking across subdomains:\nAlways default to having cookieDomain : auto in your tracker settings. In GTM, I\u0026rsquo;ve shown an example in the beginning of the post.\nIn the Universal Analytics tracking code, the snippet would look like this:\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;//www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-40669554-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; This is, actually, what the Google Analytics Tracking Code, if copy-pasted from the Property settings, offers by default, so all is well if you use this.\nPersonally, I think the setting should always default to \u0026ldquo;auto\u0026rdquo; unless explicitly changed to some other value.\nHere\u0026rsquo;s a thread in Google+ which inspired me to write this post.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/remember-to-flush-unused-data-layer-variables/",
	"title": "#GTMTips: Remember To Flush Unused Data Layer Variables",
	"tags": ["data layer", "Google Tag Manager", "gtmtips"],
	"description": "Remember to set unused variables in generic dataLayer pushes to undefined when they are not used. This way the values will not &#34;leak&#34; from hit to hit.",
	"content": "Here\u0026rsquo;s a tip that\u0026rsquo;s especially important to anyone working with a single-page application. Google Tag Manager persists items in its data model until you either manually delete the variable and/or its value from the data model, or until the user browses away from the page. There\u0026rsquo;s nothing as annoying as the example in the image below, where a value that was set for an earlier Tag is resent with a new Tag, even though the purpose was to leave it out.\nTip 26: Flush unused Data Layer Variables   Let\u0026rsquo;s extend the example in the image above. Say you have a Google Analytics Event Tag, which fires on the makeMoney Custom Event Trigger, and has the following Tag settings:\nEvent Category: Make Money\nEvent Action: {{DLV - userId}}\nEvent Label: {{DLV - criminalStatus}}\nEvent Value: {{DLV - howMuch}}\nAs you can see, the naming convention I use for the Variables is pretty self-explanatory. It\u0026rsquo;s just the variable name in the Data Layer prefixed by \u0026ldquo;DLV - \u0026ldquo;.\nSo, this Tag fires when the first payload is pushed. It gets the following values:\nEvent Category: Make Money\nEvent Action: abcb-1234\nEvent Label: true\nEvent Value: 10000\nThis Event signifies that a user with ID \u0026ldquo;abcb-1234\u0026rdquo; has scored 10000 dollars in a nefarious scheme, and this event is thus labelled as a criminal act. I will send this information to the proper authorities, after first taking my cut.\nNext, the same person, on the same page, decides to make amends and scores a far more appropriate amount of cash in this cyber-scam:\nEvent Category: Make Money\nEvent Action: ddff-2211\nEvent Label: true\nEvent Value: 2000\nBut what\u0026rsquo;s that? Criminal status is still true?! But you left it out of the dataLayer.push(), why would it still be there? Shouldn\u0026rsquo;t it be empty? Is there no way for this vigilante to escape the searchlights of the police helicopter patrolling the abandoned brick factory?\nYou see, because there was no reload, the value of the criminalStatus variable in GTM\u0026rsquo;s data model remains true until overwritten, or until a page (re)load is executed.\nThis is why the following bit is important. Whenever you push a payload to dataLayer that makes a Tag Trigger, make sure you\u0026rsquo;ve accounted for any other Data Layer Variables the Tag might use. In the example we\u0026rsquo;ve used until now, if you want to make sure that criminalStatus is flushed for the second push, you would use the following syntax in the push:\ndataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;makeMoney\u0026#39;, \u0026#39;howMuch\u0026#39; : \u0026#39;2000\u0026#39;, \u0026#39;userId\u0026#39; : \u0026#39;ddff-2211\u0026#39;, \u0026#39;criminalStatus\u0026#39; : undefined });  This stores the value undefined for the variable criminalStatus, effectively making it so that the variable does not resolve, dropping the parameter from the Google Analytics hit entirely.\nSo remember to flush those unused variables!\nOne other thing. You might be concerned that when using Enhanced Ecommerce, this means that your impressions (which you send with the Page View Tag, right?) will be sent with every single other Enhanced Ecommerce hit on the page as well. Worry not! Enhanced Ecommerce is special. It leverages an older version of GTM\u0026rsquo;s data model, where objects are not merged together. With Enhanced Ecommerce, only the most recent hit is ever processed by your Tags.\nThat\u0026rsquo;s it. Go enjoy the summer!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/remove-email-addresses-from-url-parameters/",
	"title": "#GTMTips: Remove Email Addresses From URL Parameters",
	"tags": ["Google Tag Manager", "gtmtips", "pii", "query string"],
	"description": "How to purge email addresses from URL parameters when sending hits to Google Analytics. Use Google Tag Manager to fix this PII issue.",
	"content": "PII (Personally Identifiable Information) is something we need to actively combat against when using Google Analytics, as the platform explicitly forbids sending PII to Google Analytics properties in any size, form, or shape.\nOne of the most common ways of accidentally passing PII to a property is via query parameters. Many email platforms out there, for example, see no problem in including the user\u0026rsquo;s email address in the query string, especially when the user follows a link in a newsletter. This is, however, a definite no-no in Google Analytics. Thus, I wanted to create a blanket solution for proactively weeding out potential PII in your Google Analytics Tags deployed via Google Tag Manager.\nTip 25: Remove email addresses from URL parameters   For this solution to work, you\u0026rsquo;ll need to create a new user-defined variable in GTM, which returns the URL Query string. In this example, the variable is called {{URL Query}}, and it looks like this:\n  After this, you need to create the Custom JavaScript Variable itself. Let\u0026rsquo;s name it {{Return URL Query without email}}, and it looks like this:\nfunction() { var q = {{URL Query}}.length ? \u0026#39;\u0026amp;\u0026#39; + {{URL Query}} : \u0026#39;\u0026#39;; // q = decodeURIComponent(q);  var newQ = q.length ? \u0026#39;?\u0026#39; + q.replace(/\u0026amp;[^\u0026amp;@]+@[^\u0026amp;]+/g, \u0026#39;\u0026#39;).substring(1) : \u0026#39;\u0026#39;; return newQ.length \u0026lt;= 1 ? \u0026#39;\u0026#39; : newQ; }  Thanks to Phil Pearce and David Vallejo for pointing out some errors in the original script. Also thanks to Steven J in the comments of this post for suggesting to check if the new query string just has one character (\u0026rsquo;?').\nUncomment the commented line if your URL Query strings tend to have HTML encoded characters (e.g. %3D for \u0026lsquo;=\u0026rsquo;, and %26 for \u0026lsquo;\u0026amp;').\nThe JavaScript function above is a very simple regular expression search-and-replace, which looks for an email address in the URL query string. If it finds one (or more), it simply removes the offending key-value pair(s) from the query string, and returns the stripped result.\nTo implement this in your Tags, you\u0026rsquo;ll need to add the following to every single Google Analytics Tag in Fields to Set.\nField name: page\nValue: {{Page Path}}{{Return URL Query without email}}\nIt\u0026rsquo;s not perfect, it\u0026rsquo;s a bit cumbersome as you need to implement it in all Tags, but especially with large websites that invite a lot of traffic, it might save you from data loss due to Terms of Service infringement.\n"
},
{
	"uri": "https://www.simoahava.com/tags/query-string/",
	"title": "query string",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/custom/",
	"title": "custom",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-soundcloud-integration/",
	"title": "Google Tag Manager: SoundCloud Integration",
	"tags": ["custom", "google analytics", "Google Tag Manager", "Guide", "soundcloud"],
	"description": "Track interactions with your embedded SoundCloud widget using Google Tag Manager. You can send the data to Google Analytics, for example.",
	"content": "According to their website, SoundCloud is \u0026ldquo;the world’s leading social sound platform where anyone can create sounds and share them everywhere\u0026rdquo;. For artists, it\u0026rsquo;s a channel for distributing previews of their tracks, and for people like me it\u0026rsquo;s a nice way to do some API tinkering. To each their own, I guess!\n  I saw a number of requests in the Google+ Google Tag Manager community about a SoundCloud integration, so I decided to look into it to see if I could just build one.\nSoundCloud has something called the Widget API, which listens for window.postMessage calls from within the embedded SoundCloud iframes. The benefit of this versus, for example, the YouTube API is that you don\u0026rsquo;t need to do anything to the iframe itself to make this work. All you need to do is load the Widget API, and then indicate which iframe(s) you want to listen to for interactions.\nThe setup For this to work, you will need the following:\n  Custom HTML Tag which loads the Widget API, adds the listeners, and does the dataLayer.push() calls\n  Custom Event Trigger which fires your Tag when a SoundCloud event is registered\n  Data Layer Variables for Event Category, Event Action, and Event Label\n  Event Tag which fires when the Trigger is activated, and sends the event hit to Google Analytics\n  The most complex component is the Custom HTML Tag, so let\u0026rsquo;s start there.\nThe Custom HTML Tag Here\u0026rsquo;s the full Tag code. Copy-paste it into a new Custom HTML Tag:\n\u0026lt;!-- Load the SoundCloud API synchronously --\u0026gt; \u0026lt;script src=\u0026#34;https://w.soundcloud.com/player/api.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Initiate the API integration --\u0026gt; \u0026lt;script\u0026gt; (function() { try { // initWidget is called when a SoundCloud iframe is found on the page  var initWidget = function(w) { var currentSound, act, pos, q1, q2, q3, go, lab; var cat = \u0026#39;SoundCloud\u0026#39;; var widget = SC.Widget(w); // Events.READY is dispatched when the widget has been loaded  widget.bind(SC.Widget.Events.READY, function() { // Get the title of the currently playing sound  widget.getCurrentSound(function(cs) { lab = cs[\u0026#39;title\u0026#39;]; }); // Fire a dataLayer event when Events.PLAY is dispatched  widget.bind(SC.Widget.Events.PLAY, function() { act = \u0026#39;Play\u0026#39;; sendDl(cat, act, lab); }); // Fire a dataLayer event when Events.PAUSE is dispatched  // The only exception is when the sound ends, and the auto-pause is not reported  widget.bind(SC.Widget.Events.PAUSE, function(obj) { pos = Math.round(obj[\u0026#39;relativePosition\u0026#39;] * 100); if (pos !== 100) { act = \u0026#39;Pause\u0026#39;; sendDl(cat, act, lab); } }); // As the play progresses, send events at 25%, 50% and 75%  widget.bind(SC.Widget.Events.PLAY_PROGRESS, function(obj) { go = false; pos = Math.round(obj[\u0026#39;relativePosition\u0026#39;] * 100); if (pos === 25 \u0026amp;\u0026amp; !q1) { act = \u0026#39;25%\u0026#39;; q1 = true; go = true; } if (pos === 50 \u0026amp;\u0026amp; !q2) { act = \u0026#39;50%\u0026#39;; q2 = true; go = true; } if (pos === 75 \u0026amp;\u0026amp; !q3) { act = \u0026#39;75%\u0026#39;; q3 = true; go = true; } if (go) { sendDl(cat, act, lab); } }); // When the sound finishes, send an event at 100%  widget.bind(SC.Widget.Events.FINISH, function() { act = \u0026#39;100%\u0026#39;; q1 = q2 = q3 = false; sendDl(cat, act, lab); }); }); }; // Generic method for pushing the dataLayer event  // Use a Custom Event Trigger with \u0026#34;scEvent\u0026#34; as the event name  // Remember to create Data Layer Variables for eventCategory, eventAction, and eventLabel  var sendDl = function(cat, act, lab) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;scEvent\u0026#39;, \u0026#39;eventCategory\u0026#39; : cat, \u0026#39;eventAction\u0026#39; : act, \u0026#39;eventLabel\u0026#39; : lab }); }; // For each SoundCloud iFrame, initiate the API integration  var i,len; var iframes = document.querySelectorAll(\u0026#39;iframe[src*=\u0026#34;api.soundcloud.com\u0026#34;]\u0026#39;); for (i = 0, len = iframes.length; i \u0026lt; len; i += 1) { initWidget(iframes[i]); } } catch(e) { console.log(\u0026#39;Error with SoundCloud API: \u0026#39; + e.message); } })(); \u0026lt;/script\u0026gt; Make sure this Custom HTML Tag fires upon a Page View Trigger, where the Trigger Type is DOM Ready. If it doesn\u0026rsquo;t work, try changing the Trigger Type to Window Loaded. It\u0026rsquo;s possible a race condition emerges, where the Custom HTML Tag is fired before your SoundCloud widgets are loaded, and shifting the Trigger to fire on Window Loaded should remedy that.\nLet\u0026rsquo;s chop it up into pieces so we\u0026rsquo;ll understand what\u0026rsquo;s happening. This time, we\u0026rsquo;ll start from the end!\n// For each SoundCloud iFrame, initiate the API integration var i,len; var iframes = document.querySelectorAll(\u0026#39;iframe[src*=\u0026#34;api.soundcloud.com\u0026#34;]\u0026#39;); for (i = 0, len = iframes.length; i \u0026lt; len; i += 1) { initWidget(iframes[i]); }  The code above goes through all the iframes on the page. If it encounters an iframe that loads an embedded SoundCloud Widget, it calls the initWidget method, using the iframe object as a parameter. This is as simple as it gets. The cool thing is that each iframe gets its own bindings, so you can run the script with multiple SoundCloud widgets on the page!\nvar initWidget = function(w) { var currentSound, act, pos, q1, q2, q3, go, lab; var cat = \u0026#39;SoundCloud\u0026#39;; var widget = SC.Widget(w); // Events.READY is dispatched when the widget has been loaded  widget.bind(SC.Widget.Events.READY, function() { // Get the title of the currently playing sound  widget.getCurrentSound(function(cs) { lab = cs[\u0026#39;title\u0026#39;]; });  The initWidget function is called for all the SoundCloud iframes on the page. First, it declares some utility variables. Next, it uses the Widget API SC.Widget constructor to create a new widget object the API uses for the bindings.\nOn the following lines, the SC.Widget.Events.READY event is bound to the widget object. This event is fired when the embedded SoundCloud object has loaded and is ready to be interacted with. All the listeners are put into this function callback, as we don\u0026rsquo;t want to start listening for events before the embedded file has loaded, right?\nThe first thing we do is get the title of the sound, and for that we need to use the asynchronous getCurrentSound function, whose callback returns the sound object. Then, we access this object\u0026rsquo;s title key and store it in a variable. Now we have all the static variables defined, and we can create our four listeners.\n// Fire a dataLayer event when Events.PLAY is dispatched widget.bind(SC.Widget.Events.PLAY, function() { act = \u0026#39;Play\u0026#39;; sendDl(cat, act, lab); });  The first listener is bound to the SC.Widget.Events.PLAY event, which, surprisingly, is dispatched when a \u0026ldquo;Play\u0026rdquo; event is recorded in the widget. Once that happens, we set the act variable to \u0026ldquo;Play\u0026rdquo;, and invoke the sendDl (see below) method, which does the dataLayer.push(). Parameters are the cat (\u0026ldquo;SoundCloud\u0026rdquo;), act (\u0026ldquo;Play\u0026rdquo;), and lab (Sound title) variables.\n// Fire a dataLayer event when Events.PAUSE is dispatched // The only exception is when the sound ends, and the auto-pause is not reported widget.bind(SC.Widget.Events.PAUSE, function(obj) { pos = Math.round(obj[\u0026#39;relativePosition\u0026#39;] * 100); if (pos !== 100) { act = \u0026#39;Pause\u0026#39;; sendDl(cat, act, lab); } });  The next event we\u0026rsquo;ll bind is SC.Widget.Events.PAUSE, which is dispatched when a \u0026ldquo;Pause\u0026rdquo; event is recorded in the widget. It\u0026rsquo;s practically the same as the \u0026ldquo;Play\u0026rdquo; event, but we need to add one extra check there. SoundCloud auto-pauses the sound when it\u0026rsquo;s completed, so GA would receive a number of \u0026ldquo;Pause\u0026rdquo; events that were not initiated by the user. That\u0026rsquo;s why we have the check on the first line of the callback, where we basically see if the \u0026ldquo;Pause\u0026rdquo; event occurred when the position of the sound is at 100%. This would indicate that it\u0026rsquo;s an auto-pause, and we won\u0026rsquo;t invoke sendDl in that case.\n// As the play progresses, send events at 25%, 50% and 75% widget.bind(SC.Widget.Events.PLAY_PROGRESS, function(obj) { go = false; pos = Math.round(obj[\u0026#39;relativePosition\u0026#39;] * 100); if (pos === 25 \u0026amp;\u0026amp; !q1) { act = \u0026#39;25%\u0026#39;; q1 = true; go = true; } if (pos === 50 \u0026amp;\u0026amp; !q2) { act = \u0026#39;50%\u0026#39;; q2 = true; go = true; } if (pos === 75 \u0026amp;\u0026amp; !q3) { act = \u0026#39;75%\u0026#39;; q3 = true; go = true; } if (go) { sendDl(cat, act, lab); } });  The next binding is for the SC.Widget.Events.PLAY_PROGRESS. This event is dispatched every few milliseconds, and the object it returns has the relative position of the sound at the time of the event. This relative position is actually a percentage of how far the user has listened to the track. So, because I\u0026rsquo;ve chosen to send an event at 25%, 50%, 75% and 100%, I need to check if the relative position is at these milestones. I also use a couple of booleans, q1 q2 q3 go, which prevent the same milestone from being sent multiple times. The go variable ensures that the GA Event is only fired when the milestones are reached, and not for every single iteration of the PLAY_PROGRESS event.\n// When the sound finishes, send an event at 100% widget.bind(SC.Widget.Events.FINISH, function() { act = \u0026#39;100%\u0026#39;; q1 = q2 = q3 = false; sendDl(cat, act, lab); });  Finally, when the sound finishes, we send the 100% event, and we also reset the utility variables. If we don\u0026rsquo;t reset them, repeated listenings would not be recorded.\n// Generic method for pushing the dataLayer event // Use a Custom Event Trigger with \u0026#34;scEvent\u0026#34; as the event name // Remember to create Data Layer Variables for eventCategory, eventAction, and eventLabel var sendDl = function(cat, act, lab) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;scEvent\u0026#39;, \u0026#39;eventCategory\u0026#39; : cat, \u0026#39;eventAction\u0026#39; : act, \u0026#39;eventLabel\u0026#39; : lab }); };  And here\u0026rsquo;s the sendDl method. It just takes the parameters, and pushes them into a dataLayer object.\nThe Trigger To activate GTM Tags when a SoundCloud event is registered, create a new Custom Event Trigger that looks like this:\n  It\u0026rsquo;s a simple one. This Trigger will fire your Tags when the sendDl method we built above is invoked.\nThe Data Layer Variables Next, make sure you have three Data Layer Variables. One for eventCategory, one for eventAction, and one for eventLabel. They\u0026rsquo;d look something like this:\n  These are pretty generic, so you might find them useful elsewhere as well.\nThe Event Tag Finally, you need an Event Tag to carry this information to Google Analytics. Set it up as you would any other Event Tag, and make sure it fires with the Trigger you created before (Event - scEvent). Then, add the three Variables you created to their respective fields:\n  And that\u0026rsquo;s it! Now your site should be ready to collect hits from SoundCloud widgets.\nSummary and caveats First, some caveats.\nEvery now and then I noticed an annoying race condition, where the widget had loaded before the GTM Custom HTML Tag had time to complete. This means that the SC.Widget.Events.READY was dispatched too early for the listener to catch it. This race condition could be fixed by having a timeout of a second or something, which then does the bindings anyway. I didn\u0026rsquo;t write it into this solution, but it should be pretty trivial to implement once you understand how the Widget API works.\nSome other things I noticed were quota errors from the API. There\u0026rsquo;s nothing you can do about these, though I believe you can subscribe to some Premium account where these errors don\u0026rsquo;t crop up. As far as I could tell, however, they had no impact on this tracking solution, and the events were sent nevertheless.\n  Anyway, this is a pretty simple solution for tracking SoundCloud widgets on your site. I\u0026rsquo;ve tried to mirror the excellent YouTube tracking guide by Cardinal Path.\nThe Widget API has a number of other interfaces you can tap into if you want to make the solution even better. Let me know if you encountered any problems, or if you have ideas how to improve this solution!\n"
},
{
	"uri": "https://www.simoahava.com/tags/soundcloud/",
	"title": "soundcloud",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/take-the-google-tag-manager-fundamentals-course/",
	"title": "#GTMTips: Take The Google Tag Manager Fundamentals Course",
	"tags": ["Google Tag Manager", "gtmtips", "Guide"],
	"description": "Take the Google Tag Manager Fundamentals online course.",
	"content": "Are you (even marginally) interested in one of the most powerful tag management systems out there? Do you want to refresh your memory on how JavaScript works in the web? Do you want to get the most out of Google Tag Manager as the go-to system for all marketing and measurement tagging on your websites?\nTake the Google Tag Manager Fundamentals Course at the Analytics Academy, then! And take it even if you have no idea what the tool is.\nTip 24: The GTM Fundamentals course at the Analytics Academy   Google\u0026rsquo;s really amped up their support as of late, with the new and improved Tag Manager Help, the refurbished Developer Guide, and the Solutions Guide for integrating Google Analytics with Google Tag Manager.\nNow they\u0026rsquo;ve come up with an entire course around Google Tag Manager, and it\u0026rsquo;s sure to be a great one! Just sign up quick, as this first course will start on June 23, 2015. As usual, there will be a community of learners to interact with, and a flashy certificate for all who duly take and pass the course. Find the course here:\nhttps://analyticsacademy.withgoogle.com/course05/preview\nIt\u0026rsquo;s so great to see the big G really amping up their outreach, and to see more and more users take up this wonderful tool. Enjoy the course!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/add-google-tag-manager-to-your-blogger-blog/",
	"title": "#GTMTips: Add Google Tag Manager To Your Blogger Blog",
	"tags": ["blogger", "container snippet", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "How to add Google Tag Manager to your Blogger blog and avoid the encoding error you normally get.",
	"content": "This is a very simple tip, but judging by the number of queries on the Product Forums, it should prove helpful.\nBlogger is a free blogging service by Google. Like WordPress, they allow you to run hosted blogs on the blogger.com domain, and they also allow you to modify the HTML source. This, of course, means that you can add the Google Tag Manager code to the HTML template, if you wish (and why wouldn\u0026rsquo;t you!).\nThere\u0026rsquo;s just a small catch.\nTip 23: Add GTM Container Snippet to the Blogger blog template   To edit the HTML template of your Blogger blog, click the Template menu item in the main navigation of your blog settings, and then choose Edit HTML in the screen that is displayed.\n  Next, copy-paste the GTM container snippet into its rightful place just after \u0026lt;body\u0026gt;.\nNow, here\u0026rsquo;s the important thing. In the container snippet, find the following string: dl=l!='dataLayer'?'\u0026amp;l='+l:'';, and change the ampersand (\u0026amp;) to its HTML encoded counterpart (\u0026amp;), so that the string looks like this: dl=l!='dataLayer'?'\u0026amp;amp;l='+l:'';.\nThis needs to be done, as the template format used in Blogger has a strict encoding schema, and unescaped or non-encoded special characters such as ampersand will cause errors in the template if not fixed.\n"
},
{
	"uri": "https://www.simoahava.com/tags/blogger/",
	"title": "blogger",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/how-to-get-google-tag-manager-help/",
	"title": "#GTMtips: How To Get Google Tag Manager Help",
	"tags": ["Google Tag Manager", "gtmtips", "support"],
	"description": "A number of ways you can get help with your Google Tag Manager issues.",
	"content": "Google Tag Manager has a learning curve. We\u0026rsquo;ve all gone through it. The developer guide as well as the new and improved help center are very useful, but they do not answer all the questions a thorough implementation project might face. There are many ways to find answers to your questions, and I thought I\u0026rsquo;d go through some of my favorite options here.\nTip 22: Get GTM Assistance   To help you in getting help with GTM, there are two things we\u0026rsquo;ll need to go over: where to look for help, and how to ask for assistance.\nWhere to find help Here are some of the best resources for learning about GTM, and for getting assistance.\nGoogle+ Google Tag Manager community is the best place to ask your questions. It has dozens of members who are active daily, and they almost compete to be the first to answer your question. Google+ is definitely the best place to get help. You might even run into some of the GTM developers in the community as well!\nThe Official Google Tag Manager Forum is a good place to get help as well, though I find it more suitable for GTM beginners. Response activity isn\u0026rsquo;t as great as on Google+, but you might get a more thorough reply. The problem is that there\u0026rsquo;s very little editing or moderation. Since the forums aren\u0026rsquo;t always that active, wrong answers might persist for a far longer time, doing a lot of damage before anyone notices. Some GTM developers are actively responding to questions here as well.\nStack Overflow is always a good place to ask any questions, as the community has strict rules that maintain quality of both questions and responses.\nGoogle Analytics Certified Partners can help you in many ways, but in this case we\u0026rsquo;re not talking about free tips and assistance anymore, but actually hiring a consultant to help you with your issues.\nOn top of these four places, Google search is your friend, my friend. Please, please, please use Google to see if someone has tackled your problem already. At the very least, you\u0026rsquo;ll find blogs such as this one or the awesome Bounteous blog, where you\u0026rsquo;ll find many articles and blog comments that should inspire you to find the help you need.\nI know I sound like a broken record, but take a look at the Google+ community. It\u0026rsquo;s the most awesome place to get GTM help from, as well as start up some inspiring discussions around best practices or advanced implementations, for example.\nHow to ask for help I hope I don\u0026rsquo;t sound smug or pretentious, but please read the following very, very carefully. I spend a lot of time answering questions, and if everyone asked their questions with the following steps in mind, I\u0026rsquo;d save countless hours of back-and-forth, trying to debug the issue with very little to work on.\n1) Always provide repro steps\nAlways, always, always describe your problem so that the person who\u0026rsquo;s helping you can follow exactly how you encountered the issue. And if it\u0026rsquo;s not a problem you\u0026rsquo;re looking for help with, but rather some conceptual question or something, always provide a clear and well-written description of the issue. Try to step into the reader\u0026rsquo;s shoes. Would you understand what the issue is if you read your own question?\n2) (Almost) always provide a test URL\nA URL where you have a live version of the GTM container with the issue will save hours from those who help you. Honestly, it\u0026rsquo;s the single best way to debug your issue. But it does require that the container is live. If you can\u0026rsquo;t publish it due to the problem, naturally it can\u0026rsquo;t be debugged on the live site. Also, if it\u0026rsquo;s a website in development, and behind a VPN or something, it\u0026rsquo;s understandable you won\u0026rsquo;t be able to share access. One excuse which I just do not understand is \u0026ldquo;it\u0026rsquo;s my client\u0026rsquo;s website, and I can\u0026rsquo;t share the URL because of privacy issues\u0026rdquo;. If it\u0026rsquo;s a public website, that\u0026rsquo;s the worst excuse ever, and you\u0026rsquo;ll have a hard time trying to get people to help you if you can\u0026rsquo;t meet them halfway.\n3) Screencast, screenshots, Preview/View/Edit access\nThese would be awesome to have when debugging. A screencast is great, but it might be difficult to figure out just what things you\u0026rsquo;ll need to include in the cast. Screenshots are a must, but nothing beats access to the container. Preview access lets you share Preview mode of the container with anyone, which is a great thing, but View or Edit access to the container is definitely the best and fastest way to get your issue sorted.\nSummary If you\u0026rsquo;ve got the dough, either contact a Google Analytics Certified Partner, or give a shout on Google+ for freelance help.\nOther than that, you\u0026rsquo;re dependent on the goodwill of others, so do your best in facilitating their debugging work to your best ability. Keep in mind that people who volunteer do so on their own time, so the more information you can give the better. Nothing\u0026rsquo;s worse than having to repeatedly ask for more information.\nNaturally, one of the most difficult things in asking for assistance is knowing what\u0026rsquo;s relevant information. But if you can share the URL, add some screenshots, and describe any errors / issues you\u0026rsquo;ve witnessed, you\u0026rsquo;re off to a great start.\nHappy hunting! It gets easier, I promise.\n"
},
{
	"uri": "https://www.simoahava.com/tags/support/",
	"title": "support",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/data-collection/",
	"title": "data collection",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/data-is-difficult/",
	"title": "Data is difficult",
	"tags": ["data collection", "google analytics", "Google Tag Manager"],
	"description": "Analytics is difficult. Data is difficult. Digital marketing is difficult. They&#39;re all supposed to be difficult.",
	"content": "Data is difficult. Growing a business is difficult. Measuring success is difficult. And you know what? They should be difficult. Otherwise we\u0026rsquo;d all be equally stupid, whereas now those of us ambitious enough to exert themselves are winning the race.\n  And it\u0026rsquo;s not just working with data that\u0026rsquo;s difficult. The whole Web is a mess! Search engine optimization consultants, for example, are trigger-happy in doling out advice about server-side redirects without stopping to consider the implications of what they\u0026rsquo;re recommending.\nBut it still seems to me that those working in web analytics give up first. The passive, phlegmatic, lazy approach to implementing a data collection platform and to the subsequent analysis can be seen in many, many organizations out there.\nIt\u0026rsquo;s because data is difficult.\nNever mind the analysis. We all know how much experience, expertise, and industry knowledge it requires to derive value-adding insight from a data set. But implementing a platform so that data quality is constantly re-evaluated and tweaked is an intimidating task for many.\nThis seems to stem from the fundamental misunderstanding that data collection is plug-and-play. No, implementing Google Analytics is not very difficult, as it\u0026rsquo;s simply a copy-paste operation out-of-the-box. With Google Tag Manager it can be even easier. But this is just the start. When first installed, all Google Analytics is good for is tracking Pageviews and little else. All the things that really fascinate us, like call-to-action interactions, eCommerce transactions, and content engagement, require additional work to implement.\nYes, data requires work. Data quality isn\u0026rsquo;t acquired, it\u0026rsquo;s earned. Tools like Google Tag Manager and Google Analytics shouldn\u0026rsquo;t be designed to make data and analysis thereof easier. No, their function is to make data more manageable, so that maximum data quality can be achieved with minimum effort. This way, any data organization can pursue the famous 90/10 rule, where 90% of resources (time/money/effort) should be focused on people and only 10% on tools.\n\u0026ldquo;Easy data\u0026rdquo; is one of the misconceptions making waves through the industry and saturating the digital space with practitioners who just refuse to give a damn. I\u0026rsquo;ve spent a lot of time talking and thinking about these misconceptions, and I\u0026rsquo;ve managed to group them under three observations, formulated here into rules.\nRule of Data Passivity The rule of data passivity can be best summed up as a rebuttal of the classic \u0026ldquo;Data Beats Opinion\u0026rdquo; quote, attributed to Jim Lecinski from Google.\nWith the Rule of Data Passivity, I maintain that data itself does nothing. It beats nothing. It tells nothing. It shows nothing. Data is not an active agent: it\u0026rsquo;s a passive medium.\nPure opinion can, and will, \u0026ldquo;beat\u0026rdquo; data, if data is of bad quality or if the interpretations are flawed. Naturally, success based on instinct alone is difficult to find, but so is success founded upon polluted data. Indeed, the most consistent, data-driven triumphs are derived at the convergence of informed decision-making and optimized data collection.\n  Data passivity also leads to dashboard blindness. When looking at a dashboard, we expect to derive insight at a glance. As if the pie charts, the tables, and the scatter plots were telling us how our business is doing and what we should do next. But they don\u0026rsquo;t!\nThe \u0026ldquo;green for good, red for bad\u0026rdquo; labels and interpretations in the dashboard have to be calculated by someone. And they need to align perfectly for each business and each objective. If we expect that a universal platform like Google Analytics will be able to tell us everything we need to know at a glance with no customization required, we\u0026rsquo;re sorely mistaken.\nIt\u0026rsquo;s because data is passive that smart analysts should, and hopefully will, be always in demand. They are the ones who take the metrics in the reports and churn them into meaningful visualizations in the dashboards. They are the ones that make sure real-time data flow is as informative as possible. But this, again, requires work. Data is difficult, remember?\nData is easy to hide behind, both in triumph and in failure. For this reason, it\u0026rsquo;s of paramount importance to ensure the quality of data at data collection, and to hire analysts who can interpret the data in a way that\u0026rsquo;s most beneficial for the business.\nPlug-and-play analytics might work for a while, if you\u0026rsquo;re lucky. But if you want to actually use data to make a difference in your business, customization is no longer optional.\nRule of Data Subjectivity The rule of data subjectivity is important to consider when talking about data quality. I have often said, in one form or another, that:\nData quality is directly proportional to how well the data collection mechanism is understood. Take Google Analytics, for example. For many, Bounce Rate represents a key metric in evaluating content engagement. But this is because of some weird marketing ploy or super-conspiracy, where Bounce Rate has been turned into a true data demon, an undeniably evil metric with the power to destroy like no other. Consider the following, however, before condemning Bounce Rate:\n  A Bounce is not a session with just a single Pageview. It\u0026rsquo;s a session with a single interaction.\n  Google Analytics does not know how much time a session lasts if the session is a Bounce.\n  A high Bounce Rate on a page with your phone number and address can be a good thing.\n  All these three things question the \u0026ldquo;evilness\u0026rdquo; of Bounce Rate. If you don\u0026rsquo;t measure call-to-action engagement on a page, you should not read anything into the Bounce Rate of sessions that only visited this page. Why? Because if you did measure the call-to-action, you would see a lower Bounce Rate, since an event hit from interacting with the call-to-action would negate the bounce.\nSimilarly, even if you don\u0026rsquo;t have anything else except the Pageview to measure on the page, you do not know by default how much time the visitor actually spends on the page during a bounced session. This is because Google Analytics calculates Time On Page as the distance in time between two Pageview hits. Session Duration, instead, is calculated as the distance in time between any two interactions. Both are unavailable for bounced sessions. The visitor might spend 18 hours on the page, drinking in every bit of information, for all you know.\nAlso, if the phone number or the address of your store is literally all someone might want to know, it makes sense that your contact page has a high Bounce Rate. That means it\u0026rsquo;s been optimized to appear in search results, and after landing on the page, the visitor finds what they were looking for immediately. In fact, it would be a negative user experience to force the visitor to browse other pages (and thus record a lower Bounce Rate).\nThese are just some examples of how the quality of Bounce Rate as a metric is directly proportional to how well its collection and processing mechanism is understood. And web analytics platforms are teeming with similar examples.\nAnother way to consider the rule of data subjectivity is to consider how data quality can shift from bad to good, depending on the vantage point and the question asked.\nA data set with nothing but Pageviews is bad data for tracking visitor engagement, since it\u0026rsquo;s missing key things like content interaction, but it is good data for measuring the relative read counts of your articles.\nLinkedIn endorsements are bad data when figuring out if someone is truly skilled at something, but they are good data when trying to identify what the general perception of someone\u0026rsquo;s abilities is.\nTwitter retweets are bad data for identifying meaningful content, but good data for measuring the viral effect of a powerful headline.\nRule of Data Scarcity The rule of data scarcity is almost philosophical in nature. Web analytics measurement is limited by technology. There are only so many things that we can track with JavaScript and HTTP requests, and there is only so much processing power that server-side algorithms can exhaust when inferring meaning out of the incoming hit stream. This is why it\u0026rsquo;s a good practice to start pulling this web analytics data out of the system at some point, so that it can be combined with other data sources.\nHowever, no matter how much you collect and combine, you will never have all the data. The rule of data scarcity thus dictates that data will always be incomplete, and an arbitrary line has to be drawn somewhere in the data collection mechanism.\nWhen can you say you have \u0026ldquo;enough data\u0026rdquo;?\nFor example, if you want to measure content engagement with Google Analytics, it\u0026rsquo;s commonplace to measure scroll tracking. This way you\u0026rsquo;ll know how many people scroll down your articles, and you can use this as a rudimentary indicator of read-through rates.\nBut what is the increment of scrolling that constitutes an event? 1%? 5%? 25%? Should you also measure the time spent on the article? What about if someone just scrolls furiously fast to the bottom of the content, perhaps looking for comments? Should you also measure mouse movement? Perhaps someone is scrolling, but actually they\u0026rsquo;re just looking for sidebar content or your contact details?\nThe questions you can ask are infinite, because the data that you can collect is (almost) infinite. You will need to draw a line somewhere, and this requires deliberation. It\u0026rsquo;s important that you ask questions, check if the data responds to these questions (positively or negatively), and then adjust the questions and reformulate your hypotheses.\nMeaningful data Ultimately, data collection boils down to a simple thing: gathering meaningful data. What meaningful means is something that must be negotiated uniquely for each business case, each project, each product, each organization, and each platform.\nA data organization, i.e. an organization that is serious about using data to power their work, is never just a data collection or a data processing or a data reporting body. No, turning metrics into meaningful actions that drive your business requires that all aspects of this process should be observed almost religiously.\nThe reason I\u0026rsquo;m highlighting data collection is because there seems to be a disconnect between how data is collected, how it is accessed, and how it is reported on. The three rules I write about above are not just about data collection, as they are very much aligned with processing and reporting as well.\nHowever, if you screw up data collection, you screw up all subsequent stages. You need to get it right from the get-go, otherwise you\u0026rsquo;ll berate yourself for the decisions you made or neglected to make along the way.\nIn the end, all I\u0026rsquo;m saying is that data is difficult. There are no such things as \u0026ldquo;power users\u0026rdquo; of a platform like Google Analytics. There are just \u0026ldquo;users\u0026rdquo; and then people who have given up or have never bothered to try.\nTools and platforms should not try to make analysis easier by dumbing things down. No, they should facilitate data collection and processing, so that 90% of resources could actually be spent on doing the analysis.\nExperience, education, and a data-driven mindset are the ingredients to successful analytics. Going beyond the default feature set of a platform; integrating, combining, and visualizing data; and tweaking the data collection mechanism to better reflect your business objectives are things you will need to pick up along the way as well.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/spam-filter-insertion-tool/",
	"title": "Spam Filter Insertion Tool",
	"tags": ["api", "google analytics", "JavaScript"],
	"description": "Introducing the spam filter insertion tool web app to help you get rid of referral spam.",
	"content": "Last weekend, I wrote a very simple web app that automatically creates a number of referral spam filters to tackle the problem that seems to have everybody all riled up.\n  For a nice recap of the situation, take a look at this post by Jeff Sauer, or this article by Mike Sullivan.\nThis isn\u0026rsquo;t an opinion piece, even though I\u0026rsquo;ve got a great number of opinions about this issue. If you want to read some discussion surrounding this, take a look at this Google+ thread.\n(UPDATE: I have taken down the demo tool. You can still download the source code below if you wish, but there are more robust ways to block GA spam than creating an individual filter for each referral.)\nThe tool is called the \u0026ldquo;Spam Filter Insertion Tool\u0026rdquo;, or SFIT for short (I dare you to start using that in your everyday lingo).\nDownload the source code The real meat of the solution is in the GitHub repo.\nYou can download the source code, install the application on your own web server, and use it for your own purposes.\nThe application has access to the following features of the Google Analytics Management API:\n  READ the list of accounts, properties, and profiles you have access to\n  INSERT or UPDATE profile filters on the account level\n  INSERT profile filter links, which attach the newly-created or updated filters to the selected profiles\n  To get it up and running, you will need to register a new project in the Google Developers Console, create a new Client ID for a web application, as well as a public API key. Remember to activate the Google Analytics API as well!\nHow it works When you click Initialize, the tool requests your authentication to do all sorts of horrible things to your GA account. Do not worry! This tool mainly only INSERTs and READs. The only exception is if you already have these spam filters installed on your GA account, but they are outdated, in which case the tool will automatically update them to their newest versions!\nOnce you\u0026rsquo;ve authenticated your account, you will be served a drop-down menu from which you can choose any GA account you have EDIT access to. Why EDIT access? Because that\u0026rsquo;s the required access level for new filter creation. You read that right! To create filters, you need EDIT access on the account level.\n  Anyhow, choose an account and the tool should shortly load with a multiple selection menu, where you can pick the profiles to which you want to link the filters.\nOnce you click the Create and apply filters, the tool does just that. First, it creates the filters on the account level, after which it links each filter to the profiles you selected.\nThe list of filters is the one maintained in this Lone Goat resource. Whenever they update the list, I update these filters. Which brings me to the\u0026hellip;\nCaveats These filters only help with referral spam. They will not help you with polluted Measurement Protocol hits, or with spam that doesn\u0026rsquo;t come in as referral traffic, or with spam that comes in as referral traffic but isn\u0026rsquo;t in the filters yet. There are many methods to combat this issue, and you might want to check the couple of links I had in the very beginning for ideas.\nAnyway, feel free to use the tool and let me know if there are issues.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/universal-analytics-plugins-explained/",
	"title": "Universal Analytics Plugins Explained",
	"tags": ["JavaScript", "plugins", "universal analytics"],
	"description": "Guide to how plugins work in Google Analytics.",
	"content": "There are many tools and methods to make Google Analytics more manageable. Google Tag Manager is probably the best known of these, and you can find many, many articles about GTM on these pages as well.\nHowever, today I want to tell you about one of the features of Universal Analytics that hasn\u0026rsquo;t, as far as I know, received too much attention. It\u0026rsquo;s funny, because at the same time almost everyone uses the feature in the shape of eCommerce, enhanced link attribution, and cross-domain tracking. I\u0026rsquo;m talking, of course, about Universal Analytics plugins.\n  Plugins are JavaScript objects, which you can load and execute using the Universal Analytics global object interface. This is accessed, by default, using the ga() function.\nPlugins allow you to do a number of useful things:\n  Decouple tracker object access from JavaScript embedded in the page HTML, allowing you to crete, execute, and maintain often used ga() commands in library files instead of directly in the page\n  Quickly access the tracker object of choice, without having to prefix every command with the tracker name, or to loop through all the existing trackers to find the correct one\n  Execute the plugin commands synchronously, which means you can be 100 % sure that anything the plugin does will be available to commands that come after it in the command queue\n  All the benefits listed above facilitate a plug-and-play integration between a platform and Google Analytics. If you\u0026rsquo;re the developer of the platform, you can create a plugin that does most of the legwork, such as assigning a new Custom Dimension with its respective value, and the user only has to load the plugin to reap the benefits.\nHow plugins work Before getting into a technical description of plugins themselves, there\u0026rsquo;s something we need to clear up about the Universal Analytics tracker object first. Here\u0026rsquo;s a typical tracking code on a typical website:\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;//www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; If you want to understand what the code above does statement by statement, this and this are good places to start.\nWhat\u0026rsquo;s important, however, is that the analytics.js library is loaded asynchronously. The loading starts when the immediately invoked function expression (IIFE) is executed, but the browser\u0026rsquo;s JavaScript engine moves right to the two ga() commands before the analytics.js library has loaded. This means that the two ga() calls are not actually interacting with analytics.js, but instead they are pushing commands into a command queue, which is processed first-in first-out as soon as the library has loaded.\n  This, in turn, has a significant implication on what you can do with the code. Consider the following example:\n\u0026lt;script\u0026gt; ...the IIFE here... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;, { \u0026#39;dimension1\u0026#39; : ga.getAll()[0].get(\u0026#39;clientId\u0026#39;) }); \u0026lt;/script\u0026gt; If you know your tracker methods, you\u0026rsquo;ll see that this attempts to send the clientId of the tracker object as a Custom Dimension with the page view hit.\nThis code will, however, result in the error: ga.getAll is not a function. The reason for this error is due to the library not having loaded yet when the JavaScript engine processes the command in the ga() call. It\u0026rsquo;s the analytics.js library that sets up the interface for the global object, not the tracking code.\nThere are two ways to overcome this. You can pass a callback function as an argument to the ga() call, and any code within that function will not be executed until the library has loaded. The second method would be to use a plugin.\nWhen you call the ga('require', 'pluginName') method, this request is added to the command queue. When the analytics.js library has finally loaded, the command queue will be processed in order. Once the processing reaches this \u0026lsquo;require\u0026rsquo; command, the queue will halt until a \u0026lsquo;provide\u0026rsquo; call with the same plugin name is found in or added to the queue. This means that the following code would work nicely:\n\u0026lt;script\u0026gt; ...the IIFE here... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-1\u0026#39;, \u0026#39;auto\u0026#39;); // Load the getClientId plugin, which gets the // clientId and sets it as dimension1 on following hits ga(\u0026#39;require\u0026#39;, \u0026#39;getClientId\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; Even though the tracker interface is not available when the \u0026lsquo;require\u0026rsquo; command is added to the queue, the interface will be available once the queue is being processed.\nHow to create a plugin Let\u0026rsquo;s create the getClientId plugin from the previous chapter. The plugin code itself consists of two required components:\n  A constructor that is executed when the plugin is loaded\n  A \u0026lsquo;provide\u0026rsquo; command that links the constructor with the plugin name\n  To create the plugin getClientId, you would need something like the following code loaded on the page:\nvar GetClientId = function(tracker) { tracker.set(\u0026#39;dimension1\u0026#39;, tracker.get(\u0026#39;clientId\u0026#39;)); }; var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;] || \u0026#39;ga\u0026#39;]; if (ga) { ga(\u0026#39;provide\u0026#39;, \u0026#39;getClientId\u0026#39;, GetClientId); }  The first lines are for the constructor. It\u0026rsquo;s just a JavaScript function, but it turns into an object constructor with the ga('require', 'getClientId') call, which creates a new object instance of GetClientId.\nIf object-oriented JavaScript confuses you, take a look at the excellent JavaScript track at codecademy.\nAnyway, the tracker object used in the call to the constructor is automatically passed as an argument to the function. This means that you can run commands on the tracker object, and you don\u0026rsquo;t have to worry about interacting with the right tracker or sending the hits to the right Google Analytics property.\nThe next lines first make sure that the ga() command exists. The ga() function is created in the Universal Analytics tracking snippet, so if your plugin code is executed before the browser parses the tracking snippet, it might not work.\nFinally, the ga('provide', 'pluginName', Constructor) command is run so that the plugin can be found with its respective \u0026lsquo;require\u0026rsquo; call.\nTo briefly recap, here is how plugins are loaded and linked to trackers:\n  The Universal Analytics tracking snippet creates the ga() function and the command queue\n  All function calls to ga() are added to this command queue\n  If you have a plugin, both its \u0026lsquo;require\u0026rsquo; and \u0026lsquo;provide\u0026rsquo; calls will be added to the queue as well\n  Once the analytics.js library has loaded, the commands in the queue are processed in order\n  If a \u0026lsquo;require\u0026rsquo; command is encountered, the queue processing halts indefinitely, until a \u0026lsquo;provide\u0026rsquo; call with the same plugin name is run\n  The entire plugin constructor function is executed before queue processing continues\n  Below is an example of the loading order in action. The plugin \u0026lsquo;require\u0026rsquo; call is between the \u0026lsquo;create\u0026rsquo; and \u0026lsquo;send\u0026rsquo; calls, and the library itself is loaded after the Universal Analytics tracking code snippet. This is to secure that the ga() function is created before the plugin.\n\u0026lt;script\u0026gt; ...the function expression here... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;require\u0026#39;, \u0026#39;getClientId\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://www.simoahava.com/scripts/getClientId.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; Note that the \u0026lsquo;require\u0026rsquo; command waits for the respective \u0026lsquo;provide\u0026rsquo; command indefinitely. This means that if you misspell the plugin name, or if you only have the \u0026lsquo;require\u0026rsquo; command there for some reason, the queue processing will not proceed. So try to avoid breaking your analytics implementation, please!\nExample: SimoPlugin library Here\u0026rsquo;s an example of a plugin I might be tempted to use over and over again. In it, I have two special features:\n  A generic event pusher, which I can use with all my trackers to send events to their respective properties\n  A hit duplicator, which duplicates the hit to the Universal Analytics collection endpoint, and sends it as an HTTP request to any custom endpoint of my choice\n  I\u0026rsquo;m storing the plugin in a file called simoPlugin.js, which I then load asynchronously after my tracking code:\n\u0026lt;script\u0026gt; ...the IIFE here... ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-1\u0026#39;, \u0026#39;auto\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;UA-1234567-2\u0026#39;, \u0026#39;auto\u0026#39;, {\u0026#39;name\u0026#39; : \u0026#39;rollup\u0026#39;}); ga(\u0026#39;require\u0026#39;, \u0026#39;simoPlugin\u0026#39;); ga(\u0026#39;rollup.require\u0026#39;, \u0026#39;simoPlugin\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); ga(\u0026#39;rollup.send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://www.simoahava.com/scripts/simoPlugin.js\u0026#34; async\u0026gt;\u0026lt;/script\u0026gt; As you can see, I have my normal tracker and a tracker with the name rollup, and I\u0026rsquo;m loading the plugin for both trackers using the typical multiple tracker syntax.\nThe way the hit duplicator works is that whenever a \u0026lsquo;send\u0026rsquo; command is used, e.g. ga('send', 'pageview'), the exact payload to the Universal Analytics endpoint is copied and sent to a (fictional) endpoint of mine at _http://process.simoahava.com/collect_:\n  By duplicating hits like this, you can create your own data collection mechanism. You can even use a cloud endpoint, which will give you a lot more processing power and scale. You might even use this to overcome the schema conspiracy I\u0026rsquo;ve been ranting about before.\nThe generic event pusher works by sending a custom event object to a plugin method called trackEvent:\nga(\u0026#39;simoPlugin:trackEvent\u0026#39;, { \u0026#39;cat\u0026#39; : \u0026#39;Link Click\u0026#39;, \u0026#39;act\u0026#39; : \u0026#39;Outbound\u0026#39;, \u0026#39;lab\u0026#39; : \u0026#39;http://www.google.com\u0026#39;, \u0026#39;di\u0026#39; : 4, \u0026#39;dv\u0026#39; : document.location.pathname });  As you can see, an object literal is passed as the second argument of the ga() call. This argument, in turn, is passed to the trackEvent method in the plugin code. The method takes the arguments, makes sure no undefined fields are sent, and pushes an event hit to the property associated with the tracker that was used to call the plugin method. If I wanted to send the call to my rollup tracker, I would have used the syntax ga('rollup.simoPlugin:trackEvent').\nsimoPlugin code The code stored in the simoPlugin.js library looks like this:\n(function() { // Assign the ga variable to the Google Analytics global function  var ga = window[window[\u0026#39;GoogleAnalyticsObject\u0026#39;] || \u0026#39;ga\u0026#39;]; // Helper function for registering the Plugin  var providePlugin = function(pluginName, pluginConstructor) { if (ga) { ga(\u0026#39;provide\u0026#39;, pluginName, pluginConstructor); } }; // Constructor for simoPlugin  // Copies payload to custom host  var SimoPlugin = function(tracker) { this.tracker = tracker; // Copy the original hit dispatch function  var originalSendHitTask = this.tracker.get(\u0026#39;sendHitTask\u0026#39;); // Modify the existing hit dispatcher to send a local copy of the hit  this.tracker.set(\u0026#39;sendHitTask\u0026#39;, function(model) { originalSendHitTask(model); // Send the original hit as usual  var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;http://process.simoahava.com/collect\u0026#39;, true); xhr.send(model.get(\u0026#39;hitPayload\u0026#39;)); }); }; // Set up a generic event dispatcher  SimoPlugin.prototype.trackEvent = function(evt) { var c = evt[\u0026#39;cat\u0026#39;]; var a = evt[\u0026#39;act\u0026#39;]; var l = evt[\u0026#39;lab\u0026#39;] || undefined; var v = evt[\u0026#39;val\u0026#39;] || undefined; var x = {}; x[\u0026#39;nonInteraction\u0026#39;] = evt[\u0026#39;ni\u0026#39;] || false; if (evt[\u0026#39;di\u0026#39;]) { x[\u0026#39;dimension\u0026#39; + evt[\u0026#39;di\u0026#39;]] = evt[\u0026#39;dv\u0026#39;] || undefined; } this.tracker.send(\u0026#39;event\u0026#39;, c, a, l, v, x); }; providePlugin(\u0026#39;simoPlugin\u0026#39;, SimoPlugin); })();  First things first: the whole thing is wrapped in an immediately invoked function expression. This is a good habit in general, since it will scope all variables within to function scope, and thus you\u0026rsquo;ll avoid polluting the global namespace. Here\u0026rsquo;s good overview of why IIFEs rock: I Love My IIFE - Greg Frank.\nThe next few lines are for making the plugin loader more generic. Since it\u0026rsquo;s possible to change the global function name in the analytics.js tracking code, we\u0026rsquo;ll need to make sure the plugin loader works even if the function name is no longer ga.\nThe providePlugin is a generic helper function which is largely redundant, but it will provide useful if you\u0026rsquo;re loading multiple plugins in the same library.\nThe constructor We create the constructor function in these lines:\nvar SimoPlugin = function(tracker) { this.tracker = tracker; // Copy the original hit dispatch function  var originalSendHitTask = this.tracker.get(\u0026#39;sendHitTask\u0026#39;); // Modify the existing hit dispatcher to send a local copy of the hit  this.tracker.set(\u0026#39;sendHitTask\u0026#39;, function(model) { originalSendHitTask(model); // Send the original hit as usual  var xhr = new XMLHttpRequest(); xhr.open(\u0026#39;POST\u0026#39;, \u0026#39;http://process.simoahava.com/collect\u0026#39;, true); xhr.send(model.get(\u0026#39;hitPayload\u0026#39;)); }); };  As you can see, the first argument to the function will be the tracker object used to load the plugin. This means that we can manipulate and use this tracker object interface without having to worry about accidentally sending hits to wrong GA properties!\nI use this.tracker = tracker; to store a reference to the tracker object into each instance of the plugin created with this constructor. This is why commands like ga('simoPlugin:trackEvent') and ga('rollup.simoPlugin:trackEvent') access a different tracker object. They both have their own, unique bindings of this, and thus they both have their own, unique tracker object references.\nThe next line accesses the sendHitTask property of the tracker object. Whenever a \u0026lsquo;send\u0026rsquo; command is used with the Universal Analytics global function, a series of tasks are executed. sendHitTask is one of these, and it is used to send the hit payload to the Universal Analytics collection endpoint.\nThe last call in the block is for updating the sendHitTask task in the tracker. This update uses the original sendHitTask (now stored in a variable called originalSendHitTask), and sends the payload to the Universal Analytics collection endpoint as before. The next three lines create a new HTTP request to the endpoint of your choice. The payload of this request is the exactly same payload that is sent to Universal Analytics.\nBy using this code, you will be able to create a perfect copy of the hit sent to Universal Analytics, and you can send this copy anywhere you want, such as a local hit processor, or a custom endpoint in a cloud server, for example.\nBecause this code is in the constructor, it is automatically applied to all \u0026lsquo;send\u0026rsquo; commands that take place on the page after the plugin has been loaded.\nThe event pusher The event pusher is defined as a method of the SimoPlugin object prototype. The reason we\u0026rsquo;re applying it to the prototype is because we want it to be available to all instances created with the SimoPlugin constructor. If we\u0026rsquo;d leave the prototype object out of the declaration, the trackEvent method would be added to an object literal called SimoPlugin, and not the object prototype. This means that the instances created from this prototype would not be able to use that method.\nAgain, if this is confusing, be sure to read up on object-oriented JavaScript. It\u0026rsquo;s interesting stuff, and it really shows what a complex and powerful programming language JavaScript is!\nThe event pusher code looked like this:\nSimoPlugin.prototype.trackEvent = function(evt) { var c = evt[\u0026#39;cat\u0026#39;]; var a = evt[\u0026#39;act\u0026#39;]; var l = evt[\u0026#39;lab\u0026#39;] || undefined; var v = evt[\u0026#39;val\u0026#39;] || undefined; var x = {}; x[\u0026#39;nonInteraction\u0026#39;] = evt[\u0026#39;ni\u0026#39;] || false; if (evt[\u0026#39;di\u0026#39;]) { x[\u0026#39;dimension\u0026#39; + evt[\u0026#39;di\u0026#39;]] = evt[\u0026#39;dv\u0026#39;] || undefined; } this.tracker.send(\u0026#39;event\u0026#39;, c, a, l, v, x); };  This code takes an object as an argument. This object is sent with the ga() command when the method is invoked:\nga(\u0026#39;simoPlugin:trackEvent\u0026#39;, { \u0026#39;cat\u0026#39; : \u0026#39;category\u0026#39;, \u0026#39;act\u0026#39; : \u0026#39;action\u0026#39;, \u0026#39;lab\u0026#39; : \u0026#39;label\u0026#39;, \u0026#39;val\u0026#39; : 1, \u0026#39;ni\u0026#39; : false, \u0026#39;di\u0026#39; : 1, \u0026#39;dv\u0026#39; : \u0026#39;dimensionValue\u0026#39; });  There are fields for Category, Action, Label, and Value, and I also let you determine whether the hit is non-interaction or not. Finally, you can also add one dimension with the index number and value exposed in the parameters.\nBecause Category and Action are required fields for events, the code will fail if these are not in the object. All the other fields can be left out, which is why I have them resolve to undefined if they don\u0026rsquo;t exist in the event object. The exception is the nonInteraction field, which defaults to false.\nUsing this generic event pusher, it\u0026rsquo;s pretty trivial to send simple AND complex events to Universal Analytics.\nThe final line, this.tracker.send(), sends the event using the tracker object\u0026rsquo;s send() method. As you can see, we\u0026rsquo;re using this.tracker, which accesses the interface of the tracker object used to call the method. The beauty of object-oriented JavaScript, right here ladies and gentlemen! If we didn\u0026rsquo;t use the power of object instances like this, we\u0026rsquo;d need to loop through all the trackers on the page to find the one we want to use. Way too complex!\nPlatform integrations Well, if the event pusher didn\u0026rsquo;t persuade you with its potential, and if you\u0026rsquo;re left unimpressed by the hit duplicator, another great idea for plugins is to integrate a SaaS platform and Google Analytics together.\nFor example, let\u0026rsquo;s say you have a platform through which you can create a new Custom Dimension in a Universal Analytics property, and then you can populate it with some value. This would happen programmatically, and the platform would use the Google Analytics Management API to create the Custom Dimension.\nOnce the dimension is created, you\u0026rsquo;d dynamically generate the JavaScript plugin library to include these programmatically created Custom Dimensions. Then you use the plugin commands to send hits to the trackers, using the correct Custom Dimensions.\n  This makes the integration between the platform and the website very plug-and-play, and you don\u0026rsquo;t need to ask the users to look up the correct dimension number, add it to the inline code, and risk some silly misunderstanding ruining the integration.\nIt\u0026rsquo;s such an elegant way of associating a website with your platform with just two lines of code at best (one for loading the library, and one for the ga('require', 'pluginName') call).\nSummary Plugins are an excellent way of associating often made calls with their respective tracker objects.\nAlso, they allow you to decouple all of the Universal Analytics commands from inline code. This allows you to write complex handlers for these commands without polluting the page template.\nPlugins are, however, quite an advanced use case for data collection. They require a very good understanding of JavaScript and of how the Universal Analytics library works.\nEspecially if you\u0026rsquo;re working with platform integrations and want to minimize the amount of work that users willing to implement the plugin need to do, I strongly recommend talking to your developers about creating a custom plugin for the integration.\nUnfortunately, plugins are not supported by the Google Tag Manager Universal Analytics tag template yet, so you might have to use a Custom HTML Tag to load plugins with GTM. See this article for inspiration.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/use-the-all-pages-trigger-correctly/",
	"title": "#GTMtips: Use The All Pages Trigger Correctly",
	"tags": ["Google Tag Manager", "gtmtips", "triggers"],
	"description": "How to use the All Pages trigger in Google Tag Manager.",
	"content": "Here\u0026rsquo;s a very quick tip this time, but one that\u0026rsquo;s caused a lot of headache for many Google Tag Manager users.\nTip 21: Use the All Pages Trigger correctly   Let\u0026rsquo;s face it, All Pages isn\u0026rsquo;t really an intuitive Trigger name. Many seem to interpret it as: enable the Tag to fire on all pages, but use the other Trigger (e.g. Link Click) to actually fire it.\nAs it turns out, the All Pages Trigger will fire the Tag on all pages. And it does so instantly, as soon as the GTM library has loaded. This means that a Tag such as the one in the screenshot above will fire twice:\nFirst when the gtm.js event is triggered in dataLayer upon the loading of the container snippet.\nAgain when the second, Event Trigger is activated upon a Link Click.\nYou might have guessed by now that only the second Trigger is relevant here. If the Event Tag is using Variables like Click URL or Click ID in its fields, these fields will most likely return undefined or (not set) in Google Analytics when the All Pages Trigger fires. This is because at the time of the All Pages event (gtm.js), no click has yet been registered, and these Variables will resolve to\u0026hellip;nothing.\nSo, my friends, use the All Pages Trigger as it\u0026rsquo;s supposed to be used: to fire a Tag on every single page with GTM installed on it, at a very early stage in the page load sequence. Examples of Tags that might use the All Pages Trigger would be the Google Analytics Page View Tag, some very open-ended remarketing Tag, Custom HTML Tags which setup a JavaScript framework for the rest of the Tags to use, and so on.\nFor more information about Triggers, be sure to read my Trigger guide as well!\n"
},
{
	"uri": "https://www.simoahava.com/personal/100th-post-big-changes/",
	"title": "100th Post: Big Changes",
	"tags": ["netbooster", "personal", "reaktor"],
	"description": "My 100th blog post.",
	"content": "A year ago, I wrote a Year In Review post for one of the craziest 365 days of my life, both personally (got married), and professionally (started at NetBooster, and toured the world talking about Google Analytics and Google Tag Manager). Now it\u0026rsquo;s time for another recap, and the chance to announce a big change in the Simoverse.\n  No, I will not be joining Google (made you think so!).\nGoodbye NetBooster I will be leaving NetBooster, where I have been the Head of Analytics for the Nordic countries over the past months. I held a number of positions before, e.g. SEO Manager, Production Director, and Web Analytics Supergeek (I made up the last title), but I was always gravitating towards data and analytics, so the last appointment was a very satisfying one.\nLeaving is going to bittersweet, as the future looks very interesting and challenging (more on that below), but at the same time I consider NetBooster to be my alma mater, and it\u0026rsquo;s not easy for me to say goodbye to the company that really made me feel like I\u0026rsquo;ve found my place in the world.\nNetBooster is an amazing company, with a very clear vision of where they want to be. They have a lot of crazily talented individuals and experts doing their best to push towards this vision. For a listed company, I always thought that the pace at NetBooster was extraordinary without sacrificing strategy or quality of work.\nI also got to work with some of the most intelligent people I know: Kristoffer Ewald, Mark Edmondson, Christian Pluzek, Thomas Hubert, Daniel Carlbom, Krister Collin, Antti Raami, just to name a few. Their professionalism alone made it an amazing adventure for me, and I felt dwarfed by their incredible work ethic.\nI know things are in good hands, as Fanny Le Béguec, a veteran in the analytics circuit, has taken over the responsibilities for driving Nordic analytics strategy. I couldn\u0026rsquo;t be happier that she\u0026rsquo;s in the ropes, and I\u0026rsquo;m sure she\u0026rsquo;ll make everyone forget who I ever was in no time.\nAs for where I\u0026rsquo;m going?\nHello Reaktor I\u0026rsquo;m going to be joining probably the coolest company in Finland. Reaktor is a full-service technology house, and they employ some of the best developers, designers, and innovators in Finland.\nI will be joining them as a Senior Data Advocate, bringing with me my own expertise on web analytics implementation, data collection, and data integration. I am also looking forward to setting up a proper digital analytics culture in Finland, since Reaktor has a very strong track record of community building.\nFor me, this is an amazing opportunity to accumulate new skills in data science, big data, and front-end development.\nI will continue my Google nerdom, and I hope to \u0026ldquo;Googlify\u0026rdquo; Reaktor as well, even though they have a non-commitment to any single platform, which I very much respect.\nI will also continue the outreach I\u0026rsquo;ve been doing thus far, keeping up with my blogging, being very active as a Google Developer Expert, and keeping close ties to Google Analytics and Google Tag Manager development. I\u0026rsquo;ll continue touring the conference circuit as well, just under a different banner this time.\nAll in all, a very exciting change for me. Hopefully, it will be a big change for the digital analytics community in Finland as well, if and when we start paving the way for no-strings-attached knowledge transfer that\u0026rsquo;s already blooming elsewhere in Europe.\nBlog stats Well, my traffic has continued to grow:\n  There was a dip around Christmas and the winter holidays, and in February-March I foolishly added a broken Event which I let inflate my sessions for way too long (4-5 weeks). Regardless, I\u0026rsquo;m still amazed how a niche blog can get so much attention, and I\u0026rsquo;m forever grateful to anyone interested enough to read these articles.\nMy top 10 articles over the past 365 days, measured by Unique Page Views (in parentheses) are:\n  Macro Guide For Google Tag Manager - 11 Feb 2014 (45594)\n  Advanced Form Tracking In Google Tag Manager - 7 Apr 2014 (36961)\n  Macro Magic For Google Tag Manager - 21 Mar 2014 (21881)\n  Some Awesome Google Tag Manager Resources - 26 Feb 2014 (16230)\n  Auto-Event Tracking In Google Tag Manager - 2 Oct 2013 (15216)\n  eCommerce Tips For Google Tag Manager - 6 Oct 2014 (14133)\n  Google Tag Manager: Track Social Interactions - 7 Nov 2013 (11781)\n  Google Tag Manager: Playing By The Rules - 3 Mar 2014 (11184)\n  Universal Analytics: Weather As A Custom Dimension - 19 Sep 2013 (10316)\n  Why Don’t My GTM Listeners Work? - 14 Feb 2014 (9541)\n  Lots of older articles in that list, but I\u0026rsquo;ve written a lot of nice stuff in 2015 as well, so let\u0026rsquo;s hope the top 10 looks different in a year or so.\nOverall, I\u0026rsquo;ve written 138606 words over the 100 articles.\nSince starting the blog, I\u0026rsquo;ve had the pleasure of receiving visits from 151901 users, across 335925 sessions, spanning 572925 pageviews. Amazing stuff!\n  I\u0026rsquo;ve received a crazy 1665 comments altogether on my posts, which completely astounds me. The community is just so strong and active!\nI\u0026rsquo;ve written a couple of tools, and I\u0026rsquo;ve started a #GTMtips post series, which already has 20 (hopefully) useful tips for anyone interested in using GTM. Please use the hashtag in social media if you have cool stuff you want to share with others!\nOnwards and upwards The next 365 days will surely be amazing as well, as I\u0026rsquo;m just starting to find my groove. I have the most amazing wife with me, and we bought a lovely house in Finland last summer.\n  Thank you for taking the time to read this, and if you ever see me speaking in a conference or hanging out in an event, come say hi. Always happy to make new friends!\n"
},
{
	"uri": "https://www.simoahava.com/tags/netbooster/",
	"title": "netbooster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/personal/",
	"title": "personal",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/reaktor/",
	"title": "reaktor",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/access-the-tracker-object-in-your-page-view-tag/",
	"title": "Access The Tracker Object In Your Page View Tag",
	"tags": ["custom html", "Google Tag Manager", "tracker object", "universal analytics"],
	"description": "How to access the Google Analytics tracker object before the analytics.js library has properly loaded. This tip is relevant for Google Tag Manager.",
	"content": "If you read my previous post on fetching the Client ID from the Universal Analytics tracker object with Google Tag Manager, you might have agreed with me that it sucks you can\u0026rsquo;t access the tracker object interface in real time using Google Tag Manager. This is because all of the set commands you add to a Universal Analytics tag template take place before the analytics.js is loaded and the tracker object is properly created.\nThe other issue with the tag template is that there\u0026rsquo;s no set way to access the tracker object anyway. I mean, you can use the set command, sure, but you can\u0026rsquo;t really use get or, for that matter, make use of any other interface methods you\u0026rsquo;d want to such as plugins (more on them in this post).\n  So, in this post I want to show a method you can use to actually mine the tracker object for whatever information you want, and at the same time ensure that the data is usable by the time your all-important Page View Tag fires. This is a notable improvement to the method I talked about in the previous post, which was to basically use a Window Loaded Trigger to fire a non-interactive Event Tag with the Client ID.\nThis article was inspired and intellectually fuelled by the amazing Carmen Mardiros (follow her: @carmenmardiros).\nThe method You\u0026rsquo;ll have to use Custom HTML for this. I know, I know! It\u0026rsquo;s a huge step backwards from the awesome, templated world you\u0026rsquo;re used to. Using a Custom HTML Tag is necessary precisely for the reasons I listed in the beginning. Until the Tag Template supports adding arbitrary JavaScript code between the create and send commands in the template, you\u0026rsquo;ll have to use a workaround.\nThe upside is that all you need the Custom HTML code for is to setup the tracker object, and then push an initialization event in to dataLayer, which will subsequently fire your actual Page View Tag. So, you\u0026rsquo;ll end up with one extra Tag and a very slight delay to your Page View Tag, but the delay is so minimal it doesn\u0026rsquo;t really make a difference.\nHere\u0026rsquo;s what will take place:\n  Tracker object for your UA-code is created\n  The Client ID is extracted from the tracker object as soon as it\u0026rsquo;s available\n  When the Client ID has been extracted, it\u0026rsquo;s pushed into dataLayer together with the initialization event\n  This event fires your Page View Tag, and you can use a normal Data Layer Variable to retrieve the Client ID\n  The Custom HTML Tag So, start by creating a new Custom HTML Tag, and make sure it fires on the earliest possible Trigger - usually the All Pages Trigger.\nAs for the code within, you\u0026rsquo;ll get that from your Google Analytics Admin, under Property Settings -\u0026gt; Tracking Info -\u0026gt; Tracking Code.\n  Copy-paste that into your Custom HTML Tag. Now, proceed to remove the line which says ga('send', 'pageview');. We\u0026rsquo;re removing this because we still want to use the actual tag template for sending the pageview hit.\nNow, if there are any modifications you need to do, such as adding additional trackers, you can set them up here. This guide has been written with a fairly basic setup in mind.\nNOTE! Any modifications to the tracker, such as allowLinker : true, custom cookie settings, and so forth need to be setup on this Custom HTML tracker, so that the Client ID you will be fetching from this custom tracker matches the one used by your main Page View Tag.\nThe next part is important. The analytics.js library is loaded asynchronously, which means that you don\u0026rsquo;t actually know when the tracker object has been created and its get method can be invoked. So, the library lets you pass a callback function to the ga object, which will be executed once the library has loaded and the tracker has been created. As I mention in the beginning: this feature is not available in the default tag template, which is why we need this workaround.\nTo add the callback, you add the following rows of code after the ga('create', ...); expression:\nga(function(tracker) { ...any methods which need the tracker object });  Any code within the callback function will be executed only once the library has been loaded, the tracker has been created, and the tracker object responds to interface methods. In this callback function, we\u0026rsquo;ll first fetch the Client ID, and then push it into dataLayer together with an initialization event:\nga(function(tracker) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;trackerReady\u0026#39;, \u0026#39;cid\u0026#39; : tracker.get(\u0026#39;clientId\u0026#39;) }); });  As you can see, I\u0026rsquo;m invoking the get command of the tracker object, and asking to retrieve the Client ID from the tracker. This wouldn\u0026rsquo;t be possible without the ga(function(tracker) {}) call.\nMy personal, finished example for the Custom HTML Tag looks like this. Remember, any customizations you need for the tracker object need to go here as well, so be sure to read up on how to set up advanced configurations for your Universal Analytics tracking code.\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;//www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;{{GA Tracking Code}}\u0026#39;, \u0026#39;auto\u0026#39;, {\u0026#39;allowLinker\u0026#39; : \u0026#39;true\u0026#39;}); ga(\u0026#39;require\u0026#39;, \u0026#39;simoPlugin\u0026#39;); // Any plugins you want to load would go here  ga(function(tracker) { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;trackerReady\u0026#39;, \u0026#39;cid\u0026#39; : tracker.get(\u0026#39;clientId\u0026#39;) }); }); \u0026lt;/script\u0026gt; Just a few comments on the customizations:\n  {{GA Tracking Code}} is a Lookup Table Variable which returns the property ID depending on a few conditions.\n  ga('require', 'simoPlugin'); is where you would load any plugins you want to use with your Tags. This is missing from the tag templates, and I really hope we\u0026rsquo;ll get the chance to load plugins soon.\n  That\u0026rsquo;s it for the Custom HTML Tag. The rest is very simple stuff.\nData Layer Variable and Custom Event Trigger The two other things you\u0026rsquo;ll need are a Data Layer Variable for the Client ID, and a Custom Event Trigger for the trackerReady event.\n  Nothing complicated about this. The Trigger is really simple as well:\n  Once you have these two setup, all you need to do is modify your existing Page View Tag to accommodate these changes.\nMy Tag, for example, now has the Client ID as a Custom Dimension, and it\u0026rsquo;s set to fire with the Event - trackerReady Trigger we just created.\n  Summary This is one of those articles that I hope becomes obsolete soon. The current Universal Analytics tag templates do not let you run arbitrary JavaScript (e.g. load plugins) or access the tracker object after it has been created, but before the \u0026lsquo;pageview\u0026rsquo; hit is sent.\nThat\u0026rsquo;s why you need this workaround. On the other hand, this isn\u0026rsquo;t exactly a very complex thing to do, as you\u0026rsquo;re only creating one additional Tag to set up the tracker and channel all the subsequent tags to not fire until the tracker object callback function has been executed.\nTime will tell when the Universal Analytics tag template supports these features. Personally, I hope soon. Google Tag Manager is already the de facto implementation mechanism for Universal Analytics, but if it doesn\u0026rsquo;t support advanced configuration of the tracker object, larger organizations especially might be deterred from migrating until such features are supported out-of-the-box.\n"
},
{
	"uri": "https://www.simoahava.com/tags/tracker-object/",
	"title": "tracker object",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/improve-data-collection-with-four-custom-dimensions/",
	"title": "Improve Data Collection With Four Custom Dimensions",
	"tags": ["client id", "custom dimensions", "Google Tag Manager", "Guide", "user id"],
	"description": "Add four crucial custom dimensions to all your Google Analytics hits with Google Tag Manager. The dimensions include hit timestamp, session ID, client ID and user ID.",
	"content": "Since writing my rant about the schema conspiracy of web analytics platforms, I\u0026rsquo;ve been giving the whole idea of hit-level data collection a lot of thought. Sessionization is very heavily implemented in Google Analytics, which is understandable, but the regular Google Analytics API just doesn\u0026rsquo;t give you the kind of information you\u0026rsquo;d need, if you wanted to stitch hits together differently in your own backend. In fact, there are four distinct levels of aggregation that are not exposed via the API, even though I think they should:\n  Hit timestamp - You can\u0026rsquo;t query for the exact timestamp of a Google Analytics hit via the API. You can\u0026rsquo;t get it in your custom reports, either. However, this might be very meaningful information, especially if you want to verify data integrity across systems.\n  Session ID - Hits are grouped together into sessions. However, using the API alone, you will not be able to easily identify if two distinct hits belong to the same session.\n  Client ID - Sessions are bunched under a Client ID. Every instance of the _ga cookie gets a unique Client ID, which is how the Google Analytics backend knows that the same \u0026ldquo;User\u0026rdquo; has visited the site multiple times. This Client ID is not exposed in any dimension you can query via the API.\n  User ID - The User ID feature in Universal Analytics groups together hits, sessions, and Client IDs. It is the highest possible level of abstraction and aggregation available in Google Analytics. However, the data rows do not expose if a hit/session belongs to a specific User ID.\n  So, the purpose of this post is to give you the tools to annotate your incoming hit-stream data with all the information listed above. For data integration, this is almost a necessity, especially if you have a complex mesh of systems across which you want to join arbitrary data.\nIn the following chapters, we\u0026rsquo;ll build four Custom Dimensions and four data collection methods that will let you include this information in your data set. We will, of course, be using Google Tag Manager to make things more manageable.\n1. The Method I\u0026rsquo;ve created a Custom Dimension for each of the four use cases. Two of the dimensions are session-scoped, and two are hit-scoped. The dimensions are:\n  Client ID - session-scoped Custom Dimension that collects the Client ID set by Google Analytics\n  Session ID - session-scoped Custom Dimension that collects a randomized Session ID\n  Hit Timestamp - hit-scoped Custom Dimension that collects the actual timestamp of each hit in local time, with the timezone offset included\n  User ID - hit-scoped Custom Dimension that collects the User ID that gets set when someone logs into your website\n    Why a hit-scoped Custom Dimension for User ID, you ask? Well, the whole privacy discussion around user tracking is complicated, and I would rather be poked repeatedly in the eye with a dead ferret than be drawn into it. By tracking User ID with a hit-scoped Custom Dimension, you\u0026rsquo;ll only collect the data from logged-in users. As soon as they log out, or if they re-enter the website having logged out, User ID will not be sent with the hits. If the Custom Dimension were session-scoped, or even user-scoped, you would be collecting User ID for potentially logged-out visitors as well, and that might be in the grey area privacy-wise.\nThe end result of combining all this information will be something like this:\n  In this (fictional) data export, you can see 7 unique hits, made by two different client IDs (e.g. different browsers or devices), which span across three distinct sessions, but are all made by the same, logged-in user. With layered information like this, you can build intelligent models using GA data alone, but the possibilities it offers for data integration are remarkable as well.\nThe most difficult one of these solutions to implement, by far, is Client ID collection, so we\u0026rsquo;ll start with that.\n2. Client ID (UPDATE 5 April 2018: I recommend using this customTask method instead for sending the Client ID in a Custom Dimension. The tracker object method outlined below is more complicated and far more unreliable. With customTask, you can send the Client ID with any tag you want, with 100% accuracy each time.)\nThe difficulty with Client ID is that the ga interface you use to retrieve the Client ID doesn\u0026rsquo;t perform well in real time, especially for Tags firing very early in the page load sequence (e.g. your Page View Tag).\nYou could get the Client ID from browser cookies, but if it\u0026rsquo;s a first-time visitor to your site, chances are that the cookie hasn\u0026rsquo;t been set by the time the Page View Tag fires, and you\u0026rsquo;ll miss this information. Also, if there are multiple trackers on the page, how do you know which _ga cookie to access? You don\u0026rsquo;t.\nSo, I\u0026rsquo;ve opted for a different approach. I\u0026rsquo;m sending the Client ID using a non-interaction Event Tag, which fires when the page has loaded. This almost certainly guarantees that the ga interface is up and running, and I can use it to pull the Client ID for the correct tracker. The correct tracker is identified by querying the property ID (UA-XXXXXX-X) associated with the tracker object.\nThis particular data collection method will require the following ingredients:\n  Session-scoped Custom Dimension, to collect the data in Google Analytics\n  Custom JavaScript Variable, which gets the correct Client ID\n  Window Loaded Trigger, which fires when the window has loaded if the Custom JavaScript Variable returns a valid value\n  Event Tag, which sends a non-interaction event to Google Analytics when the Window Loaded Trigger fires\n  2.1. Session-scoped Custom Dimension This one is easy. Browse to the Google Analytics Admin of the web property you want to track to, select Custom Definitions -\u0026gt; Custom Dimensions, and create a new Custom Dimension that looks like the one below.\n  The important thing is to choose Session as the scope, and to make note of the index assigned to it.\nNote that if you wish, you could just as well scope this to User instead. The Client ID, by definition, is the same for the user throughout, so it might make sense to scope it accordingly. User-scoped dimensions are a bit questionable in terms of privacy, but in this case I don\u0026rsquo;t see any issue, as you\u0026rsquo;re just exposing a dimension that exists anyway. Thanks to Michael Hayes for pointing this out in the comments!\n2.2. Custom JavaScript Variable The Custom JavaScript Variable is named {{Get Client ID for current Tracker}}, and it needs the following code:\nfunction() { try { var trackers = ga.getAll(); var i, len; for (i = 0, len = trackers.length; i \u0026lt; len; i += 1) { if (trackers[i].get(\u0026#39;trackingId\u0026#39;) === {{GA Tracking Code}}) { return trackers[i].get(\u0026#39;clientId\u0026#39;); } } } catch(e) {} return \u0026#39;false\u0026#39;; }  The function contents are wrapped in a try...catch block, so any errors and problems with loading the ga interface are gobbled up. If you want, you can add your own error debugging code into the catch block. The key thing is to make sure the Event Tag doesn\u0026rsquo;t fire if there\u0026rsquo;s a problem with retrieving the Client ID. This means that you might miss some hits, but since we\u0026rsquo;re sending the information to a session-scoped Custom Dimension, you only need one successful hit sent during the session.\nThe code is designed so that it cycles through all the GA trackers on the page. Once it encounters a tracker object which tracks to the property ID returned by the {{GA Tracking Code}} variable, it returns the Client ID associated with this object.\nThis means that you will need to have a variable called {{GA Tracking Code}}, which returns a valid property ID (UA-XXXXXX-X). On my website, for example, it\u0026rsquo;s a Lookup Table Variable, which returns my main property ID for everyone else, but for me it returns a different property ID. This is because I use this secondary property for debugging implementations.\n2.3. Window Loaded Trigger The Window Loaded Trigger is pretty simple. It\u0026rsquo;s your basic Page View Trigger, where you set the Trigger Type to Window Loaded. However, you will need an additional condition in it.\n  The condition Get Client ID for current Tracker does not equal false ensures that the Trigger only fires if the Custom JavaScript Variable you just created returns a valid value.\n2.4. Event Tag The Event Tag is very basic, except for two customizations. First, you need set its Non-Interaction value to true. This prevents the Event from being calculated into interaction metrics like Session Duration and Bounce Rate.\nAlso, you\u0026rsquo;ll need to add a Custom Dimension to the Tag, via More Settings -\u0026gt; Custom Dimensions. Add the index number of the dimension you created in step 1 to the Index field, and add the Variable reference you created in step 2 to the Value field.\n  You can see how the fields should look from the image above.\n2.5. End result If you did everything correctly, you should see your sessions populating with a new Custom Dimension that you can add to your reports, and pull out of GA via the API or via the reporting interface, if you wish.\n  The dimension contains the Client ID of the visitor. You can then use this in your backend, for example, when you want to stitch hits sent from the same _ga cookie together in meaningful ways.\n3. Session ID For Session ID, we\u0026rsquo;re using a randomized string that is sent with each Pageview hit to Google Analytics. The string changes with each Pageview, but this doesn\u0026rsquo;t matter. Because you\u0026rsquo;re sending it to a session-scoped Custom Dimension, only the last value you send will be applied to the hits in the session.\nThe required components are:\n  Session-scoped Custom Dimension, to collect the data in Google Analytics\n  Custom JavaScript Variable, which returns a valid Session ID string\n  Small modification to your Page View Tag, so that the Session ID is sent to Google Analytics\n  3.1. Session-scoped Custom Dimension This is pretty much the same step you went through in the previous exercise.\n  Just remember to make note of the index number, again.\n3.2. Custom JavaScript Variable The Custom JavaScript Variable is aptly named {{Random Session ID}}, and it has the following code:\nfunction() { return new Date().getTime() + \u0026#39;.\u0026#39; + Math.random().toString(36).substring(5); }  This script creates a pretty unique, randomized session ID. It does it by taking the hit timestamp in Unix time, adding a period, and following with a random string of alphanumeric characters. Because of the timestamp (accurate up to milliseconds), it\u0026rsquo;s very improbable that two similar session IDs are ever created.\nAn example of a session ID would be: 1427856715104.jdubr7umobt9.\n3.3. Modified Page View Tag In your Page View Tag, add a new Custom Dimension under More Settings -\u0026gt; Custom Dimensions. Set the index number you got from step 1, and set the value to the Variable reference {{Random Session ID}} you just created.\nBy using only the Page View Tag, you\u0026rsquo;ll be sending the Session ID with each page load. Only the last Session ID you send will remain, however, and all the hits in the session will automatically be annotated with this ID, thanks to the session-scoped Custom Dimension. If this is confusing, remember to read up on Custom Dimensions!\n3.4. End result By virtue of the Custom Dimension, you now have an identifier with which you can stitch together arbitrary, discrete hits in Google Analytics.\n  Together with the hit timestamp, you can start building realistic visit paths, if that suits your fancy.\n4. Hit timestamp Hit timestamp is something you should send with every single hit you send to Google Analytics. This means that you\u0026rsquo;ll need to modify all your Google Analytics Tags, which might seem like a chore.\nAccuracy is, of course, completely up to you, and you can opt to only send the timestamp with Pageviews and Transactions, instead.\nWhat you\u0026rsquo;ll need:\n  Hit-scoped Custom Dimension, to collect the timestamp in Google Analytics\n  Custom JavaScript Variable, which returns a valid timestamp string\n  Modification to all your tags, to which you want the timestamp to be attached\n  4.1. Hit-scoped Custom Dimension There\u0026rsquo;s nothing spectacular about this one. Create a new Custom Dimension in GA Admin, and set its scope to Hit.\n  Remember to make note of the index.\n4.2. Custom JavaScript Variable The Custom JavaScript Variable needs to return the timestamp in String format. Now, there are many ways you could do this, for example:\n  Get timestamp in Unix time (milliseconds since Jan 1, 1970), adjusted for client timezone\n  Get timestamp in Unix time, converted to UTC\n  Get timestamp as an ISO string, adjusted for client timezone\n  Get timestamp as an ISO string, converted to UTC\n  Get custom string, adjusted for local time or converted to UTC\n  Something completely different\n  In my setup, I wanted the timestamp to be customized for my own tastes. That means that I\u0026rsquo;m parsing it to resemble an ISO timestamp, but I\u0026rsquo;m using client local time including the timezone offset, so I can see just which timezone the user is in. So, for example, if a visitor comes from Finland, which is GMT+3 (stupid daylight savings time), the hit timestamp might look like this:\n2015-04-03T18:55:27.466+03:00\nThis translates to April 3rd, 2015, at 6:55PM Helsinki time.\nSo, to get something like this, some JavaScript is required. Create a new Custom JavaScript Variable, and name it {{Hit Timestamp Local Time With Offset}}. Add the following code within:\nfunction() { // Get local time as ISO string with offset at the end  var now = new Date(); var tzo = -now.getTimezoneOffset(); var dif = tzo \u0026gt;= 0 ? \u0026#39;+\u0026#39; : \u0026#39;-\u0026#39;; var pad = function(num) { var norm = Math.abs(Math.floor(num)); return (norm \u0026lt; 10 ? \u0026#39;0\u0026#39; : \u0026#39;\u0026#39;) + norm; }; return now.getFullYear() + \u0026#39;-\u0026#39; + pad(now.getMonth()+1) + \u0026#39;-\u0026#39; + pad(now.getDate()) + \u0026#39;T\u0026#39; + pad(now.getHours()) + \u0026#39;:\u0026#39; + pad(now.getMinutes()) + \u0026#39;:\u0026#39; + pad(now.getSeconds()) + \u0026#39;.\u0026#39; + pad(now.getMilliseconds()) + dif + pad(tzo / 60) + \u0026#39;:\u0026#39; + pad(tzo % 60); }  This code has been gratefully copy-pasted form this StackOverflow discussion.\nThis script works across all browsers, and returns a parsed string timestamp, with the timezone offset appended to the string.\n4.3. Modified Tags I send this timestamp with every single Tag that\u0026rsquo;s firing on my site, but if you feel like this is overkill, you can choose to only send it with business-critical hits you\u0026rsquo;ll use with other backend data, for example.\nThe only thing you need to do is add the Custom Dimension to any Tag you want to send it with. The setting looks like this:\n  Remember to set the Index accordingly. Go back to Google Analytics Admin, and look for the Custom Dimension you created in Step 1 to get the correct index number.\n4.4. End result What you\u0026rsquo;ll get is something like this:\n  It\u0026rsquo;s all your transactions, coupled with the accurate hit timestamp in local time when the transaction was recorded. The timezone offset helps you compare data with your backend, if it uses server time or some fixed timezone in its own data collection.\n5. User ID For User ID, you\u0026rsquo;ll need to have it already implemented in some way or another. In this example, we\u0026rsquo;ll pull the ID from dataLayer, but you might be using a 1st Party Cookie instead, which means you\u0026rsquo;ll need to modify the code accordingly.\nRemember that tracking Users across sessions and devices is a tricky business both technologically and ethically. I\u0026rsquo;ll leave things like consent, opt-out, anonymisation, and privacy to linger in the nether regions of your mind, so remember to ensure that what you\u0026rsquo;re doing is considered OK by at least one other person in the right state of mind.\nWe\u0026rsquo;re using a hit-scoped Custom Dimension again, but you can choose what level of accuracy and stitching to implement. I\u0026rsquo;ve reasoned for hit-level accuracy in the beginning of this article (wow, that was a LONG time ago), and I think the reasoning is well-founded. Also, depending on what you want to do with the data in the backend, you might choose to send the User ID with all hits or with just some hits.\nTo expose User ID as a Custom Dimension in your hits, you\u0026rsquo;ll need:\n  Hit-scoped Custom Dimension, to collect the User ID in Google Analytics\n  Data Layer Variable, which picks up the User ID from dataLayer\n  Modified Tags, to which you want the User ID to be attached\n  5.1. Hit-scoped Custom Dimension The Custom Dimension is simple, of course. Just go to Google Analytics Admin, browse to Custom Definitions under the web property you want to set this up with, and create a new Custom Dimension of Hit scope:\n  As before, make note of the Index.\n5.2. Data Layer Variable How you actually retrieve the User ID depends on how you expose it in your website. A very good method is to populate it in dataLayer by a server-side process, which renders it together with the rest of the page. This way the User ID will be cemented in the page template, and you can use it with your critical tags that fire early on in the page load sequence.\nI use dataLayer to implement User ID, so all I need to create is a Data Layer Variable that picks up the User ID from the data model, and returns the value stored within. On my site, the variable is named {{DLV - userId}}, and it looks like this:\n  As you can see, I\u0026rsquo;m not setting a default value in the Variable. This means that if userId is not set in dataLayer, this Variable will resolve to undefined, and the Custom Dimension will be dropped from any Tag that uses it. This is a wonderful feature of the analytics.js library, and it really helps in keeping your Tag setup nice and lean.\n5.3. Modified Tags Next, add the Variable you just created into all the Tags you want to associate with logged in users. I send it with every single hit, because I want a comprehensive analysis of what my visitors do on the site.\n  Remember to set the Index correctly according to what you setup earlier in Google Analytics Admin.\n5.4. End result What you\u0026rsquo;ll get is an extra annotation on all your hits from logged in users:\n  Now, let\u0026rsquo;s not kid ourselves. If you\u0026rsquo;re sending User ID with just a handful of hits, and you\u0026rsquo;re also collecting Client ID, you can extrapolate User ID in your backend to all the hits done by the Client IDs associated with the User ID dimension. Like I wrote in the beginning of this article, the ethical, legal, and privacy-related considerations are yours to make alone.\n6. Summary This article explores something I feel passionate about: meaningful data collection. Google Analytics uses a lot of information that isn\u0026rsquo;t exposed in the reporting interface or the APIs, even though this information is central to how the platform aggregates the hits coming in from digital properties.\nBeing able to access this type of granular data shouldn\u0026rsquo;t be reserved for BigQuery users alone, so the solutions in this post help you add an extra level of accuracy to the stream of data flowing to the tracking platform. You can then pull this data out, combine it with other backend data, and build powerful models that will allow you to optimize your digital properties better than before.\n"
},
{
	"uri": "https://www.simoahava.com/tags/user-id/",
	"title": "user id",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/migration-to-v2-using-the-migration-assistant/",
	"title": "#GTMtips: Migration To V2 Using The Migration Assistant",
	"tags": ["Google Tag Manager", "gtmtips", "migration"],
	"description": "How to migrate your V1 Google Tag Manager container to the new version using the migration assistant.",
	"content": "On March 30th, right on (the latest) schedule, the Migration Assistant tool was published for Google Tag Manager V2. This tool lets you opt-in to account migration for your legacy Google Tag Manager Accounts. Migration means simply that the accounts will be converted to V2 accounts, and you will have access to all the new features the upgrade provides.\nIn this #GTMtips post, we\u0026rsquo;ll go over the migration steps (it\u0026rsquo;s pretty simple), and I\u0026rsquo;ll leave you with a couple of tips on how to get started with the new features.\nTip 20: Launch the Migration Assistant   Browse to http://tagmanager.google.com/. That\u0026rsquo;s the URL for GTM V2. When you enter the site, the Migration Assistant should pop-up immediately. You can choose to launch the assistant there and then, or you can cancel and do this later.\n  If you do choose to cancel, you can launch the Migration Assistant by scrolling down to where the \u0026ldquo;Legacy Accounts\u0026rdquo; are listed on the V2 start page. In the title bar for the \u0026ldquo;Legacy Accounts\u0026rdquo; is a button that will also launch the migration tool.\n  When you do choose to launch the Migration Assistant, all your legacy accounts will be listed. You can select one or many from this list. After selecting the accounts, clicking Migrate will start the migration process.\n  The migration process is designed to be completely transparent. You will not need to edit the container snippet on the page template, nor will you need to republish the container version.\n  It will take some hours for the migration(s) to complete, so be patient.\nOnce the migration is done, you can start using the V2 features in your container. Here are two things you might want to get acquainted with.\n1. Trigger-based Auto-Event Tracking If you had setup auto-event tracking in your legacy container, you\u0026rsquo;ll have used the Event Listener Tag templates. However, in V2, auto-event tracking is now Trigger-based, and does not require a separate tag setup anymore.\nTake a look at my guide for auto-event tracking in GTM V2. What you\u0026rsquo;ll need to do is take the conditions from your old \u0026ldquo;Firing rules\u0026rdquo; (now converted to custom Triggers), and use them in the dedicated auto-event Trigger types: e.g. Click, Link Click, and Form Submit.\nFor example, if you had a Firing Rule that looked like:\n{{event}} equals gtm.formSubmit\n{{element id}} equals contactForm\nIn V2, you would create a Form Submit Trigger, and in its \u0026ldquo;Fire When\u0026rdquo; settings, you would add the condition:\nForm ID equals contactForm\nThis does require that you activate the built-in Form ID variable as well (see below).\nAnyway, follow the guide, and you\u0026rsquo;ll have a better idea of what you need to.\n2. Built-in variables In V2, the most common variables you\u0026rsquo;ll need are available as built-in variables. You can read more about them in my Variable Guide for GTM V2. I suggest you use these, as they will keep the container size down, and activating them is as simple as checking a box in the Variables screen.\nWhen you migrate from V1, you\u0026rsquo;ll have a bunch of pre-defined legacy variables such as {{url}}, {{url path}}, {{element url}}, etc. These all have equivalent built-in variables, so one of the things you might want to do is activate the respective built-in variables, and then modify your Tags and Variables to use these instead.\nHere\u0026rsquo;s a pretty sweet way to replace your user-defined variables with their respective Built-in Variables, courtesy of Brian Kuhn:\n  Make sure the Built-in Variable has been deactivated\n  Rename the user-named variable (e.g. {{url}}) to its respective Built-in Variable (e.g. Page URL)\n  Reactivate the Built-in Variable, and choose Overwrite in the pop-up that\u0026rsquo;s displayed\n  This will update all your Tags, Variables, and Triggers to use the Built-in Variable instead.\n  Do note that this isn\u0026rsquo;t required, but it\u0026rsquo;s a good idea to migrate completely to V2, so that you can leverage the new features as much as possible.\n"
},
{
	"uri": "https://www.simoahava.com/tags/migration/",
	"title": "migration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/debugging-tag-execution-properly/",
	"title": "#GTMtips: Debugging Tag Execution Properly",
	"tags": ["debug", "Google Tag Manager", "gtmtips"],
	"description": "Tips and guide for debugging tag execution properly in Google Tag Manager.",
	"content": "One of the unfortunate misunderstandings regarding the wonderful Google Tag Manager Preview mode is what it actually means when GTM reports \u0026ldquo;Tags Fired On This Event\u0026rdquo;. For many, this seems to indicate that whatever code the Tag was meant to execute also completed successfully. However, this is not the case.\nTip 19: Debugging Tag execution vs. actual requests   Let\u0026rsquo;s get the distinction straight right away:\nGoogle Tag Manager debug panel tells you when a Trigger has been invoked by certain conditions, and the Tag which uses this Trigger has its JavaScript injected into the Document Object Model.\nThis does not mean that the expected result of the Tag code (e.g. fire a pixel, load an iframe, perform arbitrary JavaScript) has completed successfully.\nThis is a very clear distinction, and it basically means that looking at the debug panel alone, you cannot debug whether or not the execution of your Tags is performing expectedly.\nFor example, if you have some Google Analytics blocker running in the browser, GTM\u0026rsquo;s debug panel will tell you that the Tag has \u0026ldquo;fired\u0026rdquo;, but the browser will prevent the Tag code from completing its request to Google Analytics servers.\nHow to debug the whole process The best way to ensure that Tags are firing correctly is to pay attention to two things: network requests and the JavaScript console.\nThe former will tell you if requests are successfully completing, and the latter will tell you if the JavaScript has errors in it (or other, unexpected behavior).\nNetwork requests can be debugged with your browser\u0026rsquo;s Developer Tools:\n  Chrome / Mac: Open the Developer Tools with Cmd + Opt + I and navigate to \u0026ldquo;Network\u0026rdquo;\n  Chrome / Windows: Open the Developer Tools with F12 and navigate to \u0026ldquo;Network\u0026rdquo;\n  Firefox / Mac: Cmd + Opt + Q\n  Firefox / Windows: Ctrl + Shift + Q\n  What you\u0026rsquo;ll see is the list of requests your browser has made during the lifespan of the page. You can locate a specific type of request using the filter feature (Chrome). For example, to see hits to the Universal Analytics endpoint, I would filter for requests with the word \u0026ldquo;collect\u0026rdquo;. That\u0026rsquo;s because collect is the URI where all Universal Analytics hits are dispatched to.\nNext, I\u0026rsquo;ll focus on the Status column. Typical errors are e.g. 404 (not found) and 500 (internal server error). A successful request would have the response code 200.\n  If you want an even clearer view to what\u0026rsquo;s happening on the site, complete with an analysis of dataLayer state, take a look at my favorite debugging tool, WASP.\nTo debug JavaScript errors, open the same Developer Tools, but navigate to Console instead. You should see a verbose output of all JavaScript errors that the current page has dispatched, and you might see some errors in your GTM tags as well.\n  Do note that debugging JavaScript errors can be pretty difficult especially if you use frameworks like jQuery. You will have to plough through the call stack trace (if there is one), and even then the results might be inconclusive. That\u0026rsquo;s why it\u0026rsquo;s a good thing to cooperate with a developer who understands the existing scripting conditions of your website, and who can provide input on which errors are related to which scripts.\nSo, remember:\nDebug Panel tells you when a Trigger causes a Tag to be injected on your site.\nBut:\nNetwork debugger and JavaScript console tell you if the Tag code executed with expected results.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/send-weather-data-to-google-analytics-in-gtm-v2/",
	"title": "Send Weather Data To Google Analytics In GTM V2",
	"tags": ["api", "Google Tag Manager", "Guide", "JavaScript", "v2", "weather"],
	"description": "How to collect and send weather data from users who visit your website. You can send this information to Google Analytics using Google Tag Manager.",
	"content": "In 2013, I wrote a guide for Universal Analytics and Google Tag Manager on how to poll for weather conditions, and send this information to Google Analytics as a custom dimension of the session. The guide was intended as a technical introduction to Google Tag Manager, and I think it succeeded in that.\nHowever, GTM has changed a lot over the last 1.5 years, and I\u0026rsquo;ve made some improvements to the method along the way. So I wanted to update the guide for the new version of Google Tag Manager (V2), and introduce some changes to the code that make it more flexible and efficient.\n(UPDATE 21 August 2017) - I updated this article to work with the Weather Unlocked API (thanks to Kévin Coppens for the tip), due to openWeatherMap upgrading their SSL plan to a pretty expensive level. Do note that Weather Unlocked requires the use of an API key, and passing the API key client-side is risky, and opens up your API plan to abuse. For those who want to take this weather analysis seriously, I really recommend moving to a 100% server-side solution, where the weather data is polled before the page itself is rendered, and the data is written in the dataLayer of the page.\nSince this is a whopper of a post, I\u0026rsquo;ve got a table of contents right here:\n1. What it does The idea behind the solution is to poll a weather service, so that all sessions in Google Analytics would be annotated with information on what the weather conditions were at the time of the session. The underlying question we\u0026rsquo;re trying to answer is: \u0026ldquo;Does weather play a part in how people interact with your website or your brand?\u0026rdquo;. This might be a very relevant question for businesses very dependent on weather conditions, such as those with golf courses and ski slopes.\nThe technical method is split into three parts:\n  Geolocation - we need to get an approximation of the geographic area the visitor is browsing from.\n  Weather API - we will use this geolocation data to query for the weather conditions in the visitor\u0026rsquo;s region.\n  Data Layer - finally, we will store all the information we get in the dataLayer object to be available for our Tags.\n  It\u0026rsquo;s not the simplest of processes, and there are many ways you can optimize it (I\u0026rsquo;ll expound these in the relevant chapters). Also, one of the main reasons for writing this guide in the first place is to show a prototype for any generic API call you might want to make. So it\u0026rsquo;s not just weather, but share price, traffic details, basketball results, and so on. If there\u0026rsquo;s an API for it, you can use this method to enrich your visitors\u0026rsquo; sessions with new Custom Dimensions and Metrics.\nThe solution itself follows the following flow, visualized with my mad PowerPoint skills:\n  It\u0026rsquo;s a fairly straightforward process, with only two junctions: session cookie state and success / failure of the API call itself.\n2. Core requirements Regardless of how you wind up doing the solution, you will require some core components. These are:\n  geoPlugin API key and Weather Unlocked API credentials\n  Custom HTML Tag to poll the API and store the information in dataLayer\n  Trigger to fire an Event Tag when the data is in dataLayer Data Layer Variables to pull the information from the dataLayer into the Event Tag\n  1st Party Cookie Variable to maintain session state\n  Session-scoped Custom Dimensions set up in Google Analytics to receive the API data from the Tag\n  Universal Analytics Event Tag to send the data to Google Analytics\n  We\u0026rsquo;ll create the Custom HTML Tag later, since it requires a bit more work and deliberation than the other core requirements.\n2.1. Trigger The Trigger is simple. It\u0026rsquo;s just a Custom Event Trigger that fires when an \u0026lsquo;event\u0026rsquo; key with value \u0026lsquo;weatherDone\u0026rsquo; is pushed into dataLayer. Give it a descriptive name, such as Event - weatherDone.\n  Make sure the Trigger looks like the one in the screenshot above.\n2.2. Data Layer Variables In this solution, we\u0026rsquo;ll use two Data Layer Variables. One to hold a simplified description of the weather conditions (e.g. \u0026ldquo;Cloudy\u0026rdquo;, \u0026ldquo;Rain\u0026rdquo;, \u0026ldquo;Storm\u0026rdquo;), and one to store the temperature, also simplified (e.g. \u0026ldquo;5°C - 15°C\u0026rdquo;).\nSo, create two Data Layer Variables, one for Variable Name weather, and one for Variable Name temperature.\n  The Variable above is named DLV - weather.\n  Give the second Variable a name such as DLV - temperature. This is the naming convention I use (DLV stands for Data Layer Variable, of course), but you can name them as you wish.\n2.3. 1st Party Cookie Variable We\u0026rsquo;ll be artificially maintaining session state using a custom cookie, written by the script and refreshed each time the page loads. The reason we want to maintain state like this is because we only want to send weather data once per session.\nThe API call to the weather service is not light-weight. We want to avoid making calls like this as much as we can, because that way the page load process won\u0026rsquo;t be hindered by slowly loading asynchronous API calls like this. So, when the visitor first lands on the site, a cookie is created that mimics GA\u0026rsquo;s session logic by having a 30-minute expiration. Naturally, this isn\u0026rsquo;t the entire session logic (we\u0026rsquo;re missing stuff like referral exclusions, cross-domain tracking, etc.), but it\u0026rsquo;s close enough to save us from making unnecessary calls.\nThe cookie script is in the actual API script introduced later in this article, but the Variable you\u0026rsquo;ll need to create should look like this:\n  2.4. Session-scoped Custom Dimensions Because we have two measurements we want to send as extra data, we\u0026rsquo;ll need two Custom Dimensions to store them. Now, you might wonder: \u0026ldquo;Why not use Custom Metrics for temperature? It\u0026rsquo;s numeric, after all?\u0026rdquo;. That\u0026rsquo;s a good question, and the reason is that you can\u0026rsquo;t calculate on Custom Metrics yet. So all temperature data that you would be sending would end up as cumulative across your reports. It doesn\u0026rsquo;t make sense to ask \u0026ldquo;What was the sum of all temperatures for converting visits from Helsinki?\u0026rdquo;, but it makes a lot of sense to ask \u0026ldquo;What was the average of all temperatures for converting visits from Helsinki?\u0026rdquo;. The latter, unfortunately, is not yet available through Google Analytics.\nAnyway, create two session-scoped Custom Dimensions, and name them so that you\u0026rsquo;ll find them when going through your dimension lists. I\u0026rsquo;ve used simply \u0026ldquo;Weather\u0026rdquo; and \u0026ldquo;Temperature\u0026rdquo;.\n  It\u0026rsquo;s important to make note of the dimension index number for both new dimensions. We\u0026rsquo;ll need this information for our Event Tag.\n2.5. Universal Analytics Event Tag The Event Tag has two \u0026ldquo;customizations\u0026rdquo;, if you will. First of all, we\u0026rsquo;re sending this hit as a non-interaction event. This means that it will not affect the bounce rate of the session. The reasoning for this is that weather data isn\u0026rsquo;t something that results from a visitor action. It\u0026rsquo;s just meta information about the session, so it\u0026rsquo;s not interactive.\nUse the Trigger you created in step 2.1. to fire this Event Tag.\nWhy not use a Page View Tag, then? Another great question. Well, the API calls we\u0026rsquo;ll be making in the Custom HTML Tag take time to complete, since they\u0026rsquo;re executed asynchronously. This means that we won\u0026rsquo;t be able to pinpoint the moment in the page load sequence when the data is actually sent, but we will know it will be deferred for as long as the API call takes. If the call takes up a long time to complete, it will delay our Page View Tag from firing. This, I think, is a definite no-no. Page Views are a crucial part of session-based hit collection, and missing them simply is not an option.\nThere\u0026rsquo;s no down-side to using a non-interaction event.\nAnyway, the Tag I\u0026rsquo;ve used here has the following event parameters:\nEvent Category - Weather\nEvent Action - {{DLV - weather}}\nEvent Label - {{DLV - temperature}}\nNon-Interaction - True\n  Next, scroll down to More Settings, and expand Custom Dimensions. Here, you will need to add two new rows of Custom Dimensions to accommodate the two dimensions that are hungrily waiting for data to pass through the GA interface.\nAt this point, you will need to remember or look up the dimension index numbers from Google Analytics Admin. In my example, they\u0026rsquo;re simply 1 (for Temperature) and 2 (for Weather), but remember to adjust if necessary.\n  Add the respective Data Layer Variables to the value fields, as in the image above.\nThat\u0026rsquo;s it for the core requirements.\nThe next bit gets quite complicated.\n3. Building the script If you remember from the beginning, this technical solution falls into three parts: geolocation, weather query, and storing the information in dataLayer.\nTo get started, you will need a Custom HTML Tag that uses the All Pages Trigger. Give it a descriptive name. I\u0026rsquo;ve used Utility - HTML - Weather Query, but, again, you are free to go with your own naming convention.\nThis code has been rewritten to work with HTTPS-protected sites. If your site isn\u0026rsquo;t using HTTPS, much of this will be overkill and needlessly expensive, but I do recommend you upgrade your site security as soon as possible.\nThe code within, in all its JavaScript glory, is this:\n\u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; (function() { var fetchWeatherData = function(longitude, latitude) { // You need to sign up to Weather Unlocked for an account. Once you do,  // you will find your APP ID and APP KEY in your account dashboard.  var wuAppId = \u0026#39;paste_your_weather_unlocked_app_id_here\u0026#39;; var wuAppKey = \u0026#39;paste_your_weather_unlocked_app_key_here\u0026#39;; jQuery.getJSON(\u0026#39;https://api.weatherunlocked.com/api/current/\u0026#39; + latitude + \u0026#39;,\u0026#39; + longitude + \u0026#39;?app_id=\u0026#39; + wuAppId + \u0026#39;\u0026amp;app_key=\u0026#39; + wuAppKey) .done(function(data) { window.dataLayer.push({ event: \u0026#39;weatherDone\u0026#39;, weather: data.wx_desc, temperature: data.temp_c }); }).fail(function(jq, status, msg) { console.log(\u0026#39;Weather request failed: \u0026#39; + status + \u0026#39; - \u0026#39; + msg); }); }; var geoLocateUser = function() { // This is your API key for GeoPlugin, which you can purchase at  // http://www.geoplugin.com/premium#ssl_access_per_year  var geoPluginAPIKey = \u0026#39;paste_your_api_key_here\u0026#39;; jQuery.getJSON(\u0026#39;https://ssl.geoplugin.net/json.gp?k=\u0026#39; + geoPluginAPIKey + \u0026#39;\u0026amp;jsoncallback=?\u0026#39;) .done(function(data) { fetchWeatherData(data.geoplugin_longitude, data.geoplugin_latitude); }).fail(function(jq, status, msg) { console.log(\u0026#39;Geolocation failed: \u0026#39; + status + \u0026#39; - \u0026#39; + msg); }); }; if (typeof {{Session alive}} === \u0026#39;undefined\u0026#39;) { geoLocateUser(); } // Reset \u0026#34;session\u0026#34; cookie with a 30-minute expiration  var d = new Date(); d.setTime(d.getTime()+1800000); var expires = \u0026#34;expires=\u0026#34;+d.toGMTString(); document.cookie = \u0026#34;session=1; \u0026#34;+expires+\u0026#34;; path=/\u0026#34;; })(); \u0026lt;/script\u0026gt; Oh my, that\u0026rsquo;s a lot of code. In the next chapters I\u0026rsquo;ll walk you through just what the heck is going on here.\nIt\u0026rsquo;s important to notice that I\u0026rsquo;m loading jQuery at the very top of the script. Also, I\u0026rsquo;ve wrapped the whole script itself in an immediately invoked function expression to scope all variables to the function and thus avoid polluting the global namespace.\nIf you\u0026rsquo;re already loading jQuery on the site, you shouldn\u0026rsquo;t load it again in this Tag, but then you will need to ensure the Custom HTML Tag fires only after jQuery has completely loaded. If you load jQuery asynchronously, it leaves you little choice but to fire the Custom HTML Tag upon the Window Loaded Trigger, since that\u0026rsquo;s the only moment you can be sure that jQuery has been loaded.\nFor error logging, I simply invoke the .fail() callback of the JSON requests to the two APIs (geoPlugin and Weather Unlocked). In case of an error, the message is simply written to the console, but you can also invoke a dataLayer.push() to log the error elsewhere, such as Google Analytics.\n3.1. Geolocation There are many ways to do geolocation, but for usability\u0026rsquo;s sake I use an API call for this as well. If you\u0026rsquo;re serious about this solution, you might want to install a geolocation service on your own web server, so that you\u0026rsquo;ll avoid needing to make any extra API calls in the client.\nGeolocation hinges on one thing: the visitor\u0026rsquo;s IP address. There are many ways to get this data, as it is public information, exposed in the HTTP headers of the visitor\u0026rsquo;s requests. You might have guessed at this point that the accuracy of the solution is thus dependent on what type of proxies or VPNs the visitor might be using. This is something we will not be able to work around, and thus you will need to take the geolocation data, as any other data in the public web, with a grain of salt.\nWhat we\u0026rsquo;ll need back from the geolocation service are the latitude and longitude of the visitor. IP-based geolocation is usually accurate down to the city the visitor is from, but in many cases it might be a much broader spectrum, such as an entire state or even country. Nevertheless, we\u0026rsquo;ll take what we can get.\nThe method in the script uses an all-in-one API for both getting the IP address and retrieving the longitude and latitude of the visit. The service is called geoPlugin, and the SSL plan is fairly inexpensive.\nvar geoLocateUser = function() { // This is your API key for GeoPlugin, which you can purchase at  // http://www.geoplugin.com/premium#ssl_access_per_year  var geoPluginAPIKey = \u0026#39;paste_your_api_key_here\u0026#39;; jQuery.getJSON(\u0026#39;https://ssl.geoplugin.net/json.gp?k=\u0026#39; + geoPluginAPIKey + \u0026#39;\u0026amp;jsoncallback=?\u0026#39;) .done(function(data) { fetchWeatherData(data.geoplugin_longitude, data.geoplugin_latitude); }).fail(function(jq, status, msg) { console.log(\u0026#39;Geolocation failed: \u0026#39; + status + \u0026#39; - \u0026#39; + msg); }); };  In this method, you first need to write your API key into the geoPluginAPIKey variable. You\u0026rsquo;ll get this API key by subscribing to a very cheap plan at geoPlugin. Next, we use the JSONP endpoint of the geoPlugin API to fetch the geolocation data for the user. It\u0026rsquo;s all wrapped in jQuery\u0026rsquo;s extremely useful getJSON() method.\nThe asynchronous request is supplemented with the done() callback, which is invoked if everything goes well. In this callback, we pass the longitude and latitude information to the next function in the chain: fetchWeatherData().\nIn case of an error, the error information is output into the browser console.\n3.2. Building the API call So now we\u0026rsquo;ve got geolocation down. The next thing we need to do is use this information to poll for weather data.\nI use the Weather Unlocked service for this. It has a pretty generous free plan, and it supports both SSL and non-SSL requests. (UPDATE: Looks like Weather Unlocked Developer API for SSL requests requires a small subscription fee now.)\nOnce you\u0026rsquo;ve signed up to the service, you will find your APP ID and APP KEY in the dashboard.\n  The fetchWeatherData() method itself looks like this:\nvar fetchWeatherData = function(longitude, latitude) { // You need to sign up to Weather Unlocked for a free account. Once you do,  // you will find your APP ID and APP KEY in your account dashboard.  var wuAppId = \u0026#39;paste_your_weather_unlocked_app_id_here\u0026#39;; var wuAppKey = \u0026#39;paste_your_weather_unlocked_app_key_here\u0026#39;; jQuery.getJSON(\u0026#39;https://api.weatherunlocked.com/api/current/\u0026#39; + latitude + \u0026#39;,\u0026#39; + longitude + \u0026#39;?app_id=\u0026#39; + wuAppId + \u0026#39;\u0026amp;app_key=\u0026#39; + wuAppKey) .done(function(data) { window.dataLayer.push({ event: \u0026#39;weatherDone\u0026#39;, weather: data.wx_desc, temperature: data.temp_c }); }).fail(function(jq, status, msg) { console.log(\u0026#39;Weather request failed: \u0026#39; + status + \u0026#39; - \u0026#39; + msg); }); };  It\u0026rsquo;s fairly similar to the geolocation method. We use the latitude and longitude information to query the Weather Unlocked API. You\u0026rsquo;ll need your APP ID and APP KEY handy for this!\nUpon a successful request, the response is parsed for the description of the current weather (data.wx_desc) as well as the current temperature (data.temp_c). These are added to the dataLayer object under their respective keys.\nNote that all Ajax calls are made using jQuery. Now, the benefits and pitfalls of being dependent on an external, bloated framework like jQuery can be debated, but jQuery handles Ajax requests beautifully. It\u0026rsquo;s perfect for what we\u0026rsquo;re trying to achieve here, so I don\u0026rsquo;t mind using the framework.\nAnd that\u0026rsquo;s about it! The solution is fairly simple, relying on fairly simple, chained asynchronous requests. If one fails, then the rest doesn\u0026rsquo;t get executed, so you might want to add some error handling if you want to know what your success rate is. But since we\u0026rsquo;re using session-scoped Custom Dimensions, you can already get a success rate by looking at how many sessions have weather data out of all sessions.\nJust one thing remaining.\n3.3. Maintaining state with a cookie The final bit of the puzzle is to fire the tag just once per session. We do this by only starting the whole process if a custom 1st party cookie has not been set. The {{Session alive}} Variable returns undefined if the cookie is not set, so we can check against this before executing the first step (the geolocation).\nif (typeof {{Session alive}} === \u0026#39;undefined\u0026#39;) { ...the geolocation call... } // Reset \u0026#34;session\u0026#34; cookie with a 30-minute expiration  var d = new Date(); d.setTime(d.getTime()+1800000); var expires = \u0026#34;expires=\u0026#34;+d.toGMTString(); document.cookie = \u0026#34;session=1; \u0026#34;+expires+\u0026#34;; path=/\u0026#34;;  At the end of the script, regardless of whether the cookie was set or not, we create / update the cookie with a new 30-minute expiration.\nAnd that\u0026rsquo;s it. Easy, right?\nRemember to test it carefully in Preview mode. Check the JavaScript console for any errors, test that the cookie logic works, and monitor Real-Time reports in Google Analytics to verify that data is flowing in.\n4. Summary So this is the weather script for Google Tag Manager V2. Feel free to swap the Weather Unlocked API with any other API endpoint you might want to query. You will need to study the API, modify the parameters, and parse the data differently, but the logic is the same.\nIt would be cool to know if weather analysis has brought you any insights. A great idea for modifying the script would be to not just track today\u0026rsquo;s weather, but the weather next weekend or over a holiday. Does a warm Easter contribute to fewer bookings in the ski slopes, and does a rainy weekend ahead make green fee sales plummet at the local golf course?\n"
},
{
	"uri": "https://www.simoahava.com/tags/v2/",
	"title": "v2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/weather/",
	"title": "weather",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-url-fragments-as-pageviews/",
	"title": "#GTMtips: Track URL Fragments As Pageviews",
	"tags": ["Google Tag Manager", "gtmtips", "history change trigger", "history listener"],
	"description": "How to track dynamic changes to the page URL, especially URL fragments, as individual page views in Google Analytics. The guide is for Google Tag Manager.",
	"content": "(Updated 15 July 2015: Added a huge simplification. Jump straight to the update at the end.)\nURL fragments are strings of characters that follow a hash mark (#) in the URL. Typically, they are used for anchor links, where following a link keeps you on the same page but jumps the browser to some anchored position. They\u0026rsquo;re also the tool of choice for single-page apps, where content is served dynamically without page reloads.\nIn Google Analytics, fragment changes are not tracked by default, and the URL paths that are passed to GA with your Pageview hits are stripped of these fragments. With Google Tag Manager, this can be remedied with a History Change Trigger and some Variable magic.\nTip 18: Track URL fragments as Pageviews   When the URL fragment of the page changes, it doesn\u0026rsquo;t cause a reload. This is problematic for your Analytics tracking, as you probably have the All Pages Trigger firing your Page View Tag, and this Trigger only fires once per page load.\nHowever, when the URL fragment of a page changes, what happens behind the scenes is interesting: a browser history event is dispatched, and this can be hooked onto. Google Tag Manager provides the perfect way to do it with the History Change Trigger. It activates when it captures an event dispatched by a the URL fragment change.\nSo, to track fragment changes, all we need is the History Trigger and some Variables, so that we can get a nice, lean, flexible setup running.\n1. The Variables First, head on over to the Variables section of Google Tag Manager, and make sure you\u0026rsquo;ve checked the New History Fragment and History Source Built-In Variables.\n  New History Fragment stores the new URL fragment when it changes, and History Source will let us specify that we don\u0026rsquo;t the tag to fire on all browser history events; just the one that signals a fragment change.\nNext, you\u0026rsquo;ll need a Custom JavaScript Variable that will produce a new, well-formed URI that we can then send to Google Analytics as the page path of the URL fragment. So, create a new Custom JS Variable, name it {{get path with fragment}}, and add the following code within:\nfunction() { return {{Event}} === \u0026#39;gtm.historyChange\u0026#39; \u0026amp;\u0026amp; {{New History Fragment}} ? {{Page Path}} + \u0026#39;#\u0026#39; + {{New History Fragment}} : undefined; }  This simple, one-liner JavaScript function checks first if the Event that invoked this Variable was a history change. If it was and the URL fragment isn\u0026rsquo;t empty, the next thing it does is return the current page path (e.g. /home/) and concatenates the new URL fragment to the string, separating the two with the \u0026lsquo;#\u0026rsquo; symbol (e.g. /home/#contact-us). If the Event was not a history change, the Variable returns the undefined value to ensure that the default path is sent with the Pageview.\nThe reason we\u0026rsquo;re sending the Page Path with the \u0026lsquo;#\u0026rsquo; symbol in place is because we want to enable the direct link \u0026ldquo;Visit this page\u0026rdquo; functionality in Google Analytics reports. Thanks to Stéphane Hamel and Phil Pearce for this tip.\n  2. The Trigger Create a new Trigger, and name it Event - History Fragment Change.\nChoose History Change as the Event, and set the following condition in the Fire On section of the Trigger:\nHistory Source equals popstate\n  We\u0026rsquo;re using popstate as a condition, because we don\u0026rsquo;t want this Trigger to go off on other instances where the History Change Trigger can fire. If you don\u0026rsquo;t have an AJAX or a single-page site, you wouldn\u0026rsquo;t have to worry about this, but this is a nice way of delimiting a rather generic Trigger to only fire in relevant situations.\n3. The Tag Finally, take your regular Page View Tag, and add the Trigger from (3) to it. This means that the Page View Tag will now have multiple Triggers. Most likely, you\u0026rsquo;ve used the All Pages Trigger, so now the Tag will fire both when All Pages matches and when a browser history event is detected.\nNext, scroll down to More Settings -\u0026gt; Fields To Set. Add a new field, name it page, and set its value to {{get path with fragment}}.\n  Save the Tag and start testing.\nHow it works When the page loads, the All Pages Trigger fires your Page View Tag. At this point, GTM will look at the page field and find the {{get path with fragment}} Variable there. It proceeds to execute the JavaScript within. However, the one-liner JavaScript only works if the value of the {{Event}} Variable is gtm.historyChange, which is not the case when the All Pages Trigger fires ({{Event}} is gtm.js in that case). So, the Variable returns undefined, which ensures that the regular Document Path is sent with the Tag instead.\nWhen someone clicks on a link which only has an anchor in the href, such as \u0026lt;a href=\u0026quot;#contact-us\u0026quot;\u0026gt;Contact Us\u0026lt;/a\u0026gt;, the browser dispatches the history change event, and it\u0026rsquo;s picked up by the History Change Trigger you created. This causes the Page View Tag to fire again.\nThis time, as GTM executes the JavaScript code again, the value of {{Event}} resolves to gtm.historyChange, and the Variable returns a string where the URL path of the page, e.g. /home/, is concatenated with the fragment text #contact-us, resulting in /home/#contact-us.\nThings to keep in mind If your site is an AJAX or single-page site, there might be other cases where popstate is dispatched. If this is the situation, you will need to add an additional check into the Custom JavaScript Variable, which compares the Built-In Variables for New History Fragment and Old History Fragment. If they are the same, then it means that the fragment didn\u0026rsquo;t change and popstate was dispatched for some other reason. In this case, the Variable needs to return undefined as well.\nYou might want to edit the Custom JavaScript Variable to check for the URL fragment when the page is loaded as well, so that direct visits to a URL fragment will be recorded accordingly in Google Analytics. This would require something like the following:\nfunction() { return ({{Event}} === \u0026#39;gtm.js\u0026#39; || {{Event}} === \u0026#39;gtm.historyChange\u0026#39;) \u0026amp;\u0026amp; {{New History Fragment}} ? {{Page Path}} + \u0026#39;#\u0026#39; + {{New History Fragment}} : undefined; }  Let me know if you have other issues with this solution. For most use cases, this should work just fine.\nUPDATE The solution above has some problems. First, it doesn\u0026rsquo;t include a possible query string in the returned path. Also, if the page is loaded, i.e. there\u0026rsquo;s no history change, the hash isn\u0026rsquo;t included in the returned path.\nSo, a huge simplification is called for. The {{get path with fragment}} should actually look like this:\nfunction() { return window.location.pathname + window.location.search + window.location.hash; }  Then you can just have your Page View Tag fire on both a Page View Trigger as well as the History Change Trigger. This Variable, once added to the page field, will return the path, the query string (if there is one) and the URL hash (if there is one).\nSometimes I think of way too complicated solutions for simple problems :-)\n"
},
{
	"uri": "https://www.simoahava.com/tags/history-change-trigger/",
	"title": "history change trigger",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/history-listener/",
	"title": "history listener",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/metrics/",
	"title": "metrics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/schema/",
	"title": "schema",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/sessions/",
	"title": "sessions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/the-schema-conspiracy/",
	"title": "The Schema Conspiracy",
	"tags": ["google analytics", "metrics", "schema", "sessions"],
	"description": "There are some problems with Google Analytics&#39; sessionization schema. I outline them in this article.",
	"content": "A schema is something that data processing platforms such as Google Analytics apply to the raw hit data coming in from the data source (usually a website). The most visible aspect of Google Analytics\u0026rsquo; schema is how it groups, or stitches, the arbitrary, hit-level data coming in from the website into discrete sessions, and these are actually grouped under yet another aggregate bucket: users.\nBut you already know this. You\u0026rsquo;re looking at metrics like Sessions, Bounce Rate, Conversion Rate, and you\u0026rsquo;re using them or variations of them as KPIs in your dashboards and whatnot. Right?\nTake a look at the group of metrics below:\n  These are some of the go-to metrics people use to assign meaning to the data stream coming in from the website. The thing about these metrics is that they are very heavily sessionized. They are entirely dependent on an arbitrary schema, which many fail to understand or to even question. Change the definition of a session even a little, and every single one of these metrics will have a different value.\nAnd herein lies the problem I now dub the Schema Conspiracy. I know, I know, it\u0026rsquo;s a tad dramatic. But the implications are dramatic as well.\nWhen you use Google Analytics, or any schema-applying data processing platform, you are subscribing to the schema imposed by the platform. You don\u0026rsquo;t have a say in it. In GA, you can make minor changes to the definition of a session, using tools like Session timeout and Referral Exclusion List, but the fact remains that the schema for sessions in Google Analytics remains universal, generic, and completely arbitrary; three qualities that should not exist when using data to optimize for business growth.\n  In Google Analytics, a session can be defined roughly as an uninterrupted browsing experience, which expires after 30 minutes of inactivity. So you enter a website, do stuff there, and 30 minutes after the last interaction the session expires. Naturally, it\u0026rsquo;s more complex than this, but as a rough description this should suffice.\nFurther reading: How a session is defined in Analytics\nNow ask yourself this: how does this mirror anything that happens in the real world? Not really, right? Shouldn\u0026rsquo;t the concept of a session be grounded in something less ephemeral than a completely arbitrary sequence of hits on the website, combined with a strange, inexplicable 30 minute timeout?\nYou might not see the relevance of any of this, and you might be completely satisfied with Google Analytics\u0026rsquo; concept of a session, and you are, of course, entitled to this.\nBut consider Conversion Rate, for example. Conversion Rate is the ratio of sessions with a conversion to all sessions. Sessions, sessions, sessions. If you\u0026rsquo;re using Conversion Rate as a KPI, you must realize you\u0026rsquo;re optimizing against a completely fictitious metric.\nThink of it like this. You might need 14 sessions to convert when buying a new boat. You might need only 6 sessions to convert when buying a new computer. But in the end you\u0026rsquo;re still just one user that converted, regardless of the number of sessions it took to do so. The key here is that you had a singular intent: to buy a boat or a new computer. This intent spanned a number of sessions, highlighting the disconnect between sessions and behavior even more.\n  I think this is very problematic indeed. Companies optimize against a metric that is very superficial and ephemeral, and completely unrelated to the intent of the visitor. You shouldn\u0026rsquo;t be interested in the number of sessions that converted, you should be interested in increasing the number of customers you have, by understanding intent and nurturing it into a purchase.\nNow, I\u0026rsquo;m cynical enough to see the justification for this arbitrary sessionization: granularity of attribution. That\u0026rsquo;s why a change in campaign source initiates a new session, even if the session hasn\u0026rsquo;t expired yet. Your advertising channels need the attribution for successful conversions, which is why this sessionization logic has been honed to give a nice, big, fat number for your acquisition metrics.\nDon\u0026rsquo;t get me wrong, I think it\u0026rsquo;s valuable to see all the channels that turned a non-converted user into a new customer. But the reality is that sessions don\u0026rsquo;t convert, users do. Attribution, too, should be balanced between the touch-points that led me to fulfil some intent I had. Following an ad starts a new session on the website, but my intent might be the same as before. The ad might have made the intent more targeted, more specific, but I\u0026rsquo;m still very much a single user on the path to conversion.\nOvercoming the problem at hand You\u0026rsquo;re pretty much out of luck if you want to apply your own schema to your Google Analytics data. Even though Google Analytics Premium boasts hit-level data through BigQuery, it\u0026rsquo;s still sessionized. The data tables stitch the hit-level data into sessions before you can access the data. This, I think, sucks big time.\n(UPDATE: Check Carmen Mardiros\u0026rsquo; comment and Pedro Avila\u0026rsquo;s comment in the comments of this article for workarounds to getting hit-level data through the API and BigQuery.)\nI get why the UI shows a sessionized data set, as applying your own, complex sessionization schema would require an astounding amount of processing power. But why not provide raw data through the API?\nSo, there\u0026rsquo;s nothing you can do with GA\u0026rsquo;s schema. That\u0026rsquo;s just how it is. You can\u0026rsquo;t even see proper user-level data, either, since that\u0026rsquo;s sessionized as well. Consider the following Custom Segment:\n  It looks like it should show data for all users that have converted at some point in the past, right? Right. And wrong.\nThe segment above shows me a cohort of users who have converted during the selected timeframe. But that\u0026rsquo;s not what I should be interested in. I should be able to segment between converted and not-converted visitors, regardless of the timeframe!\nNo, a user-scoped Custom Dimension won\u0026rsquo;t help either, as if I\u0026rsquo;m looking at a timeframe before the user converted, it will show me the user as a non-converter.\nThings like this drive me crazy. If I had access to raw, hit-level data, and if I could build my own stitching schema on top of that, I would be able to bend the processing and reporting aspects of GA to my will, improving the quality of data for my business alone. That\u0026rsquo;s what my dashboards should be showing! That\u0026rsquo;s what should be driving my business!\nFinal thoughts So what is a perfect schema? There\u0026rsquo;s no such thing. Just as each business is different, each schema should be different as well.\nOptimally, the schema should be a living thing, constantly in flux, because your visitors are living things, constantly in flux. An intelligent schema would mirror this, perhaps even learning autonomously along the way.\nOptimally, the schema wouldn\u0026rsquo;t be satisfied with just your website data. Your visitors are multi-dimensional, so the schema should be multi-dimensional as well.\nOptimally, the schema would let you optimize against metrics that are relevant for your business, and for your business alone. Your visitors are your business, so the schema should be optimized against visitors as well.\nFinally, the schema used by Google Analytics is perfectly fine. Just don\u0026rsquo;t interpret it as something it\u0026rsquo;s not. Google Analytics\u0026rsquo; sessionization does not reflect the real world, the Conversion Rate metric should not be an indicator of the state of your business, and completely sessionized metrics like Bounce Rate, Session Duration, etc. should never be used as KPIs alone.\nUsing a single, sessionized, flawed metric as a KPI is like only telling the punchline of a joke.\nFurther reading: Avinash Kaushik - Excellent Analytics Tip #26: Every Critical Metric Should Have A BFF!\nThere are tools out there that bridge the gap between Business Intelligence and web analytics. They let you build the interpretations for your raw data in any way you choose. It does require effort, however. A custom schema requires that you understand your audience behavior on a completely new level.\nFurther reading: Snowplow Analytics\nDo you agree with me? Or do you think I\u0026rsquo;m making mountains out of molehills? I\u0026rsquo;m not advocating for an upheaval of how these tools work, but I am campaigning very strongly for critical thinking.\nSo, the next time you use Google Analytics\u0026rsquo; Conversion Rate metric for anything, just pause for a second and think about what this metric means for your business. Try to come up with a sentence like: \u0026ldquo;The uplift we\u0026rsquo;re seeing in Conversion Rate means our business is\u0026hellip;\u0026rdquo; and then finish with what the change in Conversion Rate means for your business.\nYou might find it\u0026rsquo;s quite difficult.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/add-the-event-key-to-datalayer-pushes/",
	"title": "#GTMtips: Add The &#34;event&#34; Key To dataLayer Pushes",
	"tags": ["events", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "Quick Google Tag Manager tip for adding the event key to all dataLayer push commands.",
	"content": "In Google Tag Manager, every single Tag requires a Trigger to fire. Every single Trigger requires an Event condition to activate. Sometimes, these Event conditions are obfuscated under template semantics, but you can also create a Custom Event Trigger, where you specify the value of the \u0026lsquo;event\u0026rsquo; key in dataLayer that fires your tag. You can read more about the relationship between GTM events and Tags in these two posts:\n  #GTMtips: Rules In A Nutshell\n  Trigger Guide For Google Tag Manager\n  The key takeaway is that only an \u0026lsquo;event\u0026rsquo; key push into dataLayer has the power to fire a Tag. So, the tip of this post is to (always) include the \u0026lsquo;event\u0026rsquo; key when you use the push() method of dataLayer.\nTip 17: Add the \u0026lsquo;event\u0026rsquo; key to your dataLayer pushes   Now, the reason I suggested to always have the key in each dataLayer.push() is because there\u0026rsquo;s no way to fire your Tags if there\u0026rsquo;s no \u0026lsquo;event\u0026rsquo; key present. In Enhanced Ecommerce, for example, this is critical. Each push() into the dataLayer with an Enhanced Ecommerce payload exists only until the next Enhanced Ecommerce payload push(). If you don\u0026rsquo;t have a Tag firing with an Enhanced Ecommerce push, you will not be able to access this particular state of dataLayer later, if you\u0026rsquo;ve already managed to push another \u0026lsquo;ecommerce\u0026rsquo; object. To put it into perspective, take a look at the following code:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;impressions\u0026#39; : [{ \u0026#39;name\u0026#39; : \u0026#39;product1\u0026#39;, \u0026#39;id\u0026#39; : \u0026#39;12345\u0026#39; }] } }); // Some code dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;impressions\u0026#39; : [{ \u0026#39;name\u0026#39;: \u0026#39;product2\u0026#39;, \u0026#39;id\u0026#39; : \u0026#39;23456\u0026#39; }] }, \u0026#39;event\u0026#39; : \u0026#39;impressionsPushed\u0026#39; });  So you might think that having a Tag fire when Event equals impressionsPushed would send both impression objects (product1 and product2) to GA, but you\u0026rsquo;re wrong. Only the second push is processed. That\u0026rsquo;s why it\u0026rsquo;s important to have an \u0026lsquo;event\u0026rsquo; key in the first push as well. Or, it would be even better to combine these into a single push, but there might always be technical reasons why this isn\u0026rsquo;t possible.\nSo that\u0026rsquo;s key takeaway 1:\nAn \u0026lsquo;event\u0026rsquo; key in a push ensures that you can access the state of dataLayer at the time of the push in your tags.\nThe second thing I want to show you might be surprising to some. You can actually add the \u0026lsquo;event\u0026rsquo; key to pre-container-snippet pushes as well! This means that you can have Tags fire before the Pageview event, i.e. gtm.js.\nThis is because GTM processes the past states of dataLayer as well, which have been defined before the container snippet started to process the data structure.\nIf you take a look at the #GTMtips picture for this post, you can see an example of this. I have a dataLayer.push() before the container snippet, and it also includes the \u0026lsquo;event\u0026rsquo; key with value loggedIn.\nAs you can see in the debug panel, this Event is processed before the Pageview event in the message bus, meaning that the execution of any Tags that have Event equals loggedIn as their Trigger would start before Tags that fire on the All Pages Trigger, for example.\nSo, key takeaway 2: Pre-container-snippet \u0026lsquo;event\u0026rsquo; pushes can fire Tags before the Pageview Event.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/matches-css-selector-operator-in-gtm-triggers/",
	"title": "&#34;Matches CSS Selector&#34; Operator In GTM Triggers",
	"tags": ["auto-event tracking", "Google Tag Manager", "Guide", "triggers"],
	"description": "How the Matches CSS Selector operator works in Google Tag Manager triggers.",
	"content": "Be honest, can you think of anything that\u0026rsquo;s more unfair than this:\n  A new Google Tag Manager feature, published at 02:07 AM my time, and with an easter egg hunt involved?! Of course it was the infuriating Charles Farina who found the new feature and claimed the prize. Curses! (Just kidding Charles, you\u0026rsquo;re still awesome.)\n  Anyway, there\u0026rsquo;s a new GTM feature in town, and oh boy, this time it\u0026rsquo;s a big\u0026rsquo;un! Without further ado, allow me to introduce a new Trigger operator - the matches CSS selector:\n  It\u0026rsquo;s modus operandi is quite simple. You create a Trigger condition, where an HTML element is checked against a CSS selector. If the CSS selector applies to the HTML element, the condition passes.\nCSS selectors and GTM CSS selectors are patterns that you check for in any given HTML element. Traditionally, CSS selectors have been used to modify styles of the given elements. Nowadays, we can actually use them to target HTML elements for other purposes as well, especially since frameworks such as jQuery, which target elements using CSS selectors, have become ubiquitous. There are also the vanilla JavaScript querySelector(), querySelectorAll() and matches() DOM methods that allow you to pick elements based on CSS selectors.\nFor a nice list of currently supported CSS3 selectors, check out the W3Schools guide: CSS Selector Reference.\nHere are some CSS selectors you might find very useful. These are all auto-event tracking related, because the possibilities of auto-event tracking just opened up in a completely new way with the introduction of this new Trigger operator.\n  Instead of targeting Click ID or Form Class, you must now always provide an HTML element to the Trigger condition. So, you\u0026rsquo;ll need to use Click Element or Form Element (they\u0026rsquo;re the same thing) to pattern-match against your auto-event target element.\n   Selector Description     .thisclass Matches if element has class “thisclass”   .thisclass.thatclass Matches if element has class “thisclass” and class “thatclass”   #thisid Matches if element has ID “thisid”   #main .navlink Matches if element has class “navlink” and is a descendant of an element with the ID “main”   div#main \u0026gt; .navlink Matches if element has class “navlink” and is the direct child of a DIV element with the ID “main”   :checked Matches if element is checked (radio button or checkbox)   [data-title*=\u0026quot;chairman mao\u0026rdquo;] Matches if element has attribute “data-title” with the string “chairman mao” somewhere in its value   a[href$=\u0026rdquo;.pdf\u0026rdquo;] Matches if element is a link (A) with a HREF attribute that ends with “.pdf”   .contactmail:only-child Matches if element has class “contactmail” and is the only child of its parent    As you can see, you can do pretty creative stuff with it. The most significant asset is, by far, the chance to see ancestral relationships. You can now check if the element that was clicked or submitted is the child or direct descendant of any given element. What an ingenious way to fire tags only on clicks under the main navigation, for example!\nRemember that you can add multiple selectors just as with CSS by using a comma between the selectors:\nClick Element - matches CSS selector - video,video *\nThis condition would match clicks on a HTML5 \u0026lt;video\u0026gt; element or any of its descendants.\nYou\u0026rsquo;re limited by CSS3, mainly. You can\u0026rsquo;t do descendant checks, so you can\u0026rsquo;t have a CSS selector which only matches an element if it has a specific element as its child. You\u0026rsquo;ll still need JavaScript for this, unfortunately.\nLuckily, CSS4 is already pretty far in its draft stages. It brings a slew of amazing new features, which will only make this CSS selector Trigger even more powerful.\nTechnical details Just to wrap this post up, here\u0026rsquo;s a technical description of the new operator (thanks Brian Kuhn!). There\u0026rsquo;s nothing revolutionary about it, proving how GTM still leverages well-founded practices rather than inventing the wheel again each time.\nThe operator uses the matches() method with variations depending on which browser you\u0026rsquo;re using. The matches() method lets you check if a given element matches against a CSS selector:\nelement.matches(\u0026#39;.thisclass\u0026#39;);  The code above would evaluate to true if the given element has the class \u0026ldquo;thisclass\u0026rdquo;.\nmatches() isn\u0026rsquo;t supported by all browsers, and e.g. Internet Explorer only supports it at version 9.0. For antiquated browsers, GTM falls back to checking each node that matches a given selector, and it returns true if the given element is among these nodes.\n"
},
{
	"uri": "https://www.simoahava.com/tags/auto-event-tracking/",
	"title": "auto-event tracking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/fields-to-set/",
	"title": "fields to set",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/leverage-usebeacon-beforeunload-google-analytics/",
	"title": "Leverage useBeacon And beforeunload In Google Analytics",
	"tags": ["fields to set", "Google Tag Manager", "Guide", "JavaScript"],
	"description": "Leverage useBeacon and beforeunload to fire Google Tag Manager events to Google Analytics just as the user is about to leave the page or close the browser.",
	"content": "This nifty little solution will let you calculate the time spent on pages that are not tracked in Google Analytics by default. These include both bounced landing pages and exit pages. Bounced pages and exit pages lack the necessary subsequent pageview, which Google Analytics uses to calculate time-based metrics.\nBefore you go on, read this excellent article by Yehoshua Coren:\nREAL Time On Page in Google Analytics\nYehoshua gives a very nice use case for the technical solution I\u0026rsquo;m about to explore. He also leverages the Page Visibility API to get an even more accurate overview of visitors who actually digest content, and how much of that time that content is visible on their screens. Fundamental stuff, read it!\nSo what we\u0026rsquo;re actually doing here is this:\n  On each page, use the default gtm.start Data Layer Variable to calculate the time when the document has started loading\n  When the user decides to leave the page, either by closing the browser or navigating to another page, first calculate the time when the beforeunload event is dispatched. Then, use the sendBeacon() API to send a User Timing hit to Google Analytics without having to worry about the unload process cutting the request short.\n  Hopefully, you\u0026rsquo;ll end up with data like this:\n  Here you can see the User Timings recorded for pageviews of bounced sessions.\nHere\u0026rsquo;s another report:\n  In this report, you can view Exit Pages with the time spent on each page as a secondary dimension.\nNaturally, this data would be far more useful when extracted out of the GA interface into a spreadsheet, where you can actually make calculations with the Custom Dimension values, for example. Hopefully, at some point, we\u0026rsquo;ll have the possibility to calculate our own Custom Metrics, at which point it will make more sense to send this information as a metric instead. Also, read Yehoshua\u0026rsquo;s article I linked to in the beginning of the post. He uses Custom Metrics, which makes actually a lot more sense if you want to extract the data.\nNevertheless, until such a time that we can calculate on metrics, this is a useful method for obtaining a more accurate time on page across your sessions (so also for bounces and exit pages).\nTo get it working, you\u0026rsquo;ll need the following components:\n  New Custom Dimension, session-scoped, to capture this information in GA\n  Data Layer Variable to capture the value for gtm.start\n  Data Layer Variable to store the time on page\n  Custom HTML Tag, which sets the beforeunload listener and does the dataLayer.push() when the page unload begins.\n  Timing Tag which uses the quite new useBeacon field available in Universal Analytics. This field is basically a helper for setting up the sendBeacon() request.\n  1. Custom Dimension Let\u0026rsquo;s start with the Custom Dimension. We\u0026rsquo;re using a session-scoped Custom Dimension for one simple reason: it will always have the last value sent during the session. This means that since we\u0026rsquo;re sending the custom time on page on every single pageview, the session-scoped Custom Dimension should always have the exit page time for the session! Once we have this in place, we can add the custom dimension as a secondary dimension in the Exit Page report, giving us the dwell time for exit pages only.\nNote that the session-scoped Custom Dimension fails if the user is inactive long enough for the session to expire (30 minutes by default). So it might actually be a good idea to modify the timing script to only allow values up to 1800000 milliseconds (30 minutes).\n  Make note of the index the new Custom Dimension gets. This is important when you\u0026rsquo;re setting up the Event Tag.\n2. Data Layer Variable for gtm.start The next step is to create the Data Layer Variable for gtm.start. You might wonder what this \u0026ldquo;gtm.start\u0026rdquo; is, but it\u0026rsquo;s actually a property in the very first dataLayer object pushed into the Array by GTM, when the container snippet starts loading:\n  The value for this variable is a timestamp in milliseconds of Epoch time. You don\u0026rsquo;t have to worry about what this means, since all we\u0026rsquo;re going to use this for is to calculate the difference between page unload time and gtm.start to get an approximation of how long the user spent on the page. The Data Layer Variable would look like this:\n  3. Data Layer Variable for timeonpage You\u0026rsquo;ll also need to create a Data Layer Variable for timeonpage, which is where we\u0026rsquo;ll store the time on page, pushed in the beforeunload callback. So create another Data Layer Variable that looks like this:\n  4. Custom HTML Tag The next step is our custom beforeunload listener. Create a new Custom HTML Tag, and set it to fire with the All Pages Trigger. Add the following code within:\n\u0026lt;script\u0026gt; window.addEventListener(\u0026#39;beforeunload\u0026#39;, function() { window.dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;unloadEvent\u0026#39;, \u0026#39;timeonpage\u0026#39; : new Date().getTime() - {{DLV - gtm.start}} }); }); \u0026lt;/script\u0026gt; This attaches the beforeunload listener to the global window object. When the unload process begins, i.e. when the user chooses to leave the page, the callback is invoked, and a dataLayer.push() is executed, with a custom \u0026lsquo;event\u0026rsquo; value and also a value for the \u0026lsquo;timeonpage\u0026rsquo; Variable we just created.\nThe time on page is calculated by getting the timestamp for the beforeunload event, and subtracting the gtm.start timestamp from it. The result is the time in milliseconds between these two events. If you want to have a safeguard for session expiration, cap this time at 1800000 milliseconds, which is 30 minutes.\nNow all we need is the Timing Tag, which sends the timeonpage value both as a Timing value and as a Custom Dimension value.\n5. User Timing Tag User Timing is a hit type that you can use to send your own timing events to Google Analytics. A common use case is to measure the load time of linked assets, such as huge, bloated JavaScript libraries (I\u0026rsquo;m looking at you, non-minified jQuery!).\nBut you can use User Timings for anything on the site that can be measured in milliseconds. So it works perfectly with page load time as well.\nBefore you create the Tag, you\u0026rsquo;ll need the Trigger that makes the Tag fire. The Trigger is simply a Custom Event Trigger, that fires with event name unloadEvent:\n  Next, create a new Google Analytics / Universal Analytics Tag, attach the Trigger you just created to it, and set the Tag fields to look something like this:\n  If you were to save this now and publish your container, the solution would be very unreliable. This is because the beforeunload event signals the browser to start the unload process, and the unload process is brutal. Any threads that are running once the browser reaches the unload stage are cut off, and all requests are cancelled. This is because the browser doesn\u0026rsquo;t want to let anything impede the user\u0026rsquo;s desire to leave the page.\nPerfectly understandable.\nIt is for this reason that we\u0026rsquo;ll leverage yet another little-known API: navigator.sendBeacon(). This API turns any request made in its scope into an asynchronous, uninterruptible call to whatever endpoint you choose. So, even if the browser window closes or you navigate from the site, the request is allowed to complete before the browser instance is unloaded from memory.\nGoogle Analytics were quick to react to this API, and they published their own shorthand for it: the useBeacon field. Read David Vallejo\u0026rsquo;s nice review about this new feature to get acquainted.\n!!! UPDATE !!! The useBeacon has been deprecated. Use the transport field name instead, and set its value to beacon. Read about the field here.\nAnyway, useBeacon turns the call into a POST request (instead of the usual GET), and passes it asynchronously to the GA endpoint using navigator.sendBeacon().\nTo add this feature to the User Timing Tag, add useBeacon as a Field To Set, and set its value to true.\nAlso, add the {{DLV - timeonpage}} as a Custom Dimension, using the index number you got from Step 1.\nSo now the More Settings of your User Timing Tag should look like this:\n  Save the Tag, Preview \u0026amp; Debug the Tag, and Publish when you\u0026rsquo;re ready.\nThen read the caveats below. Or actually, it would be good if you read these before you publish.\nCaveats There is actually only one major caveat here. navigator.sendBeacon() has horrible browser support. Like, dismal. Basically, Internet Explorer and Safari do not support it all. This is a big setback, as IE is one of the most popular desktop browsers, and Safari is among the most popular mobile browsers.\nThe thing is, you don\u0026rsquo;t really have to write any fallback functions for browsers that don\u0026rsquo;t support the API. The Universal Analytics library detects if navigator.sendBeacon() is supported, and if it isn\u0026rsquo;t, the hit is sent normally.\nDepending on your site, there\u0026rsquo;s still a good chance that the hit gets sent. This depends on how long it takes for the browser to unload your site.\nIf you want to play it safer, you could write your own click handler that intercepts external links, fires the Timing Tag, and only then lets the link redirect proceed. This would cover exits from your site to other sites nicely, but it wouldn\u0026rsquo;t help with the most interesting use case of people closing browser windows and tabs.\nThis is a problem we\u0026rsquo;ll just have to live with. However, as an eternal optimist, I see this as a solution that can only get better with time. Once navigator.sendBeacon() gets better support and once we get calculated Custom Metrics, this solution will be so awesome.\nRight now it\u0026rsquo;s more of a prototype, but as Yehoshua shows you, it can already have very interesting analytics applications.\nWell, I\u0026rsquo;d be remiss if I didn\u0026rsquo;t mention one other, small caveat. Triggering code on beforeunload invalidates the back-forward cache (BFCache) in Firefox. This means that the page state of the page is no longer cached, and if you\u0026rsquo;re trusting e.g. form field values to this cache, you\u0026rsquo;ll need to make adjustments.\nSummary In this quite simple solution we\u0026rsquo;re leveraging some pretty cool APIs again. The point is partly to give you a tool to get better data out of Google Analytics, but at the same time we\u0026rsquo;re doing what I love best: using JavaScript and Google Tag Manager to get data from unexpected places.\nAs I say in the previous chapter, this solution will only get better with time.\nHopefully, once navigator.sendBeacon() get better browser support, it will become the default request mechanism for all Google Analytics hits. It just makes so much sense. Also, it means that you won\u0026rsquo;t need to protect your tags with setTimeout() calls or the Wait For Tags method in Google Tag Manager.\nBut from the look of things, this is still a long way off.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/setting-google-analytics-fields-in-gtm/",
	"title": "#GTMtips: Setting Google Analytics Fields In GTM",
	"tags": ["Google Tag Manager", "gtmtips", "universal analytics"],
	"description": "How to set Google Analytics tracker fields using Google Tag Manager.",
	"content": "Due to a recent change in the UI (see entry for February 12, 2015 in the release notes), the large and ever-expanding group of fields you could set for your Universal Analytics tags has been mostly removed. Instead, the often obscure Fields to Set selection has been promoted to the top of More Settings, and you control most of the Universal Analytics fields through this selector.\nIn this #GTMtips, we\u0026rsquo;ll take a quick look at what\u0026rsquo;s changed, how it\u0026rsquo;s changed, and how to work with this new, slimmed-down tag template.\nTip 16: How To Set Google Analytics Fields In GTM   First, some justification for this bold move. The Universal Analytics JavaScript library, analytics.js, has a lot of fields you can set. If just a few of these fields are represented with their dedicated drop-downs in the Tag settings, it would mean that your container would be artificially bloated. We\u0026rsquo;re working with JavaScript, remember. Less is more.\nDemoting all settings to Fields to Set is a democratic move, giving you, the user, full rights to choose which fields to include in the Tag, and more importantly, the right to exclude all the fields you do not need to use.\nThe major change is thus that all relevant Universal Analytics fields should from now on be set through Fields to Set. The fields can be found in the drop-down menu for Field Name. You can add any field name you want (naturally, only valid field names actually work), you can get the field name from a Variable, or you can use the helpful auto-complete to find the correct field.\n  How do you know what fields to add, you ask? Well, browse over to Analytics.js Field Reference for the full list. Remember, be wise in your selection. There\u0026rsquo;s no point in adding the field for eventCategory in a Page View Tag.\nI respect the fact that for many this might mean a downgrade of the user experience. After all, you\u0026rsquo;re left with a less obvious interface. However, I\u0026rsquo;m courageous enough to argue that you only use a handful of fields anyway, and if you know your Universal Analytics, you\u0026rsquo;ll already know what their field names are. Also, checking the Field Reference is not such a huge effort.\nAn improvement would be that the drop-down list would also enable you to quickly check what the field you selected actually is (e.g. the \u0026ldquo;proper name\u0026rdquo;). This would save you the trouble of going to the Field Reference.\n"
},
{
	"uri": "https://www.simoahava.com/seo/dynamically-added-meta-data-indexed-google-crawlers/",
	"title": "Dynamically Added Meta Data Indexed By Google Crawlers",
	"tags": ["Google Tag Manager", "meta description", "SEO", "serp"],
	"description": "You can add meta HTML tags to your website using Google Tag Manager, and Google&#39;s search crawlers will crawl and index them.",
	"content": "Quick history. On May 23, 2014, the following announcement was made on the Google Webmaster Central Blog:\nIn order to solve this problem, we decided to try to understand pages by executing JavaScript. It's hard to do that at the scale of the current web, but we decided that it's worth it. We have been gradually improving how we do this for some time. In the past few months, our indexing system has been rendering a substantial number of web pages more like an average user's browser with JavaScript turned on. Read the full announcement here.\nAnyway, since I see the world through GTM-tinted shades, I instantly figured that this should extend to not only the presentational layer, but to semantic information as well. At the time, I was thinking in terms of the Search Engine Results Page (SERP) and the Meta Description, which prompted me to ask the following in Google+ from Gary Illyes, who made the announcement:\n  As you can see, I didn\u0026rsquo;t get a direct answer, so I decided to run some tests. I created some test pages, and used Google Tag Manager to inject a Meta Description with a simple Custom HTML Tag (that fired on the test page only):\n\u0026lt;script\u0026gt; var m = document.createElement(\u0026#39;meta\u0026#39;); m.name = \u0026#39;description\u0026#39;; m.content = \u0026#39;This tutorial has some helpful information for you, if you want to track how many hits come from browsers where JavaScript has been disabled.\u0026#39;; document.head.appendChild(m); \u0026lt;/script\u0026gt; This simple tag creates a new \u0026lt;meta name=\u0026quot;description\u0026quot; content=\u0026quot;This tutorial...\u0026quot;\u0026gt; tag and adds it as the last child of the head HTML element.\nSo I published the test pages, had Google crawl them and ended up with\u0026hellip;failure. For some reason, it didn\u0026rsquo;t work and I left it be, evangelizing to people to keep on adding the Meta Data directly in the page template.\nThen, at SuperWeek Hungary this January, I had the pleasure of meeting Gary Illyes in person, and I relayed to him my test results. To add some additional context, I had just tested successfully and written about injecting structured data JSON-LD through GTM. Anyway, Gary was adamant that the crawlers should understand injected meta tags, so I ran another series of tests.\nAgain, failure. But being your typical Finn, I refused to give up. So I ran three more tests.\nTest 1: Pure Test Page The first test I did was for a new page using my blog template, which had basically no content. Its title was \u0026ldquo;Test page\u0026rdquo; and the Meta Description was injected. After creating the page and setting up the tag in GTM, I published the page and asked Google to crawl the page via Webmaster Tools.\nHowever, the SERP refused to show the injected Meta Description. After asking about it, I was told that sometimes the crawlers index a page before rendering the JavaScript, and that I should make a dramatic change to the page to force Google to recrawl it.\nI added a lot of text and some images, but nothing changed so I abandoned the test as a failure.\nTest 2: Real Content The next test I ran was against real content. I had just written a blog post, and I decided to publish it with an injected Meta Description. This time, I was sure that the crawlers would render the Meta Description, as this was a real page with valuable content, and the Meta Description reflected this content very well.\nThis test was\u0026hellip;a failure. This time, I was told that sometimes Google\u0026rsquo;s crawlers decide not to render the JavaScript version. The justification was that pages with heavy JavaScript would create an enormous load for Google\u0026rsquo;s crawlers to work with, which is why they might not render the JavaScript at all in favour of sparing resources.\nInteresting reasoning. This means that the crawlers are not just going to take all the JavaScript you feed them with, but rather they still have an internal decision-making mechanism to determine whether or not to render the dynamic content.\nTest 3: Real-like Content, Little JavaScript I created a dummy page with real content and an actual call-to-action:\n  As you can see, I\u0026rsquo;ve pulled all my CRO chops in creating this page. Do NOT comment on how it looks, it\u0026rsquo;s just a test!\nAnyway, on this page I injected the Meta Description again, asked Google to crawl it and the test was\u0026hellip;A SUCCESS:\n  The Meta Description in the SERP result above has been injected with Google Tag Manager. So it IS true:\nGoogle\u0026rsquo;s crawlers index dynamically injected meta data as well.\nSorry if this was a no-brainer to you - but I had so many unsuccessful tests behind me that I wanted to be sure.\nImplications First of all, as you can probably see from my tests, this isn\u0026rsquo;t a 100 % sure method. So don\u0026rsquo;t delegate the creation and deployment of Meta tags to Google Tag Manager or any other dynamic, client-side solution. The best way is still to add Meta tags to the page template.\nHowever, this does open up a world of possibilities for single-page apps, for example. Instead of using a complicated setup of hashbangs and server-side responses, it just might be possible to serve Meta tags purely with JavaScript in the future. Right now I don\u0026rsquo;t think it\u0026rsquo;s robust enough to trust your entire app logic with, but in the future, who knows.\nThe incredibly interesting and reassuring thing here is that Google\u0026rsquo;s crawlers are really taking dynamic web pages seriously. This is a huge step in building an actual representation of the web, instead of just crawling source code that might have very little to do with what visitors to your website actually experience and find relevant.\n"
},
{
	"uri": "https://www.simoahava.com/tags/meta-description/",
	"title": "meta description",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-non-javascript-visits-google-analytics/",
	"title": "Track Non-JavaScript Visits In Google Analytics",
	"tags": ["Google Tag Manager", "JavaScript", "Google Analytics"],
	"description": "How to collect data from users who do not have JavaScript enabled in their browser. You can use Google Tag Manager to collect this information to Google Analytics.",
	"content": "One of the big mysteries in browser-based data collection platforms like Google Analytics is what happens when the visitor is not being tracked. This is most obvious in cases where the user explicitly opts out of tracking, when the user does not have JavaScript active in their browser, in bounced sessions, and on exit pages.\nOpt-outing means that the user explicitly prohibits a website from tracking them. In some cases, it\u0026rsquo;s possible that opt-out is the default, and the user must explicitly opt-in to allow GA to record their visits. In this article, I will of course not give you ideas how to circumvent this, as you must respect the user\u0026rsquo;s wishes.\nBrowsing with JavaScript disabled is surprisingly common, even though it makes a large part of the web unusable. If you disable JavaScript in your browser, the browser will no longer be able to run script blocks or arbitrary JavaScript functions. DOM methods still work, for example, but you won\u0026rsquo;t be able to use persistent storage such as cookies in your GTM container.\n  Bounced sessions and exit pages are problematic, since certain metrics such as Time on Page can only be calculated with two pageviews. This means that bounced sessions, which have a maximum of one pageview, will not be able to tell you how much time a visitor spent on your site. Similarly, an exit page does not have a subsequent pageview, which means that time on page is lost for these as well.\nI\u0026rsquo;ve got two use cases I want to explore. Because I want to keep things short(er than usual), I\u0026rsquo;ve split this into two articles. The first one, this, concerns sending pageview hits from browsers with JavaScript disabled.\nThe second article (read it here) covers using User Timings to capture time on page for bounced sessions and exit pages.\nI\u0026rsquo;m implementing both use cases using Google Tag Manager. As usual, these articles aren\u0026rsquo;t just about tackling specific GA use cases. They\u0026rsquo;re also about introducing cool JavaScript APIs and lesser-known Google Analytics features to the non-developer public.\nCollect pageviews from non-JS browsers To collect pageviews from browsers where JavaScript has been disabled, we\u0026rsquo;ll need to leverage a number of advanced features of both Google Tag Manager and Google Analytics. The steps we\u0026rsquo;re going to take are these:\n  Add a dataLayer key-value pair into the iframe of the noscript element in the GTM container snippet.\n  Use a Custom Image Tag to fire a Measurement Protocol pixel when this dataLayer key-value pair is detected.\n  Filter these MP hits into their own Google Analytics profile.\n  (NOTE: The point of this solution is to show how the iframe can be used to leverage the Data Layer Variable in GTM. There\u0026rsquo;s actually a much more elegant way to check if the user doesn\u0026rsquo;t have JavaScript enabled in their browsers. Check Duncan\u0026rsquo;s comment below for the solution!)\nThis means that when a browser that\u0026rsquo;s disabled JavaScript enters the site, the noscript tag is executed, and the dataLayer key-value pair causes the Custom Image Tag to fire. This tag is a pageview hit to the Universal Analytics endpoint using Measurement Protocol. This way you can collect data from visitors who have JavaScript disabled!\n1. Edit the container snippet So, the first thing you need to do is edit the container snippet. When a browser without JavaScript tries to render the container snippet, it will not execute any of the code within the script element. Instead, it finds the noscript block, and renders the contained HTML code.\nGoogle Tag Manager loads an iframe, which is tailored to your container. By passing key-value pairs as query parameters to this iframe, you add data into the internal data model of Google Tag Manager, and it can be used by the Data Layer Variable even on browsers without JavaScript enabled.\nSo, in this use case, I want to send a key-value pair to Google Tag Manager, which I can then use as a Trigger condition to fire a Tag only for browsers which render the iframe. As you can see below, I\u0026rsquo;m sending nojscript=true to GTM. This is the non-JavaScript equivalent of dataLayer.push({'nojscript' : 'true'});\n  2. Data Layer Variable for nojscript Next, we\u0026rsquo;ll need a Data Layer Variable that accesses this key-value pair, and a Custom Event Trigger which fires the tag when nojscript=true is found in the data model.\n  This Data Layer Variable will retrieve the value of the key nojscript from the data model if such a key exists.\n3. Custom Event Trigger The Trigger which fires the Tag when nojscript=true looks like this:\n  As you can see, the Event for the Trigger is still gtm.js. This is the equivalent of the \u0026ldquo;All Pages\u0026rdquo; Trigger. However, you need to specify a new condition, which demands that the Data Layer Variable for nojscript resolve to true.\nSo, quick recap. Now we\u0026rsquo;ve modified the container snippet to push nojscript=true into Google Tag Manager\u0026rsquo;s data model when the user\u0026rsquo;s browser does not have JavaScript enabled. Then, we have a Data Layer Variable which picks up this information from the data model. Finally, we have a Trigger which fires any Tag it is attached to, when the value of nojscript is true.\n4. Custom Image Tag Now, we need the Tag itself. Create a new Custom Image Tag:\n  Use the Trigger you just created as the only Trigger for this Tag.\nCustom Image Tags are still supported even if the visitor\u0026rsquo;s browser does not use JavaScript. This is because the special iframe document that is loaded in the noscript can still render normal HTML tags, which an img very much is.\nThe URL of the img tag is a Measurement Protocol pixel call. If you know your Universal Analytics, you\u0026rsquo;ll know that every single hit from your web properties uses the Measurement Protocol, since it\u0026rsquo;s basically just a pixel that is loaded from the URL https://www.google-analytics.com/collect. All the fields and settings for each hit are provided as parameters to this call. So, this is what the Measurement Protocol hit for the Custom Image Tag would look like:\nhttps://www.google-analytics.com/collect?v=1\u0026amp;t=pageview\u0026amp;dl={{Page URL}}\u0026amp;dt=No%20JavaScript\u0026amp;cid={{Random Number}}\u0026amp;tid=UA-XXXXXXX-X\u0026amp;gtm=GTM-XXXX\nThis sends a very simple pageview hit with no extra parameters or dimensions. The Document Location, which GA uses to parse the page path from, is taken from the {{Page URL}} Built-In Variable, the Document Title of the page is \u0026ldquo;No JavaScript\u0026rdquo;, and the Client ID of the hit is a random number, generated by another Built-In Variable. Remember to substitute your own Google Analytics tracking code for the \u0026amp;tid= parameter. Also, if you want to mimic other GTM hits, add the \u0026amp;gtm= parameter with your container ID.\nAdd any other parameters you might like to. For the full list of available parameters, see this guide.\nI use \u0026ldquo;No JavaScript\u0026rdquo; as the page title because I use that to create the Include filter in Google Analytics:\n  Naturally, you\u0026rsquo;ll want to add a similar Exclude filter to your main reporting profiles.\nIn the Measurement Protocoll call, we can\u0026rsquo;t use a stored client ID in the hit, because cookies are inaccessible in browsers where JavaScript is disabled. This means that you will not be able to stitch hits together as sessions, or sessions as users. Each hit is a new session, essentially. This is why it won\u0026rsquo;t make sense to include these hits in your main reporting profile.\nThere are ways around this restriction. You\u0026rsquo;ll need to build a server-side script which takes the client ID from the _ga cookie sent with each GET request, and renders this as another URL parameter in the iframe URL in the GTM noscript snippet. However, this is way beyond the scope of this simple guide.\nOnce you publish this setup, you should end up with a profile in Google Analytics collecting pageview hits from all visitors who do not have JavaScript enabled in their browsers. You can use this data to get an idea of just how many hits you are actually missing because of these over-cautious visitors. You can also use the average for Pages / Session to get an estimate of how many sessions these hits comprise.\nSummary The idea behind this guide was to introduce the elusive noscript tag of the GTM container snippet. You can use it to push key-value pairs into Google Tag Manager\u0026rsquo;s data model, and you can then use this information to fire any Tags you want to reserve for users without JavaScript.\nIf you have marketing platforms that have specified a \u0026lt;noscript\u0026gt; alternative, you would use this solution to compile the image tag that\u0026rsquo;s usually provided as the JavaScript-less option.\nRemember to come back in a little time to read the next part of this guide: how to get an accurate time on page metric for bounced sessions and exit pages!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-file-downloads-in-gtm-v2/",
	"title": "#GTMtips: Track File Downloads In GTM V2",
	"tags": ["auto-event tracking", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "How to setup Google Analytics tracking for visitors who click file download links. Set up the method using Google Tag Manager.",
	"content": "In this #GTMtips post, we\u0026rsquo;ll go over a simple method for tracking file downloads in Google Tag Manager, specifically the new, V2 interface. Also, \u0026ldquo;tracking file downloads\u0026rdquo; means sending Events to Google Analytics, so this is a GA for GTM guide as well.\nTip 15: Set up file download tracking in GTM   Since we\u0026rsquo;re using Google Analytics as the tracking platform, we\u0026rsquo;ll need the following ingredients to make this setup work:\n  Event Tag - which fires when a downloadable file link is clicked, and sends the Event to Google Analytics\n  Auto-Event Variable for click URL path - to capture the URL path of the clicked link\n  Event Trigger - which uses the Link Click trigger type and checks if URL path contains any of the extensions you want to track\n  So start with the Auto-Event Variable, and follow these steps:\n  Create new Variable of type Auto-Event Variable\n  Name it Click URL Path\n  Choose Element URL as the Variable type\n  Choose Path as the Component Type\n  I like this method, since using the Component Type \u0026ldquo;Path\u0026rdquo; strips the URL so that only the URI path remains. This means that query parameters, fragments, hostnames, protocols, and ports are all excluded from the string, leaving a much cleaner output.\nFor the Trigger, you need to create a Link Click trigger, where the Click URL Path is matched against a regular expression of all the file extensions you want to monitor. If the whole concept of Link Click tracking is unfamiliar to you, remember to check my guide on auto-event tracking.\n  Create a new Trigger, and choose Click as the Event Type\n  Name it Event - Link Click on downloadable\n  Choose Link Click as the Trigger Type\n  Select Wait for Tags and Check Validation if you wish\n  If you selected either Wait for Tags or Check Validation, choose Page URL matches RegEx .* as a filter in the \u0026ldquo;Enable When\u0026rdquo; step of the Link Click Trigger creation\n  For \u0026ldquo;Fire On\u0026rdquo;, add the following condition: Click URL Path matches RegEx .(ext1|ext2|ext3)$\n  It should look like this:\n  Couple of things to note here. First, if you select either \u0026ldquo;Wait for Tags\u0026rdquo; or \u0026ldquo;Check Validation\u0026rdquo; in the Link Click Trigger settings, you will have an extra step called \u0026ldquo;Enable When\u0026rdquo;. This step is for when GTM should listen for link clicks, so I suggest you put Page URL matches RegEx .* as the only condition.\nSecond, you will need to add the file extensions you want to track in the \u0026ldquo;Fire On\u0026rdquo; condition. For example, if I want to track all PDF, XLSX, PNG, and DOCX downloads on my site, the regular expression would look like this:\n.(pdf|xlsx|png|docx)$\nThis would match any Click URL Path which ends in .pdf, .xlsx, .png, or .docx.\nIf your site has file extensions in capital letters (PNG, XLSX), you\u0026rsquo;ll need to change the operator from matches RegEx to matches RegEx (ignore case).\nFinally, you\u0026rsquo;ll need the Tag itself. It\u0026rsquo;s just your run-of-the-mill Event Tag, and you need to attach the Trigger you just created to it.\nFeel free to be creative with what you add as the Event fields. I\u0026rsquo;ve used a simple Click URL Variable to get the entire URL of the clicked link, but you could do some macro magic and return just the filename or the file extension (see my macro magic article for instructions).\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/track-outbound-links-in-gtm-v2/",
	"title": "#GTMtips: Track Outbound Links In GTM V2",
	"tags": ["auto-event tracking", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "How to track links that lead out of your website in Google Analytics. Setup the tracking using Google Tag Manager.",
	"content": " NOTE 30 Oct 2019: I\u0026rsquo;ve published a new article on outbound link tracking in Google Tag Manager, which makes the whole process much, MUCH simpler. I strongly recommend you read that article instead.\n Tracking outbound links is important for many. Identifying the exit paths is almost as important as tracking entrances. In this simple #GTMtips post, I\u0026rsquo;ll show you how to track outbound links with a simple Trigger + Auto-Event Variable combination in the new Google Tag Manager interface.\nFor more information about triggers, variables, and auto-event tracking, here are some of my previous articles on these topics:\n  Auto-Event Tracking In GTM 2.0\n  Variable Guide For Google Tag Manager\n  Trigger Guide For Google Tag Manager\n  Setting up the Variable and the Trigger   Tracking outbound links hinges around two things: a Variable which captures the hostname of the clicked link, and a Trigger, which fires your Event Tag when the hostname is not your domain.\nTo set up the Variable, create a new Auto-Event Variable, and call it Click URL Hostname. You can choose whatever name you want, of course, but this mimics the pattern of the existing Built-In Variables.\nThe Variable should have the following settings:\nType: Auto-Event Variable\nVariable Type: Element URL\nComponent Type: Host Name\nIf you want, you can choose to \u0026ldquo;Strip www.\u0026rdquo;, meaning the Variable will return the same, www-less value for both www.mydomain.com and mydomain.com.\nFor the Trigger, you\u0026rsquo;ll need to create a new Click Trigger which has a filter for your site\u0026rsquo;s hostname.\n  Create new Just Links trigger\n  Check \u0026ldquo;Wait for Tags\u0026rdquo; and \u0026ldquo;Check Validation\u0026rdquo;\n  For the \u0026ldquo;Enable this trigger when\u0026hellip;\u0026rdquo; condition, set Page URL contains /\n  Select the radio button for Some Link Clicks\n  Set the last condition to: Click URL Hostname does not contain mydomain.com\n  Substitute mydomain.com with your own domain. You can choose another operator, such as RegEx matching, or the exact match \u0026ldquo;does not equal\u0026rdquo; operator, depending on how accurate you want the pattern match to be.\nTo finish off the Trigger, check my Auto-Event Tracking guide if the options confuse you. The most difficult bit of the new Trigger-based listener is the \u0026ldquo;Enable When\u0026rdquo; setting, which determines when the Trigger is actively listening for link clicks. Just add a blanket match to this: Page URL contains /, since it\u0026rsquo;s quite likely that you want to track link clicks on all your site\u0026rsquo;s pages, right?\nFinally, to make your Event tag fire with this new Trigger, just add it to the Tag:\n  If you\u0026rsquo;re wondering how this Trigger works with links that have relative targets, don\u0026rsquo;t worry. The Trigger works for these as well, since it doesn\u0026rsquo;t access the href attribute of the A element. Rather, it grabs the value from the DOM property named href, which will always have a fully-formed URL value with the hostname and all.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/release-gtm-tools-v2-0/",
	"title": "RELEASE: GTM Tools V2.0",
	"tags": ["Google Tag Manager", "gtm tools", "Guide", "tools"],
	"description": "Introducing the latest version of GTM Tools for Google Tag Manager.",
	"content": "(UPDATE 3 Apr 2017: There is a newer version of GTM Tools out, so please ignore this article and read this one instead.)\nSo, the time has come to update my GTM Tools. I released the first toolset in October 2014, and it performed its duties just well enough. Sure, the UI was ugly as hell, and there were bugs along the way, but for cloning containers, macros, and rules, and for visualizing containers, it was just good enough.\nI\u0026rsquo;ve been working on a new version of the toolset, designed specifically for accounts created in the new UI of Google Tag Manager. There\u0026rsquo;s no \u0026ldquo;backwards compatibility\u0026rdquo;, so this version of GTM Tools will, as I wrote, only work with accounts and containers created in the new UI.\n  This article is intended to be the \u0026ldquo;User guide\u0026rdquo; for the tool, so I\u0026rsquo;ll jump straight to the subject matter right after this disclaimer:\nI am not a professional product developer, and GTM Tools v2.0 is not a commercial product.\nThis toolset is my own, personal, intellectual property, with no purpose of making money out of it or making it the best possible product out there. It\u0026rsquo;s got bugs, the code is pretty nightmarish in some places, and I haven\u0026rsquo;t done thorough testing. It\u0026rsquo;s a toolset that you might find useful or then you might not.\nI would still appreciate that you e-mail me (simo(at)simoahava.com) any bugs, errors, or freezes that you come across. Just remember to check the Known Issues part of this guide first.\nThe tool is located in this URL:\nhttp://www.gtmtools.com/\n1. Login And Authentication GTM Tools v2.0 uses your Google Account for authentication. This means that when you first open the tool website, you will need to Sign in with your Google credentials.\n  Once you\u0026rsquo;ve signed in, you will need to authorize GTM Tools v2.0 for access to Google Tag Manager and your Google profile. Specifically, here are the authorization scopes you allow access to:\n  If you refuse to allow access, you will not be able to use the toolset.\nOnce you\u0026rsquo;re in the actual tool interface, you can see the profile you\u0026rsquo;ve logged in with in the upper right corner of the page. You can click this link and Sign out of your Google Account at any time, or you can click Account to access your Google Account settings.\n  2. Home Page The first page you\u0026rsquo;ll see in the tools is the home page. This page is a placeholder, and you should use the navigation bar in the top of the page to move on in the site.\n  The navigation has the following selections in the home page:\n  Home – Takes you back to the home page\n  GTM Account – This lists all the GTM Accounts you have access to with the signed in Google Account. Note that this list includes GTM Accounts created for the old GTM interface, but these will not work in GTM Tools v2.0, so try to avoid accessing them.\n  Library – Takes you to the Asset Library where you can find your stored Containers\n  Cart – Shows you how many items you have in your Cart, and by clicking the button you will be taken to the Cart page\n  Your Profile – Clicking this shows a drop-down menu, where you can choose to access your Google Account settings and/or Sign Out of your Google Account.\n  3. Account Page **NOTE!** Due to processing reasons, you will only be able to interact with a **PUBLISHED VERSION** of any given Container. If there is no published version, you will not be able to perform any actions on the Container. This is something I intend to fix as soon as the API provides better access to the Container Draft. When you choose a GTM Account in the Accounts navigation, you will be taken to a page that lists all the Containers in the selected GTM Account. If you click a Container name, you will be taken to the respective Container page.\n  If you click the small down arrow next to a Container name, you\u0026rsquo;ll see quick links for the following Container actions:\n  Inspect - Takes you to the Inspect Container page, where you can view information about the Container, and where you can add / remove assets from the Container into the Cart\n  Visualize - Takes you to the Visualize Container page, where you can view a visualization of the Container\n  Clone - Opens a modal dialog that lets you clone this Container\n  Read more about these actions in the following sections of this guide.\n4. Container Page The container page is here more for structural reasons than to add any added value. You can move through it to the individual actions (Inspect, Visualize, Clone), which you can also do from the Account page, as you just learned.\n  The following chapters will include details about the various actions you can take.\nInspect Container On the Inspect Container page, you can see a list of all Tags, Triggers, and Variables in a Container. You will also see information about the current published version by clicking the Version Information panel.\n  The number on the right-hand-side of a panel title tells you how many assets are in each respective category.\nBy expanding an asset category, you\u0026rsquo;ll see a list of all the assets under that category.\n  You will also see three buttons:\n  Green plus + for adding the asset to your Cart\n  Red minus - for removing the asset from your Cart\n  yes in the Links column if there are dependencies (i.e. linked assets) that you should probably add to your Cart as well\n  The green plus will be greyed out if the asset is already in the Cart, and the red minus will be greyed out if the asset is not yet in the Cart. If there are no dependencies, you will only see a dash in the \u0026ldquo;Links\u0026rdquo; column.\nDo not add an asset to Cart if another asset with the same name has already been added. This is not supported by the current version of GTM Tools v2.0, and I haven’t yet prevented this in the code.\nWhen you click on the yes button for dependencies, a modal dialog will open up which lists all the dependencies of the current asset. This means that these dependencies are linked to directly from the asset itself, or from one of the linked assets. It\u0026rsquo;s strongly recommended that you include all linked dependencies when adding an asset to the Cart.\n  You can add a dependency to the cart by clicking the Add link, after which you\u0026rsquo;ll see the text Added next to the dependency.\nVisualize Container The Visualize Container page first shows you a brief description of what the tool does. Once you click the Start visualization button, a full-screen modal dialog will open, and you will be able to see a visualization of all the assets in the Container as well as any links between them.\nThe asset colors are:\n  Grey - Built-In Variables\n  Green - Tags\n  Blue - Variables\n  Red - Triggers\n    If you hover your mouse over an asset, any links to or from that asset will be highlighted. The path color is red if the link is from the selected asset, and the path color is green if the link is to the selected asset.\nHovering over the asset will also show information about it in the small box that appears in the center of the visualization.\nClicking an asset name freezes the paths, so that it\u0026rsquo;s easier for you to navigate to the other end of the path.\nClicking Select Hermit Nodes will highlight all the assets that have no links to or from other assets.\n  You can use the search box to find assets. Start typing, and the assets that match whatever you\u0026rsquo;ve typed will be highlighted as you type.\n  Clone Container There are two ways to clone a Container in GTM Tools v2.0. Due to architectural reasons, they are a bit different.\nThe first way is through the Account page and the Container page. So either you choose Clone Container from the drop-down menu next to the Container name in the Account page, or you click the Clone Button on the Container page itself.\n  When you choose this Clone option, you will be able to choose the GTM Account where this Container will be cloned to. You can also choose the same GTM Account as the one from where you’re cloning the Container from.\n  Once you\u0026rsquo;ve chosen the Account and clicked Clone, the process begins, and the source Container with all its assets is cloned to the target Account.\nIf there already is a Container with this name in the target Account, the Container name will be prefixed with \u0026ldquo;copy of \u0026ldquo; during the process.\nThe second way of cloning a Container is with your custom-created Containers. This means that you choose to Clone either directly from the Cart page or from your Asset Library page.\n  If you choose this option, it will be possible to merge the stored Container with an existing Container, or you can choose to create an entirely new Container, if you wish.\n  If you choose New Container, you will need to give the new Container a name. When you click Clone, the new Container will be created in the target Account, and all assets are cloned. If there is already a Container with this name, the new Container’s name will be prefixed with \u0026ldquo;copy of\u0026rdquo; during the process.\nIf you choose an existing Container, the contents in your Cart or in the stored Container will be merged with the assets in the target Container. This means that if there is a naming conflict, i.e. an asset with the same name already exists in the target Container, the asset\u0026rsquo;s name will be prefixed with \u0026ldquo;copy of\u0026rdquo;, and any links to the asset in other cloned assets will be updated accordingly.\nRenaming Containers and assets like this makes merging Containers possible while still preserving the established links between assets in the source Container.\nIf you choose to merge the assets to an existing Container, no existing assets in the target Container are modified in any way, so you don\u0026rsquo;t have to worry about data or integrity loss.\nOnce the Cloning process begins, there\u0026rsquo;s no way to interrupt it.\n5. Cart Page On the Cart page you can see all the assets that you have stored in your Cart. You store assets in the Cart through the Inspect Container page. The assets are listed first by GTM Account name, then by Container name, and finally by asset type.\n  Clicking the Remove link next to an asset removes the asset from your Cart.\nClicking the Clone to container button opens a modal dialog that lets you clone the Cart contents into an existing Container or a new Container.\nClicking the Save cart button opens a modal dialog that lets you save the Cart contents into your Asset Library. This way you can save your favorite Container configurations to be used later.\n  Clicking the Empty cart button flushes the Cart contents.\n6. Asset Library The Library page shows you all your stored Containers. When you click a Container name, you will see how many Tags, Triggers, and Variables are in the stored Container. You\u0026rsquo;ll also be able to see when the Container was created, as well as the description you gave the Container when you saved it.\n  Clicking the Clone button lets you clone this Container into an existing Container or a new Container.\nClicking the Visualize button takes you to the Visualize Container page, where you can see a visualization of all the assets stored in the Container.\n  Clicking the Delete button opens a modal dialog which confirms this action. If you choose to delete the Container, you\u0026rsquo;ll see a success message shortly, after which the page will automatically reload.\n7. Known Issues Here are some of the issues I know exist in the toolset.\n  If you try to access accounts created in the old version of GTM, you will run into trouble. Unfortunately, there\u0026rsquo;s no way to weed these out efficiently. This won\u0026rsquo;t be a problem after all accounts are migrated to the new GTM.\n  You can add an asset to the Cart even if another asset with the same name is already in the Cart. However, if you do so, you will run into an error when trying to clone the Container. I\u0026rsquo;m going to prevent this is the code, but it didn\u0026rsquo;t make it into this version.\n  Currently, all Container actions work only with published versions of the Container. This is a limitation, and I hope to improve it as soon as the API provides better support for accessing the Container draft. At the very least, I intend to allow you to choose the version you want to interact with.\n  Most common cause for errors is with naming conflicts. I\u0026rsquo;ve tried to fix most of these by automatically prepending \u0026ldquo;copy of \u0026ldquo; in front of the Container or asset you\u0026rsquo;re trying to clone, but I may have missed some use cases.\n  I cache most of the things you work with to reduce the number of API calls that are made. However, the cache is purged for Containers and assets whenever significant changes are made. Nevertheless, there might be situations where you don\u0026rsquo;t see a change even though you just performed an operation with the tools or did something in the GTM interface. In this case, I suggest you wait 15 minutes and then check again, as that is the expiration time for the cache. I will add a switch to manually flush your own cache, but it\u0026rsquo;s not in this version yet.\n  8. Latest releases 17 Nov 2016   Updated error handling to be more informative and less intrusive.\n  Migrated from the Channel API to Firebase Realtime Database. As a result, the progress bar is slightly slower (working on it), but it\u0026rsquo;s more stable.\n  Created a new container for Library storage since the first one was full. Sharding is done automatically.\n  Minor refactoring here and there, but the codebase is still pretty awful.\n  10 Dec 2016  Migrated from cookies in Cart storage to AppEngine Datastore.  9. Summary As I hopefully made clear in the introduction, this toolset is my own playing ground. It\u0026rsquo;s not a fully-formed platform, it\u0026rsquo;s not a sponsored product, and it doesn\u0026rsquo;t have a team of engineers working on it 24/7. Thus I hope you will find it useful, and I\u0026rsquo;ll do my best to fix bugs and new features, but don\u0026rsquo;t expect Premium-level support from me in making things right.\nUse the toolset at your risk.\nThere\u0026rsquo;s no risk to your existing assets, since I don\u0026rsquo;t have any overwrite features in the toolset. The only thing you can botch up is cloning something into something else, and in that case only the thing you were cloning will suffer. Easy enough to clean up afterward in your GTM account.\nI still hope you find the toolset useful, and I would very much appreciate any feedback that you might want to direct to my developer team (i.e. me).\n"
},
{
	"uri": "https://www.simoahava.com/analytics/enrich-serp-results-using-gtm/",
	"title": "Enrich SERP Results Using GTM",
	"tags": ["Google Tag Manager", "Guide", "SEO"],
	"description": "You can use Google Tag Manager to enrich your content with structured data. This will make your pages stand out in search engine results.",
	"content": "Google has a myriad of ways to make the search engine results page (SERP) livelier. When you input a search query, the engine\u0026rsquo;s mission is to provide you with the most relevant information with as few clicks as possible. Often, this means that you\u0026rsquo;ll see the answer to your query directly in the SERP:\n  See also Dr. Pete\u0026rsquo;s excellent description of variation in the SERP (note that this post is from 2013, and not all the data types are relevant today). As you can see, there are many ways for a site to annotate data found within, and that way provide it for Google to utilize if it so chooses. There\u0026rsquo;s no guarantee that your structured data is picked up by the search engine, but that shouldn\u0026rsquo;t deter you from marking up your content anyway.\nNow, to business. Google recently released a structured data testing tool that accepts JSON-LD as its input. JSON-LD is an offshoot of the JavaScript Object Notation (JSON) data format, mainly in that you can specify linked data nodes between different notation objects.\nThe structured data supported by JSON-LD is still quite limited, but the amount of options will surely amp up in the near future. Currently, you can markup your site content for:\n  All Knowledge Graph features\n  Event Rich Snippets\n  Sitelink search boxes\n  (UPDATE 3 Feb 2016) Support for Reviews and Products! Thanks Claudia Kosny for the tip.\nThe cool thing about this new release is the following quote from the support pages:\n[JSON-LD] lets you embed a block of JSON data inside a script tag anywhere in the HTML...Also, Google can read JSON-LD data even when it is dynamically injected into the page's contents, such as by Javascript code or embedded \"widgets\". Read the last part again. Doesn\u0026rsquo;t that sound like a Custom HTML Tag in Google Tag Manager? You bet it does!\nMarkup structured data using Google Tag Manager You can dynamically inject JSON-LD specifications on your pages using Google Tag Manager. The only stipulation is that the code must be run during the initial page load. This means that you can\u0026rsquo;t dynamically enhance the structured data on the site after the window has loaded, that is, after gtm.load has been pushed into dataLayer. So as long as your structured data tags are executing with the All Pages Trigger, for example, you should be fine.\nHere are some examples of Custom HTML Tags for structured data. Make sure the Tags fire with the All Pages Trigger.\nAnnotate social profiles for Knowledge Graph If your site is included in the Knowledge Graph, it might be a good idea to add your social profiles directly to the SERP, as Marimekko have done:\n  Here\u0026rsquo;s what you need to write into the Custom HTML Tag:\n\u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34; : \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34; : \u0026#34;Person\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://www.simoahava.com/\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;Simo Ahava\u0026#34;, \u0026#34;sameAs\u0026#34; : [ \u0026#34;http://fi.linkedin.com/in/simoahava\u0026#34;, \u0026#34;http://plus.google.com/+SimoAhava\u0026#34;, \u0026#34;http://www.twitter.com/SimoAhava\u0026#34; ] } \u0026lt;/script\u0026gt; The code above would annotate my Knowledge Graph box (if I ever reach such levels of stardom) with my social profiles directly in the SERP.\nEnable sitelinks search box This, I think, is one of the coolest additions to the SERP. On some sites, you can see a search box directly in the sitelinks. This search box is tied together with the internal search engine of the site, allowing you to directly search for content within the site!\n  To get this to appear, the syntax of the code in the Custom HTML Tag needs to look like this:\n\u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;WebSite\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.simoahava.com/\u0026#34;, \u0026#34;potentialAction\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;SearchAction\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;https://www.simoahava.com/?s={search_term}\u0026#34;, \u0026#34;query-input\u0026#34;: \u0026#34;required name=search_term\u0026#34; } } \u0026lt;/script\u0026gt; Here, I provide the URL and the necessary search query parameter that operate the internal site search engine on my site. Now, if someone were to enter a search term directly in the SERP sitelinks, they will be transported to the search results page within my site for that particular query.\nThings to note Remember that you can combine your various types of structured data into a Custom HTML Tag, and you should do so to reduce the number of tags in your container. The two examples from above would be combined into a single structured data push like this:\n\u0026lt;script type=\u0026#34;application/ld+json\u0026#34;\u0026gt; [{ \u0026#34;@context\u0026#34; : \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34; : \u0026#34;Person\u0026#34;, \u0026#34;url\u0026#34; : \u0026#34;https://www.simoahava.com/\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;Simo Ahava\u0026#34;, \u0026#34;sameAs\u0026#34; : [ \u0026#34;http://fi.linkedin.com/in/simoahava\u0026#34;, \u0026#34;http://plus.google.com/+SimoAhava\u0026#34;, \u0026#34;http://www.twitter.com/SimoAhava\u0026#34; ] }, { \u0026#34;@context\u0026#34;: \u0026#34;http://schema.org\u0026#34;, \u0026#34;@type\u0026#34;: \u0026#34;WebSite\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.simoahava.com/\u0026#34;, \u0026#34;potentialAction\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;SearchAction\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;https://www.simoahava.com/?s={search_term}\u0026#34;, \u0026#34;query-input\u0026#34;: \u0026#34;required name=search_term\u0026#34; } }] \u0026lt;/script\u0026gt; Here I\u0026rsquo;ve included the two JSON objects, and inserted them into a single Array (see the square brackets that wrap the two objects), delimited by a comma.\nOnce you\u0026rsquo;ve created your Tag and published the container, remember to test your site with the Structured Data Testing Tool:\n  One thing I noticed with the tool is that it doesn\u0026rsquo;t always work, especially if the HTML of the site is complex. This doesn\u0026rsquo;t mean the structured data didn\u0026rsquo;t validate, it just means there was some error in the process.\nRemember that since this is GTM, you can use your Variables to make the structured data injector more dynamic. Also, if you\u0026rsquo;ve defined dataLayer in your page template, and it has information you think would be useful in the structured data markup, you can pull this data using Data Layer Variables within your structured data markup. This means that you can create a really flexible structured data injector by leveraging GTM\u0026rsquo;s own functionalities.\nThis feature of Google\u0026rsquo;s search crawlers has been long in the waiting. It makes so much sense to be able to inject annotations about your content with JavaScript, instead of having them applied directly to the page template by the CMS. Also, the fact that you can inject the object anywhere, not just in the \u0026lt;head\u0026gt; of the document, is a great asset, since GTM, by default, injects to the end of the \u0026lt;body\u0026gt;.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/create-a-generic-event-tag/",
	"title": "#GTMtips: Create A Generic Event Tag",
	"tags": ["events", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "How to create a generic event tag using Google Tag Manager.",
	"content": "With Google Tag Manager, there are a million different ways to make your tagging setup leaner and more flexible. The reason this should be a priority is because the UI isn\u0026rsquo;t perfect. The more tags you have, the more difficult it becomes to manage your assets.\nIn this #GTMtips post, I show you one of my favorite ways to put your container on a diet.\nTip 13: How to create a Generic Event Tag   I\u0026rsquo;ve seen a lot of containers that suffer from the same problem. They have many Event tags which only differ by the Event that triggers them and/or by the tag fields of Event Category / Action / Label / Value. There\u0026rsquo;s an easy way to reduce redundancy here. It\u0026rsquo;s the Generic Event Tag (you guessed it!).\nTo set it up, you need the following ingredients:\n  One Universal Analytics Tag, Event tag type\n  Four Data Layer Variables, each for one of the tag fields\n  One Trigger, Custom Event type\n  So create the Data Layer Variables first. You need one for each of the most-used Google Analytics Event fields. If you never use Event Value, you can leave it out, and if you use Non-Interaction a lot, you can create a Variable for it, too.\nThe Data Layer Variables have the following composition:\n  So change the fields for Variable Name (the name you\u0026rsquo;ll use in the Tag fields) and Data Layer Variable Name accordingly. I chose the following Data Layer Variable Names:\n  eventCategory - for Event Category value\n  eventAction - for Event Action value\n  eventLabel - for Event Label value\n  eventValue - for Event Value value (value value, eh\u0026hellip;)\n  You can choose other names of course, if you wish. Doesn\u0026rsquo;t make a difference.\nNext, you\u0026rsquo;ll need the Trigger. I\u0026rsquo;ve chosen GAEvent as the Event name to match, but you can, again, choose whatever you wish. This is what the Trigger should look like:\n  Nothing too complicated, right?\nFinally, we\u0026rsquo;ll need the tag. When you create the tag, you need to add the Trigger you just created in Step 3 of the tag creation process (What triggers this tag to fire?). Then, you need to add the Variables you created earlier in their respective fields. Here\u0026rsquo;s what my Tag looks like:\n  As you can see, it\u0026rsquo;s just a simple case of adding the Variables to their designated fields, and making sure that the Tag fires whenever the GAEvent value is pushed into dataLayer.\nFinally, you operate this tag with the following command:\ndataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39; : value_for_Event_Category, \u0026#39;eventAction\u0026#39; : value_for_Event_Action, \u0026#39;eventLabel\u0026#39; : value_for_Event_Label, \u0026#39;eventValue\u0026#39; : value_for_Event_Value });  So whenever this code is executed, the Generic Event Tag is fired with the values you added to the dataLayer.push().\nHere\u0026rsquo;s a tip, though. Because Data Layer Variables are linked together with GTM\u0026rsquo;s data model, it\u0026rsquo;s a good practice to always push the undefined value with any of the \u0026lsquo;eventXXXXX\u0026rsquo; fields that you do not use. Otherwise it\u0026rsquo;s possible that the Tag uses some older value you pushed earlier on the page, and you\u0026rsquo;ll have weird-looking events.\nSo, for example, if I want to send an Event to GA which only uses the Event Category and Event Action fields, the push() would look like this:\ndataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39; : \u0026#39;Form Submit\u0026#39;, \u0026#39;eventAction\u0026#39; : {{url path}}, \u0026#39;eventLabel\u0026#39; : undefined, \u0026#39;eventValue\u0026#39; : undefined });  This will ensure that any values eventLabel and eventValue might have had in the data model are erased and will not interfere with your Tag.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/add-konami-code-to-your-site/",
	"title": "#GTMTips: Add Konami Code To Your Site",
	"tags": ["easter egg", "Google Tag Manager", "gtmtips"],
	"description": "Use Google Tag Manager to add a Konami code listener to your website. You can use it to create a fun secret for your website visitors.",
	"content": "You\u0026rsquo;ve probably heard of the Konami Code. It\u0026rsquo;s a cheat code in many Konami games, where the cheat is executed with a sequence of key presses on the keyboard. Since then, it\u0026rsquo;s become one of the staples of video game folk lore, and many websites, games, and applications have their own easter eggs activated with the Konami code.\nThe sequence of keys is:\nup, up, down, down, left, right, left, right, B, A\nFeel free to try it on this site!\nIn this #GTMtips post, I\u0026rsquo;ll show you how to implement the Konami Code on your website with a single, very simple Custom HTML Tag.\nBefore I go on, kudos for the idea goes to the awesome Gerry White, who graciously allowed me to steal his brilliant idea.\nTip 12: Add the Konami Code to your website   The implementation is simple. All you need is a Custom HTML Tag which fires on All Pages. The code in the tag looks like this:\n\u0026lt;script\u0026gt; var els, i, len, title; var konamiCode = \u0026#39;38,38,40,40,37,39,37,39,66,65\u0026#39;; var keyPresses = []; var checkKonami = function(e) { keyPresses.push(e.keyCode); if (keyPresses.slice(keyPresses.length-10).join() === konamiCode) { runKonami(); } }; var runKonami = function() { els = document.getElementsByTagName(\u0026#39;h2\u0026#39;); for (i = 0, len = els.length; i \u0026lt; len; i++) { title = els[i].textContent || els[i].innerText; title = title.trim(); els[i].innerHTML = title.split(\u0026#39;\u0026#39;).reverse().join(\u0026#39;\u0026#39;); } }; document.addEventListener(\u0026#39;keyup\u0026#39;, checkKonami); \u0026lt;/script\u0026gt; Here\u0026rsquo;s what happens in the script:\nvar els, i, len, title; var konamiCode = \u0026#39;38,38,40,40,37,39,37,39,66,65\u0026#39;; var keyPresses = [];  The three lines above are variable declarations for the script. The first line defines a bunch of utility variables we\u0026rsquo;ll need later on. The konamiCode variable is a String with the sequence of keys required to execute the easter egg. The key codes and their respective keys are: 38 - up arrow, 40 - down arrow, 37 - left arrow, 39 - right arrow, 66 - B key, 65 - A key.\nThe third line declares the keyPresses Array, whose job is to keep a record of all keys pressed on the page.\nvar checkKonami = function(e) { keyPresses.push(e.keyCode); if (keyPresses.slice(keyPresses.length-10).join() === konamiCode) { runKonami(); } };  The checkKonami function is the callback for the \u0026lsquo;keyup\u0026rsquo; event (see below). In essence, it\u0026rsquo;s called every time a key press is registered on the page. It takes the event object as a parameter (e).\nThe first line, keyPresses.push(e.keyCode);, pushes the key code for the registered key press into the keyPresses Array. The next three lines are an if-block, where the last 10 key codes are evaluated. These are joined into a comma-separated String object, and then compared with the sequence of key codes in the konamiCode String. If these two match, that is if the last ten key presses have the same codes in the same sequence as the konamiCode String, the function runKonami is called.\nvar runKonami = function() { els = document.getElementsByTagName(\u0026#39;h2\u0026#39;); for (i = 0, len = els.length; i \u0026lt; len; i++) { title = els[i].textContent || els[i].innerText; title = title.trim(); els[i].innerHTML = title.split(\u0026#39;\u0026#39;).reverse().join(\u0026#39;\u0026#39;); } };  In the runKonami function you can run whatever code you want to when the Konami code is correctly registered on the website. In my example, I take the text content of every single H2 element on the page, and I reverse the character order.\ndocument.addEventListener(\u0026#39;keyup\u0026#39;, checkKonami);  The final line of JavaScript adds the event listener for the \u0026lsquo;keyup\u0026rsquo; event, and denotes checkKonami as the callback to be invoked each time such an event is registered.\nThis was hopefully a fun example of how to add a custom event listener on the site, and how to manipulate the DOM in a Custom HTML Tag.\n"
},
{
	"uri": "https://www.simoahava.com/tags/easter-egg/",
	"title": "easter egg",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/macros/",
	"title": "macros",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/variable-guide-google-tag-manager/",
	"title": "Variable Guide For Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "macros", "variables"],
	"description": "Comprehensive guide to variables in Google Tag Manager. Included are a look under the hood as well as a plethora of examples how to use variables to make your container rock.",
	"content": "(Updated 21 February 2019)\nThe current version of Google Tag Manager was released in October 2014. With the release, we saw a brand-spanking new UI, a lot of new functionalities (revamped auto-event tracking, for example), plus a new terminology to cope with. We moved away from the programming-centric concepts of Macros and Rules to the more tactile variables and triggers.\nIt\u0026rsquo;s difficult to rank the changes. The new Auto-Event Tracking is perhaps most impactful, but the improvements done to triggers and variables, when compared to the previous version of GTM, require attention as well.\n  Thus, I give you my variable Guide for Google Tag Manager. For reference, feel free to take a look at the old macro guide, since especially the different variable types have been left largely untouched.\nIntroduction to Variables In computing terms, variables denote compartments in computer memory, which are reserved for storing values. This is a significant achievement in efficiency, because it means that values can be reused across functions, procedures, and environments. The alternative would be to treat each representation of the same value as unique, temporary, and disposable, creating a mass of redundancy and inefficiency, and making the current computing landscape a whole lot different.\nIn Google Tag Manager, the term variable is used to denote a helper function that your tags, triggers, and other variables can invoke to retrieve values from. Thus, the idea is very similar to the broader concept of computing variables explored in the previous paragraph.\nIn GTM, invoking the variable function is done with a specific syntax:\n{{variable name}}\nAs we\u0026rsquo;ll learn later on, you can\u0026rsquo;t invoke variables wherever you\u0026rsquo;d want to. But in an approved context, the syntax {{variable name}} would run the underlying function of the variable with the name \u0026ldquo;variable name\u0026rdquo;, and pass the value returned by that function to its execution context (the tag, trigger, or variable where the syntax was used).\nOne of the first things all Google Tag Manager containers which run Google Analytics tags should do is this:\n  Here, instead of always typing the tracking code for your web property (UA-XXXXXX-X) into each GA tag, you can create a Constant variable, which stores the value. Thus, whenever the variable name is invoked using the correct syntax, the tracking code returned by the Constant variable will be included in the appropriate tag field. This is something I\u0026rsquo;ve actually written a #GTMtips post about this.\nI will go so far as to say that variables can make or break GTM. When skeptics ask me what is the main benefit of having GTM on the site, I always end up talking about variables. They add a level of flexibility and customization that can really make your tagging lean, efficient, performance-driven, and time-saving.\nAt the same time, variables can be difficult to fathom, especially when we get to the technical details (see below), or when we try to tackle the amazingly multi-faceted and deviously difficult Custom JavaScript variable. I hope this guide will help you get to the bottom of variables, and encourage you to find your own ways of performing magic tricks with your GTM container.\nTechnical details and how-to For you to be able to invoke a GTM variable, you need to be working in a script context, or the template field needs to support variable references.\nA supported template field can be uncovered by looking for the little variable symbol next to a field:\n  As you can see, the \u0026ldquo;Tag firing priority\u0026rdquo; field will not be able to invoke a variable, but the \u0026ldquo;Auto Link Domains\u0026rdquo; will. In fact, if you click the little variable icon, you will see a drop-down list from which you can pick the variable you want to use. This way you don\u0026rsquo;t have to worry about correct syntax, as GTM will do it for you.\n  The other places where you\u0026rsquo;ll frequently run into variables are:\n  Triggers\n  Custom HTML tags\n  Custom JavaScript variables\n  With triggers (remember to read my Trigger guide as well!), you will always need to specify a variable as the target of the trigger condition. Every single trigger condition needs a variable that is evaluated against some value. In fact, the condition builder is created so that you can\u0026rsquo;t possibly not choose a variable. It\u0026rsquo;s obligatory!\n  Note that the value (right-hand) field of the trigger condition is NOT a variable context!\nAs for Custom HTML tags and Custom JavaScript variables, you can call variables from the scripts within, but you won\u0026rsquo;t have the helpful drop-down menu to assist you. You will need to manually type the call, using the correct syntax and the correct (case-sensitive) variable name:\n\u0026lt;!-- Sample Variable call in a Custom HTML Tag --\u0026gt; \u0026lt;script\u0026gt; (function() { alert({{custom alert string}}); })(); \u0026lt;/script\u0026gt; The code above will pop-up a browser alert, where the content will be the string returned by the variable named custom alert string. Note! If the variable doesn\u0026rsquo;t return a string, you might run into problems with the alert method, so you need to be well aware of return types and values at all times.\nIf you try to use a variable that doesn\u0026rsquo;t exist (or whose name you mistyped), you won\u0026rsquo;t be able to perform any operations on the container version (create, publish, etc.). You\u0026rsquo;ll run into an error message that looks like this:\n  So remember to check all the places where you\u0026rsquo;ve manually typed a variable call. It\u0026rsquo;s also possible that you\u0026rsquo;ve jumped the gun and called a variable, but then you forgot to create the variable itself. Retrace your steps and fix any broken links.\nOne cool thing that GTM does for you is this: when you change a variable name, all references to the variable are automatically updated. Even the ones you\u0026rsquo;ve typed in manually into Custom HTML tags and Custom JavaScript variables! This is incredibly time-saving, since you don\u0026rsquo;t have to go through every single reference, rewriting the code to match the new name.\nVariables are resolved according to the following process:\n  If the variable is not attached to a trigger or a tag, the variable is never resolved (unless, of course, the variable is referenced from some other variable that is resolved).\n  If the variable is referenced in a trigger that is not attached to any tags, the variable is never resolved.\n  If the variable is referenced in a trigger that is attached to a tag, the variable is resolved every single time an event key is pushed into dataLayer.\n  If the variable is referenced in a tag, the variable is resolved every single time the tag fires.\n  The last one is interesting. When a trigger causes a tag to \u0026ldquo;fire\u0026rdquo;, the process is actually this:\n  Variable calls are transformed into the actual function calls (google_tag_manager['containerId'].macro('gtm10'))\n  The tag code is injected on the site\n  The tag code is executed, and all variable functions within are resolved\n  As you can see, a variable is actually a method of the google_tag_manager object. The parameter that is given to the macro() method is basically \u0026lsquo;gtm\u0026rsquo; plus a unique sequential number. This means that it\u0026rsquo;s very difficult to identify which macro() parameter is associated with which variable, which is why debugging has become an all-important tool in understanding how variables work.\nThe other, common case of variable resolution has to do with trigger conditions. When an \u0026lsquo;event\u0026rsquo; key is pushed into dataLayer, every single trigger that is attached to a tag in your container is evaluated for the \u0026lsquo;event\u0026rsquo; condition. In addition to checking the event value, all variables referenced in the trigger are evaluated, too.\nYou might want to re-read the previous paragraphs to grasp the idea, but this has very severe repercussions on how variables should (and should not be used):\nVariables must never have side effects\nRemember how variables should only be used to return values? Well, sometimes you might be tempted to use a variable to change the state of the global object or to set or push stuff into dataLayer. Don\u0026rsquo;t do it! Because variables can be resolved in multiple ways, and not just in tags as you\u0026rsquo;d expect, you might find yourself creating infinite loops, pushing stuff into dataLayer multiple times, or severely hurting page performance.\nfunction() { // DON\u0026#39;T DO THIS:  window.dataLayer.push({ ... }); // OR THIS:  alert(\u0026#34;test\u0026#34;); // OR THIS:  window.temp = getTempValue(); // DO THIS:  var a = window.thisValue; var b = getSomeOtherValue(); return a + \u0026#39;: \u0026#39; + b; }  If you\u0026rsquo;ve made it this far, congratulations. As a developer, I think it\u0026rsquo;s all-important to understand how variables work beyond the obvious, if only to realize the potential that variables have in improving the efficiency of your work with GTM.\nBuilt-in Variables The new version of Google Tag Manager introduced Built-in variables. These are helpful, most commonly used variables, whose existence is based on binary logic: active / inactive. If the Built-in variable is activated, it can be used as any other variable, and if it\u0026rsquo;s not activated, it can\u0026rsquo;t.\nBuilt-in variables are thus no more than a shorthand for exploiting some of the most frequently used variable types in Google Tag Manager. I find it a bit odd that they\u0026rsquo;re not all activated by default, since there\u0026rsquo;s absolutely no harm in them being available for use.\nYou will find the Built-in variables displayed prominently in the top of the variables screen of your Google Tag Manager Container:\n  If the Built-in variable is listed here, it is active in the container. To add new Built-in variables, click the red CONFIGURE button, and check the box next to each variable you want to add to the container.\n  As with all variables, if the Built-in variable cannot be resolved (e.g. there was no Click action and you want to use Click ID), the variable returns the JavaScript undefined type.\nHere are the Built-in variables with a brief description of what they return:\nPages   Page URL - returns a string containing the full URL of the current page without hash fragment (https://www.simoahava.com/article?parameter=true)\n  Page Hostname - returns a string containing the the hostname of the current page (www.simoahava.com)\n  Page Path - returns a string containing the relative path of the current page (/article)\n  Referrer - returns a string containing the URL of the page which brought the visitor to the current page (https://www.simoahava.com/home/), from document.referrer\n  Utilities   Event - returns a string containing the value stored in the \u0026lsquo;event\u0026rsquo; dataLayer key\n  Environment Name - returns the name of the environment currently being previewed (either via the Share Preview link or the environment\u0026rsquo;s container snippet)\n  Container ID - returns a string containing the container ID (GTM-XXXXXX)\n  Container Version - returns a string containing the current container version\n  Random Number - returns a number, randomized between 0 and 2147483647\n  HTML ID - returns the identifier of the Custom HTML tag. Used with tag sequencing.\n  Errors   Error Message - returns a string containing the error message dispatched by a JavaScript Error trigger\n  Error Line - returns a string containing the line number where the error was thrown\n  Error URL - returns a string containing the URL of the script where the error was thrown\n  Debug Mode - returns a Boolean (true/false) depending on if the user is in GTM debug mode or not\n  Clicks   Click Element - returns an HTML element that was the target of an auto-event action; this object is retrieved from the gtm.element key in dataLayer\n  Click Classes - returns a string contained in the className attribute value of the auto-event element\n  Click ID - returns a string contained in the id attribute value of the auto-event element\n  Click Target - returns a string contained in the target attribute value of the auto-event element\n  Click URL - returns a string contained in the href or action attribute value of the auto-event element\n  Click Text - returns a string contained in the textContent / innerText attribute value of the auto-event element\n  Forms These are exactly the same as the Click variables. I\u0026rsquo;m not sure why we need two sets of variables, when one generic \u0026ldquo;Auto-Event\u0026rdquo; type would suffice.\nHistory   New History Fragment - returns a string containing the new URL fragment after a page history change auto-event action is registered\n  Old History Fragment - returns a string containing the previous URL fragment\n  New History State - returns an object containing the new history state after a pushState() has been registered\n  Old History State - returns an object containing the old history state\n  History Source - returns a string describing the event that initiated the history change (e.g. popstate or pushState)\n  Videos   Video Provider - returns a string containing the video service being tracked (currently YouTube only supported).\n  Video Status - returns a string with the status of the video that caused the trigger to fire. Could be one of 'start', 'pause', 'buffering', 'progress', or 'complete'.\n  Video URL - returns a string with the URL of the embedded video.\n  Video Title - returns a string with the title of the embedded video.\n  Video Duration - returns a number with the total length of the video in seconds.\n  Video Current Time - returns a number with the time mark where the user currently is (i.e. when the event was triggered).\n  Video Percent - returns a number with the percentage mark where the user currently is.\n  Video Visible - returns true or false, depending on if the video was visible in the browser viewport when the event was triggered.\n  Scrolling   Scroll Depth Threshold - returns a number with the value of the scroll tracking threshold that was crossed (e.g. 25 for 25 percent or 25 pixels, depending on which the trigger is configured with).\n  Scroll Depth Units - returns a string with 'percent' or 'pixels', depending on which threshold type the trigger is tracking.\n  Scroll Direction - returns a string with 'vertical' or 'horizontal', depending on which direction is being tracked with the trigger.\n  Visibility   Percent Visible - returns a number with the percentage of visibility for the element whose visibility is being tracked (e.g. 50 if half of the element is in the viewport).\n  On-Screen Duration - returns a number with the total cumulative time (in milliseconds) that the element has been in the viewport when the trigger fires.\n  As you can see, the Built-in variables are just a quicker way to access some of the most common variable types. Read on, and check especially the chapter on variable types, as that will help you understand Built-in variables better as well.\nDebugging Variables In the amazingly wonderful Debug Mode, there\u0026rsquo;s a tab for variables:\n  By clicking that tab, you can explore the state of each variable upon every single dataLayer interaction. As you probably know, every single push to the dataLayer has the potential to change the state of the data used by GTM, which is why you must be able to observe this state with every interaction. And this is what the variables section of the Debug pane enables you to do.\nFor example, here are two different states of dataLayer. The first state is when the Container snippet is first loaded, and the \u0026lsquo;event\u0026rsquo; key is pushed with the value \u0026lsquo;gtm.js\u0026rsquo;:\n  As you can see, I\u0026rsquo;ve selected the \u0026ldquo;Page View\u0026rdquo; event in the left-hand-side navigation of the Debug pane. This corresponds to a dataLayer.push(), where the \u0026lsquo;event\u0026rsquo; key was populated with \u0026lsquo;gtm.js\u0026rsquo;.\nNext, I select the \u0026ldquo;Window Loaded\u0026rdquo; event from the navigation. Here\u0026rsquo;s what the \u0026lsquo;event\u0026rsquo; key looks like in this particular state:\n  As you can see, the value of the \u0026lsquo;event\u0026rsquo; key has changed. So with the debug pane, you can explore the values stored in each variable at any given dataLayer interaction.\nIf a variable has the value undefined, it means that the variable did not resolve. If you see this value in a state where you are certain the variable should have a proper value, it means there\u0026rsquo;s something wrong with your variables, tags, or triggers, and you need to look into it more carefully.\nBy clicking the Tags tab, you can examine what your variables returned in any tag that has fired (or not):\n  You can also look at tags which didn\u0026rsquo;t fire, and focus on the trigger. The visual display will tell you which variable condition did not pass the check, and you\u0026rsquo;ll know there\u0026rsquo;s something for you to debug again:\n  The Debug tool is incredibly useful, as it allows you to double-check how variables work in your current setup.\nVariable Types   There are a number of useful variable types for you to choose from, and you can use the Custom JavaScript variable to create custom variables of your own. Many of the variable types have some cool customization options as well, so you should take the time to study their many uses.\n1. HTTP Referrer   Use the HTTP Referrer variable to identify details about the page that brought the visitor to the current one. The value is retrieved from the document.referrer property. Note: There is a Built-In variable for this (Referrer).\nRETURNS\nString with the URL of the referring page. You can specify a URL component if you wish (see the URL variable for more information about the various component types).\nUSE CASE(S)\nYou could create a trigger which fires when HTTP Referrer does not contain your own domain. This would mean that the user arrived from outside your site to the current page.\n2. URL   The URL variable can be used to access components of the current page URL (default) or of any URL string returned by a variable. This is a very versatile variable type, and is especially useful for traversing query parameters and hash fragments in your URLs. Note: There are Built-In variables for this (Page URL, Page Hostname, Page Path).\nThe Component Types you can choose are:\nFull URL - returns the full URL without the hash fragment, e.g. \u0026lsquo;https://www.simoahava.com/?home=true'.\nProtocol - returns the protocol of the URL, e.g. \u0026lsquo;https\u0026rsquo;.\nHost Name - returns the hostname of the URL without the port number, e.g. www.simoahava.com. You can choose to Strip \u0026lsquo;www.' to strip the \u0026lsquo;www\u0026rsquo; subdomain from the hostname.\nPort - returns the port number used in the URL, or \u0026lsquo;80\u0026rsquo; for HTTP / \u0026lsquo;443\u0026rsquo; for HTTPS, if the URL has no port number.\nPath - returns only the pathname in the URL. You can also specify Default Pages to strip pages with names like \u0026lsquo;index.html\u0026rsquo; or \u0026lsquo;index.php\u0026rsquo; from the return string.\nQuery - returns the entire query parameter string (without leading \u0026lsquo;?'), if you don\u0026rsquo;t specify a query key. If you do specify a query key, only the value of this key is returned, or undefined if no such key is found in the URL.\nFragment - returns the value of the URL\u0026rsquo;s fragment without the leading \u0026lsquo;#\u0026rsquo;, e.g. \u0026lsquo;anchor1\u0026rsquo;.\nYou can expand the More Settings tab to find a source selector. In this selector, you can choose the variable whose return value the URL variable will access.\nRETURNS\nThe return value for the URL type you specified in the Component Type selection, or undefined if no such component is found in the URL variable. By default, the URL variable that is accessed is the page URL, but you can choose any variable which returns a string with a URL in it.\nUSE CASE(S)\nThis is, again, a very versatile variable. For example, check the following article for an example of how to use the URL variable to fix your site\u0026rsquo;s internal search tracking:\n Fix GA Site Search With Google Tag Manager  3. First Party Cookie   The 1st Party Cookie variable returns the value for the first browser cookie with the name you specify in the Cookie Name field. For example, if you have a cookie called \u0026ldquo;session\u0026rdquo;, you can use the 1st Party Cookie variable to retrieve the value for this particular cookie.\nRETURNS\nString containing the value stored in the cookie, or undefined, if no such cookie exists.\nUSE CASE(S)\nI\u0026rsquo;ve used cookies a lot in my guides. Here are two examples for using cookies:\n  #GTMtips: Prevent Repeat Transactions\n  #GTMtips: Once userId, Always userId\n  4. Custom JavaScript   The Custom JavaScript variable is surely the most versatile variable in the set. You can use it to run arbitrary JavaScript on the page. It creates a script context, meaning you can also call other variables from within using the appropriate syntax.\nThe Custom JavaScript variable needs to follow two simple rules. First, the script must be wrapped in an anonymous function block (function() { ... }). Second, the function must have a return statement (return somevalue;).\nThe third, unwritten rule is that the function should only return a value. You shouldn\u0026rsquo;t use a Custom JavaScript variable to modify the global namescape by pushing values to dataLayer for example. If you want to tamper with global variables from a function, it\u0026rsquo;s better to create a Custom HTML tag for this purpose.\nRETURNS\nDepends on what you have in the return statement. You can return any variable or value, even other functions, other GTM variables, or nothing (a simple return; is the equivalent of returning the undefined value).\nUSE CASE(S)\nMany of my articles use the Custom JavaScript variable to some extent. Take a look at these to get started:\n  Custom Event Listeners For GTM\n  Simple RegEx Table For Google Tag Manager\n  Macro Magic For Google Tag Manager\n  5. Data Layer Variable   The Data Layer variable is extremely versatile as well. When you create a Data Layer variable, you specify the Data Layer key whose value you want to retrieve. When the variable is resolved, GTM will look for the most recent value for the key in the internal data model. For primitive values (Strings, numbers, Booleans, functions), the variable will return whatever was most recently pushed into the key. For plain objects and Arrays, the variable will return the result of a recursive merge, where only shared keys are replaced.\nYou can use dot notation to access both Data Layer variable keys which have a dot in their name (e.g. gtm.element), or to access properties of DOM element objects (e.g. gtm.element.dataset.name).\nYou can use dot notation to access Array members as well. Square notation won\u0026rsquo;t work, so replace the square notation with dots: products[0].name becomes products.0.name.\nRETURNS\nThe value stored in the Data Layer variable whose name you point out in the Data Layer variable Name field. You can also retrieve the value of an object property, if you are sure that the variable holds an object. To access Array members, use dot notation instead of square notation. You can also specify a Default Value which will be returned if no variable with the given name can be found from the Data Layer when the variable is resolved. If you don\u0026rsquo;t give a default value, the Data Layer variable will return undefined in case no variable with the given name is found.\nUSE CASE(S)\nThe Data Layer variable is your best friend when you want to make the most of Auto-Event Tracking. The Built-In variables and the Auto-Event variable types only give you a handful of DOM properties to choose from in the auto-event element. Use the Data Layer variable to traverse the gtm.element object as you wish.\nRemember to read my two earlier, in-depth articles about the Data Layer:\n  The Data Layer\n  Google Tag Manager\u0026rsquo;s Data Model\n  6. JavaScript Variable   The JavaScript variable returns the value stored in the global JavaScript variable you specify. Note, this is NOT the same as the Custom JavaScript variable, which is a function declaration.\nRETURNS\nThe value stored in the global JavaScript variable that you specify. If no such global variable exists, the undefined value is returned instead.\nUSE CASE(S)\nHere\u0026rsquo;s an example of using the JavaScript variable with a custom tag. In this example, I show you how to fire a single tag multiple times by increasing a global JavaScript variable counter with each iteration, and then fetching the value of this variable in linked tags and variables.\n Fun With Google Tag Manager: Part 2  7. Undefined Value   The Undefined Value variable is extremely simple (so simple, in fact, that I\u0026rsquo;m wondering why it\u0026rsquo;s not a Built-in variable). Its sole purpose is to return the JavaScript undefined value. So it\u0026rsquo;s essentially the same as a Custom JavaScript variable with:\nfunction() { return; }  RETURNS\nThe variable returns the undefined JavaScript value.\nUSE CASE(S)\nUse it whenever you might want to use the undefined value effectively. For example, you might want to use it in a Lookup Table or a RegEx Table, returning undefined when you want to ignore the value of the table in some cases (such as if you want to drop a Custom Dimension from a hit.\n8. Auto-Event Variable   Auto-Event variables are used to access the target element of an auto-event action (e.g. Click, Error, Form Submit). When you create a new Auto-Event variable, you need to specify just which component of the target element you want to access.\nElement - Accesses the DOM Element itself that was the target of the auto-event action. This element is stored under the key gtm.element in the Data Layer, and you can create your own, customized auto-event variables using the Data Layer variable, and traversing the gtm.element object as you would any other DOM element. For example, to get the value stored in the ID attribute of the auto-event element\u0026rsquo;s parent, you\u0026rsquo;d create a Data Layer variable which points to gtm.element.parentElement.id. Note: There are Built-In variables for this (Click Element and Form Element).\nElement Classes - Returns the value of the class attribute of the auto-event element. Stored in the Data Layer under the key gtm.elementClasses. Note: There are Built-In variables for this (Click Class and Form Class).\nElement ID - Returns the value of the id attribute of the auto-event element. Stored in the Data Layer under the key gtm.elementId. Note: There are Built-In variables for this (Click ID and Form ID).\nElement Target - Returns the value of the target attribute of the auto-event element. Stored in the Data Layer under the key gtm.elementTarget. Note: There are Built-In variables for this (Click Target and Form Target).\nElement Text - Returns the value of either the textContent or innerText property of the auto-event element. The return value is trimmed of whitespace and normalized to account for differences in how browsers interpret element text. Note: There are Built-In variables for this (Click Text and Form Text).\nElement URL - Returns the value of either the href or the action attribute of the auto-event element. You can further specify just which URL component you want to access (see the section for the URL variable type for more information). Stored in the Data Layer under the key gtm.elementUrl. Note: There are Built-In variables for this (Click URL and Form URL).\nHistory New URL Fragment - Returns the new URL fragment set with a browser history event. Stored in Data Layer under the key gtm.newUrlFragment. Note: There is a Built-In variable for this (New History Fragment).\nHistory Old URL Fragment - Returns the old URL fragment replaced in the browser history event. Stored in Data Layer under the key gtm.oldUrlFragment. Note: There is a Built-In variable for this (Old History Fragment).\nHistory New State - Returns the new state object set with a browser history event. Stored in Data Layer under the key gtm.newHistoryState. Note: There is a Built-In variable for this (New History State).\nHistory Old State - Returns the old state object replaced in the browser history event. Stored in Data Layer under the key gtm.oldHistoryState. Note: There is a Built-In variable for this (Old History State).\nHistory Change Source - Returns a string denoting the event that triggered the history change event (popstate, pushState, replaceState, or polling). Stored in Data Layer under the key gtm.historyChangeSource. Note: There is a Built-In variable for this (History Source).\nRETURNS\nThe Auto-Event variable returns the value appropriate for the selected element type. If no relevant auto-event has been registered, the variable returns the Default Value (if set), or undefined.\nUSE CASE(S)\nThere are many use cases for the Auto-Event variable. To get you started, I suggest you take a look at the following articles:\n  Auto-Event Tracking In GTM 2.0\n  Advanced Form Tracking In Google Tag Manager\n  Google Tag Manager: The History Listener\n  9. DOM Element   You can use the DOM Element variable to retrieve the text content of any given DOM Element. You can also use it to retrieve the value of any attribute of the DOM Element.\nRETURNS\nThe text content of the given DOM Element, or the value of the given attribute (optional). If no DOM Element with the given ID or CSS selector is found, the variable returns the null value.\nUSE CASE(S)\nYou can use this to access any arbitrary DOM Element on the page. This becomes useful if you want to fire an event only if a certain element is on the page.\n10. Element Visibility   The Element Visibility variable lets you know if any particular element was visible in the browser viewport when the trigger fired. Visibility requires that the element be positioned above the fold of the page in the active browser tab. In other words, the element must actually be in sight of the user. The only exception is if there is some other window in front of the browser window where the element is otherwise visible. In this case, GTM would consider the element to still be visible, even though it\u0026rsquo;s not technically viewable by the user.\nRETURNS\nThe variable returns either True / False indicating if the element was visible or not, respectively, or a percentage of how much of the element was visible when the variable was resolved. You can choose which output type to use in the settings of the variable.\nUSE CASE(S)\nYou can set the visibility variable to check an element with CSS selector body, and minimum 1 percent visible. This variable would then tell you if the page was visible in the viewport when the variable was called.\n11. Constant   The Constant variable is a prime example of how variables are reusable. If you have any string of characters that you need to use often, or which you might need to update in the future, it\u0026rsquo;s best to store it as a Constant variable instead.\nRETURNS\nThe Constant variable returns the string you choose to type in the Value field.\nUSE CASE(S)\nThe obvious use case is your Google Analytics web property ID. By storing the UA-XXXXX-X code in the Constant variable, you won\u0026rsquo;t need to look it up every single time you create a new GA tag.\n12. Custom Event   The Custom Event variable returns the value of the \u0026lsquo;event\u0026rsquo; key in the Data Layer. For example, if you run the following code: dataLayer.push({'event' : 'thisEvent'});, then the Custom Event variable would hold the value \u0026lsquo;thisEvent\u0026rsquo; after the push. Note: There is a Built-In variable for this (Event).\nRETURNS\nThe value stored in the \u0026lsquo;event\u0026rsquo; key in the Data Layer.\nUSE CASE(S)\nHonestly, I can\u0026rsquo;t figure out what this variable is for. If I had to guess, it\u0026rsquo;s a remnant of the old GTM, where you could accidentally delete the {{event}} macro and then use the Custom Event macro to bring it back. In the new UI, there\u0026rsquo;s a Built-In variable for Event, and at the time of writing there\u0026rsquo;s also the internal variable _event which you can\u0026rsquo;t delete or deactivate. So there really is no need to create a new Custom Event variable.\n13. Environment Name   The Environment Name variable is similar to Custom Event in that it doesn\u0026rsquo;t really add anything to GTM. There already is a Built-In variable for \u0026ldquo;Environment Name\u0026rdquo;, which you should use instead of creating this User-Defined variable.\nRETURNS\nString with the current environment name if using an environment snippet, or the draft version identifier if in Preview mode. It won\u0026rsquo;t return anything for Live or Latest versions.\nUSE CASE(S)\nUse it to fire tags only if in a certain environment. Remember to read my Environment Guide while you\u0026rsquo;re at it!\n14. Google Analytics Settings   The Google Analytics Settings variable returns a set of Universal Analytics tag settings. This can be used to configure multiple tags at once, consolidating their Custom Dimensions and Fields to set, for example.\nNote that the Google Analytics Settings variable can only be used in a Universal Analytics tag. You can\u0026rsquo;t invoke the variable in other contexts.\nRETURNS\nA configuration of Google Analytics Settings to be used in a Universal Analytics tag.\nUSE CASE(S)\nUse the Google Analytics Settings variable to consolidate tag settings across your Universal Analytics tags. For inspiration, read this article.\n15. Lookup Table   The Lookup Table variable performs any number of lookups that you specify, returning the value of the first match. Since this is a Lookup Table, the value lookup is always exact match and case-sensitive. You can create your own, custom Lookup Table if this is too strict for you.\nThe Input variable specifies the variable which will be used as the input in the lookups. On each row that you add to the table, you give an output value to be returned by the variable, if the input variable is matched with the Input field value of the row. You can chain Lookup Tables together, creating a powerful, efficient, and flexible value lookup for your tags and variables.\nRETURNS\nThe value in the Output field of the first row that is matched against the Input variable. You can also give a Default Value which will be returned if no match is made. If you don\u0026rsquo;t specify a Default Value, the variable will return the undefined value if no match is made.\nUSE CASE(S)\nTake a look at my original article for some ideas:\n Google Tag Manager: The Lookup Table Macro  Bounteous has also a really interesting guide on automating the Lookup Table variable creation process.\n16. Random Number   The Random Number variable returns a random number between 0 and 2147483647. Note: There is a Built-In variable for this (Random Number).\nRETURNS\nA number, randomized between 0 and 2147483647.\nUSE CASE(S)\nYou can use the Random Number variable to sample your visitors.\n17. RegEx Table   The RegEx Table variable lets you create a pattern-matching table. It functions similarly to the Lookup Table, with the obvious difference that lookups are exact match only, whereas regular expressions are far more flexible in what you can match against. There are some other additional features in the RegEx Table, which you can read about here.\nRETURNS\nWhatever you have defined in each individual Output field, and the Default Value field.\nUSE CASE(S)\nYou can use the RegEx Table with a Just Links trigger to determine what type of link was clicked. See the screenshot above for hints how to do this.\n18. Container ID   This is also a Built-In variable, so use that instead of creating a new User-Defined variable.\nRETURNS\nThe public ID (GTM-XXXXXX) of the Container.\nUSE CASE(S)\nThis is important in tag sequencing, as it\u0026rsquo;s used to signal when a Custom HTML tag has finished completion. You should also send the Container ID as a Custom Dimension in your GTM hits - that way you can see in Google Analytics which hits were sent from which container.\n19. Container Version Number   This is pretty self-explanatory. The Container Version Number returns the version number of the container that is implemented on the site, or QUICK_PREVIEW if you are previewing the workspace draft. Note: There is a Built-In variable for this (Container Version).\nRETURNS\nString with the current GTM Container version number, or QUICK_PREVIEW, if the workspace draft is in preview mode.\nUSE CASE(S)\nAssign a Google Analytics Custom Dimension for the container version ID, which will help you analyze the impact of changes various versions of the GTM container have had. This is a great way to debug your GTM and GA implementations.\n20. Debug Mode   The Debug Mode variable returns true if the user is viewing the container in Preview mode, and false if not. Note: There is a Built-In variable for this (Debug Mode).\nRETURNS\ntrue when the user is in Container Debug mode, and false when not.\nUSE CASE(S)\nWith the Lookup Table variable, you can collect all your GA hits in Debug mode to a separate, test property. Read about this idea in my Macro Magic article.\nYou can also send the Debug Mode value as a Custom Dimension, if you want to collect all hits to the same GA property. This way you can filter with the Custom Dimension and collect all your debug hits to a separate Google Analytics profile.\nSummary I\u0026rsquo;m bold enough to claim that once you understand variables, you can call yourself a Google Tag Master. It\u0026rsquo;s not just what the different variable types are. It\u0026rsquo;s how you use them together to create sense out of the complexity that almost any tag implementation brings in its wake.\nJust remember to pay heed to the technical details as well. If you don\u0026rsquo;t understand the process of how variables are resolved, you\u0026rsquo;ll often run into unexpected situations. Most often it\u0026rsquo;s a \u0026lsquo;race condition\u0026rsquo;, where the variable is trying to access some other data source which isn\u0026rsquo;t ready yet.\nBe respectful of the \u0026lsquo;no side effects\u0026rsquo; rule as well. Do not use variables for anything else except to build a well-formed return statement. If you feel like you need to increase the complexity of your function calls, use a Custom HTML tag instead.\nWere you interested in some other aspect of variables that I didn\u0026rsquo;t cover here? Or do you have a great use case in mind for some variable type? Sound off in the comments, thanks!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/track-content-enhanced-ecommerce/",
	"title": "Track Content With Enhanced Ecommerce",
	"tags": ["ecommerce", "enhanced ecommerce", "Google Tag Manager", "Guide"],
	"description": "Use Google Tag Manager to track your site content via the Enhanced Ecommerce reports in Google Analytics.",
	"content": "My fingers have been tingling to write this article. Ever since I implemented Enhanced Ecommerce on my blog a couple of weeks ago, I\u0026rsquo;ve been getting such an impressive amount of useful data that it\u0026rsquo;s mind-boggling.\nIn this article, I\u0026rsquo;ll walk you through the steps I went to implement the solution, along with examples of the data I can now access through Google Analytics\u0026rsquo; reporting interface. As you might have guessed, if you\u0026rsquo;ve read my articles before, I implemented Enhanced Ecommerce with Google Tag Manager. Note, this is not a step-by-step guide, but should rather provide inspiration for you to think out-of-the-box when it comes to Google Analytics\u0026rsquo; features.\nEcommerce and content, huh?   You might be surprised at what the premise of this article is. Usually, Ecommerce plugins are used to track transactions on your web store, so what does this have to do with content? Well, I don\u0026rsquo;t have a web store on my blog, but every reader who reads an article is valuable to me. If they are valuable, there must be some way to measure their value, and perhaps use this data to benchmark against future products (i.e. articles) I want to create.\nEnhanced Ecommerce gives you a bunch of useful reports, which you can use to track not only store transactions but any kind of user interaction on your site. You just have to see your site in terms of funnels. This means that you need to translate Ecommerce terminology to match the conceptual framework of your site, whether it\u0026rsquo;s a blog, a web store, a news portal, or a brand site.\nOn my blog, the terminology ended up something like this:\n  Product: A blog article.\n  Product price: The number of words in the article.\n  Product impression: At minimum the title, but usually title + ingress combination of articles on various category pages, and in article sidebars.\n  Product list: A page or widget which holds a number of Product impressions. For example, my home page, category pages, tag pages, and related posts lists are all considered Product lists in my Enhanced Ecommerce setup.\n  Product list click: A click action on an article title or \u0026ldquo;Read more\u0026hellip;\u0026rdquo; link in the product lists.\n  Product detail view: When the article is loaded.\n  Add to cart: When scrolling begins on the article. I can assume that the reader wants to \u0026ldquo;buy\u0026rdquo; it if they start scrolling.\n  Checkout: The Checkout funnel in my setup is based on scroll tracking. The first step is when the reader reaches one third of the article, the second step when they scroll two thirds, and the last step is when the article end is reached.\n  Purchase: Purchase occurs when the checkout funnel is passed through, and a minimum of 60 seconds has elapsed since the article was loaded. This is an arbitrary number of seconds I simply chose to weed out casual readers from actual readers.\n  UPDATE: Thanks to an idea from Robert Petković, I updated the collection method with Product Detail impressions and Add To Cart actions. These were missing from the first version of this article. I also changed the checkout funnel to reflect scroll depth.\nHow to set it up First, here\u0026rsquo;s the Git repository link for this solution: eec-gtm.\nI\u0026rsquo;ll reveal a dirty secret from the get-go: I scrape the DOM for my setup. It\u0026rsquo;s definitely not the most robust way to go, but since I\u0026rsquo;m the developer, the marketer, the owner, and the content creator on my site, I can be safe to know that any changes to the page template are totally under my own control.\nThe way it SHOULD work is to leverage dataLayer. You have a number of options when doing this. You could, for example, store every single product on the page into dataLayer when the page is rendered, and then pick the relevant objects when impressions are loaded, or when user actions like clicks take place. Another way to go is to store the products in some other global JavaScript variable, which is, perhaps, a bit easier to access, but it does pollute the global namespace which should generally be avoided.\nAnyway, I scrape. I\u0026rsquo;m a scraper. I did it for science, for progress, for technology. And because I was a bit lazy and didn\u0026rsquo;t want to customise my WordPress hooks. But, in short, here\u0026rsquo;s how my setup works.\n1. Product impressions On every page which has product lists, I build the ecommerce.impressions Array as soon as the DOM has loaded. In this Array, each object is a single article title (+ ingress) that\u0026rsquo;s visible in one of the possible product lists. Product lists on my site are:\n  Main posts - the home page listing\n  Category posts - if a visitor has chosen to see all posts in a given category\n  Tag posts - if a visitor has chosen to see all posts tagged with a specific tag\n  Search results - the list of results you get if you use internal search\n  Recent posts - the \u0026ldquo;Recent posts\u0026rdquo; widget in the sidebar\n  Recent comments - the \u0026ldquo;Recent comments\u0026rdquo; widget in the sidebar\n  GTM Tips - the \u0026ldquo;GTM Tips\u0026rdquo; widget in the sidebar\n  List position is determined by the order of posts in the list. An individual product object would look something like this:\n{ \u0026#39;category\u0026#39; : \u0026#39;analytics\u0026#39;, \u0026#39;id\u0026#39; : \u0026#39;TriggerGuideForGoogleTagManager\u0026#39;, \u0026#39;list\u0026#39; : \u0026#39;Main posts\u0026#39;, \u0026#39;name\u0026#39; : \u0026#39;Trigger Guide For Google Tag Manager\u0026#39;, \u0026#39;position\u0026#39; : 2 }  As you can see, it\u0026rsquo;s very simple. category is the WordPress category assigned to the post (there\u0026rsquo;s only every one category on my articles), id is a truncated version of the article title, name is the name of the article, and list and position define where the impression was listed.\nSo for every impression, I push an object like above into the Array. I also have a Promotion view for my \u0026ldquo;Were you looking for my GTM posts?\u0026hellip;\u0026rdquo; info box on the home page of my site, but this hasn\u0026rsquo;t proven very useful, so I might remove it.\nOn any given page, the Array might look something like this:\n  A very simple setup for a very simple purpose. I send this Array with a Non-Interaction: True Event tag, because I don\u0026rsquo;t want to delay my pageview from firing until the impression Array is built, and I don\u0026rsquo;t want impressions to affect bounce rate.\n2. Product list clicks I track product list clicks using a Link Click trigger. When someone clicks on an article title or the \u0026ldquo;Read more\u0026hellip;\u0026rdquo; link on the product list, I push details about the clicked product into dataLayer, together with the \u0026lsquo;productClick\u0026rsquo; value for the \u0026lsquo;event\u0026rsquo; key, which then triggers an Event tag.\n  Firing this event lets me see the effectiveness of my product lists. It gives me information about how the different lists fare in light of the entire customer journey from product impression to purchase.\n3. The Checkout Flow The checkout flow combines Product Detail impressions, Add To Cart actions, and the checkout flow itself.\nA Product Detail impression is sent as a Non-interaction: True event as soon as the article is loaded. This impression can thus be interpreted as the reader quickly checking out whether or not they want to read the article.\nThe Add To Cart action occurs when the user starts scrolling. The payload is sent with a normal Event tag to Google Analytics. I consider scrolling to be revealing of the reader\u0026rsquo;s intention to consume the content, but it\u0026rsquo;s not a checkout yet, as they might just want to skim the first paragraph.\nThe checkout flow itself is pretty cool. I use the scroll tracking plugin Justin Cutroni wrote about on his site. I\u0026rsquo;ve modified it to work with Google Tag Manager, and I also customised it to work with \u0026ldquo;Purchases\u0026rdquo; as well (see next chapter).\nHere\u0026rsquo;s how it works right now:\n  I calculate the length of the content DIV in pixels.\n  When the viewport of the user\u0026rsquo;s browser reaches one third of this length, the first checkout step is sent as a normal Google Analytics event. This step is labelled \u0026ldquo;Read one third\u0026rdquo;.\n  When the browser reaches two thirds of the content length, the second checkout step is sent as a GA event. This step is labelled \u0026ldquo;Read two thirds\u0026rdquo;.\n  When the browser reaches the end of the content DIV, the final checkout step is sent as a GA event. This step is labelled \u0026ldquo;Reached end of content\u0026rdquo;.\n  Finally, if the user has reached Step 3 and spent a minimum of 60 seconds on the article page, the \u0026ldquo;Purchase\u0026rdquo; event is sent as well (see next chapter).\n    You\u0026rsquo;ll need to modify this funnel and the setup to match the type of content you write. You might want to change the dwell time from 60 seconds to something else, and you might want to add more steps to the checkout funnel (25 %, 50 %, 75 %, for example). For me, this level of granularity was enough.\n  That\u0026rsquo;s a sample checkout object for the second step of the funnel. As you can see, I have price as one of the properties of the article. Here\u0026rsquo;s the kicker: price is the number of words on the article. Naturally, I\u0026rsquo;ve turned it into a \u0026ldquo;.99\u0026rdquo; number to make it more realistic as an actual price :-) You\u0026rsquo;ll see the usefulness of this once I get to the reports.\n4. Purchase Like I wrote in the previous chapter, a \u0026ldquo;Purchase\u0026rdquo; event is pushed when the checkout funnel is completed, and the visitor has spent 60 seconds on the site. The purchase itself is a perfectly standard Enhanced Ecommerce Purchase object, which might look like this:\n  The transaction ID is basically epoch timestamp plus a string of random characters. The quantity of products in a transaction will always be 1.\nThe analysis So, let\u0026rsquo;s go over my favorite reports and segments. First, there\u0026rsquo;s the Shopping Behavior report:\n  As you can see, this shows the interactions during the selected timeframe. In this example, the timeframe is just one day. I\u0026rsquo;ve removed the absolute numbers, but I\u0026rsquo;ll update this screenshot once I have more data. The behavior funnel is fairly logical. There\u0026rsquo;s around 20 % drop-off on each step, with a larger abandonment leading up to purchase. 15 % of people never open an article, which is interesting! This could also be a measurement error thanks to my DOM scraping, but it\u0026rsquo;s still understandable considering the amount of traffic I get on my blog.\nAlso, 25 % of the people who start scrolling never reach one third of the article. This is also interesting. It means that there\u0026rsquo;s something in the first paragraphs that drives the reader away.\nThis data should next be segmented and carefully analysed. How can I optimise the funnel further? Two immediate concerns I have is the overall low conversion rate (only 25 % of my readers end up reading an entire article while spending more than 60 seconds on the article page), and the fact that only 40 % of people who start reading end up reading the article thoroughly. This is, of course, a sign of normalcy in the blogosphere, but it\u0026rsquo;s definitely something I want to improve.\n  Now this is interesting! Half of my readers start in the checkout funnel, and only half of these reach the end and read for more than 60 seconds. Talk about selective reading! My content is pretty lengthy by average, so it\u0026rsquo;s interesting to see if article length is a factor here. Or maybe some people just jump straight to the comments, which is perfectly understandable. Actually, I should track this as well! Mental note.\nSo there\u0026rsquo;s a big disconnect between starting to read and reaching the end of content. I might have to do something about this. Like, writing more interesting articles. But it\u0026rsquo;s still respectable how many people actually take their time to read the article.\nNaturally, one problem with this Checkout Funnel report is that the checkout funnel varies from article to article. Longer articles have a far higher threshold for hitting the funnel steps (since the steps are dependent on the pixel height of the content DIV), which means I should see a far higher rate for checkouts on shorter articles.\nNext, we have the Product Performance report:\n  Oh, this is so much fun. During the week, almost six million words have been \u0026ldquo;Purchased\u0026rdquo; on my articles! This means that six million words worth of article content passed through the checkout funnel into the Purchase column. Awesome!\n(UPDATE: I\u0026rsquo;m still waiting to get more data before updating this chapter with Buy-to-Detail and Cart-to-Detail rate analyses.)\nIf you look at Average Price, you can see that the list is topped by some of my longer articles. It\u0026rsquo;s still heart-warming to see some shorter ones on the top 10 list, delivering me \u0026ldquo;word revenue\u0026rdquo;.\nSales Performance isn\u0026rsquo;t very useful, since transactions are pretty arbitrary on my blog. I\u0026rsquo;ll jump straight to Product List Performance (sorry about the poor screenshot):\n  Key takeways from this report are that my sidebar widgets aren\u0026rsquo;t really useful. I should probably get rid of them as soon as I can think of something value-adding to put there instead. Well, the \u0026ldquo;Recent posts\u0026rdquo; list has a pretty high number of clicks, so I might preserve that.\n(UPDATE: I\u0026rsquo;m still waiting to get more data before updating this chapter with Product Adds To Cart analysis.)\nMy top-performing list is naturally the home page listing. It has a very respectable CTR of 16.47 %. \u0026ldquo;Category posts\u0026rdquo; and \u0026ldquo;Tag posts\u0026rdquo; are much less popular, but even they attract clicks quite a bit.\n\u0026ldquo;Search results\u0026rdquo; is doing very well, which is important. I want people to find what they were searching for. Naturally, I follow my most searched-for terms like a hawk, getting content ideas at the same time.\nSince I have all this amazing data at my fingertips, I can create a bunch of awesome segments as well:\n  Whales: Revenue per user \u0026gt; 10000 - To track readers who\u0026rsquo;ve \u0026ldquo;purchased\u0026rdquo; more than 10,000 words in their lifespan.\n  Passers-by: Transactions per user = 1 - To track readers who\u0026rsquo;ve only \u0026ldquo;purchased\u0026rdquo; a single article in their lifespan.\n  Skimmers: Transactions per user = 0 AND Event Action exactly matches Checkout - To track readers who\u0026rsquo;ve started the checkout flow but never completed a \u0026ldquo;purchase\u0026rdquo;.\n  Loyal readers: (Include Users) Revenue per hit \u0026gt; 3000 - To track readers who only read my longer articles.\n  And many other segments as well! Now I can segment my channels to see which channels bring the most valuable readers. Note that I don\u0026rsquo;t automatically consider it more valuable to have a reader read my longer articles, which is where the whole \u0026ldquo;words as price\u0026rdquo; idea falls down. I enjoy writing short articles as well, and especially my #GTMTips posts are usually a bit shorter length-wise.\nSummary I hope this post has been inspiring. The data I\u0026rsquo;m getting into my reports is so interesting and actionable. It does require tweaking, however, so I might need to work with the checkout funnel, in order to optimise the flow of reading from article load to the end of content.\nWith the help of one of my blog commenters, I\u0026rsquo;ve updated the setup to include Product Detail impressions and Add to Cart actions, but it will still take a week or so to get more data, so you can expect another update to this article soon.\nLet me know if you\u0026rsquo;ve tried something like this or if you have other ideas for tracking content with Enhanced Ecommerce! I\u0026rsquo;m sorry I can\u0026rsquo;t really give you a step-to-step guide at this point, but as I\u0026rsquo;ve done this with DOM scraping, I don\u0026rsquo;t really want to flaunt the solution, since I don\u0026rsquo;t consider it best practices. But they key thing in this article is to inspire you to think of your content in terms of funnels and transactions. I really love the new Enhanced Ecommerce reports, and I hope I\u0026rsquo;ve shown you how flexible they are for other uses as well than just web stores.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/prevent-repeat-transactions/",
	"title": "#GTMTips: Prevent Repeat Transactions",
	"tags": ["ecommerce", "Google Tag Manager", "gtmtips"],
	"description": "Use this method to prevent transactions from being sent to Google Analytics each time the user reloads the thank you page. This is necessary for accurate Ecommerce reporting.",
	"content": "In this tip, we\u0026rsquo;ll take a look at how to leverage a custom first-party cookie to prevent repeat hits of any kind. This is most useful for transactions, since a common problem with Google Analytics (traditional) eCommerce tracking is that a transaction hit is sent again upon subsequent entries to the receipt page, for example using the Back button of the browser. In some cases, and this is not a good practice, a receipt e-mail is sent to the user with a link back to the receipt page, where the transaction is sent over and over again upon entry.\nTip 11: Use a 1st Party Cookie to prevent hit multiplication   Here is the use case:\n  User lands on the receipt page\n  The transaction tag checks if the transaction ID is in the transaction cookie\n  If the ID is in the cookie, the transaction tag does not fire\n  If the ID is not in the cookie, the transaction tag fires\n  The transaction ID is added to the cookie in the hitCallback of the tag\n  So it\u0026rsquo;s very similar to what I wrote some time ago about firing a tag just once per session.\nFor this to work, you\u0026rsquo;ll need four variables:\n  Data Layer Variable for the key transactionId in the transaction payload on the receipt page\n  1st Party Cookie Variable, which retrieves the value of the cookie where all the user\u0026rsquo;s transaction IDs are stored\n  Custom JavaScript Variable, which returns true if the transaction ID in the receipt page is found in the cookie\n  Custom JavaScript Variable, which returns the callback function, where the current transaction ID is added to the cookie after the transaction tag has fired\n  Data Layer Variable: {{dlv transaction id}} This one is easy. Just create a new Data Layer Variable called {{dlv transaction id}}, and have it point to the variable where the transaction ID is stored on the receipt page (should be transactionId).\n1st Party Cookie: {{cookie user transactions}} Create a new 1st Party Cookie variable called {{cookie user transactions}}, and have it refer to whatever you want your cookie to be called. In this example, I\u0026rsquo;ll use user_transaction_ids.\n  Custom JavaScript: {{js is transaction id in cookie}} The purpose of this variable is to return true if the transaction ID on the thank you page is found in the cookie. This means that one of the conditions of the Transaction Tag firing trigger is a check for if the cookie value is not true. This would mean that the transaction ID has not been fired yet by this user. The code looks like this:\nfunction() { return /(,|^){{dlv transaction id}}/.test({{cookie user transactions}}); }  Custom JavaScript: {{js hitcallback set transaction cookie}} In this Custom JavaScript variable, we\u0026rsquo;ll add the value returned by {{transactionId}} to the cookie that\u0026rsquo;s referenced in the variable we just created above. Since I\u0026rsquo;m assuming the visitor can have multiple transactions, the cookie will be appended with the value, not overwritten. The expiration date depends on the lifetime of a single transaction ID. In my case, I\u0026rsquo;m assuming that they\u0026rsquo;re always unique, so I have an expiration time of two years (completely arbitrary figure). If you know that transaction IDs are not unique, you should make the expiration shorter to reflect this.\nHere\u0026rsquo;s what the {{js hitcallback set transaction cookie}} should have within:\nfunction() { return function() { var d, expires; var cvalue = \u0026#39;\u0026#39;; // Run the code only if a transaction is found in the data layer  if ({{dlv transaction id}}) { d = new Date(); d.setTime(d.getTime() + (2*365*24*60*60*1000)); expires = \u0026#39;expires=\u0026#39;+d.toUTCString(); // If the cookie already exists, append not overwrite  if ({{cookie user transactions}}) { cvalue = {{cookie user transactions}} + \u0026#39;,\u0026#39;; } document.cookie = \u0026#39;user_transaction_ids=\u0026#39; + cvalue + {{dlv transaction id}} + \u0026#39;; \u0026#39; + expires + \u0026#39;; path=/\u0026#39;; } }; }  Set up the tag Finally, you need to set up the tag. The only modifications you need to make are to the firing trigger and the hitCallback field. For the firing trigger, you need to add the following condition:\n{{js is transaction id in cookie}} does not equal true\nThis means that the tag will only fire if the transaction ID on the page template is not found in the cookie. Makes sense, right?\nNext, you need to set up the hitCallback field. So scroll down to More Settings -\u0026gt; Fields To Set, and fill in the values like this:\n  And that should do it.\nSummary Here\u0026rsquo;s what should take place when a user lands on the receipt page:\n  The Transaction Tag checks if the transaction ID on the page template (stored in {{dlv transaction id}}) can be found in the value returned by the cookie variable {{cookie user transactions}}.\n  If the ID exists in the cookie, it means that a transaction has already been fired with this ID, so nothing is done.\n  If the ID doesn\u0026rsquo;t exist, the transaction tag fires.\n  After the tag has completed execution, the hitCallback function, {{js hitcallback set transaction cookie}} is invoked.\n  This function checks if the cookie already exists. If it does, it appends the current transaction ID to the end of the cookie (using a comma as the separator). If the cookie doesn\u0026rsquo;t exist, a new cookie is created with the transaction ID as its only value.\n  This way any repeat visits to the page with the same transaction ID in the template will not cause the tag to fire again.\n  It looks like a complicated way to do things, and on some level it probably is. However, GTM and GA are stateless in the user\u0026rsquo;s browser. The only things they persist are the tracking cookies. Everything else has to be done by the user, which is why we need to manually create the cookies and the logic behind them.\nUPDATE: David Vallejo has written a great guide on how to achieve the same in Enhanced Ecommerce. Check it out!\n"
},
{
	"uri": "https://www.simoahava.com/tags/rules/",
	"title": "rules",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/trigger-guide-google-tag-manager/",
	"title": "Trigger Guide For Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "rules", "triggers"],
	"description": "Comprehensive guide to Google Tag Manager&#39;s triggers. This trigger guide goes under the hood and gives educational examples of each trigger in use.",
	"content": " Updated 27 March 2019\n In the new version of Google Tag Manager, one of the most visible and profound changes to the previous version is how tags are fired (and blocked). First of all, there\u0026rsquo;s the obvious terminological distinction: we talk about triggers now, not rules. Second, triggers have become an integral part of the tag creation workflow, and as such have far more significance in the user interface than before.\n  The following text is a standalone article. It\u0026rsquo;s not an update to the guide I wrote for the previous GTM version: Google Tag Manager: Playing By The Rules, and many of the concepts covered therein are still valid.\nTriggers introduce many new features and functionalities (a new way of doing auto-event tracking, for example), which require special attention.\nHere are the contents of this guide:\nAs always, feel free to add your thoughts in the comments after this post, if there\u0026rsquo;s some aspect of triggers that wasn\u0026rsquo;t adequately covered.\n1. What\u0026rsquo;s changed There are three major changes, in my opinion, to how triggers compare with the rules of the previous GTM version.\nI) Triggers are integrated into the workflow Sure, this is more a cosmetic thing, but it\u0026rsquo;s significant. In the previous version of Google Tag Manager, the entry-level for beginners was quite high. This was partly due to the fact that it wasn\u0026rsquo;t clear just what firing rules and blocking rules do. It got even more confusing once you started learning about stuff like implicit events and conditions, and it didn\u0026rsquo;t help that the UI gave very little hints about what to do next.\nIn the new UI, everything is about the workflow. There\u0026rsquo;s actually a question there now: What triggers this tag to fire? Nothing about firing rules or blocking rules, nothing about event conditions, and so forth. Just the simple, quite self-explanatory question. Of course, to understand what \u0026ldquo;fire\u0026rdquo; means, you will need to do some exploration, but the question sets the scene: a tag needs a trigger to fire.\n  So the biggest obvious change is in the UI. Triggers are an integral part of the workflow, and it\u0026rsquo;s impossible to not notice them, as you may have with rules in the previous version.\nII) You don\u0026rsquo;t need to explicitly state the \u0026lsquo;event\u0026rsquo; condition The second major change is how you input the event condition. If you\u0026rsquo;ve read my previous posts about rules, or if you\u0026rsquo;ve paid attention to the developer guides, you\u0026rsquo;ll know that every single tag needs an \u0026lsquo;event\u0026rsquo; push to fire. With \u0026lsquo;event\u0026rsquo; push I\u0026rsquo;m of course referring to a dataLayer.push() command that gives a value to the key labelled \u0026lsquo;event\u0026rsquo;.\n  This time around, you don\u0026rsquo;t need to explicitly state the required Event condition (i.e. the value that \u0026lsquo;event\u0026rsquo; key needs to have for the tag to fire). Instead, you rely on the available trigger types to set this condition up for you. In fact, the only time you have to explicitly state the name of the event is if you use a Custom Event type, and even then you don\u0026rsquo;t have to worry about setting it up as a proper condition. You just give the event its name in the required field (you can also use RegEx matching if you want more flexibility):\n  The point is that since Event is such a super-important part of any tag or trigger, it\u0026rsquo;s taken out of the normal condition-based setup for triggers and elevated to a grander status.\nIII) Auto-event tracking has changed I\u0026rsquo;ve already written a guide on this, and I\u0026rsquo;ll cover some of the concepts in this article as well, but this is really significant. Auto-event tracking is no longer tag-based, as it was in the previous version. Instead, you specify the event (click, link click, form submit, error, history event, or timer) by choosing one of the respective trigger types. Once you create a trigger for an auto-event type, GTM automatically starts listening for these events on your site.\n  This is incredibly convenient, since it reduces clutter in your tags, and it makes event tracking a very central part of any tagging setup (as it should be).\nThese are, in my view, the biggest changes. As a GTM fanboy, my honest and utterly biased opinion is that the improvements are amazing. There\u0026rsquo;s a learning curve, and I know that improvements are made to the UI constantly. But the way that triggers have been integrated into the tag workflow, without compromising any of the features carried over from rules, is a wonderful display of design skill.\n2. Technical overview It\u0026rsquo;s not like I\u0026rsquo;ve reverse engineered the GTM container JavaScript library (well, not all of it), but there are some interesting things to consider when working with triggers.\nFirst of all, as I said above, triggers require a dataLayer.push() or a pre-container-snippet declaration to fire a tag. If a push() command doesn\u0026rsquo;t have an \u0026lsquo;event\u0026rsquo; key, it becomes merely a \u0026ldquo;message\u0026rdquo; that\u0026rsquo;s added to the message bus. It does nothing for tags. You can see this in the debug mode. If there\u0026rsquo;s an \u0026lsquo;event\u0026rsquo; key in the message, the instance gets the name of the event (unless it\u0026rsquo;s one of the three default GTM events, see below) or just \u0026ldquo;Message\u0026rdquo; if there\u0026rsquo;s no \u0026lsquo;event\u0026rsquo; key in the command:\n  So if there\u0026rsquo;s an \u0026lsquo;event\u0026rsquo; key in the push(), a data layer helper object activates and goes through all the active triggers in your container. If any one of these matches the value of the \u0026lsquo;event\u0026rsquo; key, and if all the other conditions in the trigger pass, the tag is injected into the site and its code is executed.\nVery little has changed, then. You will still need to push \u0026lsquo;event\u0026rsquo; values to fire tags. The three default events are still gtm.js (pushed when the container snippet is first rendered in the page template), gtm.dom (pushed when the DOM has loaded), and gtm.load (pushed when the window has loaded):\n  One thing that has changed is how multiple triggers of same type on the same page are handled.\nSince there\u0026rsquo;s no longer a listener tag that you work with, it\u0026rsquo;s more than possible that you can have many link click triggers, for example, activating tags on the same page. It doesn\u0026rsquo;t mean that GTM always attaches a new event listener to the document node, because that would be an exercise in redundancy. Rather, all the triggers that use the same auto-event, e.g. gtm.linkClick, are evaluated when the event occurs.\nThis is fine in most cases, but it\u0026rsquo;s also possible that you have two Link Click triggers, where on one you have \u0026ldquo;Check Validation\u0026rdquo; set to ON, and on the other it\u0026rsquo;s OFF. This means that the first one only fires if the default action of the link click has not been prevented by other scripts, and the latter fires regardless. Since there\u0026rsquo;s just one listener controlling the firing of your tags, GTM leverages the gtm.triggers key in the auto-event object to specify which trigger should fire upon the event:\n  The value of the key is containerID_triggerID. So in this particular example, I had two Link Click triggers firing on the page, and one of them had \u0026ldquo;Check Validation\u0026rdquo; ON. I then clicked a link where I had prevented the default action of the click with event.preventDefault(). Thus, the gtm.triggers key tells our tags that only the trigger where \u0026ldquo;Check Validation\u0026rdquo; was OFF (id 27) is allowed to fire.\nThis makes for a very economical but still extremely robust setup for your auto-event triggers.\n3. Triggers in the workflow There are two obvious paths to creating triggers: 1) through the tag workflow, and 2) via the Triggers menu.\nBoth paths take you essentially through the same steps.\nIn the tag workflow, you can enter the trigger selection screen in three different ways:\n  If the tag has no triggers, then clicking the big empty space with \u0026ldquo;Choose a trigger to make this tag fire\u0026hellip;\u0026rdquo; will open the trigger selector.\n  If the tag already has triggers, then clicking the blue plus symbol lets you add new triggers to the tag.\n  You can also click the ADD EXCEPTION link to add triggers that block the tag from firing.\n    In the trigger selection screen, clicking the blue plus button in the top right corner takes you to trigger creation mode.\nYou can also enter trigger creation mode by browsing to Triggers and clicking the red NEW button in the UI.\nRegardless of which path you take, this is what you\u0026rsquo;ll see:\n  Read on for information how to create your new trigger.\n4. New trigger creation When you start creating a new trigger, you should already have a good idea of which GTM event should fire your tag.\n  The trigger type is essentially a combination of the GTM event and the type of interaction (or event) you want GTM to start listening to.\nHere are the trigger types currently available:\nPage View:\n  DOM Ready - fires the trigger on the gtm.dom event, once the browser has loaded the document object model.\n  Page View - fires the trigger on the gtm.js event, as soon as the GTM container has loaded.\n  Window Loaded - fires the tirgger on the gtm.load event, dispatched once the entire page and all linked resource have completed loading.\n  Click:\n  All Elements - fires the trigger on the gtm.click event, dispatched when any element is clicked on the page.\n  Just Links - fires the trigger on the gtm.linkClick event, dispatched when a link (\u0026lt;a\u0026gt;) HTML element is clicked on the page.\n  User Engagement:\n  Element Visibility - fires the trigger on the gtm.elementVisibility event, when an element becomes visible on the page.\n  Form Submission - fires the trigger on the gtm.formSubmit event, dispatched when a form submission is detected.\n  Scroll Depth - fires the trigger on the gtm.scrollDepth event, dispatched when the user scrolls the page.\n  YouTube Video - fires the trigger on the gtm.video event, dispatched when a video is viewed on the page.\n  Other:\n  Custom Event - fires the trigger when an event key is pushed into dataLayer with a custom value.\n  History Change - fires the trigger on the gtm.historyChange event, dispatched when a window history event is detected.\n  JavaScript Error - fires the trigger on the gtm.pageError event, dispatched when an uncaught JavaScript error is thrown on the page.\n  Timer - fires the trigger on the gtm.timer event, dispatched when the timer trigger interval is met.\n  Trigger Group - the trigger group fires when all the included triggers have fired at least once.\n  There\u0026rsquo;s bound to be more in the future. Especially the various events you can listen to with JavaScript are still vastly under-utilised (check this post for ideas how to extend them).\n  The next step is to choose any trigger-specific settings. With the DOM Ready trigger, for example, you can delimit the trigger to only fire on specific page URLs.\nIf you choose All (event type), the only condition in the trigger will be the event type. That means that your tag will fire every single time the event you specified is pushed into dataLayer. So if, for example, you chose a Click / All Elements trigger type, and you specify All Clicks as the filter, your tag will fire every single time a click is registered on the site (overkill much?).\n  If you choose Some (event type), you will need to specify the other condition(s) for your tag to fire. These can be anything you like, such as data layer variable values, page path matches, etc. As before, you can specify multiple conditions, but if you do, every single one of these conditions must pass for your tag to fire. There\u0026rsquo;s no either-or relationship here. Conditions are final.\nWith \u0026ldquo;Link Click\u0026rdquo;, \u0026ldquo;Timer\u0026rdquo; and \u0026ldquo;Form\u0026rdquo; triggers, you\u0026rsquo;ll see some extra settings in the Trigger, depending on what settings you choose. With \u0026ldquo;Timer\u0026rdquo;, you can set up the timer that will fire your tag after a given interval (or given intervals), and with Link Click and Form triggers, you might need to specify the conditions for the listener itself (see next chapter).\nAnd that\u0026rsquo;s it. If you\u0026rsquo;re in the tag creation workflow, you will now return to your tag, where you can add other triggers or an exception.\nIf you want to add multiple triggers to a single tag, do note that multiple triggers on the tag are in an either-or relationship. So having multiple triggers on a tag will make the tag fire when any of the triggers fire, which means that your tag can fire multiple times on a page unintentionally.\nHowever, if the underlying event on these multiple triggers is the same (e.g. Click), your tag will, by default, fire only once for every click event regardless of if there\u0026rsquo;s overlap in the triggers. You can change this behavior by opening the tag\u0026rsquo;s advanced settings:\n  Adding exceptions is simple enough. Just click ADD EXCEPTION in the tag\u0026rsquo;s trigger selection, and choose a trigger which will block the firing of the tag.\nWith exceptions, blocking triggers will always win against firing triggers. However, blocking triggers always need a firing trigger with the same underlying event, otherwise they\u0026rsquo;re useless. This is because when an \u0026lsquo;event\u0026rsquo; key is pushed into dataLayer, the blocking trigger can only block a trigger which fires on the same \u0026lsquo;event\u0026rsquo;, since they are evaluated at the same time against the same value of the \u0026lsquo;event\u0026rsquo; key (complicated, I know!).\nRead more about it here: #GTMTips: Block Your Tags With Trigger Exceptions.\n5. Click and Form triggers Since I\u0026rsquo;ve already written about these in my latest auto-event tracking guide, I want to just briefly explain what makes these two triggers special.\nFirst of all, skip the generic Click / All Elemenets trigger. Nothing special about that. It just listens to all clicks on your site, regardless of what element you click.\nThe Just Links trigger, however, only listens for clicks which propagate up to a link (\u0026lt;a/\u0026gt;) node. This means that you can click on a SPAN in a BUTTON in a DIV, but as long as there\u0026rsquo;s a link wrapper somewhere up the ancestral tree, and as long as the event propagates, GTM will register the event as a link click.\nThe Form trigger waits for a submit() event to be dispatched. This means that if some script on your site hijacks the form event and proceeds with some proprietary Ajax function, for example, GTM\u0026rsquo;s listener will not be able to pick it up.\nSo remember these two things: Just Links triggers require a click on a link, and Form triggers require a valid browser form submit event. Both need propagation to work.\nYou might have noticed the two clickable options on these two particular trigger types: Check Validation and Wait For Tags. Once you check either, you\u0026rsquo;ll see an extra step, Enable When, in the Trigger settings:\n  The point with this step is that you\u0026rsquo;ll be able to specify a condition for when the trigger is actively listening for the specified event. The most common condition types you\u0026rsquo;ll use here are page conditions, since you might want to specify that the Form Submit trigger only listens for submit events on pages with forms.\nThe reason this step was introduced is due to how the two checkable options, especially Wait For Tags, work. So let\u0026rsquo;s do a quick overview.\nCheck Validation, when checked, will require that a valid action is propagated to GTM\u0026rsquo;s listeners. With both Just Links and Form this means that there\u0026rsquo;s no event.preventDefault() called by other scripts on the event object before it reaches GTM\u0026rsquo;s handlers.\nHowever, in many cases it\u0026rsquo;s not just that the default action of the event is prevented, in which case GTM\u0026rsquo;s listeners will still pick up the event if Check Validation is OFF. Often you\u0026rsquo;ll see that propagation is stopped as well, which prevents GTM\u0026rsquo;s listeners from picking anything up. Be sure to see my previous two posts on the topic.\nWait For Tags ensures that all tags that fire on the trigger execute first before proceeding with the action of the event. So if it\u0026rsquo;s a Just Links trigger, the redirect (or whatever is the action) is programmatically halted long enough for all dependent tags to complete execution, after which the action is resumed. Same thing with forms.\nAnd this is the reason you need to specify the secondary filter. Since GTM is the one that halts the default action of the event, it\u0026rsquo;s possible that it screws something up in the propagation path. I\u0026rsquo;ve seen cases where the \u0026ldquo;Wait For Tags\u0026rdquo; option caused a pop-up blocker to pick up otherwise perfectly innocent lightboxes. Only by deactivating \u0026ldquo;Wait For Tags\u0026rdquo; was the problem solved.\n NOTE!!! The \u0026ldquo;Wait For Tags\u0026rdquo; can be very invasive on single-page apps, where link handling is completely customized. On single-page apps, an \u0026ldquo;internal\u0026rdquo; link should never lead to a redirect. GTM, however, does not know this. By pausing the event and then proceeding with the default action, it\u0026rsquo;s possible that \u0026ldquo;Wait For Tags\u0026rdquo; causes links to redirect when they shouldn\u0026rsquo;t.\n  Thus you should always be very careful with Wait For Tags and ONLY activate it on pages where you have THOROUGHLY tested it doesn\u0026rsquo;t interfere with the site\u0026rsquo;s default functionality.\n 6. Triggers in the API Since I\u0026rsquo;ve such a huge fan of the GTM API (see my GTM Tools), I wanted to say a few quick words about triggers and the API.\nTriggers are a pollable resource just like anything else in the API. You can list them, retrieve them, update them, delete them, create them, etc.\n  However, in their current state, there\u0026rsquo;s a complication. If you want to copy a tag from one container to another, the problem is the trigger ID. Each trigger has an ID, which, I think, is roughly the first available number in a sequence starting with 1. If you use a trigger in a tag, the tag will refer to this trigger using this ID.\nNow, when you want to copy this tag to another container, you create a new tag in the target container with this tag resource as the body. The problem is that the target container can already have a trigger with the ID, since they all follow the same logic when assigning the ID!\nThis means that when copying a tag with triggers to another container, what you actually need to do is this:\n  First create the trigger(s) in the new container\n  Use the object you receive in the response to see what the new trigger IDs are\n  Update the tag resource with the new trigger IDs\n  Create the tag in the target container\n  It\u0026rsquo;s a pretty complex operation for something as simple as resource cloning, and I hope it will resolve to a simpler solution in the future.\nYou don\u0026rsquo;t have this problem with variables, as variables are referred to by name, not ID.\n7. Tag Sequencing One important thing to know about triggers and tag sequencing is that if a tag is part of a tag sequence (either as a Setup or Cleanup tag), all its triggers are ignored.\nIn other words, tag sequencing trumps triggers. If a tag fires either before or after the main tag in a sequence, it will be completely controlled by the triggers of the main tag. The only way to prevent the tag from firing in a sequence is to pause it.\n8. Summary That\u0026rsquo;s all I have to say about triggers, for now. I\u0026rsquo;m cautiously apprehensive about possible changes to the UI, as I\u0026rsquo;m sure there will be in the near future.\nTo sum it up, triggers work like a charm. It\u0026rsquo;s so important for usability to have them integrated into the workflow as they are now. At the same time, they haven\u0026rsquo;t lost any of their power, quite the contrary. The new, Trigger type -based approach to underlying events is an excellent addition, since it helps us focus on what\u0026rsquo;s important, instead of having to battle with confusing condition syntax.\nThere are things to be improved in the workflow. I don\u0026rsquo;t especially enjoy having to work through the Variables page to prepare my triggers (if the variables I need do not exist), and I think there\u0026rsquo;s still some unification of the UI called for, since, for example, the Some Pages view is so very different from all the other trigger choices.\nPlease sound off in the comments if I\u0026rsquo;ve missed something obvious. I would love to keep this guide as up to date as possible.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/check-if-google-analytics-is-in-page-template/",
	"title": "Check If Google Analytics Is In The Page Template",
	"tags": ["Google Tag Manager", "Guide"],
	"description": "Guide for creating a variable which checks if Google Analytics is hard-coded in the page. This is useful if you want to check for duplicate implementations via Google Tag Manager.",
	"content": "One of the recurring problems in migrating to Google Tag Manager is how to make the transition as smooth as possible. Usually it requires that we agree with the developers on a time when the old code is removed, and at that moment we need to make sure the GTM tags point to the right UA code. This is, of course, only one use case for migrations, as some people do the entire migration in a staging environment, and some just don\u0026rsquo;t care if they lose a little bit of data along the way.\nHowever, the most seamless way to do the transition is to automate it. Make GTM somehow check for the existence of on-page GA, and if it finds it, the tags will not fire. As soon as the on-page GA is no longer found, GTM can go rampart.\nThis is a recurring discussion in our Google+ GTM Community, but this time it began in Twitter. Here\u0026rsquo;s the gist of it:\nI need a GTM rule for GA code existing on the website - suggestions? #measure\n\u0026mdash; Peter O\u0026#39;Neill (@peter_oneill) November 27, 2014  In this post, I wanted to take a shot at what my good friend Peter is looking for, with what I think is the best way to do this.\nNow, there are a number of ways you could go about checking for on-page GA, including:\n  Check for existence of GA cookies\n Very unreliable as cookies persist after removing on-page code, and if migration is to the same version of GA, the cookies are the same    Serialize page template, and regex match for script loaders or calls to the tracking objects\n Crazy solution (that I just came up with), and isn\u0026rsquo;t very reliable. Fails completely if scripts are loaded in external JS files    Ask developers to add dataLayer.push() that tells the status of on-page GA\n By far the most reliable solution out there, but the reason most people are looking for a solution like this is to minimize developer intervention    Check for existence of ga or _gaq objects\n Works well if you can bear the wait for the tracking library to load AND execute, as the objects are created in the library code. On some sites, this might be a too long wait, especially if the library is loaded asynchronously. Also, you can rename the ga object, so you\u0026rsquo;d need to identify it first.    But I want to show you what I think is the best way to do it. Feel free to disagree, and I find myself disagreeing with me as well, especially on a complex site! In my opinion, the following solution does the check as early as possible, and it\u0026rsquo;s as reliable as it can be, even though there are some caveats.\nSolution: look for the \u0026lt;script\u0026gt; elements This solution looks for the existence of \u0026lt;script\u0026gt; elements, where either ga.js, analytics.js, or dc.js are loaded.\nI think it\u0026rsquo;s pretty reliable, as if the script element is injected in proper form, it means that the libraries have loaded or have begun to load.\nHere\u0026rsquo;s the Custom JavaScript Macro for this. It returns \u0026ldquo;true\u0026rdquo; if it finds any of the libraries you specify.\nfunction() { var scripts = document.getElementsByTagName(\u0026#39;script\u0026#39;), ga = true, // set to false if you don\u0026#39;t want to check for ga.js  ua = true, // set to false if you don\u0026#39;t want to check for analytics.js  dc = false, // set to false if you don\u0026#39;t want to check for dc.js  i = len = 0; if (ga || ua || dc) { for (i, len = scripts.length; i \u0026lt; len; i += 1) { if (ga \u0026amp;\u0026amp; /www\\.google-analytics\\.com\\/ga\\.js/.test(scripts[i].src)) { return true; } if (ua \u0026amp;\u0026amp; /www\\.google-analytics\\.com\\/analytics\\.js/.test(scripts[i].src)) { return true; } if (dc \u0026amp;\u0026amp; /stats\\.g\\.doubleclick\\.net\\/dc\\.js/.test(scripts[i].src)) { return true; } } } return false; }  So now you can add {{check for scripts}} equals true as a blocking trigger in your GTM GA tags.\nThere are some shortcomings to this method. For example, the page can load the libraries but not do any tracking. This can be true for legacy setups, where only the _gaq.push() commands have been removed, but the library load remains. So naturally, you will have to audit the site before implementing this solution.\nI tested this out a couple of times on my own blog, using it as the blocking rule of my pageview tag, and it seemed to work fine.\nNaturally, since the ga.js snippet is added to the end of the page (stupid, synchronous script), you might need to wait for gtm.dom before checking the blocking rule, especially on heavy pages.\nSummary Remember, this simple solution is only for the transition from on-page GA to GTM. The only thing it does is prevent you from having to sit in front of a computer, constantly refreshing the page to see if the on-page code has been removed.\nA migration requires vigilance, so even if you manage to automate the transition with this solution, you will still need to be alert throughout the process. But you knew this already (didn\u0026rsquo;t mean to sound condescending).\n"
},
{
	"uri": "https://www.simoahava.com/tags/lookup-table/",
	"title": "lookup table",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/simple-regex-lookup-table-for-google-tag-manager/",
	"title": "Simple RegEx Table For Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "lookup table", "variable"],
	"description": "Simple way to build a regular expression lookup table in Google Tag Manager.",
	"content": "When our good friends in the Google Tag Manager developer team first introduced the Lookup Table Macro, we were excited. For many of us, it soon became the weapon of choice especially when used as a management and optimization tool for the container itself.\n  However, the macro wasn\u0026rsquo;t considered perfect. In fact, the most frequently heard request had to do with the core functionality of the feature itself: the macro should support operations, that is, predicate logic. It\u0026rsquo;s not enough to just have equal match lookups; people wanted support for operations such as \u0026ldquo;is x larger than y\u0026rdquo; or \u0026ldquo;does y contain x\u0026rdquo;.\nThe thing is, I don\u0026rsquo;t agree with changing the Lookup Table Macro to support these types of operations. Sure, a table whose values you can query with more complex operations than simple lookups would be awesome, but it wouldn\u0026rsquo;t be a lookup table anymore. We\u0026rsquo;d need a different variable type for those use cases.\nIn this post, I\u0026rsquo;ll take a look at just what makes a lookup table a lookup table, and I\u0026rsquo;ll also give you a nifty Custom JavaScript variable that lets you create a regular expression table by yourself. This table lets you query an input variable (e.g. {{Page Path}}) against a number of regular expressions (rows). If a match is made, then some value is returned. So, it\u0026rsquo;s essentially a variation of the Lookup Table, but with regular expressions instead of exact match lookups.\nLook it up! Even though I\u0026rsquo;m a product of the unsurpassed Finnish education system, I suck at ornithology. So you\u0026rsquo;ll excuse me for the following, clumsy metaphor.\nConsider the homing pigeon. It has an intimate knowledge of a location, and it flies to that location. If there is nothing there, it gets confused and poops. If it does find a recipient or a message, it does its thing and coos happily.\nWell, when you have a lookup table, it\u0026rsquo;s the same thing. You use a variable reference to pinpoint to a specific cell in a table. If this cell exists, any value stored within is returned. If the cell doesn\u0026rsquo;t exist, the script gets confused and poops an undefined or an error.\nThis is what makes lookup tables so incredibly efficient. It\u0026rsquo;s all based on binary logic.\nThere are no complex operations, no predicates to be evaluated. It\u0026rsquo;s just a question of \u0026ldquo;does table X have a value under label Y\u0026rdquo;.\nIn JavaScript, a lookup table can be a plain object (most common), or an Array, or even a String. Basically, it can be any Array-like structure.\nIf you use a plain object as a lookup table, it\u0026rsquo;s common to call it an associative array or a hash table, but we\u0026rsquo;ll call them lookup tables here for clarity.\nSo, you can perform lookups on all Array-like structures. The three examples listed above can be used for lookups like this:\n// Plain object var newValue = objectTable[\u0026#39;key\u0026#39;]; // Array var anotherValue = arrayTable[3]; // String var newestValue = \u0026#39;String\u0026#39;[5];  With the plain object, you can also use dot notation in some cases.\nAs you can see, you\u0026rsquo;re directly requesting a specifically labelled value in the table, and if it exists, it\u0026rsquo;s returned to you without any further operations.\nNow, if you were to introduce predicate logic into the mix, with something like table['[Kk]ey'] (fictional example), it would mean that the lookup should check every single cell until a match is made in the table to see if they have either \u0026lsquo;Key\u0026rsquo; or \u0026lsquo;key\u0026rsquo; as their label.\nThis is because with JavaScript data structures, a label can only ever be one thing. With lookup tables, you\u0026rsquo;re requesting for a given label without any variations, e.g. \u0026lsquo;Key\u0026rsquo;, and only that label is thus queried in the table. This is because programming logic dictates that only one cell can exist in the table with that label.\nAs soon as you add predicate logic into the mix, you\u0026rsquo;re forcing the lookup to check every single cell until a match is made, because you can\u0026rsquo;t label cells with regular expressions or dynamic values (e.g. \u0026lsquo;Key/key\u0026rsquo;).\nThe difference between the binary check of the lookup vs. the traversal of a more complex operation becomes clearer when thinking in terms of performance.\nQueries on a lookup table are said to work in constant time. Since you\u0026rsquo;re querying for a specific label in a table of arbitrary size, the complexity of the operation will always be the same. Either the label exists or it doesn\u0026rsquo;t. The table can be huge or it can be miniscule, the performance is always the same.\nPerformance is usually indicated with Big O notation. The notation for constant time (i.e. the lookup) would be O(1).\nWhen using predicate logic, you achieve O(1) only if you match the query with the first cell that is checked. Every subsequent cell that is checked for a match incurs a linear decrease in the performance. Thus, comparison logic is said to work in linear time.\nDescribing linear time operations with O(1) would be fairly optimistic. For this reason, Big O notation tends to describe the worst-case scenario. The worst-case scenario of linear time would be that the value is in the very last cell that is queried. Thus, the notation would be O(n), where n is the number of cells in the table.\nThis also means that the larger the table, the more expensive the operation becomes, in terms of performance.\nWith small tables this difference is pretty trivial, but with large tables and multitudes of chained variables, the performance hit can be significant, especially if it takes time to make the match, and the labels are arbitrary enough that you can\u0026rsquo;t use facilitating data structures or search algorithms.\nSo if you\u0026rsquo;re concerned about performance, and you should be if it\u0026rsquo;s a web page, always use the Lookup Table variable.\nRegEx Lookup Table with Custom JavaScript Well I know you\u0026rsquo;re not satisfied with my explanation, and you\u0026rsquo;re still craving for a more flexible way to fetch values from a table.\nI hope the GTM developers will, at some point, introduce another variable type that\u0026rsquo;s essentially a lookup table but where you can specify the predicate logic used row-by-row.\nUntil then, you can make do with workarounds such as the script below.\nCopy the following code into a new Custom JavaScript Variable:\nfunction() { // Set inputVariable to the input you want to assess  var inputVariable = {{Page URL}}; // Set defaultVal to what you want to return if no match is made  var defaultVal = undefined; // Add rows as two-cell Arrays within the Array \u0026#39;table\u0026#39;.  // The first cell contains the RegExp you want to match  // the inputVariable with, the second cell contains the  // return value if a match is made. The third cell (optional),  // contains any RegEx flags you want to use.  //  // The return value can be another GTM variable or any  // supported JavaScript type.  //  // Remember, no comma after the last row in the table Array,  // and remember to double escape reserved characters: \\\\?  var table = [ [\u0026#39;/home/?$\u0026#39;, \u0026#39;Home Page\u0026#39;], // Row 1  [\u0026#39;\\\\?location=\u0026#39;, \u0026#39;Contact Us Page\u0026#39;], // Row 2  [\u0026#39;/products/[123][0-9]\u0026#39;, \u0026#39;Products 10-39\u0026#39;, \u0026#39;i\u0026#39;] // Row n (last)  ]; // Go through all the rows in the table, do the tests,  // and return the return value of the FIRST successful  // match.  for (var i = 0, len = table.length; i \u0026lt; len; i += 1) { var regex = new RegExp(table[i][0], table[i][2]); if (regex.test(inputVariable)) { return table[i][1]; } } return defaultVal; }  Here\u0026rsquo;s how the variable works:\n  First, you give it the input: some variable or value that you want to assess in the table rows\n  Next, you insert the rows\n  Rows are actually Arrays within the table Array\n  1. The first cell contains the regular expression you want to evaluate against the input (note, you will need to _double_ escape reserved characters!) 2. The second cell contains the value that is returned if a match is made 3. The third cell is optional, and can contain any regular expression flags (e.g. 'g', 'i') you might want to use   Finally, there\u0026rsquo;s a little for-loop which loops through each row of the table Array, checking the regular expression against the input variable. If and when a match is made, the specified return value is returned by the function\n  If no match is made, the specified default value is returned\n  Remember to edit the rows to match your table. Since it\u0026rsquo;s plain text JavaScript, you could also create the table in Excel (formatting it with the square brackets and all), and then just copy-paste it as plain text to the JavaScript macro body.\nSummary As always, this solution is educational first, a proof-of-concept second, and a usable, out-of-the-box workaround last. So feel free to modify it to your own purposes, or just ditch it completely.\nThe key takeaway from this article should be an understanding of how Lookup Tables work, and how much more complicated they would get if operational logic would be introduced as well. For that reason, my feature request remains that the Lookup Table would be kept as it is, but a new variable type would be introduced, where you can specify the operation on a row-by-row basis. This way, everyone wins.\nBy the way, if you\u0026rsquo;re interested in performance, JavaScript, and other data structures and search algorithms, take a look at this book:\nMichael McMillan: Data Structures and Algorithms with JavaScript\nThe book has the basics summed up really well. The next step would be to grab a book about design patterns and more complex data structures. It\u0026rsquo;s all very educational.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/fix-problems-with-gtm-listeners/",
	"title": "#GTMtips: Fix Problems With GTM Listeners",
	"tags": ["auto-event tracking", "event listeners", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "Tips for how to fix problems with Google Tag Manager&#39;s triggers. Typical issues are that the Just Links of Form triggers do not work.",
	"content": "I\u0026rsquo;ve written about this before here and here, but this issue remains probably the biggest problem users have when implementing Google Tag Manager.\nTip 10: Resolve conflicts with GTM\u0026rsquo;s listeners   The tip title is actually wrong. You\u0026rsquo;re not fixing Google Tag Manager listeners. Rather, you\u0026rsquo;re resolving conflicts that other scripts on your page might introduce.\nGTM\u0026rsquo;s event listening is based on something called event delegation. Event delegation makes use of the document object model (DOM) and its tree-like hierarchy.\nWhen a click occurs on a node, such as a link element, the click event begins to bubble up the DOM tree. It passes through every single ancestral node on its way to the top. This means that instead of attaching a listener to every single link element, GTM attaches the listener on the top-most document node (the document itself). This way it will capture the event once it has bubbled up all the way to the top. It\u0026rsquo;s much more economical to listen for events this way, as you don\u0026rsquo;t have to pollute the elements with individual handlers.\n  A conflict occurs when somewhere along this way up, the event\u0026rsquo;s path is obstructed. The term we use here is that its propagation is stopped. If this happens, GTM\u0026rsquo;s listeners never capture the event, and thus they will never work with your tags.\nThe most common way that propagation is stopped is:\nreturn false; in a jQuery event handler\nIf there\u0026rsquo;s a return false; in your jQuery event handler, propagation is stopped. The return false; statement in jQuery combines both preventDefault() AND stopPropagation() on the event object. There\u0026rsquo;s an easy fix to this: invoke ONLY preventDefault(). This should work almost always, unless there\u0026rsquo;s a specific reason you want propagation to stop.\nFor example:\n// Before - propagation stopped $(\u0026#39;a#toTop\u0026#39;).on(\u0026#39;click\u0026#39;, function() { // some code  return false; }); // After - propagation not stopped $(\u0026#39;a#toTop\u0026#39;).on(\u0026#39;click\u0026#39;, function(e) { e.preventDefault(); // some code });  preventDefault() prevents the default action of the click but it doesn\u0026rsquo;t stop propagation. I\u0026rsquo;d say around 95% of the time, this is enough for your dynamic scripts.\nThere are other ways to stop propagation (such as an explicit call to e.stopPropagation()), but the jQuery scenario is by far the most common one.\nFix it! The first thing you want to try is to uncheck Check Validation in your link click or form submit trigger. If Check Validation is on, it means that GTM\u0026rsquo;s handlers will not fire even with a proper preventDefault(). The listener will only fire if an uninterrupted / untouched event object propagates to the document node.\nThe second thing is to look through your scripts and try to find the offending jQuery / other handlers. Look for hints of propagation being stopped. Fix them yourself or ask your developers to fix them. Look for custom AJAX functions as well, especially with forms. With forms, it\u0026rsquo;s possible that a submit() is never called. Instead, an AJAX POST request is sent to a backend validator or something.\nThe third thing you should try is the best: talk to your developers. Educate them. Tell them that GTM\u0026rsquo;s listeners require that clicks and form submits propagate to the document node. If they get that, they should be able to fix it. If they don\u0026rsquo;t understand this, make them learn the basics of JavaScript again.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-analytics-page-load/",
	"title": "Google Analytics And The Page Load",
	"tags": ["dom", "google analytics", "Google Tag Manager", "Guide"],
	"description": "An article describing how Google Analytics interacts with, and is influeced by, the page load of the web browser. The rendering process has implications for your web analytics setup.",
	"content": "If you use Google Analytics, Google Tag Manager, or any JavaScript-based data collection or analytics platform, have you ever stopped to wonder how they actually work? I mean, you obviously care about getting the data in, but are you taking the machinations of these tools for granted?\nThis is something I\u0026rsquo;ve been thinking about for a long while, because I\u0026rsquo;m not so sure that many who work with these platforms actually understand how the browser and the web page interact.\nThat doesn\u0026rsquo;t make sense. And it scares me a little.\nSince the predominant paradigm for web analytics today revolves around JavaScript, it\u0026rsquo;s important to have a sense of what the weak links in this technology are. Also, even before JavaScript kicks in, there are certain aspects of the page load process that you need to observe to get better quality data.\nFortunately, even the most basic understanding of these leads to elucidation: No, the tools do not cover 100 % of the visits to your site, no, \u0026ldquo;pageviews\u0026rdquo;, \u0026ldquo;sessions\u0026rdquo;, and \u0026ldquo;users\u0026rdquo; should not be taken literally, and no, it\u0026rsquo;s most often not Google Tag Manager or Google Analytics that\u0026rsquo;s at fault when tracking on your site fails. You\u0026rsquo;ll find the real culprit by opening the JavaScript console on your darling website, but don\u0026rsquo;t forget to close this dimension rift to the nether pits of the hell of all hells once you\u0026rsquo;re done, lest some of the demon spawn gets through.\n  The core of this article boils down to a simple statement:\nThe only way to evaluate the quality of the data you use is to understand how it was collected. Keeping this in mind, I want to take a look at the page load sequence in the user\u0026rsquo;s browser, and what implications the different parts of the load process have on Google Analytics tracking.\nThe sequence of events in the page load   This short essay (or long rant, however you want to see it) is split into a number of sections. Each section represents a stage in the intricate process of what takes place in the user\u0026rsquo;s browser when they visit your website. Each part of this process has implications for how you can and should track the interactions this user has with your content.\n  The Request - What happens when a user types an address into the location bar of the browser, and what are the pressure points for web tracking.\n  The Render - How the browser turns the source code into a living document.\n  The Race - How asynchronous JavaScript works, what race conditions are, and what you should watch out for.\n  The Interaction - Once the page is loaded, how are user interactions measured, and what are the biggest pitfalls here.\n  As always, I wrap up with a summary.\n  Everything starts with the request.\nWhen a user types your website address into the browser\u0026rsquo;s location bar, the desired outcome is usually that they see a web page. In order for this to happen, the browser uses the HTTP protocol to issue a request to the machine that hides behind the address.\nIf there is a web server at that endpoint, if that endpoint has mapped the requested address to some resource, and if the request is valid (there\u0026rsquo;s no additional authentication required, firewalls don\u0026rsquo;t block the request, no security policies are violated etc.), the web server sends an \u0026ldquo;OK\u0026rdquo; response, most commonly with a status code 200, and the document that was mapped to the address in the response body.\nThis document is usually an HTML template file, i.e. the page source code, and the browser\u0026rsquo;s job is to turn this document into a dynamic web page.\nI mean, obviously, it\u0026rsquo;s a much more intricate and complex process than this, but the key interaction here is what happens when the web server receives the request.\nFor successful collection of data to Google Analytics, there are some things here that you need to be aware of.\n1) 404 - Resource not found If there is no resource mapped to the address the user requested, it means that the web server has to respond with a \u0026ldquo;resource not found\u0026rdquo; error. Now, at this point your web server should be configured so that if there is no resource to send to the user, a \u0026ldquo;Page not found” template is served instead.\nYou should have GA running on this page with a tag that tracks pageviews. 404 page tracking is a wonderful addition to the toolset of any savvy webmaster-slash-analyst.\nIf you haven\u0026rsquo;t set up a template, the user might see the default error page served by your web server (they\u0026rsquo;re usually pretty horrible), or, lacking that, the browser will cook up an error message instead (even more horrible). The key here is that if you don\u0026rsquo;t have a custom template with the GA tracking code, you won\u0026rsquo;t be able to track these hits in Google Analytics. For example, my GTM Tools serves the following, unhelpful, untrackable default page if no resource is found:\n  This is a bad practice. Use a custom template instead.\nCheck the Bounteous post if you want to see a great way of tracking 404 errors in GA and GTM!\n2) Redirects If you\u0026rsquo;ve set up a redirect server-side, you must remember preserve query string parameters in the redirect! If you strip them out in the redirect, you\u0026rsquo;ll risk losing important campaign tracking information.\nThis is essential to preserving data quality!\n3) Single-page apps If your website is essentially a single-page app, where instead of the expected HTTP GET request, the user\u0026rsquo;s browser sends POST calls, the rules of traditional page tracking change a little.\nYou can no longer trust the reset that occurs with every page load, since there are actually no page loads after the first one. Instead, you\u0026rsquo;re going to have to come up with a new terminology for tracking your visitors. For some it\u0026rsquo;s scroll tracking, via events or virtual pageviews, and for some it\u0026rsquo;s tracking user interactions with the page, such as clicking on tabs or call-to-action buttons, and for some it\u0026rsquo;s invoking GTM\u0026rsquo;s history listener, which reacts to changes in browser state.\nIf you\u0026rsquo;re using Google Tag Manager, an important aspect of these single-page apps is that the dataLayer object is not reset as it is with page refreshes. That\u0026rsquo;s because in a web context, the painstakingly rendered web page with all its variables, objects, elements, texts, links, images, and other clutter is recreated with every page refresh. In other words, objects on a given page do not persist to other pages. With a single-page app, there is no page refresh, so there\u0026rsquo;s no purge. The data layer is alive for the entire time the user stays on the page, meaning it\u0026rsquo;s more than possible that anything you store into dataLayer and send to GA might be sent with subsequent hits as well.\nWith single-page apps, then, you need to be aware of the state of dataLayer at all times, and if necessary you need to emulate a page refresh by deleting keys that you don\u0026rsquo;t want to persist:\ndataLayer.push({ \u0026#39;key_to_be_deleted\u0026#39; : undefined });  4) Don\u0026rsquo;t be idiot\u0026hellip;idiosyncratic Finally, an important aspect of interacting with the web server is that many analytics platforms expect standard behavior from your server. So don\u0026rsquo;t whip up your own custom responses if just the regular \u0026ldquo;200 OK” will do. If you don\u0026rsquo;t serve standard responses, it\u0026rsquo;s possible that callbacks are not executed, hits don\u0026rsquo;t get sent, and valuable data is lost again.\nThe key takeaway here is this:\nAdhere to best practices in template handling and server responses - your analytics tool will thank you with good data.   When the browser receives the source code, it does something inexcusable. It gives the website a break. You see, one of the reasons JavaScript is reviled and browser wars are raging is because errors are glossed over. Your site can have the most horrendous source code, but depending on the browser, it turns into an amazing, well-crafted, dynamic web document.\nIt\u0026rsquo;s much like what Auto-Tune does to talentless singers. The music industry uses Auto-Tune to turn pitchy Idol dropouts into pop stars. In much the same way, the web browser turns a hideous piece of anti-code into a web page without actually telling the developer bluntly that they\u0026rsquo;ve done a poor job.\nNow, we can argue that Auto-Tune is a good thing, and we can argue that it\u0026rsquo;s great that the browser helps you like this. But having a machine correct your singing is no excuse to stop trying to reach the correct pitch naturally, and having a browser fix your source code problems is no reason not to aim for good markup.\nFor analytics, this browser behavior has some implications.\nBecause every browser wants to be a bit better than the next, they have their own ways of parsing source code. Sometimes these idiosyncrasies are subtle enough to not make a dent, but sometimes they erupt into full-blown compatibility issues, polyfills, and disgruntled developers. At the heart of this tragedy is the analyst, who only wanted to know how many people leave a specific form field empty when submitting the contact form.\n// Classic example of browser compatibility checking if (document.addEventListener) { // For real browsers  document.addEventListener(\u0026#39;click\u0026#39;, handleClick); } else { // For IE8 and earlier  document.attachEvent(\u0026#39;onclick\u0026#39;, handleClick); }  To combat these problems in the render process, it\u0026rsquo;s often wise to adopt a framework. If you\u0026rsquo;re tracking the \u0026ldquo;traditional way\u0026rdquo;, jQuery is something you might want to take a look at. If you\u0026rsquo;re using Google Tag Manager, well, you\u0026rsquo;re already using a JavaScript framework. By leveraging the built-in features of GTM, such as auto-event tracking and tag templates, you\u0026rsquo;re externalizing cross-browser woes to Google Tag Manager. It\u0026rsquo;s the framework\u0026rsquo;s job to take care that any features you want to use work across the wide variety of browsers and devices.\nIn some cases, even the smartest browser won\u0026rsquo;t help you. These are called single points of failure, because they essentially disrupt the current execution context when an error occurs. If you run JavaScript such as the Universal Analytics tracking snippet in your page template, a single syntax error will break the current script context and the code will fail. Again, your browser is probably trying to be helpful and doesn\u0026rsquo;t raise any alarms if this happens, so you\u0026rsquo;ll need to do some debugging. Make the JavaScript console of your browser your new best friend. Use it religiously for debugging.\n  It\u0026rsquo;s a shame if your entire tracking plan for a website fails because of a mistyped quote or a missing semicolon. Also, remember to watch out for formatted quotes. If you copy-paste from a document where quotes are formatted in some unorthodox way (such as Microsoft Word), you might run into trouble with JavaScript compilers, which expect regular, unformatted quotes in the code.\nSo the key takeaway in the render process is:\nMake sure that your [markup is valid](http://validator.w3.org/), and that your [JavaScript is flawless](http://jslint.com/). If your JavaScript is getting too complex to manage, take a look at a sustainable framework that will take some of the management overhead away.   JavaScript is single-threaded. This means that at any given time, only a single line of JavaScript can be executed. Thus, JavaScript is, by nature, blocking. Every line of JavaScript that is executed on the page template blocks the template until the line of code has been executed.\nBefore asynchronous JavaScript became the norm, we resorted to adding potentially risky JavaScript to the end of the document, at the very bottom. This reduced the risk of breaking down your entire web page if the JavaScript refused to agree with the browser. Also, if really large, external JS files were loaded, these calls were added to the end of the page as well, because the loading and the execution of the external file were queued in succession, meaning some severe page blocking would take place.\nWell, today, we have asynchronous JavaScript. At the heart of asynchronous JavaScript are callbacks, which are functions that are executed once some action, usually a network request, is processed. So if, for example, your script performs an HTTP request, asynchronous JavaScript initiates the request, but immediately returns to the next line of code in the page template. As the request progresses, the callbacks are executed as soon as there\u0026rsquo;s a spot open in the event queue. This way the code doesn\u0026rsquo;t block the page load, as it\u0026rsquo;s run \u0026ldquo;in the background\u0026rdquo;, only executing the callbacks once they are invoked by the original function.\nAnother place where you\u0026rsquo;ll see asynchronous requests is when loading external JS libraries. Google Analytics and Google Tag Manager are both loaded asynchronously. This means that the request for loading the JavaScript file is sent asynchronously, and when the file is loaded and available, only then is the code within executed (and when there\u0026rsquo;s a spot in the event queue). This way you can add these external library loads to the top of the page, ensuring that their load process begins as early as possible into the page load, but without risking them blocking your other code from running.\nThe downside of asynchronous JavaScript is that we can\u0026rsquo;t really predict when a given script will finish executing. We\u0026rsquo;ll know when it finishes thanks to the callbacks, but we can\u0026rsquo;t pinpoint this moment in the page load sequence beforehand.\nThis behavior leads to something called race conditions. Race conditions occur when two scripts are loaded asynchronously, and the latter requires that the former complete first. An example would be event tracking in GA. You don\u0026rsquo;t want to send an event hit before the pageview hit completes, because otherwise you\u0026rsquo;ll end up with warped sessions, where landing page is (not set):\n  If race conditions are at play, it doesn\u0026rsquo;t matter that the event hit began its execution after the pageview. What matters is that the event can finish first, because we can\u0026rsquo;t control asynchronous execution once it begins.\nFor GA and GTM, it\u0026rsquo;s important that you recognize potential race conditions on your page. If you want to increase the odds of the pageview being sent before any subsequent hits, for example, you could set all other hits to wait at least until the DOM of the page has been loaded. This safety measure usually adds a tiny overhead to the execution of the tag that waits for DOMComplete, and it more often than not means that the pageview has had ample time to complete before any further hits are sent.\nIf it\u0026rsquo;s not business critical data, you can even use the window load event as the hook for your other hits. The window has loaded when the page and all associated scripts have finished loading. If your code pushes data to GA after window load, it\u0026rsquo;s almost 100 % certain that the pageview has finished executing. However, if you have a heavy page full of big, uncached images (boo!), and heaps of external JS files to load (boo boo!), it might be too long a wait for some impatient visitors. Thus any hits that wait for window load might not get sent, as the user has already navigated to another page.\nThe best way to make sure that asynchronous scripts load in sequence, and thus to avoid race conditions, is to use the callback of a successful hit as the trigger for the subsequent hits. So in your pageview, you can use the hitCallback parameter to specify what happens once the tag has successfully fired. In this callback function, you can tell your other tags that it\u0026rsquo;s ok to fire. With Google Tag Manager, using hitCallback together with dataLayer makes it really easy to introduce some order into things.\nUnderstanding race conditions is really important with eCommerce as well, especially if you use client-side HTTP requests to get the transaction data. It\u0026rsquo;s all too common to see a transaction tag executing before the transaction data is available.\nThe takeaway of The Race is:\nMap out the dependencies between your tags, eradicate race conditions by using callbacks, and leverage asynchronous loading as much as possible.   Well all this page load stuff is marginally interesting, but it might not play a role in your idea of what web analytics is for, even though it should. But what you\u0026rsquo;re definitely interested in is how to track interactions on the site.\nInteraction is a loaded term, because it\u0026rsquo;s often difficult to pinpoint just what we mean by a user interaction. A click is surely an interaction, but is scroll an interaction? What about mouse movement, is that interaction? How about the shifting of the user\u0026rsquo;s gaze to a different part of the site, surely that\u0026rsquo;s not interaction? Well, it might be.\nThe thing is that depending on what you want to track, you might be facing a lot of problems in implementing the system that collects the data. Something as simple as a click handler can be difficult to implement if you\u0026rsquo;re not aware of what possible conflicts there are with your other JavaScript handlers.\nThis conflict resolution covers around 75 % of the most frequently asked questions about Google Tag Manager. I\u0026rsquo;ve written about this extensively before, so if you want in-depth details around the event propagation problem, take a look at this article:\nWhy Don\u0026rsquo;t My GTM Listeners Work?\nThe key here is to understand how the document object model works, especially in terms of event delegation. Often it\u0026rsquo;s most economical to add just a single listener on a page, and then just pick up events using event delegation. However, if you have conflicting script on the page, this might not work, and you have to come up with a workaround. Here\u0026rsquo;s a simplified version of the problem:\n  This visualization shows how the submit() event starts propagating up the document tree, and once it reaches GTM\u0026rsquo;s listeners on the document node the auto-event tracking kicks in. It also shows you what happens if there\u0026rsquo;s an intruding call to stopPropagation() on the event object.\nBut that\u0026rsquo;s GTM. When you\u0026rsquo;re tracking with traditional GA, you can again use jQuery for easy, cross-browser event handling. The thing is that the more you leverage different frameworks, the more possibilities there are that things go awry.\nWith interaction tracking, it\u0026rsquo;s so important to establish a discussion with your front-end developers. If you start injecting stuff onto the site that doesn\u0026rsquo;t play well with existing script infrastructure, you\u0026rsquo;ll have far bigger problems than data pollution. Your forms might not work any more, tabs which were supposed to change the content dynamically start redirecting you to completely different pages, elements flicker in and out annoyingly, and so forth.\nSo the key takeaway for interaction tracking is this:\nOnly track what you think is useful to track, and always test thoroughly for conflicts The Summary I hate just scratching the surface in articles, but we\u0026rsquo;re already past 3,000 words and if you made it this far you deserve a huge thank you.\nAnalytics is all-encompassing. It can permeate every single platform you use, aggregating information from a wide variety of data sources in one, centralized location. However, to get your web analytics working to the max, I strongly believe that an understanding of the page load process, even on a high level, is crucial. This article only scratched the surface, so you might want to take a look at the following resources to expand your mind:\n  The Request: How Browsers Work\n  The Request: Google Analytics And 301/302 Redirects\n  The Render: Optimizing Content For Different Browsers\n  The Render: QuirksMode: Compatibility overview\n  The Race: Asynchronous JavaScript Programming\n  The Race: #GTMTips: hitCallback And eventCallback\n  The Interaction: Custom Event Listener for GTM\n  The Interaction: Google Tag Manager Events Using HTML5 Data Attributes\n  "
},
{
	"uri": "https://www.simoahava.com/gtm-tips/migrate-containers-to-new-ui/",
	"title": "#GTMtips: Migrate Containers To New UI",
	"tags": ["container", "Google Tag Manager", "gtmtips", "new ui"],
	"description": "Tips on how to migrate containers from the first version of Google Tag Manager to the revamped, second version.",
	"content": "If you haven\u0026rsquo;t lived in a barrel, you should know by now that a new version of Google Tag Manager has been released. You can find the new version at http://tagmanager.google.com/, and there\u0026rsquo;s already a bunch of good articles about the new UI out there. I want to point out two: \u0026ldquo;Setting up GA via GTM\u0026rsquo;s new UX\u0026rdquo; by Krista Seiden, and \u0026ldquo;Google Tag Manager Refresh – 6 Things You Need to Know\u0026rdquo; by Jonathan Weber from Bounteous. Both are wonderful run-throughs of the new features, and should get you up-to-date with what\u0026rsquo;s changed.\nOK, I\u0026rsquo;ve written something as well, namely a guide for using GTM\u0026rsquo;s event listeners in the new UI. This guide was warranted, as the whole approach to automatic event handling has changed quite a bit in the transition from the old version to the new.\nLet\u0026rsquo;s get back on track. This tip is all about migration. Officially, all old accounts will be migrated to the new UI sometime in the beginning of 2015, but if you want to take a head start, you can do the migration yourself!\nTip 9: Migrate a container to the new UI   To migrate a container, you need to export a container file in the old UI, and then you need to import this as a new version in an existing container in the new UI.\nNote! Since you\u0026rsquo;re using a new container in the new UI to import the old container into, the container ID (GTM-XXXXX) will change. This means that if you want to use this new container on your site, you will need to edit the container snippet accordingly!\nSo here\u0026rsquo;s the process in easy-to-follow steps.\n1) Export container In the old account, go to the container you want to export. Technically, you\u0026rsquo;re not going to export a container, since that\u0026rsquo;s just a chassis for all the assets within. Instead, you\u0026rsquo;re going to export a container version. This means that after you click Export Container, you will need to select the version you\u0026rsquo;ll be exporting.\n  After you select the version, you\u0026rsquo;ll have a chance to preview the JSON (JavaScript Object Notation) file you\u0026rsquo;ll be downloading. Unless you\u0026rsquo;re really into debugging the nuts and bolts of a container file, you can just divert your attention to the Download button under the text field. Yes, click it now.\n  2) Import container This next step requires two things:\nFirst, you need to have an account created in the new UI, and a container within that account that you can access. If you have neither, just browse to http://tagmanager.google.com/ and follow the instructions. Honestly, it\u0026rsquo;s really easy.\nSecond, you need to navigate to the container in the new UI, and you need to import the file you just downloaded in the old UI. So, go to the container, and specifically the Admin section.\n  Once you click Import Container, you will first need to choose the container file you just downloaded. So do that.\nNext, you have two options. Either Overwrite or Merge. Both create a new version of the container using your imported file, but the difference is that Overwrite is a clean refresh, where only the assets you import will exist in the new version. Merge will do what it says, and merge the assets you import with anything that existed in the latest version of the target container.\nI\u0026rsquo;m a purist when it comes to migrations. A migration needs to be clean and simple, with as little interference in the process as possible. This means that I recommend choosing Overwrite, since you\u0026rsquo;re probably working with a new account or at least a new container anyway.\nSo, after choosing the file, checking Overwrite, and clicking Continue, you should see something like this:\n  Feel free to doubt yourself at this point, it\u0026rsquo;s normal. Just click Confirm and you can fix any problems in the container version.\nIf it\u0026rsquo;s a new container you created in the new UI, you\u0026rsquo;ll most probably see the five variables in the deleted column. No need to panic. This is just because the container you\u0026rsquo;re migrating does not utilize built-in variables, as they were only introduced in the new UI. Thus, the built-in variables are deactivated upon migration, and they are also confusingly reported as being deleted.\nPossible issues First of all, double-check all your tags, triggers and variables. No, triple-check, quadruple-check, and then check again.\nIf you find problems, I mean real problems like triggers looking completely different than what they used to (variables changing place, etc.), take a screenshot, report it in the Google Tag Manager community in Google+, and then fix the discrepancies manually.\nOne thing I do suggest you do is start activating the built-in variables, and using them in your triggers and tags. Also, make sure you do the transition to the new, trigger-based auto-event tracking in your event tracking tags!\nMigrations are never easy. They\u0026rsquo;re not supposed to be. But feature parity between the old version of GTM and the new version is almost 1:1, so just by importing your container, it should work pretty nicely out of the box.\n"
},
{
	"uri": "https://www.simoahava.com/tags/new-ui/",
	"title": "new ui",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/auto-event-tracking-gtm-2-0/",
	"title": "Auto-Event Tracking In GTM 2.0",
	"tags": ["auto-event tracking", "Google Tag Manager", "gtmtips", "Guide", "new ui"],
	"description": "Introducing auto-event tracking for Google Tag Manager. Auto-event tracking means the automatic handling of events such as clicks and form submits via your Google Tag Manager container.",
	"content": "In the new version of Google Tag Manager, auto-event tracking has received a considerable usability upgrade. It might seem quirky at first, especially if you\u0026rsquo;re used to the old auto-event tracking method, but the logic behind the new setup is brilliant.\n  The most important distinction is that auto-event tracking isn\u0026rsquo;t something you control with separate tags anymore. Rather, it\u0026rsquo;s now entirely trigger-driven, meaning you activate and specify the auto-event tracking of your choice using tag triggers (triggers are what ye olde folk used to call rules).\nThe trigger types If you\u0026rsquo;ve used the new UI, you might have noticed how triggers are now grouped together logically. For example, you have your Click Triggers (which include both \u0026ldquo;Click\u0026rdquo; and \u0026ldquo;Link Click\u0026rdquo; trigger types):\n  I\u0026rsquo;ll get to how these triggers are set up in just a bit, but the important thing to understand is that the auto-event trigger controls both when your actual tag fires as well as when the auto-event listener should be active. That\u0026rsquo;s a huge simplification, since you no longer need a separate, discrete tag to control your auto-event listener with.\nThen you have your Form Triggers:\n  And you also have auto-event triggers for History events, JavaScript Errors and Timers, as before.\nSo remember:\nThe auto-event trigger determines when your tag fires AND when the listener is active.\nThat\u0026rsquo;s the new auto-event tracking in a nutshell.\nHow to set it up But if this was just a \u0026ldquo;in a nutshell\u0026rdquo; post, I would shame my reputation as someone who writes increasingly long guides when a simple, to-the-point, concise, non-repetitive, efficient, and disambiguated utterance would suffice (this sentence proves my point). So, here\u0026rsquo;s a step-by-step how to set up some useful listeners.\nWhen you create a trigger for an auto-event, such as a Click trigger or a History trigger, they will always be on. What does this mean? Well it means exactly what I wrote. For example, when you create a Click trigger – regardless of what filters you have in place for the actual tag which uses it – every single click on every single page will create the gtm.click event. Go ahead, try it! You can\u0026rsquo;t limit the Click trigger type to only fire on certain pages.\nThere are two notable exceptions to this.\nWhen you create a Timer, Link Click or a Form trigger, you will need to specify when that trigger is enabled, if you\u0026rsquo;re using the Wait For Tags or Check Validation options. This is because these listeners can behave erratically when other scripts on your page interrupt or control the event propagation flow. It\u0026rsquo;s thus not always prudent to have one of these triggers firing on a page where they might not play nicely with your other scripts.\nYou control when these special triggers are enabled using the Enable When step in the trigger settings:\n  You will only see this option on the Timer, the Link Click, and the Form trigger type, when you\u0026rsquo;ve selected Wait For Tags or Check Validation. This filter lets you delimit the trigger to only be active on certain pages, for example. It\u0026rsquo;s best to start with a broad match (Page URL matches RegEx .*), and then modify it appropriately if problems arise.\nThe conditions for when a Tag that uses this trigger should fire are set in the Fire On step:\n  I hope this distinction is now clear. Enable When is for determining when the trigger listens for events, and Fire On is for when the tag should fire if the event is recorded.\nSo, back to the step-by-step. Here\u0026rsquo;s the key process:\n  Create the tag you want to be triggered by the auto-event (e.g. Event tag for Google Analytics)\n  Add the trigger you want to use for the tag (e.g. Click or Form)\n  In the trigger settings, specify what other conditions beside the Click or Form Submit you want in place for the tag (e.g. Click ID equals X or Form URL contains Y)\n  If it\u0026rsquo;s a Link Click or Form trigger, you need to also specify on what conditions should the auto-event listener itself be active\n  Let\u0026rsquo;s go over this step-by-step. I\u0026rsquo;ll create a tag which sends an event whenever someone clicks on a link which redirects out of my domain. An external link, in layman\u0026rsquo;s terms.\n1. Built-in variables First things first. The new UI introduced something called Built-In Variables. These little checkboxes let you quickly enable your most used variables for your tags to utilize. As I want to trigger the tag only when the clicked link HREF does not point to my domain, I will need to activate the Click URL variable. In the old UI, it\u0026rsquo;s the same thing as the {{element url}} macro, just simplified here for your convenience.\n  You\u0026rsquo;ll see me using it in just a bit.\n2. Set up your tag Next, create your tag. I\u0026rsquo;ll be using a Universal Analytics / Event tag for this. Let\u0026rsquo;s just stop short of adding the trigger:\n  Nothing surprising here, right?\n3. Create your trigger Next, we need to create the Link Click trigger that makes this tag go BOOM. So, click on Click in the Triggers list to enter the Choose from existing Click Triggers (if you already have click triggers) or the Add a new trigger view if this is your first one. Whatever the case, you should now be in the process of creating a new Click trigger for your tag:\n  In Step 2, you\u0026rsquo;re asked to select Trigger Type. Choose Link Click from this list, and you should see two new options in this same step:\n  Wait For Tags - if you want the listener to wait for any tags to fire before proceeding with the original action (i.e. link redirect)\n  Check Validation - if you only want the listener to activate when a proper link click is registered\n  It\u0026rsquo;s a good idea to start with both checked, and then if it doesn\u0026rsquo;t work, debug by unchecking first one and then the other.\nNote that since we\u0026rsquo;re creating a (Link) Click trigger, the underlying condition is that a click has occurred, so you don\u0026rsquo;t have to specify the gtm.(link)click event anymore! All you need to specify in these filters are the other conditions you might want to use to have your tag fire only under specific circumstances.\nIf you choose either Wait For Tags or Check Validation, Step 3 requires that you enter the condition for when the listener should be active and listening for clicks. Like I suggested before, start with a broad match like Page URL matches RegEx .*, test it thoroughly, and if some pages prove problematic, exclude them with a URL condition.\n  Step 4 is for specifying what happens when a (link) click event is registered. If a tag has this trigger attached to it, you should now specify any other conditions you want to check to make the tag only fire for certain link clicks, for example. Because we want to track external link clicks, we\u0026rsquo;ll start by choosing Some Clicks, and then entering the following Fire On condition:\n  So, by having Fire On: Click URL does not match RegEx simoahava.com, we\u0026rsquo;re making sure that any tag that uses this trigger is only fired if the click event is registered on a link pointing away from the simoahava.com domain.\nAnd that\u0026rsquo;s it! The way you read the settings from Step 1 to Step 4 is roughly this:\nThis Link Click - External Link asset is a trigger of type Link Click. It will listen for link clicks on all pages where this container is installed on. When a link click event is registered, this trigger will activate any tag it is attached to only if the clicked link element URL does not match the regular expression pattern simoahava.com. The link click listener will also wait for any tags to fire before proceeding with the link redirection and only activate if the action is a valid link click.\nSo, this is now 95 % of auto-event tracking in the new UI. It\u0026rsquo;s trigger-based, so you just need to get your trigger right and everything will work smoothly. Save the trigger and return to the tag.\n4. Fill in the rest of your tag The rest is just regular tag creation. Remember to give your tag a name. You can use the other built-in variables (such as Page Path) to populate your fields, just like in the old UI.\n  And that should be it. Remember to test with the Preview mode to make sure the tag only fires when external clicks are registered!\n  It\u0026rsquo;s important to notice that gtm.linkClick events are registered as usual with every link click, since my Link Trigger was set to be active with {{url}} matches RegEx .*. But it\u0026rsquo;s the first set of filters that make sure my tag only fires when the clicked element HREF doesn\u0026rsquo;t contain simoahava.com.\nSummary The new setup might seem daunting at first. It\u0026rsquo;s a bit confusing now, I admit, and I hope the UI will be improved. For some, it\u0026rsquo;s been difficult to distinguish between the first set of filters on the trigger (that determine when the tag fires), and the second set of filters on the Link Click and Form trigger types (that determine when the listener is active). This should be made more intuitive.\nHowever, having trigger-based auto-event tracking is very logical, indeed. A listener should, after all, only exist to fulfil a condition on a marketing tag. They have no inherent value as tags themselves. It\u0026rsquo;s thus counter-intuitive to have a listener as a tag when its sole purpose is to create the foundations for a firing or blocking trigger.\nAs soon as you grasp the interplay between tags and triggers in this way, you\u0026rsquo;ll have a nice Eureka moment. Feel free to share that moment in the comments below :)\nI only touched upon how the new UI works, and especially the new Built-In Variables only received a cursory glance, but I honestly love working with the new UI. Everything is more streamlined and the workflow is much clearer. I hope you\u0026rsquo;ve found it useful as well!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/once-userid-always-userid/",
	"title": "#GTMtips: Once userId, Always userId",
	"tags": ["Google Tag Manager", "gtmtips", "universal analytics", "user id"],
	"description": "How to persist the User ID across sessions, even if the user is not logged in. Implement with Google Tag Manager.",
	"content": "The User ID is definitely one of the coolest things about Universal Analytics, if used correctly. It might reveal some surprising insights about your visitors, since now you\u0026rsquo;re not restricted to analysing visitors as just browser or device instances as before, but rather you can build your stories around all the touch points the user might have had on their journey to and through your web properties.\nWith this simple tip, you can extend User ID tracking to return users without them needing to authenticate. This is done with a cookie.\nNote! This means that the User ID will often persist on public computers, so the person you\u0026rsquo;re tracking might not actually be the same human (it might also be a llama with some tech skills).\nThis idea was inspired by the comments in Annie Cushing\u0026rsquo;s blog post: Why Google Analytics User Metrics Are BS (For Most Sites). Especially the comments by John Mitchell and Christopher Mason were particularly insightful.\nTip 8: User ID for non-authenticated return users   The process is pretty much this:\n  When users authenticate, the user ID should be stored in dataLayer\n  When subsequent Universal Analytics tags are fired, the \u0026amp;uid; key should first check if the User ID is in dataLayer\n  If it is, then a cookie is written in the user\u0026rsquo;s browser with this User ID, and finally the value is returned to the tag\n  If the User ID is not in dataLayer, GTM checks if it\u0026rsquo;s stored in a cookie, and if it is, the cookie value is returned\n  If there is no such cookie, nothing is returned and the User ID parameter will not get sent\n  So, for this to work you\u0026rsquo;ll need three variables. First one is {{uid in datalayer}}, and it\u0026rsquo;s a simple Data Layer Variable where the variable name it points to is whatever you have configured by your website. I\u0026rsquo;ll user userId in this example.\n  So when a user logs in, the website should push their User ID into dataLayer like so:\ndataLayer.push({ \u0026#39;userId\u0026#39; : \u0026#39;AAA-123\u0026#39;, \u0026#39;event\u0026#39; : \u0026#39;authentication\u0026#39; });  This shouldn\u0026rsquo;t come as a surprise, right? That\u0026rsquo;s how you should do it in any case. The \u0026lsquo;event\u0026rsquo; push fires the Event Tag which sends the user ID to Google Analytics. Nothing ground-breaking here, yet.\nNext, you\u0026rsquo;ll need two other variables. The first one is a 1st Party Cookie variable, which looks for whatever cookie name you\u0026rsquo;ve decided to store the user ID in. I\u0026rsquo;ll use userId again for consistency, but note that it\u0026rsquo;s a pretty common name, and you don\u0026rsquo;t want to overwrite other cookies written by your scripts.\n  Finally, you\u0026rsquo;ll need a Custom JavaScript Variable, which we\u0026rsquo;ll call {{user id}}. Its task is to perform the algorithm described in the beginning of this chapter.\nfunction() { if ({{uid in datalayer}}) { var d = new Date(); d.setTime(d.getTime()+1000*60*60*24*365*2); var expires = \u0026#39;expires=\u0026#39;+d.toGMTString(); document.cookie = \u0026#39;userId=\u0026#39; + {{uid in datalayer}} + \u0026#39;; \u0026#39;+expires+\u0026#39;; path=/\u0026#39;; return {{uid in datalayer}}; } else if ({{uid in cookie}}) { return {{uid in cookie}}; } return; }  And, finally (phew! this was supposed to be a simple tip), you need to edit your Universal Analytics tags to fetch the {{user id}} for the \u0026amp;uid;:\n  And that will do it.\nRemember, this isn\u0026rsquo;t for everyone. There\u0026rsquo;s no inherent benefit of always tracking return users unless it\u0026rsquo;s something you consider necessary for you to achieve your business goals. Sometimes it\u0026rsquo;s a business requirement itself to track return users who don\u0026rsquo;t authenticate. And don\u0026rsquo;t forget the warning about public computers. You might want to edit this script to only work for mobile phones and tablets, since it\u0026rsquo;s less likely that they have as many users as a library computer might.\n"
},
{
	"uri": "https://www.simoahava.com/tools/",
	"title": "Tools",
	"tags": [],
	"description": "",
	"content": "   I\u0026rsquo;ve written a bunch of tools to help you use and debug Google Tag Manager and Google Analytics. I\u0026rsquo;m neither a professional product developer nor am I a visual designer, and both correlate with how the tools look like and what their user experience is :)\nNevertheless, I only create tools that I want to use myself, and being a sort of power-user especially in the development side of things, I do think that these little utilities provide added value to your tag management needs.\nTo read more about each individual tool, see the respective menu item under TOOLS, or go directly from the list below.\n  GTM Tools - Tools for managing your Google Tag Manager containers and assets at www.gtmtools.com.\n  Chrome Extensions - I\u0026rsquo;ve written two extensions: GTM Sonar for debugging Google Tag Manager\u0026rsquo;s auto-event tracking, and Internalize, which lets you set yourself as internal traffic when browsing a website.\n  Google Analytics Validator - Google Sheets Add-on, which lists all your GA accounts, properties, and views. Then, it lets you build a sheet of all their Custom Dimensions and whether these Custom Dimensions have collected any data in the past 7 days. Add it to Sheets here.\n  Google Analytics Custom Dimension Manager - Google Sheets Add-on, which lets you build a source sheet of Custom Dimensions, and then apply these to any property you want (updating or skipping existing dimensions, and creating new ones if missing). Add it to Sheets here.\n  GTM Tools by Simo Ahava - Google Sheets Add-ons, which let you manage your Google Tag Manager containers, tags, triggers, and variables. Add it to sheets here.\n  customTask Builder - A tool which lets you build your own customTask script by selecting different components to include in the callback.\n  "
},
{
	"uri": "https://www.simoahava.com/analytics/introducing-gtm-tools/",
	"title": "Introducing GTM Tools",
	"tags": ["api", "Google Tag Manager", "gtm tools"],
	"description": "Introducing GTM Tools, a toolset built for the management of your Google Tag Manager accounts, containers, and assets within.",
	"content": "I\u0026rsquo;ve written a completely revamped version of this toolset for Google Tag Manager V2.\nWell, I just yesterday published the first of my GTM API tools (the Container Visualizer), and I vowed that I wouldn\u0026rsquo;t release the other tools for a number of reasons.\nThe reasons were good, in my opinion (especially the part about the tools being ugly as crap), but on the other hand I don\u0026rsquo;t want to keep anyone away from the amazing potential of the new API.\nSo here are all the tools I\u0026rsquo;ve written so far. Note that all tools have been written for the \u0026ldquo;old UI\u0026rdquo;, i.e. there are no triggers or variables that you can manage yet. I\u0026rsquo;ll get around to treating these new features in future releases.\n  Feel free to play around. There are surely errors here and there, and I\u0026rsquo;d be grateful if you could e-mail or Tweet me the trace stack what you\u0026rsquo;ll see if you encounter an error.\nYou can find the tools here: www.gtmtools.com.\nAlso, be patient. It\u0026rsquo;s still running on the free App Engine quota, because I don\u0026rsquo;t want to start asking for money from people who want to use these tools. I\u0026rsquo;ll see if I can upgrade to better quotas in the near future.\nContainer Cloner The first tool lets you copy entire containers from one account to another. Cloning a container means cloning everything in it, apart from user permissions. So you\u0026rsquo;ll see your tags, rules, and macros all whooshing from one account to another.\n  Here\u0026rsquo;s how it works:\n  Choose an account from which you want to transfer the containers\n  Choose an account to which you want to transfer the containers\n  Choose one or multiple containers from the source column\n  Click \u0026ldquo;Move Containers\u0026rdquo;\n  If everything is fine, click \u0026ldquo;CLONE CONTAINERS\u0026rdquo; to start cloning\n  You won\u0026rsquo;t be able to clone a container within the same account, nor can you copy a container if a container with the same name already exists in the target account.\nTag / Rule / Macro Cloner The three other cloners can be bunched up into one description, since they have a very similar mode of operation. Basically, you choose a source container and a target container, and then the assets you want to clone.\nThe tool prevents assets with the same name being copied, and I might add an option to either overwrite or add a (copy) or something if there is a name conflict.\nAnother thing to note is that the cloner only clones the \u0026ldquo;shell\u0026rdquo; of the tag, rule, or macro. If there are references within these assets to other assets (tags macros, rules referencing macros, and macros referencing other macros), these dependencies are NOT cloned in this version, so you\u0026rsquo;ll have to remember to go through the new assets and clone any dependencies that are missing. Otherwise your container will not publish, as it will alert that a macro which doesn\u0026rsquo;t exist has been referred to.\n  And here\u0026rsquo;s how the cloners work:\n  Choose source account and container\n  Choose target account and container\n  You can sort the results from A to Z, and you can show Macro and Tag types by clicking the respective buttons\n  Choose the assets you want to copy from the source container\n  Click \u0026ldquo;Move Tag(s) / Rule(s) / Macro(s)\u0026rdquo;\n  If the move is successful, you\u0026rsquo;ll see the assets-to-be-cloned in the target container with an asterisk (*) next to them\n  Click \u0026ldquo;CLONE TAGS / RULES / MACROS\u0026rdquo; to start the cloning process\n  Please let me know if you run into trouble. I know there are some errors that still pop up every now and then.\nContainer Visualizer I wrote about the Container Visualizer in a recent post, so be sure to read that. It\u0026rsquo;s definitely my favorite tool of the bunch!\nSummary These tools showcase the powers of the API. You can really do awesome stuff, and save a LOT of time.\nFuture releases of the tools will include:\n  Mass Update tool, which lets you add a condition to multiple rules at once, or a rule to multiple tags at once\n  Cloners for the new UI assets (triggers and variables)\n  General support for \u0026ldquo;GTM 2.0\u0026rdquo; (the new UI and assets)\n  Some sort of dependency checking for the cloners, which would let you copy all dependencies (e.g. linked macros and rules) as well\n  I\u0026rsquo;m always happy to receive feedback, but I\u0026rsquo;d like to remind you that I\u0026rsquo;m not a professional product developer, nor am I looking to productize these tools or make them something they aren\u0026rsquo;t. At least for now. I really wanted to just show what the API can do, and create little tools which I want to use myself. I\u0026rsquo;ve been using the Cloner tools for some time now, and they\u0026rsquo;ve made my life SO much easier.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/container-visualizer-google-tag-manager/",
	"title": "Container Visualizer for Google Tag Manager",
	"tags": ["api", "Google Tag Manager", "tools"],
	"description": "Introducing the container visualizer, a part of the GTM Tools toolset built for Google Tag Manager management.",
	"content": "[UPDATE:] Quite a lot of people had trouble accessing the tool after I published this post. This should now be fixed.\nSo, AWESOME stuff. The new Google Tag Manager UI and API have just rolled out, and I can finally start revealing the stuff I\u0026rsquo;ve been working on :)\nI\u0026rsquo;m not going to go into the new UI in this post. I just want to give a huge thanks to the GTM team for working on the UX with such dedication. The big problem with GTM so far has been that newcomers and non-developers (and why not developers as well) don\u0026rsquo;t get the flow of the tool. There\u0026rsquo;s no indication of \u0026ldquo;what to do next\u0026rdquo;, or \u0026ldquo;what\u0026rsquo;s taking place right now\u0026rdquo;. These are some of the things that have been remedied in this release.\nBut the API. Oh man, the API. I love it. It lets me do such cool things with it. I\u0026rsquo;ve got a bunch of tools in the prototype-stage, including:\n  Container Cloner (copy container(s) from account to account)\n  Tag Cloner (copy tag(s) from container to container)\n  Macro Cloner (copy macro(s) from container to container)\n  Rule Cloner (copy rule(s) from container to container)\n  Container Visualizer (read more below)\n  I\u0026rsquo;m not ready to go public with them for two reasons: 1) I\u0026rsquo;m still using the free App Engine plan, meaning quotas will burst if dozens of people start cloning GTM assets at the same time (yes, I\u0026rsquo;m that confident about the tools\u0026rsquo; popularity), and 2) I\u0026rsquo;m not a visual developer, so the tools look like crap.\n(UPDATE: I caved and made the other tools public as well. See this post for more info.)\nBut there is one tool I want to make public now, because I\u0026rsquo;m so furiously proud of it. So here we go.\nContainer Visualizer It\u0026rsquo;s a little tool that lets you visualize all links and relationships between the tags, macros, and rules of a container.\n  You can find the tool here: Container Visualizer (GTM Tools @SimoAhava). I haven\u0026rsquo;t tested extensively across browsers, so it might look a bit different depending on which browser you use.\nHere\u0026rsquo;s how it works:\n  First you need to authenticate your Google ID to use the API\n  Next, choose an account whose container you want to inspect\n  Then choose a container in that account\n  Now you should see the diagram with all your nodes\n  Hover over a node to see its connections\n  Click a node to freeze its state (click again anywhere to release)\n  Click \u0026ldquo;Select hermit nodes\u0026rdquo; to select nodes with no links\n  Start typing to find a specific tag, macro, or rule\n  Click \u0026ldquo;Full screen\u0026rdquo; to view a larger version of the visualization\n  I think this is a pretty darn sweet tool to see how vibrant your container is. If you have the average container, you should see a lot of links to the {{event}} macro and the {{url}} macro, but if you want a healthy container, you should see very few hermit nodes or assets with just single links.\nFor the visualization, I did some heavy modifications to the Hierarchical Edge Bundling visualization built with the amazing d3.js library.\nTechnology-wise it\u0026rsquo;s all Python, JavaScript, and HTML templates.\nI\u0026rsquo;ll try to muster up the courage to publish the other apps in my GTM Tools @SimoAhava collection soon.\nLet me know what you think about the whole idea of visualizing your data assets!\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/datalayer-declaration-vs-push/",
	"title": "#GTMtips: dataLayer Declaration Vs. .push()",
	"tags": ["datalayer", "Google Tag Manager", "gtmtips"],
	"description": "Recognize the difference between a dataLayer declaration and a dataLayer.push(), and always use the latter when working with Google Tag Manager.",
	"content": "Here\u0026rsquo;s a tip on how to avoid a horrible, horrible mistake with Google Tag Manager.\nTip 7: Always use .push() with dataLayer   When you assign a value to a variable using the equals ( = ) sign, you are actually reallocating the variable to a new value, and the garbage collection system of the runtime environment sweeps the previous value to bit heaven.\nLet\u0026rsquo;s put it simply: if you redefine dataLayer after the GTM container snippet, you will break GTM\u0026rsquo;s proprietary functions. No, I\u0026rsquo;m not trying to be dramatic, it\u0026rsquo;s the truth. You cause havoc because GTM modifies the native push() method that all Arrays have, adding stuff of little importance such as a listener which fires tags when an \u0026lsquo;event\u0026rsquo; key is pushed!.\nBy redefining dataLayer your tags will no longer fire. Oops!\nBut hey, we\u0026rsquo;re often instructed to use variable assignment when creating dataLayer before the container snippet. Right? Well, yes, that\u0026rsquo;s what we\u0026rsquo;re instructed to do but that\u0026rsquo;s not necessarily the best practice. Having just one declaration there works like a charm, but what about:\n\u0026lt;script\u0026gt; dataLayer = [{ \u0026#39;businessCriticalVar\u0026#39; : \u0026#39;businessCriticalVal\u0026#39; }]; \u0026lt;/script\u0026gt; ... some other code ... \u0026lt;script\u0026gt; dataLayer = [{ \u0026#39;anotherCriticalVar\u0026#39; : \u0026#39;anotherCriticalVal\u0026#39; }]; \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- GTM Container Snippet --\u0026gt; Well, you might have guessed it. The second assignment overwrites the first one, and dataLayer only has anotherCriticalVar available for the benefit of your tags.\nSo how to avoid this?\nSimple: use only push() when interacting with dataLayer. That way you\u0026rsquo;ll never overwrite anything in the Array, you\u0026rsquo;ll just add new stuff to the end.\nIn GTM tags (Custom HTML, for example), you can just go ahead and use dataLayer.push(), since you can be 100 % sure that dataLayer is an Array with a push() function (the container snippet takes care of this for you). But when you\u0026rsquo;re working on the page template, where it\u0026rsquo;s not necessarily a given that dataLayer has been defined as an Array, always use the following syntax to safeguard against problems:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#39;someVar\u0026#39; : \u0026#39;someVal\u0026#39; });  The first line basically checks if a global variable called dataLayer has already been declared. If it has, it\u0026rsquo;s left alone and execution proceeds to the push() block. If, however, dataLayer has NOT been defined, the first line then assigns a new, empty Array to it. This ensures that the following push() will always work.\nSo here\u0026rsquo;s the recap:\n  When working on the page template, always check whether or not dataLayer has been defined, and initialize it as a new Array if necessary\n  Always, ALWAYS, use push() when interacting with dataLayer\n  DISCLAIMER: The window.dataLayer = window.dataLayer || []; fails if dataLayer HAS been defined but not as an Array. This is probably quite rare, but you should be aware of what variable has been assigned to GTM by looking at the container snippet\u0026rsquo;s self-invoking function parameters.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/ecommerce-tips-google-tag-manager/",
	"title": "eCommerce Tips For Google Tag Manager",
	"tags": ["ecommerce", "enhanced ecommerce", "Google Tag Manager", "Guide"],
	"description": "Tips for setting up Google Analytics Ecommerce and Enhanced Ecommerce tracking via Google Tag Manager.",
	"content": "I\u0026rsquo;ve noticed that setting up eCommerce in Google Tag Manager (and now the new Enhanced ecommerce) is very difficult for many. I\u0026rsquo;m sure part of the problem is that eCommerce is for many users the moment that GTM forces you to take steps in to the developer\u0026rsquo;s domain, since it\u0026rsquo;s obvious that you\u0026rsquo;ll need to add some code to the web page.\nThis isn\u0026rsquo;t a tutorial on how to do eCommerce in Google Tag Manager. To get started, you\u0026rsquo;ll want to check out the official documentation. Sure, it might be inaccurate in places, and especially the translations are lacking, but dataLayer form and function is nicely described within.\nEnhanced Ecommerce (UA) Developer Guide Google Analytics Ecommerce\nThis article has tips and answers to some frequently asked questions around eCommerce. Hopefully they will help you understand how eCommerce and Google Tag Manager work together.\nThe process in a nutshell Here\u0026rsquo;s how eCommerce and Google Tag Manager conspire to get you the all-important data of your web transactions.\n  Your eCommerce platform and/or your Content Management Solution (CMS) render the source code\n  If the page includes transactional information, this source code must include all required details about the transaction in the dataLayer object\n  When the tag which carries the transaction to GA fires, it will look at the dataLayer object\n  If it finds all the required information within, it will send the transaction hit to GA\n    Most of the following chapters will shed more light on these points, but the key thing to remember is this:\nYour eCommerce and/or CMS platform do almost all the work in tracking your transactions.\nThe backend solution, whether it\u0026rsquo;s your CMS or a dedicated eCommerce platform such as Magento, or even a WordPress plugin (such as WooCommerce), has to do the grunt work of populating the keys in the data layer.\nWith a Python-based application, the page template might look like this:\ndataLayer.push({ \u0026#39;transactionId\u0026#39; : \u0026#39;{{ transactionId }}\u0026#39;, });  With PHP, it might look like this:\ndataLayer.push({ \u0026#39;transactionId\u0026#39; : \u0026#39;\u0026lt;?php echo $transactionId ?\u0026gt;\u0026#39;, });  With QBASIC, it might look like this:\ndataLayer.push({ \u0026#39;transactionId\u0026#39; : \u0026#39;PRINT T$; GOTO END\u0026#39; });  OK, the QBASIC script was fictitious, I was trying to be funny. But nostalgia, right? Right?\nSo don\u0026rsquo;t expect GTM to come up with values for these variables magically. You will need a developer to help you here, and you will need a cooperative backend platform.\nNow, sometimes you might be tempted to scrape the DOM for eCommerce information. That means that you have some JavaScript in place which retrieves all the necessary transactional data from elements on the page, such as headings, body text, even image ALT texts.\nThis is not the recommended approach!\nThere are many, excellent reasons for decoupling semantic information (information with no inherent value to what is rendered in the browser document) from the presentational layer. This is what the data layer was invented for. I strongly suggest you avoid taking the seemingly easy path of scraping on-page elements, especially with business-critical transactional tags.\nNote, however, that some aspects of Enhanced eCommerce as it stands today might actually benefit from a combination of DOM scraping and dataLayer utilization. We\u0026rsquo;ll go over this later in the post.\nHere are the actual tips:\n  dataLayer before GTM\n  Use a custom event\n  Make sure all the data is there\n  Don\u0026rsquo;t forget your JavaScript\n  Enhanced eCommerce is special\n  And we\u0026rsquo;ll wrap it up with a neat summary.\ndataLayer before GTM This is a very common problem. The tag that carries your transaction information fires before the transaction data is in dataLayer. D\u0026rsquo;uh!\nWhen a tag fires, it has access to the current state of all variables in the global namespace. dataLayer is one of them. If dataLayer doesn\u0026rsquo;t have the required variables on the moment that the transactional tag fires, you won\u0026rsquo;t see data from that hit in GA.\nHere\u0026rsquo;s an example. This is what your normal eCommerce tag looks like:\n  As you can see, there are VERY few options to work with. You just set Track Type to Transaction, add a rule, and you\u0026rsquo;re good to go.\nThe point is that this tag will fire as soon as the container snippet is loaded, i.e. when the container snippet pushes 'event' : 'gtm.js' into dataLayer. Because the tag looks into dataLayer for the transaction details, they have to be there by the time the tag fires!\nThus, this will work:\n\u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; dataLayer.push({ \u0026#39;transactionId\u0026#39;: \u0026#39;1234\u0026#39;, \u0026#39;transactionAffiliation\u0026#39;: \u0026#39;Acme Clothing\u0026#39;, \u0026#39;transactionTotal\u0026#39;: 38.26, \u0026#39;transactionTax\u0026#39;: 1.29, \u0026#39;transactionShipping\u0026#39;: 5, \u0026#39;transactionProducts\u0026#39;: [{ \u0026#39;sku\u0026#39;: \u0026#39;DD44\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;T-Shirt\u0026#39;, \u0026#39;category\u0026#39;: \u0026#39;Apparel\u0026#39;, \u0026#39;price\u0026#39;: 11.99, \u0026#39;quantity\u0026#39;: 1 },{ \u0026#39;sku\u0026#39;: \u0026#39;AA1243544\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Socks\u0026#39;, \u0026#39;category\u0026#39;: \u0026#39;Apparel\u0026#39;, \u0026#39;price\u0026#39;: 9.99, \u0026#39;quantity\u0026#39;: 1 }] }); \u0026lt;/script\u0026gt; \u0026lt;!-- Google Tag Manager --\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;iframe src=\u0026#34;//www.googletagmanager.com/ns.html?id=GTM-P8XR\u0026#34; height=\u0026#34;0\u0026#34; width=\u0026#34;0\u0026#34; style=\u0026#34;display:none;visibility:hidden\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-P8XR\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; As you can see, the data is before the container snippet, so the dataLayer object is populated with all the required variables by the time the tag fires.\nHowever, this will NOT work:\n\u0026lt;!-- Google Tag Manager --\u0026gt;; \u0026lt;noscript\u0026gt;\u0026lt;iframe src=\u0026#34;//www.googletagmanager.com/ns.html?id=GTM-P8XR\u0026#34; height=\u0026#34;0\u0026#34; width=\u0026#34;0\u0026#34; style=\u0026#34;display:none;visibility:hidden\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;script\u0026gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\u0026#39;gtm.start\u0026#39;: new Date().getTime(),event:\u0026#39;gtm.js\u0026#39;});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!=\u0026#39;dataLayer\u0026#39;?\u0026#39;\u0026amp;l=\u0026#39;+l:\u0026#39;\u0026#39;;j.async=true;j.src= \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39;+i+dl;f.parentNode.insertBefore(j,f); })(window,document,\u0026#39;script\u0026#39;,\u0026#39;dataLayer\u0026#39;,\u0026#39;GTM-P8XR\u0026#39;);\u0026lt;/script\u0026gt; \u0026lt;!-- End Google Tag Manager --\u0026gt; \u0026lt;script\u0026gt; dataLayer.push({ \u0026#39;transactionId\u0026#39;: \u0026#39;1234\u0026#39;, \u0026#39;transactionAffiliation\u0026#39;: \u0026#39;Acme Clothing\u0026#39;, \u0026#39;transactionTotal\u0026#39;: 38.26, \u0026#39;transactionTax\u0026#39;: 1.29, \u0026#39;transactionShipping\u0026#39;: 5, \u0026#39;transactionProducts\u0026#39;: [{ \u0026#39;sku\u0026#39;: \u0026#39;DD44\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;T-Shirt\u0026#39;, \u0026#39;category\u0026#39;: \u0026#39;Apparel\u0026#39;, \u0026#39;price\u0026#39;: 11.99, \u0026#39;quantity\u0026#39;: 1 },{ \u0026#39;sku\u0026#39;: \u0026#39;AA1243544\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Socks\u0026#39;, \u0026#39;category\u0026#39;: \u0026#39;Apparel\u0026#39;, \u0026#39;price\u0026#39;: 9.99, \u0026#39;quantity\u0026#39;: 2 }] }); \u0026lt;/script\u0026gt; This time, the transaction data is pushed after the container snippet has loaded, and there\u0026rsquo;s a very strong possibility that the data is not rendered in dataLayer when the Transaction Tag fires.\nThe same applies to any data you might try to move in a Custom HTML Tag. The Custom HTML Tag fires at its earliest with {{event}} equals gtm.js, because that\u0026rsquo;s the earliest possible event to fire your tags on. However, your transaction tag is ALSO firing on that event, meaning there\u0026rsquo;s a possibility that your transaction tag will fire first, and thus won\u0026rsquo;t find the transaction details in dataLayer. You can try to rectify this with Tag Priority, but in this case it would be better to look at the next tip.\nUse a custom event If you can\u0026rsquo;t push your data into dataLayer before {{event}} equals gtm.js fires, or if there\u0026rsquo;s even the tiniest possibility that the data won\u0026rsquo;t be rendered in time, you could just as well push a custom event of your own.\nFor example, if you\u0026rsquo;re using a Custom HTML Tag to build your eCommerce dataLayer, add an \u0026lsquo;event\u0026rsquo; push in there, which you\u0026rsquo;ll then use as the firing rule for your transactional tags:\n\u0026lt;script\u0026gt; dataLayer.push({ ... eCommerce properties ..., \u0026#39;event\u0026#39; : \u0026#39;transactionComplete\u0026#39; }); \u0026lt;/script\u0026gt; Then your transactional tag needs to fire on {{event}} equals transactionComplete. This way you\u0026rsquo;ll be 100 % certain that the data is in dataLayer by the time the tag fires, because the trigger event is pushed at the same times as the transactional data!\nMake sure all the data is there The developer guides tell you exactly what variables are required. In traditional eCommerce, here\u0026rsquo;s what your Transaction Tag will look for:\nTransaction ID - transactionId Transaction Total - transactionTotal Product Name - name Product SKU - sku Product Price - price Product Quantity - quantity\nIf any of these fields are missing from dataLayer, the Transaction Tag will not get sent.\nOh, and also: transactionId and sku need to be unique. If you have multiple transactions with the same ID, or if you have many products with the same SKU, your data will be skewed.\nWith Enhanced eCommerce, there are many more fields to observe, so be sure to read both the Google Tag Manager Developer Guide (linked to in the beginning of this post), as well as the general guide for Enhanced eCommerce in Google Analytics.\nDon\u0026rsquo;t forget your JavaScript We\u0026rsquo;re in the developers\u0026rsquo; domain here. Don\u0026rsquo;t forget that. Here are some things you should check first if your tags are not firing:\nEach object key requires a value - If you want to omit a property from the push, either send it with an empty string, or drop it out of the push altogether. Don\u0026rsquo;t send it without a value at all:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;purchase\u0026#39; : { \u0026#39;actionField\u0026#39; : , // WRONG! This key requires a value  ... }); dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;purchase\u0026#39; : { \u0026#39;actionField\u0026#39; : \u0026#39;\u0026#39;, // BETTER!  ... });  Know when to add a comma to the end of the line - If the properties are siblings of each other, that is, they have the same parent, then each line must end in a comma except the last line. Here\u0026rsquo;s how it should go:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { // No comma, starts a new hierarchy  \u0026#39;purchase\u0026#39; : { // No comma, starts a new hierarchy  \u0026#39;actionField\u0026#39; : {\u0026#39;list\u0026#39; : \u0026#39;Apparel Gallery\u0026#39;}, // Comma, followed by a sibling  \u0026#39;products\u0026#39; : [{ // No comma, starts a new hierarchy  \u0026#39;name\u0026#39; : \u0026#39;product1\u0026#39;, // Comma, followed by a sibling  \u0026#39;id\u0026#39; : \u0026#39;12345\u0026#39; // No comma, last member in current object  },{ // Comma, since there are two objects in this \u0026#39;products\u0026#39; Array  \u0026#39;name\u0026#39; : \u0026#39;product2\u0026#39;, // Comma, followed by a sibling  \u0026#39;id\u0026#39; : \u0026#39;23456\u0026#39; // No comma, last member in current object  }] // No comma, last member in current object  } // No comma, last member in current object  }, // Comma, followed by a sibling  \u0026#39;event\u0026#39; : \u0026#39;transactionComplete\u0026#39; // No comma, last member in current object });  Respect the type requirements - Let\u0026rsquo;s first address the elephant in the room. Always, always remember to use a period as the decimal separator in a number, not a comma. When passing a numeric value, the comma can be horribly misinterpreted:\n... \u0026#39;price\u0026#39; : 33,75, // WRONG! \u0026#39;quantity\u0026#39; : 1 ...  Here, the browser interprets the first comma as the property separator, and the 75, that follows is not a syntactically valid key-value pair, and will result in an error. Here\u0026rsquo;s how it should look:\n... \u0026#39;price\u0026#39; : 33.75, \u0026#39;quantity\u0026#39; : 1 ...  Now the number is actually a number, with a period as the decimal separator.\nWith enhanced eCommerce, the type for price and other monetary values is confusingly Currency, which is not a JavaScript type, but just use a string with the decimal as the separator, e.g.\ndataLayer.push({ \u0026#39;ecommerce\u0026#39;: { \u0026#39;currencyCode\u0026#39;: \u0026#39;EUR\u0026#39;, // Local currency, type string  \u0026#39;impressions\u0026#39;: [ { \u0026#39;name\u0026#39;: \u0026#39;Triblend Android T-Shirt\u0026#39;, // Name, type string  \u0026#39;id\u0026#39;: \u0026#39;12345\u0026#39;, // ID, type string  \u0026#39;price\u0026#39;: \u0026#39;15.25\u0026#39;, // Price, type string  \u0026#39;brand\u0026#39;: \u0026#39;Google\u0026#39;, // Brand, type string  \u0026#39;category\u0026#39;: \u0026#39;Apparel\u0026#39;, // Category, type string  \u0026#39;variant\u0026#39;: \u0026#39;Gray\u0026#39;, // Variant, type string  \u0026#39;list\u0026#39;: \u0026#39;Search Results\u0026#39;, // List, type string  \u0026#39;position\u0026#39;: 1 // Position, type number  }] ...  So remember: respect the type requirements. A number is written as just the plain number, using a period as the decimal separator: 15.35. No quotes there. A string is a combination of characters wrapped in single or double quotes: \u0026lsquo;T-shirt\u0026rsquo;.\nEnhanced eCommerce is special I\u0026rsquo;ve alluded to Enhanced eCommerce a couple of times already, but now it\u0026rsquo;s time to tackle the beast. If you thought there was a lot of dataLayer this and JavaScript that in the previous tips, you\u0026rsquo;re going to overload with all the development effort that Enhanced eCommerce requires.\nLet\u0026rsquo;s start with the obvious:\nYou do not use the Transaction tag type for Enhanced eCommerce.\nThe Transaction tag is for traditional eCommerce, and as the Enhanced eCommerce dev guide clearly states:\n\u0026ldquo;Important: The Enhanced Ecommerce plug-in should not be used alongside the Ecommerce (ecommerce.js) plug-in for the same property.\u0026quot;\nSo don\u0026rsquo;t confuse the two. Enhanced eCommerce gives you a far more granular and funnel-like look at how visits progress in terms of your purchase funnel.\nThe thing is, with Enhanced eCommerce all hits are piggy-backed on top of existing tags, such as Pageview and Event. The key is to remember to set Enhanced eCommerce features on in the settings of your tags, AND to use dataLayer to carry the data through the tag to Google. Again, you will need to make sure that dataLayer is initialized with all the required data before the tags fire.\n  With Enhanced eCommerce, all the previous tips apply tenfold. That\u0026rsquo;s because there are so many more variables to appreciate, so many more type requirements to observe, so many more ways to get the data flowing to GA.\nAnother important thing about the new feature is that any tag which has Enhanced eCommerce enabled will only process whatever was in the most recent \u0026lsquo;ecommerce\u0026rsquo; variable push to the dataLayer. So if you have, for example:\ndataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;impressions\u0026#39; : ... } }); dataLayer.push({ \u0026#39;ecommerce\u0026#39; : { \u0026#39;detail\u0026#39; : ... } });  Only the latter push will be processed by your tag which fires on {{event}} equals gtm.js. This is because the \u0026lsquo;ecommerce\u0026rsquo; object is still processed with \u0026ldquo;version 1\u0026rdquo; of the data layer, where repeated pushes of the same object name are not merged. Thus each new push overwrites the previous value of the object. In any case, it\u0026rsquo;s crucial that you take this into account. What it means, in practice, is:\nIf you have many Enhanced eCommerce actions / items you need to send in a single tag, you must combine them in a single dataLayer.push()!\nNeedless to say, it will be practically impossible to implement eCommerce with the new features without involving your developers. After all, you are combining on-page actions (clicks, impressions) with the information in dataLayer. Even on the best of days, those two don\u0026rsquo;t mix well.\nHere are some general tips for Enhanced eCommerce:\n  dataLayer has a maximum length of 300 items. A search page on a large web store can have hundreds and hundreds of products, especially if you utilize lazy load methods, or something similar. So try to populate dataLayer with as much information pre-load as possible, and refrain from dynamically adding data into the structure.\n  Sometimes the page elements might be dynamic. For example, you might need to account for sorting the items according to different criteria, or the same item might be in different categories, depending on what sort methods the user has chosen. In those cases you might need to scrape Page Category, for example, from the DOM, and push it into the product listing in the data model. Be sure to read this guide for information how to update existing products in GTM\u0026rsquo;s data layer.\n  Start small and scale up, if the task seems too big to comprehend. Start with tracking the shopping cart, checkout funnel, and purchases. Next plan and implement the path from product impression, to product click, to product detail view. Finally, work yourself around promotions, campaigns, and refunds, if you wish.\n  If you want to send multiple items using a single Enhanced eCommerce enabled GTM tag, you need to combine the items into a single push. Only the most recent dataLayer.push() with the \u0026lsquo;ecommerce\u0026rsquo; key is processed by your Enhanced eCommerce enabled GTM tag! An even better practice is to always include an \u0026lsquo;event\u0026rsquo; key in your post-container-snippet pushes, and use this \u0026lsquo;event\u0026rsquo; key to fire your tag with every single \u0026lsquo;ecommerce\u0026rsquo; push to dataLayer.\n  Summary The trouble with eCommerce is that it\u0026rsquo;s so deep in the developers\u0026rsquo; domain, but it involves a tool mainly directed at marketers. Naturally, this demands that communication between the technical and the non-technical stakeholders must flow naturally.\nBiggest problems occur when the developers do not understand eCommerce, its purposes, or how transactional information is passed through dataLayer into the analytics platform. Also, sometimes non-developers want to tackle eCommerce on their own, because they\u0026rsquo;ve had success with the TMS thus far, and soon they\u0026rsquo;ve bitten more than they can chew.\nThe truth is that eCommerce requires a lot of knowledge transfer between the people who plan and the people who implement. Otherwise simple problems like failure to appreciate JavaScript type requirements, or what product details need to be represented in dataLayer, will cause errors.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/hitcallback-eventcallback/",
	"title": "#GTMtips: hitCallback And eventCallback",
	"tags": ["callback", "Google Tag Manager", "gtmtips", "Guide"],
	"description": "You can use hitCallback to execute commands after the Google Analytics tag has completed, and eventCallback to execute JavaScript after the dataLayer event has completed. Both in Google Tag Manager.",
	"content": "This time we\u0026rsquo;ll take a look at two different, JavaScript-y features of Google Analytics and Google Tag Manager. Callback as a concept should be familiar to anyone who\u0026rsquo;s ever used a programming language. It\u0026rsquo;s basically a piece of code that is passed as an argument to some function, so that when this second function has completed, the callback is executed.\nFor web analytics, callbacks are hugely important, since they allow you to impose a firing order for your asynchronous tags. Asynchronous tags, as you might know, abhor order and precision, so sometimes it\u0026rsquo;s necessary to use a callback to have at least an inkling of predictability in your tag jungle.\nTip 6: hitCallback and eventCallback   Here\u0026rsquo;s the difference:\nhitCallback - hitCallback is a feature of the analytics.js collection library. It lets you provide a callback function for each tag separately. If you want to, for example, fire some tag directly after your pageview has fired, you might want to use hitCallback to push an event into dataLayer, and then use that event to fire your second tag. Or you might do something really wacky, such as use hitCallback to fire a single tag multiple times.\nThe key thing here is to make sure that the Custom JavaScript Macro which holds the callback function returns a function which does all the work. This is important, since otherwise the function expression in your callback macro would fire TWICE: first when the tag is written and executed, and again when the callback is fired.\neventCallback - eventCallback is pure GTM. If you push a dataLayer event into the message queue, you can also push the eventCallback key as well. The value of the key would be the callback function. As soon as all tags which fire on the event that you pushed have executed, your callback function will fire.\ndataLayer.push({ \u0026#39;key1\u0026#39; : \u0026#39;value1\u0026#39;, \u0026#39;key2\u0026#39; : \u0026#39;value2\u0026#39;, \u0026#39;event\u0026#39; : \u0026#39;fireTags\u0026#39;, \u0026#39;eventCallback\u0026#39; : function() { alert(\u0026#39;ALL tags which fire on {{event}} equals fireTags have now fired\u0026#39;); } });  So in the code example above, as soon as all tags that have the firing rule {{event}} equals fireTags have executed, the function in the callback will fire.\n"
},
{
	"uri": "https://www.simoahava.com/tags/callback/",
	"title": "callback",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/gtm-tips/chain-macros-lookup-tables/",
	"title": "#GTMtips: Chain Macros In Lookup Tables",
	"tags": ["Google Tag Manager", "gtmtips", "macros"],
	"description": "You can chain Google Tag Manager lookup tables together to create a powerful and flexible way to fetch dynamic data from a number of sources.",
	"content": "One of the cool things about using a tag management solution is that you can leverage variables like never before. In Google Tag Manager, these variables are referred to as macros, and you can identify a macro with the syntax of {{macro name}}. In this tip I\u0026rsquo;ll show you how you can actually call macros from other macros, using a Lookup Table as an example.\nTip 5: Chain Macros In Lookup Tables (And Other Macros)   It\u0026rsquo;s not just Lookup Tables, either. You can pretty much use a macro in any field where there is a script context (because macros are JavaScript functions and require a script context to be run). Valid fields are, for example, Custom HTML Tags, Custom JavaScript Macros, Google Analytics tag fields, and so on.\nLeveraging macros like this is a really cool way to make your tag setups more flexible AND leaner at the same time. In the picture above, I use two Lookup Tables, chained together, to achieve the following:\n If the hostname of the page is A or B, then return the respective Tracking ID If the hostname is C and Debug Mode is on, then return the respective Tracking ID If the hostname is C and Debug Mode is false, then return the respective Tracking ID  So instead of having a bunch of tags to accommodate for all this variations, I can just have a single tag and let macros do the work.\nJust remember, chaining too many macros together can quickly become a management nightmare. Only use macros to return a calculated value, never to set or push anything by themselves.\nHere\u0026rsquo;s some further reading for you:\n  Macro Guide For Google Tag Manager\n  Macro Magic For Google Tag Manager\n  "
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-data-model/",
	"title": "Google Tag Manager&#39;s Data Model",
	"tags": ["data layer", "data model", "Google Tag Manager", "Guide"],
	"description": "A detailed description and walkthrough of Google Tag Manager&#39;s internal data model and how it interacts with the on-page dataLayer structure.",
	"content": "It\u0026rsquo;s time for MeasureCamp again! As before, I want to write an accompanying post for my session, since there\u0026rsquo;s always so much more to say than the time slot allows for. So, the topic of this article is the data model used by Google Tag Manager to process digital data in your data layer.\nThis post also picks up where I left in my previous foray into the data layer. However, where the first article aimed to be generic (since the data layer should be generic), this post will look at how GTM uses the information in the generic data layer, and how it processes this information to work with the proprietary features of the tool.\n  The diagram above should elucidate my point (fear my PowerPoint-to-image skills).\nWe have data passing through your backend systems to your website. Some of this data is used to build the website with its visuals and functionalities, and some of this data is stored in the data layer to be used by other tools and applications connected to the website.\nGoogle Tag Manager doesn\u0026rsquo;t access the data layer\u0026rsquo;s structure directly, since that would compromise the data layer\u0026rsquo;s generic and tool-agnostic purpose. Rather, it pulls data from the data layer, stores it in its internal, abstract data model, and uses that to process the digital data.\nSince we live in a multi-vendor world, where web tools and applications are popping up like mushrooms after rain, it\u0026rsquo;s important to respect the generic data layer. It\u0026rsquo;s up to the sophistication of the tool itself to use this data, but it must be done in a non-invasive manner - using pull methods rather than push.\n   dataLayer Data Model     Tool-agnostic Tool-specific   Generic Unique   Accessed directly Accessed via helper   Structured Abstract    There\u0026rsquo;s a difference between data layer and data model. To some it might seem very subtle, but in reality it\u0026rsquo;s what ensures that the data layer remains a free-for-all, standardized container for data. The data model, on the other hand, is built according to each platform\u0026rsquo;s own specifications, but the way it communicates with the data layer must be clean and perhaps even standardized, since only that way can we ensure that a single tool doesn\u0026rsquo;t ruin the data layer for all.\nSetting up the test The most familiar way of accessing GTM\u0026rsquo;s data model is through the Data Layer Variable Macro. When you call this macro type, the following happens:\n  The macro polls the data model through an interface method\n  If a key with the given variable name is found, its value is returned\n  If no key is found, undefined is returned instead\n  For the purposes of this article, I\u0026rsquo;ll now create a tester, which shows you how the data model works. The tester is a Custom HTML Tag which fires upon a certain event (\u0026lsquo;dlTest\u0026rsquo;). When it fires, it prints the content of the Data Layer Variable Macro into the JavaScript console.\n  The macro itself is just a Data Layer Macro which refers to variable name testKey:\n  So now, whenever I want to see what the key testKey contains in the data model, I only have to type the following in the console:\ndataLayer.push({'event' : 'dlTest'});\nNext, I\u0026rsquo;ll publish my container, and try this out. This is what the console looks like now if I run the event:\n  The undefined is what the macro actually returns. false is returned because the event push triggered a tag.\nAdd and modify the key Let\u0026rsquo;s start simple. I\u0026rsquo;ll add some values to the key, and see how the macro reacts:\n  Here are the pushes in order:\n  'string' - string\n  1 - number\n  [1, 2, 3] - Array\n  {'key' : 'value'} - object\n  true - boolean\n  function() { return undefined; } - function\n  As you can see, the interface get() function only returns the latest value. dataLayer, however, holds all the values:\n  Here are the key takeaways:\n  The data model is not the same thing as data layer. All the values I pushed above can be found in the data layer, but only the most recent value is stored in the data model\n  When pushing a value of different type, the previous value is completely overwritten in the data model\n  That\u0026rsquo;s pretty simple, right? Well, let\u0026rsquo;s try updating the value with another value of the same type next.\n  Here are the pushes in order:\n  'string' + 'newString' =\u0026gt; 'newString'\n  1 + 5 =\u0026gt; 5\n  [1, 2, 3] + [4, 5] =\u0026gt; [4, 5, 3] *HUH?\n  {'key' : 'value'} + {'newKey' : 'value'} =\u0026gt; {'key' : 'value', 'newKey' : 'value'} *WTF?\n  The primitive values work as expected. Pushing another value of the same type just overwrites the previous value. However, the Array and the plain object behave very strangely.\nThis is because when you\u0026rsquo;re pushing an Array on top of an Array or a plain object on top of a plain object, the interface performs a recursive merge. That is, it checks whether the keys within the object or Array that are being pushed already exist. If they do, their values are updated, but all the other keys are left alone.\nIt\u0026rsquo;s easy to understand if you look at how the plain object behaves.\nFirst, you push an object with the key \u0026lsquo;key\u0026rsquo; with the value \u0026lsquo;value\u0026rsquo;. Next, you push an object with the key \u0026lsquo;newKey\u0026rsquo; with the value \u0026lsquo;value\u0026rsquo;. Now, \u0026lsquo;key\u0026rsquo; is not the same thing as \u0026lsquo;newKey\u0026rsquo;, so the plain object is updated, not replaced.\nBut what about the Array? I\u0026rsquo;m pushing [4, 5], which have nothing in common with [1, 2, 3]. Shouldn\u0026rsquo;t the end result be\n[4, 5], or [1, 2, 3, 4, 5], or even [[1, 2, 3], [4, 5]]?\nSurely [4, 5, 3] is a bug?\nNope, if you know your JavaScript. An Array is a type of an object. It, too, has keys with which you can access the values within. The keys start from 0 and go up until there are no more members in the Array. So, the first Array looks actually something like this:\n[1, 2, 3] Key 0 : Value 1 Key 1 : Value 2 Key 2 : Value 3\nThe second Array looks like this:\n[4, 5] Key 0 : Value 4 Key 1 : Value 5\nNow, the recursive merge spots these shared keys (0 and 1), and updates their values accordingly. The third key (2) is not touched, since the Array that was pushed second had no value for it.\nWe\u0026rsquo;ll explore how to add data to existing Arrays soon enough.\nRemoving a key from the data model If you have a single page app, and the data layer persist throughout the session, you might want to delete some variables from the data model every now and then. Just removing the key from dataLayer won\u0026rsquo;t be enough:\n  Here\u0026rsquo;s what happens:\n  I push \u0026lsquo;simoahava\u0026rsquo; as the value of \u0026lsquo;testKey\u0026rsquo;, this is registered by the macro\n  I delete this entire object from dataLayer\n  I verify this by looking at the contents of dataLayer\n  However, the data model still holds the latest value\n  This is actually an important feature of the data model. The data model treats dataLayer as a queue or a message bus, if you will. It operates on a first in, first out principle, meaning that as soon as something is pushed into dataLayer, it is processed and its values are stored into the data model.\nIt wouldn\u0026rsquo;t work if the data model should remove a key if it is dropped from dataLayer. You might have multiple pushes of the same key with different values (take \u0026lsquo;event\u0026rsquo;, for example). How would the data model know if you\u0026rsquo;re just cleaning up objects from the global Array structure rather than asking for them to be removed from the data model?\nA remove method in the interface might be a good idea, but it\u0026rsquo;s just as easy to take the generic approach and push undefined as the value of the key. This will store undefined into the data model as well, meaning it will be, for all intents and purposes, as if the key no longer exists.\n  That\u0026rsquo;s how simple it is.\nThe command array This is where I left you hanging earlier. Say you want to update an Array by adding members to the end or into the middle. It\u0026rsquo;s very difficult to do in a generic way, since you first need to retrieve the Array from the data model, add members to the end or to the middle, and then push it back. And all has to happen within the data layer, because you don\u0026rsquo;t want all the other tools and platforms that use the data layer to be left outside.\nThe way to do this is to use a special command array. It enables you to access methods of the value type you have stored in the data model.\nHere\u0026rsquo;s how it works. I\u0026rsquo;m going to update an Array [1, 3] first with two new members using push(), so that it becomes [1, 3, 4, 5]. Next I\u0026rsquo;ll do a splice(), where I add the number 2 to its rightful place. Observe closely.\n  As you can see, the command array has its special syntax. First of all, you need to push an Array into the data layer, not an object as you normally would.\nNext, the first member of the Array needs to be a string which holds the actual command. All the rest of the members in the command array are arguments to this command.\nThus, testKey.push(4,5) becomes ['testKey.push', 4, 5], and testKey.splice(1,0,2) becomes ['testKey.splice', 1, 0, 2].\nThis way you can do some cool things with the values stored in the data model without having to access them directly. Using the data layer ensures that other tools and applications can benefit from your modifications as well.\nCustom methods The last thing I\u0026rsquo;ll show you is how to perform some custom transformations on the values stored in the data model.\nLet\u0026rsquo;s say I\u0026rsquo;m storing a bunch of products and stores in the data layer. This data is provided by the backend system. As it turns out, one of the store names is misspelled, and this needs to be fixed in the data model. Performing a series of gets and sets would be cumbersome and very ineffective. Instead, I can just push a function which does this whole thing in a simple for-loop.\n  When you push a function into the data layer, this will be the interface of the data model on the page. It exposes two methods: get(key) and set(key, value).\nFirst, I use get() to retrieve the value of 'testKey'. Then, I do a for-loop which goes over each member in the 'testKey' Array, looking for the typo. If a typo is found, then it\u0026rsquo;s corrected there and then. Because I\u0026rsquo;m dealing with objects, you don\u0026rsquo;t have to push anything back into the data model, since you\u0026rsquo;ve actually copied an object reference, not the object itself.\nDon\u0026rsquo;t worry about that object mumbo-jumbo. The key here is that I performed a transformation on the data in the data model by using the data layer. This way other vendors and tools can benefit from the change as well. I could have just as well directly accessed the public method of the interface, but that would not be the generic way to do things.\nSummary This has been a complicated post, I know, but here are the things you should have learned.\n  The data layer on the page and the data model used by the tag manager are not the same thing\n  The data layer is generic, tool-agnostic, and can be accessed by all applications that can tap into the global namespace\n  The data model is internal to the tag manager, it\u0026rsquo;s abstract (no Arrays here), and it has a public interface with just two methods\n  Certain values (Arrays, plain objects) behave erratically when you try to update them with a regular push\n  You should always do all additions, removals, and transformations by using the data layer, and not by accessing the interface of the data model directly\n  If you want to geek it up a little, take a look at the Data Layer Helper specification in GitHub, written by GTM\u0026rsquo;s own Brian Kuhn. That\u0026rsquo;s where most of the lessons here were picked up.\nIt\u0026rsquo;s so important to understand the subtleties of the data layer and the data model. The one is (or should be) tool-independent, the other is a proprietary feature of the tool. One can be standardized to serve multiple vendors and platforms, the other should cater to the idiosyncrasies of each tool separately.\nMy presentation \u0026ldquo;Google Tag Manager For Nerds\u0026rdquo; from MeasureCamp V\n"
},
{
	"uri": "https://www.simoahava.com/analytics/use-page-visibility-api-google-tag-manager/",
	"title": "Use Page Visibility API With GTM",
	"tags": ["Google Tag Manager", "Guide", "JavaScript", "visibility"],
	"description": "How to use the Page Visibility API in Google Tag Manager for improved accuracy of web analytics tracking.",
	"content": "The Page Visibility API for web browsers is pretty sweet. It lets you poll, using some new properties of the document object, whether or not the current page is visible to the user. Visibility is hidden if the page is not open in the current browser tab instance, or if the browser window has been minimized.\n  In this post, I\u0026rsquo;ll give an example of how features of the Page Visibility API could be used with Google Tag Manager. Do note, however, that browser support for the API pretty much excludes all IE versions older than 10 from the scope of this article.\nIn this example, we\u0026rsquo;ll set up some tags, rules, and macros which will help us avoid receiving pageviews for pages which have been opened in tabs but were never read. I don\u0026rsquo;t know about you, but having a pageview for a page which wasn\u0026rsquo;t viewed sounds counter-intuitive. The pageview will only get sent once the page first becomes visible.\nThe Visibility Listener First, we\u0026rsquo;ll create a visibility listener. What it does is dispatch a browser event every time the visibility state of a page changes.\n  Create a new Custom JavaScript Macro {{visibility prefix}}\n  Add the following code within:\n  function() { var prefixes = [\u0026#39;moz\u0026#39;, \u0026#39;ms\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;webkit\u0026#39;]; if (\u0026#39;hidden\u0026#39; in document) { return \u0026#39;\u0026#39;; } // Loop through each prefix to see if it is supported.  for (var i = 0; i \u0026lt; prefixes.length; i++) { var testPrefix = prefixes[i] + \u0026#39;Hidden\u0026#39;; if (testPrefix in document) { return prefixes[i]; } } return; }    Create a new Custom JavaScript Macro {{visibility hidden}}\n  Add the following code within:\n  function() { switch ({{visibility prefix}}) { case \u0026#39;\u0026#39;: return document[\u0026#39;hidden\u0026#39;]; case \u0026#39;moz\u0026#39;: return document[\u0026#39;mozHidden\u0026#39;]; case \u0026#39;o\u0026#39;: return document[\u0026#39;oHidden\u0026#39;]; case \u0026#39;webkit\u0026#39;: return document[\u0026#39;webkitHidden\u0026#39;]; default: return; } }    Create a new Custom HTML Tag\n  Add the following code within:\n  \u0026lt;script\u0026gt; if (typeof {{visibility prefix}} !== \u0026#39;undefined\u0026#39;) { var visibilityEvent = {{visibility prefix}} + \u0026#39;visibilitychange\u0026#39;, hiddenState = {{visibility hidden}}, visibilityChanged = function() { if (typeof hiddenState !== \u0026#39;undefined\u0026#39;) { dataLayer.push({ \u0026#39;event\u0026#39; : \u0026#39;visibilityChange\u0026#39; }); } }; // Attach visibility listener to document  document.addEventListener(visibilityEvent, visibilityChanged, false); } \u0026lt;/script\u0026gt;  Set tag to fire upon {{event}} equals gtm.js  First we create a utility macro which returns the required browser prefix for setting up the listener and for testing visibility state. In the next macro, we retrieve the state of the document (true: document is hidden, false: document is visible).\nFinally, in the tag we create a listener for changes in page visibility. If visibility changes, a dataLayer event is pushed.\nWe will utilize this data in the following example.\nBlock Pageview Until Page Is Visible So the point here is to not send a pageview to Google Analytics until the page becomes visible. This applies to all your events as well, so you\u0026rsquo;ll need to add these same rules and functions (modified to match the original firing logic) to your event tags, or you might end up having sessions with only events and no pageviews. This is to be avoided.\nHere\u0026rsquo;s the logic:\n  When the pageview is first set to fire (usually {{event}} equals gtm.js), use page visibility as a blocking rule. If the page is hidden, do not fire the tag.\n  When the page becomes visible, use the visibility event as a trigger for the tag.\n  When the pageview has fired, use its hitCallback to prevent the visibility listener from triggering the tag again.\n  So, let\u0026rsquo;s get going.\n  Create a new Custom JavaScript Macro {{visibility callback}}\n  Add the following code within:\n  function() { return function() { if (typeof {{visibility prefix}} !== \u0026#39;undefined\u0026#39;) { var visibilityEvent = {{visibility prefix}} + \u0026#39;visibilitychange\u0026#39;; document.removeEventListener(visibilityEvent, visibilityChanged); } } }    In your pageview tag, create a new rule with the following two conditions, and add it to the tag. Keep all the old rules in place as well!\n  {{event}} equals visibilityChange\n  {{visibility hidden}} equals false\n    In your pageview tag, add the following Blocking Rule\n {{visibility hidden}} equals true    In your pageview tag, browse all the way down to Fields to Set, and add a new field\n  Field name: hitCallback\n  Value: {{visibility callback}}\n    And that should do it. This is a long chain of actions, but it follows the path described in the beginning of this section.\nConclusions The Page Visibility API is pretty cool, but it\u0026rsquo;s first and foremost designed to save resources. Use it to stop a video from playing when the tab is not focused, or to stop an image carousel from proceeding.\nWith Google Tag Manager, you could do a number of things, such as:\n  Block pageview for non-visible pages (explored in this post)\n  Send visibility during pageview as a Custom Dimension (to track if the page is opened in a new tab or directly)\n  Track visibility change as a GA event (to see if people focus on your content)\n  Pause Timer Listener when page is hidden, and restart it when it becomes visible\n  And so on.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/undefined-dimensions-wont-get-sent/",
	"title": "#GTMtips: Undefined Dimensions Won&#39;t Get Sent",
	"tags": ["Google Tag Manager", "gtmtips", "JavaScript", "undefined"],
	"description": "By making sure that unneeded variables are resolved to undefined, Google Tag Manager automatically drops the from tags.",
	"content": "This might not sound like a tip to you. You might think, \u0026ldquo;Dimensions won\u0026rsquo;t get sent? Sounds like a bug!\u0026rdquo;. You\u0026rsquo;re wrong. This is one of the awesome features of the GA API, and it\u0026rsquo;s key to making your tag setups leaner when sending data to Google Analytics.\nTip 4: Undefined dimensions are left out of GA hits   Note that \u0026lsquo;undefined\u0026rsquo; here means the special value undefined in JavaScript. For a refresher, check this post. If you use a macro in a dimension field of Google Tag Manager\u0026rsquo;s Universal Analytics tags, this dimension will be left out of the hit payload if its return value is undefined.\nThe hit itself will get sent, but the dimension will just be dropped out.\nHow is this awesome? Well, you might have thought until now that you need two tags for each version of the dimension reference: one for when it has a value, and one when its value is undefined. The latter version doesn\u0026rsquo;t have the dimension field populated at all.\nYou don\u0026rsquo;t have to go through this duplication of tags if you learn to use the power of undefined in your GTM macros. Just make sure the value is really undefined and not 'undefined', which is a String.\nIt\u0026rsquo;s not just Custom Dimensions this works with. Any fields that are translated into dimensions behave the same way, meaning you can use this to send pretty darn efficient virtual pageviews as well, as they use the Document Path field in your tags.\n"
},
{
	"uri": "https://www.simoahava.com/tags/undefined/",
	"title": "undefined",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/governance/",
	"title": "governance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/data-layer/",
	"title": "The Data Layer",
	"tags": ["data layer", "datalayer", "Google Tag Manager", "governance"],
	"description": "An overview of the Data Layer in Google Tag Manager, and a detailed description of its use and application in websites.",
	"content": "Writing this article is dangerous. Data Layer is two marketers short of becoming a buzz word. This occasion will be heralded by articles such as \u0026ldquo;Data Layer Is Dead\u0026rdquo;, \u0026ldquo;This Developer Implemented A Data Layer And You\u0026rsquo;ll Never Guess What Happened Next\u0026rdquo;, and other examples of the kind of content generation whose propagation should be prevented by military force. This is not one of those articles, I hope, but rather an honest look at what Data Layer is from a number of perspectives.\n  And there are many perspectives, indeed. The terminology itself is difficult to pin down. In this article, I will consider Data Layer to comprise the following definitions:\n  The description of business requirements and goals, aligned in a format that is readily transferrable to technical specifications\n  The concept of a discrete layer of semantic information, stored in a digital context\n  I will also use the variable name dataLayer to denote the data structure used by Google Tag Manager for storing, processing, and passing data between the digital context and the tag management solution. I also prefer the term digital context to website, for example, since the Data Layer can be used in a variety of context, not just a public-facing web environment.\nThe Data Layer most explored in this article is the one that is firmly rooted in the DMZ between developers and marketers. It\u0026rsquo;s very much a technical concept, since its existence is justified by the limitations imposed by certain web technologies (JavaScript, for example) upon how browsers interact with applications (Google Tag Manager, for example). At the same time, Data Layer is, and ought to be, owned at least partly by marketers, analysts, executives, designers, and communication professionals, who draft the business requirements and goals that are satisfied by data collection methods.\nIn other words, it\u0026rsquo;s very common that the governance of Data Layer is debated hotly among different stakeholders of the \u0026ldquo;data organization\u0026rdquo; within a company. Since, as we will learn, it\u0026rsquo;s a generic data model that can be used by all applications that interface with your digital data, it\u0026rsquo;s very difficult to draft a governance model that would satisfy all parties. This, too, we\u0026rsquo;ll explore in this post.\nIn the end I\u0026rsquo;ll share some great resources for learning more about Data Layer, since this post will not be a deep-dive (even though it is wordy).\nWhat Is The Data Layer To put it shortly, a Data Layer is a data structure which ideally holds all data that you want to process and pass from your website (or other digital context) to other applications that you have linked to.\nThe reason we use a Data Layer is because sometimes it is necessary to decouple semantic information from other information stored in the digital context. This, in turn, is because if we reuse information already available, there\u0026rsquo;s a risk that once modifications are done to the original source, the integrity of the data will be compromised.\nA very common example is web analytics tracking. You might have a Data Layer which feeds data into your analytics tool about the visitor. Often, this data isn\u0026rsquo;t available in the presentational layer, or in the markup at all. This data might be, for example, details about the visitor (login status, user ID, geolocation), metadata about the page (optimal resolution, image copyrights), or even information that\u0026rsquo;s already in the markup, but that you want to access in a more robust way.\nThis duplication is often seen in eCommerce data. Instead of \u0026ldquo;scraping\u0026rdquo; transactional details from the header or content of the page, it\u0026rsquo;s more reliable to use the Data Layer to carry this information, since only this way is the data uncoupled from the website proper, meaning it is less subject to errors when markup is modified.\nIf, for example, you were inclined to use data stored in a H2 heading of the HTML markup in the thank you page, a single change to the markup or the format of the information in this HTML element would compromise data collection from the site to your tracking tool. If, however, the data were stored in a Data Layer with no link to the presentational layer, there is a far smaller risk of unexpected changes occurring (though it\u0026rsquo;s definitely not impossible).\nSo, in short, the Data Layer is a data structure for storing, processing, and passing information about the context it exists in.\nThe Data Layer: The Non-Technical Perspective For the marketer, the analyst, the executive, the communications officer, or other non-developer, the Data Layer is actually a list of business requirements and goals for each subset of the digital context.\nFor a web store, for example, business requirements and goals might include transactional information (what was purchased), user data (who made the purchase), spatial and temporal details (where was the purchase made in and at what time), and information about possible micro conversions (did the user subscribe to product updates).\nFor another part of the same website, the business requirements and goals might include simply details about which social media channel brought the user to the website, or which pages the user has viewed more than once.\nThese are not technical specifications, but clearly defined lists of items that need to be collected in order to satisfy the business goals set for each business area of the website or other digital context.\nIdeally, the Data Layer carries information which can be used by as many different tools / users / stakeholders as possible, but it\u0026rsquo;s very common that idiosyncrasies emerge. This is why it\u0026rsquo;s extremely important to treat the Data Layer as a living, agile model, not a stagnated, monolithic, singular entity.\nSimilarly to any aspect of digital analytics, a Data Layer should also be treated as something that\u0026rsquo;s constantly in flux. The data it holds must be optimized, elaborated, divided, conjoined, cleaned, refactored, and questioned as often as new business requirements emerge, or when previous goals were not beneficial to the business.\nGoogle Tag Manager\u0026rsquo;s dataLayer Since there\u0026rsquo;s no existing standard for the data model explored in this article (the effort is under way, though), the Data Layer can have many technical guises. The technical perspective I\u0026rsquo;ve chosen is the one that has evolved through Google Tag Manager. This is because I think, and I\u0026rsquo;m only slightly biased here, that dataLayer is one of the more elegant implementations of a structured data model in the web environment.\ndataLayer is a JavaScript Array, which holds data in key-value pairs. The key is a variable name in String format, and values can be any allowed JavaScript type. This is an example of dataLayer with different data types:\ndataLayer = [{ \u0026#39;products\u0026#39;: [{ \u0026#39;name\u0026#39;: \u0026#39;Kala Ukulele\u0026#39;, \u0026#39;tuning\u0026#39;: \u0026#39;High-G\u0026#39;, \u0026#39;price\u0026#39;: 449.75 },{ \u0026#39;name\u0026#39;: \u0026#39;Fender Stratocaster\u0026#39;, \u0026#39;tuning\u0026#39;: \u0026#39;Drop-C\u0026#39;, \u0026#39;price\u0026#39;: 1699 }], \u0026#39;stores\u0026#39;: [\u0026#39;Los Angeles\u0026#39;, \u0026#39;New York\u0026#39;], \u0026#39;date\u0026#39;: Sat Sep 13 2014 17:05:32 GMT+0200 (CEST), \u0026#39;employee\u0026#39;: {\u0026#39;name\u0026#39;: \u0026#39;Reggie\u0026#39;} }];  Here we have values such as an Array of objects (the products), numerical values (price), an Array of Strings (stores), a date object, and a nested object (the employee name).\nThe point here is that dataLayer is generic and tool-agnostic. As long as it behaves like your typical JavaScript Array, it won\u0026rsquo;t be restricted to just one tool. The information in the dataLayer object above can be used by any application which has access to the global namespace of this page.\nHow the data within this Array is processed is thus left to the tool. In Google Tag Manager, for example, an intermediate helper object is used to process data in dataLayer, which is then stored in an internal, abstract data model within the tool itself. This ensures that dataLayer can stay generic and tool-agnostic, but the data within is processed to comply with the idiosyncratic features of Google Tag Manager.\nThe helper object used by Google Tag Manager has a number of interesting features, such as:\n  A listener which listens for pushes to dataLayer. If a push occurs, the variables in the push are evaluated.\n  Get and set methods which process / manipulate dataLayer as a queue (first in, first out), and ensure that the special values (objects, Arrays) within the data model can be updated and appended correctly.\n  The ability to access commands and methods of objects stored in dataLayer, and the possibility of running custom functions in the context of the data model.\n  These are all transparent to Google Tag Manager\u0026rsquo;s users, of course, but they explain why, for example, the Data Layer Variable Macro can access dotted variable names (gtm.element) and properties (gtm.element.id) equally, and also why you can push multiple values with the same key into dataLayer but only the most recently pushed value is available for tags which fire after the push.\nSince the abstract data model within Google Tag Manager only respects the most recent value of any variable name, the organization must decide where and when Data Layer as a business component becomes dataLayer the Array structure. This is the topic of the next chapter.\nFrom Business Goals To Technical Implementation The most common approach, I believe, is that the business requirements and goals are translated into a set of key-value pairs, which must be rendered / deployed by server-side code, so that dataLayer is populated with all the necessary data before the GTM container snippet loads.\nNaturally, you could do it with client-side code, and it doesn\u0026rsquo;t have to be pre-populated, but business-critical data is best secured if it\u0026rsquo;s rendered into dataLayer at the earliest possible moment in the page load, so that data loss is minimized if the user decides to leave the page before dataLayer has rendered.\nHere\u0026rsquo;s an example. We have a page with the following business requirements that we want to track as business goals:\n  User ID - because we want to track the entire user journey, not just session-by-session or device-by-device\n  Internal user - because we want to filter out our own employees\u0026rsquo; traffic from the data\n  Weather at time of visit - because we want to see how weather affects visit behavior\n  This is a simple, albeit nonsensical, list of business requirements that have a direct impact on how we track goals for this part of the website. This list needs to be appended with more information, such as what are example values for these variables, what is their scope (hit, session, user, product, for example), should they persist (stay on from page to page), and so on. I won\u0026rsquo;t do this now, since it\u0026rsquo;s very much up to how your organization handles projects which span across different departments or business domains.\nAnyway, an example of dataLayer, rendered before the container snippet, might look like this:\n\u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; dataLayer.push({ \u0026#39;userId\u0026#39; : \u0026#39;abf5-3245-ffd1-23ed\u0026#39;, \u0026#39;internalUser\u0026#39; : true, \u0026#39;weather\u0026#39; : \u0026#39;Cloudy\u0026#39; }); \u0026lt;/script\u0026gt; \u0026lt;!-- GTM Container Snippet Code Here --\u0026gt; As you can see, the data is rendered before the GTM container snippet, so that all tags that fire as soon as GTM is loaded can use this data.\nDo note that you can and will use dataLayer within the confines of Google Tag Manager as well, since your tags or other on-page libraries might well push data into the structure after this pre-load sequence. I don\u0026rsquo;t think these dynamic pushes or data exchanges need to be documented as carefully, since they occur solely in the domain of the tool that does the pushes. Thus, documentation and version control is left up to the sophistication of the tool itself.\nThe reason you need to put a lot of thought behind the pre-rendered dataLayer is because each new stakeholder makes the question of governance a bit more complex.\nGovernance Of The Data Layer Coming up with a good governance model is difficult. Coming up with one for a data structure which is at the mercy of a number of different parties, all with varying levels of expertise (and general interest), is even more difficult.\nNevertheless, a well-defined, structured, and formalized governance model is probably the one thing that will prevent your analytics organization from imploding due to missteps in operating with a Data Layer.\nA governance model, in this context, is a document (or documentation) which describes as clearly as possible the Data Layer, its parts, the business domains it\u0026rsquo;s deployed in, its various owners, its version history, its variables, how risk management is handled, etc.\nThis is a very fluid concept, and it really depends on the organization how they want to organize themselves around this project, but ideally this is the kind of governance model I\u0026rsquo;d be happy to work with:\nI Introduction\n Purpose of the document Who this document is for Table of contents  II Version History\n What was revised When it was revised By whom it was revised  III Ownership\n What does ownership mean Who owns the process What are the rights and privileges of the owner  IIIa Stakeholders\n Who have a stake in Data Layer (tools, platforms, departments, agencies, third parties) What is their role What are their rights and privileges  IIIb Technical Specifications\n Who owns the technical Data Layer (IT, most often, or a very enlightened marketer) What is their role What are their rights and privileges  IIIc Management\n Who owns the business requirements (head of marketing, or some similar role in the client organization) What is their role What are their rights and privileges  IV Process Distribution\n What parties use Data Layer What are their special requirements How to avoid conflicts between different stakeholders  V Risk Management\n What are the risks What is their severity What is their probability Who owns the risks (and any actions taken to mitigate them).  VI Data Layer Management Model\n How to plan for updates How to implement updates Who deploys the updates Who tests the updates Who needs to be notified Who updates the document How to avoid conflicts  VII Data Layer Technical Description\n What is the underlying data structure How is this structure translated into each tool\u0026rsquo;s own data model Are there reserved variable names or other potential sources of conflict  VIII Data Layer Variables\n Business requirements translated to data layer variables Sorted by business domain Example values, scope, parameters, expected types Where the data comes from How the data is used And so on\u0026hellip;  I know, it looks horrible. And probably unusable for many. However, having a document like this that is also constantly updated not only provides you with some contractual security, but it also keeps everyone up to date on the most recent structure and format of Data Layer.\nDoes this document need to be consulted / updated when you create a new JavaScript code snippet which calculates the number of images on the page?\nProbably not.\nDoes this document need to be consulted / updated when you\u0026rsquo;re implementing a conversion pixel which also uses Transaction Value?\nMost likely.\nDoes this document need to be consulted / updated when you\u0026rsquo;re deploying enhanced eCommerce?\nAbsolutely.\nIt doesn\u0026rsquo;t have to be larger than life or a huge complication. Just have some concrete description of Data Layer available at all times, and at the very least, agree in writing on how the Data Layer is updated and by whom. This way you\u0026rsquo;ll save a lot of trouble in the long run, when unwarranted changes are about to happen.\nConclusions And Further Reading I think Data Layer is a very difficult concept to grasp. This isn\u0026rsquo;t just because for most it\u0026rsquo;s a technical thing, but because most don\u0026rsquo;t realize it\u0026rsquo;s also very much a list of business requirements.\nTranslating business goals to a well-formed, lean, and 100 % utilized Data Layer is really difficult. I honestly think one of the biggest mistakes is to follow the waterfall model, where a huge list of requirements is jotted down in the beginning of the project, then translated into a Data Layer which appears on every single page on the site, and after that point the structure is never touched again.\nThis doesn\u0026rsquo;t work.\nThe waterfall model is flawed thanks to human fallibility. We simply can\u0026rsquo;t design or predict the final shape of something as vast as an entire layer of semantic data, which might cover almost every single aspect of our digital context. It has to be agile. There has to be a mutual understanding that the shape of the layer becomes clearer with time.\nStart small and scale up, if you have time. If you\u0026rsquo;re in a rush, focus solely on the business-critical requirements.\nWhatever you do, make sure there\u0026rsquo;s a process in place which lets you suggest modifications to Data Layer quickly. This requires a lot of lubrication, education, and knowledge transfer. That\u0026rsquo;s why I think the most important thing in any data project is to start with educating all parties about what the other parties are doing in the project. Make the marketers more development-minded and the developers more respectful of your marketing efforts.\nThat way everyone wins, and you\u0026rsquo;ll have a beautiful Data Layer in no time.\nFurther reading:\n  Google Tag Manager Dev Guide by Google\n  Make Analytics Better With Tag Management And A Data Layer by Justin Cutroni\n  Unlock The Data Layer: A Non-Developer\u0026rsquo;s Guide To Google Tag Manager by Dorcas Alexander / Bounteous\n  The dataLayer Structure in JavaScript 101 For GTM: Part 1\n  P.S. If anyone knows a really good article about governance of semantic data, I\u0026rsquo;d love to read it and link to it in this post.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/gtmtips-rules-nutshell/",
	"title": "#GTMtips: Rules In A Nutshell",
	"tags": ["Google Tag Manager", "gtmtips", "Guide", "rules"],
	"description": "Google Tag Manager&#39;s rules in a nutshell. Rules govern how tags fire based on dataLayer.push events.",
	"content": "Rules are the cornerstone of Google Tag Manager. As with any critical element in a system, they are easy to get wrong. This tip is just a refresher on how GTM firing and blocking rules work.\nTip 3: Google Tag Manager rules in a nutshell   So, let\u0026rsquo;s go through these points one-by-one.\nEvery tag requires a firing rule to work - this is a given. Without a firing rule, your tag will not be written in the document object, and it will never be executed.\nEvery firing rule requires an {{event}} condition - this is a bit more complex. Every time an \u0026lsquo;event\u0026rsquo; variable is pushed with some value into dataLayer, every single rule on every single tag will be evaluated against this value. Thus, every tag needs some value lookup for {{event}} in their firing rules.\n{{event}} equals gtm.js is the \u0026ldquo;default\u0026rdquo; event - if there\u0026rsquo;s no explicit {{event}} condition in your firing rule, GTM will evaluate the tag as having {{event}} equals gtm.js as its firing rule. This event occurs at the earliest possible moment when the GTM Container Snippet is written on the page.\nA single rule can have many conditions - but every single condition must be matched for this rule to work as a trigger. You can introduce some OR-logic into conditions by using e.g. {{event}} matches RegEx ^gtm.(js|linkClick)$, but this is generally better to do with multiple firing rules rather than this crude manipulation of rule conditions.\nA single tag can have many firing rules - this is very important! Every single firing rule that the tag has can and will fire the tag if the rule conditions are met. Thus, if you have a rule {{event}} equals gtm.dom (fire the tag after DOM has loaded) AND {{url path}} matches RegEx ^/thankyou$, this tag will fire once on every page, and twice on the /thankyou-page: first for {{event}} equals gtm.js and second for {{event}} equals gtm.dom! Note that the tag will still fire only once for a given event. So even if you manage to botch things by having three {{event}} equals gtm.js rules in a single tag, the tag will fire just once when the event occurs.\nA blocking rule overrides any firing rule - if you have a blocking rule, it will override any firing rule you might have on the tag. For example, if the firing rule is {{url}} matches RegEx .* and the blocking rule is **{{url path}} matches RegEx ^/thankyou$**, the tag will fire on all pages except the /thankyou-page. The good thing about using blocking rules is that the same rule you block this tag can be used as a firing rule on some other page, e.g. the actual /thankyou-page for your Transaction Tag!\nA tag is written and run only when the firing rules activate it - thus, the moment the firing rule activates the tag, the tag is added to the document object and any macros referred to in the tag are evaluated. This is important, since it means that by the time the tag fires, any macros it refers to have to be available. Many eCommerce setups have failed because the Transaction Tag fires before the eCommerce payload is in the dataLayer.\nOne thing about macros (this is an extra tip). When a tag fires, all macros referred to in the tag are resolved. This means that it\u0026rsquo;s extremely difficult to know, especially with large implementations, how many times any macro will be resolved upon any event, as you can refer to the same macro in as many tags as you wish. Thus, it is highly recommended that you do not use macros to set or push data.\nSo be careful! As the wise members of the GTM product team always remind us: macros should not have side effects. They should be for value retrieval only - not to set or push data.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/check-referrer-for-previous-page-url/",
	"title": "#GTMtips: Check {{referrer}} For Previous Page URL",
	"tags": ["Google Tag Manager", "gtmtips", "Guide"],
	"description": "Use the referrer variable in Google Tag Manager to see where the visitor came from to your website.",
	"content": "Here\u0026rsquo;s a simple way to check what was the source of the visitor\u0026rsquo;s arrival to the current page. It\u0026rsquo;s done by utilizing the {{referrer}} macro, which comes out-of-the-box in any GTM setup.\nTip 2: Use {{referrer}} to see where the visitor came from   You might want to also explore the Component Types and create new macros for {{referrer path}} and {{referrer host name}} for example:\n  By default, you see, the {{referrer}} macro returns the entire URL of the previous page. Sometimes it\u0026rsquo;s more economical to check only against the URL Path or the Host Name of the previous page. This is where the Component Types kick in.\nAlso, remember that if the previous page was not on the current domain (i.e. it\u0026rsquo;s an entrance to the site), this shouldn\u0026rsquo;t automatically be treated as the landing page of the session, when thinking in Google Analytics terms. A session can include many entrances to the site, if they occur within a 30 minute time window and if they are direct or cross-domain traffic.\nThis tip is useful if you want to fire tags or populate macros depending on if the user came from a specific location, for example one of your social media channels.\n"
},
{
	"uri": "https://www.simoahava.com/gtm-tips/save-gatc-constant-string-macro/",
	"title": "#GTMtips: Save GATC In A Constant String Macro",
	"tags": ["Google Tag Manager", "gtmtips", "Guide"],
	"description": "Save the Google Analytics tracking ID as a constant variable in Google Tag Manager.",
	"content": "I wanted to try something new (and, naturally, I\u0026rsquo;m running out of content ideas), so let me introduce the hashtag #gtmtips. I hope others contribute as well, but I will be adding a new tip as often as possible. I\u0026rsquo;ve got maybe 20 tips in store right now, and I\u0026rsquo;m writing new ones all the time. So without further ado, here\u0026rsquo;s\u0026hellip;\nTip 1: Save GATC In A Constant String Macro   This is an easy one, and everyone should already be doing this in one way or another. The point is that you shouldn\u0026rsquo;t use strings in fields which are often used. Save the string in a macro, so all you have to do is refer to the macro in all your tags. That way, if the string ever changes (for example, when transferring to another property), you don\u0026rsquo;t have to rewrite the field in all your tags.\nDead simple!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/persist-datalayer-google-tag-manager/",
	"title": "Persist dataLayer In Google Tag Manager",
	"tags": ["api", "datalayer", "Google Tag Manager", "Guide", "JavaScript", "localstorage"],
	"description": "A script that lets you persist dataLayer variables from one page to the next when using Google Tag Manager.",
	"content": "(Update 19 November 2018: See this article for a more elegant solution.)\nIf you know your JavaScript, you know that all variables, functions, objects, resources, and data in the document get rewritten with every page load. In other words, every single page refresh builds the page from scratch, and the state of the document before the page refresh is left drifting in the ocean of oblivion.\nGoogle Tag Manager\u0026rsquo;s dataLayer is also one such entity. It gets rewritten with every page load, so it\u0026rsquo;s not possible to have a dataLayer variable persist from one page to the other without using cookies or, as I\u0026rsquo;m going to show in this guide, the HTML5 Web Storage API. For web analytics, this is a bit of a shame. It forces us to think of things in the scope of a single page, when rarely anything worth mentioning has a lifespan that short.\nBefore we begin, take a look at what Shay Sharon expertly wrote two years ago about the persistent dataLayer. In his solution, cookies are used to carry the dataLayer across page refreshes. It\u0026rsquo;s a good patch, and it performs the task admirably. However, the thing with cookies is that they can be deleted (and usually are). Also, the length of the cookie string is limited (though 4KB is still plenty), and there\u0026rsquo;s always the fact that cookies are resolved with every single page load (since they are part of the document object).\nData in browser storage, on the other hand, is actually pretty difficult to delete on a granular level, it has a huge quota, and it\u0026rsquo;s only retrieved and stored on demand. Also, using localStorage (more about this soon), you have the data stored indefinitely, so user-level storage is very easy to implement.\n(UPDATE: Remember that browser local storage should be treated the same as cookies when observing the cookie laws of your country. Thanks to Martijn Visser for pointing this out (he\u0026rsquo;s from the Netherlands, and they have pretty much the strictest interpretation of the cookie law in place.))\n  In this guide, I\u0026rsquo;ll give you an API of sorts to save, load, replace, and delete items from the local storage. I use a 30 minute expiration for data in localStorage to mimic session-length storage.\nIntroducing localStorage localStorage stores data indefinitely in your browser\u0026rsquo;s own storage compartment. It has a larger-than-needed size limit (around 5MB), and data is handled more securely than with cookies. Cookies, you see, are written on the document every time you load a page, and this introduces potential security risks.\nItems in localStorage, on the other hand, are retrieved only when called. This way all the data you store will be picked up on demand, and all you need to do is make sure your script handles the data correctly (which is what I\u0026rsquo;ll do for you in this guide).\nIn localStorage, data is written as key-value pairs, but unlike with dataLayer, both the key and the value need to be of String type. This means that in order to save stuff from dataLayer, which can hold any types of values, to localStorage, some object serialization and type conversion needs to take place (Object =\u0026gt; String). Similarly, the reverse needs to take place when loading data from localStorage and pushing it to dataLayer.\nThe Persistent dataLayer API Without further ado, allow me to introduce the Persistent dataLayer API. Copy the following code into a Custom HTML Tag, and read on.\n\u0026lt;script\u0026gt; (function() { // Declare some utility variables  var retrievedDL = \u0026#39;\u0026#39;, getDL = {}, saveDL = {}, persistEvent = /^persist(Save|Replace)/, timeNow = new Date().getTime(), timeStorage = \u0026#39;\u0026#39;, persistTime = 1000*60*30; // Expiration in milliseconds; set to null to never expire  // Only works if browser supports Storage API  if(typeof(Storage)!==\u0026#39;undefined\u0026#39;) { retrievedDL = localStorage.getItem(\u0026#39;persistDL\u0026#39;); timeStorage = localStorage.getItem(\u0026#39;persistTime\u0026#39;); // Append current dL with objects from storage  var loadDL = function() { if(retrievedDL) { dataLayer.push(JSON.parse(retrievedDL)); // dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;DLLoaded\u0026#39;});  } } // Save specified object in storage  var storeDL = function() { for (var i = 0; i \u0026lt; dataLayer.length; i++) { if (persistEvent.test(dataLayer[i].event)) { saveDL = dataLayer[i]; delete saveDL.event; getDL = JSON.parse(retrievedDL) || {}; for (var key in saveDL) { if (saveDL.hasOwnProperty(key)) { getDL[key] = saveDL[key]; } } localStorage.setItem(\u0026#39;persistDL\u0026#39;, JSON.stringify(getDL)); } } } var deleteDL = function() { localStorage.removeItem(\u0026#39;persistDL\u0026#39;); } switch ({{event}}) { case \u0026#39;gtm.js\u0026#39;: if (retrievedDL \u0026amp;\u0026amp; timeStorage) { if (persistTime \u0026amp;\u0026amp; timeNow \u0026gt; Number(timeStorage) + persistTime) { deleteDL(); } else { loadDL(); } } break; // Delete dataLayer variables  case \u0026#39;persistDelete\u0026#39;: deleteDL(); break; // Replace dataLayer variables  case \u0026#39;persistReplace\u0026#39;: retrievedDL = null; // Save dataLayer variables  case \u0026#39;persistSave\u0026#39;: storeDL(); break; } localStorage.setItem(\u0026#39;persistTime\u0026#39;, JSON.stringify(timeNow)); } })(); \u0026lt;/script\u0026gt; The tag will need the following firing rule:\n{{event}} matches RegEx ^(gtm.js|persist(Save|Replace|Delete))\nHere\u0026rsquo;s a rundown of how the API works, before I go into the technical stuff:\n  With every page load, dataLayer variables stored in localStorage are retrieved and pushed into dataLayer.\n  When pushing an object with \u0026lsquo;event\u0026rsquo;: \u0026lsquo;persistSave\u0026rsquo;, all variables in the same push (e.g. \u0026lsquo;pageCount\u0026rsquo; and \u0026lsquo;author\u0026rsquo; in dataLayer.push({'event': 'persistSave', 'pageCount': '5', 'author': 'Simo-Ahava'});) are saved to localStorage. If a variable with the same name already exists in localStorage, it is updated with the new value.\n  When pushing an object with \u0026lsquo;event\u0026rsquo;: \u0026lsquo;persistDelete\u0026rsquo;, all dataLayer variables in localStorage are deleted.\n  When pushing an object with \u0026lsquo;event\u0026rsquo;: \u0026lsquo;persistReplace\u0026rsquo;, all existing dataLayer variables in localstorage are deleted, and all variables in the dataLayer.push() are saved.\n  So remember the following commands:\ndataLayer.push({'var1': 'value1', 'var2': 'value2', **'event': 'persistSave'**}); stores \u0026lsquo;var1\u0026rsquo; and \u0026lsquo;var2\u0026rsquo; (with values) to localStorage.\ndataLayer.push({'var1': 'value1', 'var2': 'value2', **'event': 'persistDelete'**}); deletes all saved dataLayer variables in localStorage; doesn\u0026rsquo;t store anything.\ndataLayer.push({'var1': 'value1', 'var2': 'value2', **'event': 'persistReplace'**}); deletes all saved dataLayer variables in localStorage; stores \u0026lsquo;var1\u0026rsquo; and \u0026lsquo;var2\u0026rsquo; (with values) to localStorage.\nA few additional details:\n  With every page load, variables are loaded from localStorage and pushed to dataLayer. Even though the tag itself fires on {{event}} equals gtm.js, there\u0026rsquo;s a slight delay when processing data through the Storage API. This means that usually the variables appear in dataLayer after gtm.dom but before gtm.load.\n  Every interaction with localStorage, resets the 30 minute expiration timer. If a page load occurs so that the last interaction was over 30 minutes ago, all saved dataLayer variables in localStorage are deleted.\n  Before saving the data, dataLayer variables are serialized into a String. When loading from localStorage, they are parsed from JSON back to their original types. Thus Arrays, objects, and primitive values are restored to their original types before pushed back into dataLayer.\n  Technical stuff Let\u0026rsquo;s go over the code (almost) line-by-line.\n(function() { ... })();  The function is wrapped in an IIFE (immediately invoked function expression). This is because I want to avoid using the global scope when utilizing so many different variables. Scoping the variables to this function ensures that I don\u0026rsquo;t mess with global variables of some other library, for example.\nvar retrievedDL = \u0026#39;\u0026#39;, getDL = {}, saveDL = {}, persistEvent = /^persist(Save|Replace)/, timeNow = new Date().getTime(), timeStorage = \u0026#39;\u0026#39;, persistTime = 1000*60*30; // Expiration in milliseconds; set to null to never expire  Here I introduce a bunch of utility variables. If you want to have the storage persist for ever and ever, set persistTime = null;.\nif(typeof(Storage)!==\u0026#39;undefined\u0026#39;) { retrievedDL = localStorage.getItem(\u0026#39;persistDL\u0026#39;); timeStorage = localStorage.getItem(\u0026#39;persistTime\u0026#39;); ... }  Only run the API if the browser supports HTML5 Web Storage. It\u0026rsquo;s basically only a problem with IE versions older than 8. I saw no reason to provide an alternative for them, since if you\u0026rsquo;re still using IE7 or older, you deserve a horrible browsing experience.\nCheck Shay Sharon\u0026rsquo;s excellent post I linked to in the beginning for a cookie-solution to the persistent dataLayer. That should work with your crappy, out-of-date browser.\nvar loadDL = function() { if(retrievedDL) { dataLayer.push(JSON.parse(retrievedDL)); // dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;DLLoaded\u0026#39;});  } } ... case \u0026#39;gtm.js\u0026#39;: if(retrievedDL \u0026amp;\u0026amp; timeStorage) { if(persistTime \u0026amp;\u0026amp; timeNow \u0026gt; Number(timeStorage)+persistTime) { deleteDL(); } else { loadDL(); } } break;  With every page load, parse the dataLayer variables in localStorage back to their original types, and push them into dataLayer as a single object. If you want to set a trigger event to fire your tags after the variables have been loaded from localStorage, uncomment the line with \u0026lsquo;event\u0026rsquo;: \u0026lsquo;DLLoaded\u0026rsquo;. This way you can have your dependent tags fire on {{event}} equals DLLoaded to ensure that they have access to the stored variables.\nAlso, if the variables in storage expire (default is 30 minutes since last interaction), no data is loaded and the variables are deleted from localStorage. If you\u0026rsquo;ve set persistTime = null;, then there\u0026rsquo;s no expiration for the data in storage, and the variables are stored until they are manually deleted.\nWhen a dataLayer.push(); is made so that the object that is pushed contains the property \u0026lsquo;event\u0026rsquo;: \u0026lsquo;persistSave\u0026rsquo;, the function storeDL() is run.\nFirst, the function removes the \u0026lsquo;event\u0026rsquo; property from the object that was pushed into dataLayer. This is done because you don\u0026rsquo;t want to store the trigger event itself in localStorage. (Another possibility is to just skip the \u0026lsquo;event\u0026rsquo; property when storing properties into localStorage.)\nNext, each property in this dataLayer object is pushed into the object that was found in localStorage. Thus, all variables that were already stored are updated with new values, and new variables are appended.\nFinally, the object, now serialized into a string of keys and values, is stored in localStorage, waiting to be loaded with a new page refresh.\nIf the \u0026lsquo;event\u0026rsquo; was \u0026lsquo;persistReplace\u0026rsquo;, then this the storeDL() is run as well, but all dataLayer variables in storage are replaced with the new variables.\nvar deleteDL = function() { localStorage.removeItem(\u0026#39;persistDL\u0026#39;); } ... case \u0026#39;persistDelete\u0026#39;: deleteDL(); break;  If the trigger event was \u0026lsquo;persistDelete\u0026rsquo;, all dataLayer variables in localStorage are deleted.\nlocalStorage.setItem(\u0026#39;persistTime\u0026#39;, JSON.stringify(timeNow));  Finally, every time the script is run, the current time is saved as a timestamp in localStorage.\nConclusions I\u0026rsquo;ve noticed that many people have been aching for a persistent dataLayer. I\u0026rsquo;m pretty sure this post will become obsolete as soon as the GTM team choose to deploy such a feature into the product itself, but until then, this should serve you well.\nAnd if nothing else, at least you got to learn about yet another really cool JavaScript API!\nDo you have suggestions for the API? Or maybe you have a sweet use case for persistent variables that you might want to share with others?\n"
},
{
	"uri": "https://www.simoahava.com/analytics/node-relationships-gtm/",
	"title": "Node Relationships And GTM",
	"tags": ["auto-event tracking", "event listener", "Google Tag Manager", "Guide", "JavaScript"],
	"description": "Simple variable to check whether or not two nodes are &#34;related&#34; in the document object model. You can do this via Google Tag Manager.",
	"content": "There\u0026rsquo;s a much easier, native-to-GTM way to do this now: the Matches CSS Selector.\nBehind this tragically boring title is a simple solution to many problems with Google Tag Manager\u0026rsquo;s auto-event tracking. The common denominator to these problems is poor website markup. Selectors are used sparingly, and element hierarchy is messy. This disregard for proper node relationships means you have to resort to Data Layer Variable Macros which look like\ngtm.element.parentElement.parentElement.id\nto identify the element that was the target of the event. In other words, there\u0026rsquo;s no robust, unique identifier with which you could identify the targeted element, so you have to go either up or down the Document Object Model to find something you can latch on to.\nThis is not a stable solution.\nThe more steps there are in your selector chain, the more chances there are of you either miscalculating the required steps, or some code on some page template producing a hierarchy which you didn\u0026rsquo;t account for, resulting in a loss of hits.\nIn this guide I introduce a pretty simple solution. It explores the concept of node relationships by using a simple DOM function to do so. The whole thing is still a workaround, and you should definitely try to talk with your developers about how to mark up your elements with proper identifiers.\n(By the way, for markup ideas, here are two excellent articles about using data attributes as identifiers:\nTracking Clicks Using Custom Data Attributes - Bounteous\nGoogle Tag Manager Events Using HTML5 Data Attributes - SwellPath)\nAnyway, this guide will demonstrate a pretty simple DOM (Document Object Model) function with which to check whether a given element (e.g. {{element}}) is an ancestor of some other element (e.g. a top-level menu wrapper). The rule will look something like this:\n  As usual, there\u0026rsquo;s the easy method and then there\u0026rsquo;s the advanced method (which isn\u0026rsquo;t really that advanced at all).\nThe easy method The easy method is really just about checking the node relationship between a given element, such as {{element}}, and a specific parent element. This is useful if you only have one or two cases where this is an issue.\nThe DOM method we\u0026rsquo;ll be using is Node.contains.\nHere\u0026rsquo;s the Custom JavaScript Macro, which is the motor of the solution, in all its simplicity:\n{{node contains element}}\nfunction() { var controlElement = document.querySelector(\u0026#39;#parentId\u0026#39;); return controlElement \u0026amp;\u0026amp; controlElement !== {{element}} ? controlElement.contains({{element}}) : false; }  And then your firing rule will be something like:\n{{node contains element}} equals true\n{{event}} equals gtm.linkClick\nFeel free to use any other auto-event tracking type you wish.\nThis will only fire the tag if a click occurs on a link which is a direct ancestor of an element with ID parentId. A cousin or a long-lost blood brother will not do; the nodes must have a direct ancestral relationship.\nSo that was easy, right? What about when you have multiple ancestor-descendant-relationships you want to explore? You could create a rule and macro for each, but a far more robust way is to use a Lookup Table or a Custom JavaScript macro.\nThe advanced method The idea with the advanced method is to vary the the node against which you control the event element, depending on some parameter. In this particular example I use {{url path}} to determine which control node to choose, and a Custom JavaScript macro to return the new query selector.\nSo let\u0026rsquo;s say I have three page templates:\n  Home page (url path === \u0026lsquo;/'), where I want to track clicks on links in the main navigation (id === \u0026lsquo;#mainNav\u0026rsquo;)\n  Product pages (url path === \u0026lsquo;/products/*'), where I want to track clicks in any of the three Call-to-action page elements (class === \u0026lsquo;call-to-action\u0026rsquo;)\n  All other pages, where I want to track clicks on links in the footer (tag name === \u0026lsquo;footer\u0026rsquo;)\n  Now, to complement the Custom JavaScript macro above, I want the query selector to be dynamic, depending on which of the above conditions is true. So let\u0026rsquo;s modify the macro just a little bit:\nfunction() { var controlElement = document.querySelector({{get query selector}}); return controlElement \u0026amp;\u0026amp; controlElement !== {{element}} ? controlElement.contains({{element}}) : false; }  So it\u0026rsquo;s the same, but the document.querySelector() fetches the query string from a macro called {{get query selector}}.\nAnd what does this macro look like? Well, to satisfy the three requirements I listed above, this is what you\u0026rsquo;ll get:\nfunction() { var homeRegex = /^\\/$/, productRegex = /^\\/products\\//; if(homeRegex.test({{url path}})) { return \u0026#39;#mainNav\u0026#39;; } else if (productRegex.test({{url path}})) { return \u0026#39;.call-to-action\u0026#39;; } return \u0026#39;footer\u0026#39;; }  This returns a different string every time, depending on which condition is satisfied by the URL path of the document.\nYou can now create a unique tag for each of these three variations, still referencing the same macro in the rule, because it will return true/false depending on which condition is satisfied. If you want a more verbose check than just Boolean true/false, you can modify the original macro to return the query selector itself, after which you can modify the rule accordingly:\nfunction() { var controlElement = document.querySelector({{get query selector}}); return controlElement \u0026amp;\u0026amp; controlElement !== {{element}} \u0026amp;\u0026amp; controlElement.contains({{element}}) ? {{get query selector}} : false; }  This piece of code returns \u0026ldquo;#mainNav\u0026rdquo; if the click occurred on the home page navigation, \u0026lsquo;.call-to-action\u0026rsquo; if the click occurred on any of the three call-to-action elements on the home page, or \u0026lsquo;footer\u0026rsquo; for all other cases. Then, if you have a tag which should only fire on clicks on the product page calls-to-action, the rule would look like this:\n{{event equals gtm.linkClick}}\n{{node contains element}} equals .call-to-action\nAnd that\u0026rsquo;s it for the advanced method. Remember that one of the huge perks of using a tag management solution is the opportunity to consolidate your tags and make the whole setup so much leaner. Using Custom JavaScript macros for advanced lookups or the Lookup Table macro for simple value retrievals is usually the key to reducing the weight of your implementation.\nConclusions This was a very basic guide on a sweet little method of the DOM Node object not many seem to be aware of. The point here is that instead of building vast selector chains, where each link is weaker than the one before it, you can just use an all-encompassing ancestor lookup, which reduces the chance of error.\nIf you want to get real flashy, there\u0026rsquo;s always the Node.compareDocumentPosition() method, which returns a bitmask representing the relationship between two nodes. It doesn\u0026rsquo;t take just ancestry into account, it also looks for descendant relationships, which might be useful in some cases. However, it\u0026rsquo;s not in the scope of this guide, and I\u0026rsquo;m pretty sure that most of the problems can be solved with the ancestor lookup.\nJavaScript is fun! Google Tag Manager is fun-ner!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/custom-event-listeners-gtm/",
	"title": "Custom Event Listeners For GTM",
	"tags": ["Google Tag Manager", "Guide", "JavaScript", "listeners"],
	"description": "How to create custom event listeners in Google Tag Manager. This is arguably one of the best ways to enhance Google Tag Manager tracking on your website.",
	"content": "(UPDATE 1 Oct 2014 Due to a change in how macros work in Debug Mode, the {{generic event handler}} macro no longer works when testing in Debug Mode. That means that you\u0026rsquo;ll have to test your custom listener in a live container (I know, ouch!). If you want to test in Debug Mode, you\u0026rsquo;ll have to skip using the {{generic event handler}} as a macro, and instead copy the inner function into the Custom HTML Tag, give the function a name, and use that as the callback in addEventListener or attachEvent.\nAlso, to preserve the \u0026lsquo;gtm.\u0026rsquo; namespace for native GTM features only, I have changed the prefix of the custom event name from \u0026lsquo;gtm.\u0026rsquo; to \u0026lsquo;event.')\nWhen the good folks at Mountain View introduced auto-event tracking for Google Tag Manager, a collective sigh was heard around the world (I\u0026rsquo;m just slightly exaggerating).\nFinally, the true power of GTM was unleashed.\nWith auto-event tracking, one of the more difficult aspects of web analytics, tracking user interactions beyond the page load, was greatly simplified.\n  However, as a species we are in a perpetual state of dissatisfaction.\nWhen we got the Click, the Link Click and the Form Submit, we wanted more. So then we got the History listener. Again, we wanted more. And we got the Error listener. And then we wanted more again.\nTo satisfy this undying thirst for more listeners, the community has been very helpful. In fact, before you read on, I want you to familiarize yourself with Doug Hall\u0026rsquo;s excellent post on extending GTM\u0026rsquo;s auto-event listeners. The following guide will expand upon the ideas put forth by Doug, while striving to approach the elegance of his prose and wisdom.\nI\u0026rsquo;ve also written many posts on GTM\u0026rsquo;s listeners. I\u0026rsquo;ll link to them at the end of this article.\nWhat follows is a step-by-step guide to creating a generic listener for all the various events you want to capture that are not yet captured by GTM\u0026rsquo;s own listeners. Once GTM introduces a proprietary listener for whatever event you want to handle, it would be best to start using that.\nThe listener prototype The prototype of the custom event listener will require two components:\n  A Custom HTML Tag for each listener type you want to activate (e.g. change, blur, copy)\n  A generic Custom JavaScript Macro, which returns the handler function that pushes the event into the dataLayer\n  As you can see, it looks pretty simple. And it is. As long as you observe proper design patterns and best practices, these two components will get you far.\nThe generic event handler macro Let\u0026rsquo;s start with the macro, since it\u0026rsquo;s the truly generic component here.\nThe macro returns a function, which serves as the handler for the listener you\u0026rsquo;ll create in the next step. For this function to be as generic as possible, it will be agnostic as to what the event was. It will do this by accepting the Event object as its parameter, and parsing that for all the data it needs.\nShowing is easier than telling, so here\u0026rsquo;s the code for you to copy-paste:\nMacro Name: {{generic event handler}}\nMacro Type: Custom JavaScript\nfunction() { return function(e) { dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;event.\u0026#39;+e.type, \u0026#39;gtm.element\u0026#39;: e.target, \u0026#39;gtm.elementClasses\u0026#39;: e.target.className || \u0026#39;\u0026#39;, \u0026#39;gtm.elementId\u0026#39;: e.target.id || \u0026#39;\u0026#39;, \u0026#39;gtm.elementTarget\u0026#39;: e.target.target || \u0026#39;\u0026#39;, \u0026#39;gtm.elementUrl\u0026#39;: e.target.href || e.target.action || \u0026#39;\u0026#39;, \u0026#39;gtm.originalEvent\u0026#39;: e }); } }  This, in my opinion, is a pretty close emulation of how GTM\u0026rsquo;s listeners work. What\u0026rsquo;s important is that the object pushed into dataLayer does its best to mimic the design pattern of GTM\u0026rsquo;s own listeners.\nLet\u0026rsquo;s go over the code line-by-line.\nreturn function(e) { ... } is the wrapper. The point here is that this macro must return a function, since the listener we\u0026rsquo;ll create in the following chapter requires a function or object as a parameter. If the macro wouldn\u0026rsquo;t return a function but rather run the code itself, you\u0026rsquo;d be sending some weird empty events every time the listener tag is written to the page template.\ndataLayer.push({ ... }); is where the magic happens. Here the triggered event is pushed into dataLayer by observing the design patterns of GTM\u0026rsquo;s own listeners. Now, you can observe these patterns if you want, or you can use your own syntax. I\u0026rsquo;m a big fan of symmetry, which is why I prefer to follow GTM\u0026rsquo;s syntax. This might result in some conflicts if GTM introduces a similar event listener out-of-the-box, but in that case it would be a good idea to migrate to this proprietary listener anyway.\n'event': 'event.'+e.type, pushes a value into the \u0026lsquo;event\u0026rsquo; data layer variable. It takes the type property from the event object, which is a string representing what type of listener was fired, e.g. \u0026ldquo;change\u0026rdquo;, \u0026ldquo;blur\u0026rdquo;, \u0026ldquo;copy\u0026rdquo;. So if the event was of type change, this push would actually look like: 'event': 'event.change'. (By the way, remember E-Type?)\n'gtm.element': e.target, pushes the element the event occurred on into the dataLayer variable \u0026lsquo;gtm.element\u0026rsquo;. This is a design pattern, and if you look at your click listeners and link click listeners, a similar object is always found in those as well. The idea here is that you can then use your Data Layer Variable Macro to explore properties of this \u0026lsquo;gtm.element\u0026rsquo; object if you want to dig deeper into the DOM.\n'gtm.elementClasses': e.target.className || '', pushes the class of the event target into the variable \u0026lsquo;gtm.elementClasses\u0026rsquo;. If there is no class on the HTML object, an empty string is pushed instead.\n'gtm.elementId': e.target.id || '', pushes the ID of the event target into the variable \u0026lsquo;gtm.elementId\u0026rsquo;. If there is no ID on the HTML object, an empty string is pushed instead.\n'gtm.elementTarget': e.target.target || '', pushes the target of the event target (sounds weird) into the variable \u0026lsquo;gtm.elementTarget\u0026rsquo;. If there is no target attribute on the HTML object, an empty string is pushed instead.\n'gtm.elementUrl': e.target.href || e.target.action || '', pushes either the href or the action attribute of the event target into the variable \u0026lsquo;gtm.elementUrl\u0026rsquo;. If there are no such attributes on the HTML object, an empty string is pushed instead.\n'gtm.originalEvent': e is my own addition to this design pattern. Every now and then you might want to access the original event, exposed by the listeners. With GTM\u0026rsquo;s out-of-the-box listeners, this is currently not possible, since they only expose the e.target property of this event object. However, especially if you want to do stuff like identify clicks created by code and not by the user, access to the event object is a must. I hope this will be a standard feature in GTM\u0026rsquo;s own listeners as well.\nSo that\u0026rsquo;s what the macro looks like. It\u0026rsquo;s possible some of the patterns need more work, and nothing is stopping you from extending the number of variables that are pushed into the dataLayer. I consider these variables to be the minimum set you\u0026rsquo;ll need in order to provide enough information for your tags while still remaining economical and conscious of best practices.\nThe listener tag Now that we have our generic event handler, the next step is to create a tag which sets up the listener. The key here is recognizing just which listener you want to set up. Also, for symmetry\u0026rsquo;s sake I will only show how to prime the event listener on the document node, because that\u0026rsquo;s the generic way AND the way GTM\u0026rsquo;s own proprietary listeners work. If you want, you can attach the listeners to specific DOM nodes, which reduces the risk of propagation problems, but as a solution it won\u0026rsquo;t be as generic any more.\nFirst, here\u0026rsquo;s the listener code itself. Put this in a Custom HTML Tag, and add a firing rule which uses either {{event}} equals gtm.js if you\u0026rsquo;re attaching the listener on the document node, or {{event}} equals gtm.dom if you\u0026rsquo;re listening on specific HTML elements.\n\u0026lt;script\u0026gt; (function() { var eventType = \u0026#34;change\u0026#34;; // Modify this to reflect the event type you want to listen for  if (document.addEventListener) { document.addEventListener(eventType, {{generic event handler}}, false); } else if (document.attachEvent) { document.attachEvent(\u0026#39;on\u0026#39; + eventType, {{generic event handler}}); } })(); \u0026lt;/script\u0026gt; On the first line, you specify just what type of event you want to listen for. For the full list of supported types, follow this link. Remember, you can also dispatch and listen to your own custom events, which makes this solution even more flexible. Here\u0026rsquo;s MDN\u0026rsquo;s excellent guide on creating and triggering events.\nAnyway, here\u0026rsquo;s a list of some of the most popular event types:\n  beforeunload - Fire a listener when the window, the document, and all resources are about to be unloaded (e.g. when someone is closing the browser window).\n  blur - An element has lost focus (e.g. the user has left a form field). Note, this doesn\u0026rsquo;t bubble by default, meaning a listener on the document node won\u0026rsquo;t be able to catch it. To activate event delegation, you\u0026rsquo;ll need to set the last parameter in the document.addEventListener() call to true instead of false.\n  change - The value of an element changes between receiving and losing focus (e.g. the user enters a form field, types something in, and leaves the field).\n  click - A click is registered on an element (use GTM\u0026rsquo;s Click Listener instead).\n  contextmenu - The right mouse button is clicked.\n  copy - Text is copied to the clipboard.\n  cut - Text is cut to the clipboard.\n  dblclick - A double-click is registered on an element.\n  focus - An element has received focus (e.g. the user has left a form field). Note, this doesn\u0026rsquo;t bubble by default, meaning a listener on the document node won\u0026rsquo;t be able to catch it. To activate event delegation, you\u0026rsquo;ll need to set the last parameter in the document.addEventListener() call to true instead of false.\n  keydown - A key is pressed down.\n  keyup - A pressed down key is released.\n  mousedown - The mouse button is pressed down.\n  mouseenter - The mouse pointer is moved over the element where the listener is attached. Won\u0026rsquo;t really work if the listener is on the document node.\n  mouseleave - The mouse pointer is moved off the element where the listener is attached. Won\u0026rsquo;t really work if the listener is on the document node.\n  mouseout - The mouse pointer is moved off the element where the listener is attached or one of its children.\n  mouseover - The mouse pointer is moved over the element where the listener is attached or one of its children.\n  mouseup - The pressed down mouse button is released.\n  orientationchange - The orientation (portrait / landscape) of the screen changes.\n  reset - A form is reset.\n  scroll - A document view or element is scrolled.\n  submit - A form submit is registered (use GTM\u0026rsquo;s Form Submit Listener instead).\n  When the Custom HTML Tag is written on the page, it attaches the listener of your choice on the document node. The event handler is the generic event handler function you created in the previous chapter. Then, when the event occurs, the function is executed with the event object as its parameter. This event object is then parsed and pushed into dataLayer with a bunch of properties that you can access with the Data Layer Variable Macro.\nExample with the Change Listener Here\u0026rsquo;s a simple example. I have some form fields on a web page. Whenever a value of a form field changes, i.e. a user writes / edits / deletes text in it, I want to push a virtual pageview with the URL path /form/(field-name)-(field-value). So if the form field\u0026rsquo;s name is \u0026ldquo;search\u0026rdquo; and value is \u0026ldquo;GTM\u0026rdquo;, I want to send the virtual pageview with the path /form/search-GTM.\nSo let\u0026rsquo;s get started. I have my {{generic event handler}} macro which I created earlier, and I have my Change Listener Tag firing on {{event}} equals gtm.js as you can see:\n  Next, I\u0026rsquo;ll need to create two new Data Layer Variable Macros to capture the field name and field value, respectively. First, here\u0026rsquo;s the Data Layer Variable Macro {{field name}}:\n  As you can see, I use \u0026ldquo;(not set)\u0026rdquo; as the placeholder if the field has no name attribute.\nAnd here\u0026rsquo;s the Data Layer Variable Macro {{field value}} for the field value:\n  Again, I use \u0026ldquo;(not set)\u0026rdquo; if the field has no value.\nFinally, I\u0026rsquo;ll need my virtual pageview tag. It\u0026rsquo;s just a normal Universal Analytics pageview tag, but it uses the Document Path field. Also, the firing rule for this tag needs to be {{event}} equals event.change, so that the tag fires only when a \u0026lsquo;change\u0026rsquo; event is registered by the custom listener.\n  I concatenate the string \u0026ldquo;/form/{{field name}}-{{field value}}\u0026rdquo;, since the macros are resolved at runtime, and I\u0026rsquo;ll end up with a nice, clean URL path.\nI manage to test this live by typing text into the search field and leaving the field. The event.change listener fires, and my debug panel shows that the Document Path has been processed correctly:\n  Naturally, don\u0026rsquo;t forget to check GA\u0026rsquo;s Real Time report to verify the data is flowing in correctly.\nConclusions This post was about creating a generic event handler for all your custom listener needs. I urge you to explore beyond the out-of-the-box setups that GTM provides. However, once there\u0026rsquo;s overlap with GTM\u0026rsquo;s features, I strongly suggest you leverage the tag manager\u0026rsquo;s own listeners, since that will ensure that they\u0026rsquo;ll stay up-to-date with possible changes under the hood. I also recommend that you try to observe best practices, and that you emulate GTM\u0026rsquo;s design patterns to your best ability.\nLike I said, I\u0026rsquo;ve written a lot about GTM\u0026rsquo;s listeners in various posts. Here are some guides you might enjoy reading as well:\nWhy Don\u0026rsquo;t My GTM Listeners Work? This is still one of the most asked questions I get. Please, read this post. You\u0026rsquo;ll learn about event delegation, and why so often interfering code prevents events from bubbling up to GTM\u0026rsquo;s listeners. Here\u0026rsquo;s a more recent rant on the topic: My Google+ rant.\nGoogle Tag Manager: The DOM Listener You can use MutationObserver to listen for changes on the page that occur without an actual event firing or page refreshing.\nGoogle Tag Manager: The History Listener A review of GTM\u0026rsquo;s History Listener.\nAlso, I allude to listeners in most of my Google Tag Manager posts, so be sure to read the rest of them if you have time.\nThis was a long and rather advanced guide. Go and eat an ice cream, you\u0026rsquo;ve earned it!\n"
},
{
	"uri": "https://www.simoahava.com/tags/listeners/",
	"title": "listeners",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/best-practices/",
	"title": "best practices",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/javascript-101-gtm-part-2/",
	"title": "JavaScript 101 For GTM: Part 2",
	"tags": ["best practices", "google analytics", "Google Tag Manager", "Guide", "JavaScript"],
	"description": "Second part of a two-part post which introduces core JavaScript concepts for Google Tag Manager.",
	"content": "It\u0026rsquo;s been an awesome summer, with temperatures soaring in the global warming range throughout our northern country. The heat has given me ample reason to not be near a computer, but now it\u0026rsquo;s time to mine some JavaScript wisdom again. Here\u0026rsquo;s the second part of my JavaScript for Google Tag Manager series. The first part focused on GTM specific tips and tricks, and I hope that while reading it, you were treated to another grand example of the flexibility of this wonderful tool.\nThis second part was supposed to be more about general JavaScript practices, but I saw myself going back to the DOM with every step. So the theme here is the elusive Document Object Model. It\u0026rsquo;s a defining quality of JavaScript in web development, and understanding its mechanics is key to understanding how tag management solutions work as well.\nHere\u0026rsquo;s the list of topics:\n  The Document Object Model (DOM)\n  The DOM API\n  JavaScript injection + GTM\n  Handling false\n  Here we go again. I implore you to take a look at the list of resources I linked to in the previous post. They should serve you really well in your journey towards JavaScript enlightenment.\n1. The Document Object Model (DOM) Here\u0026rsquo;s a very, very simplified account of how a browser works:\n  The browser sends a request to the web server to retrieve a web document\n  This document is just an annotated source code file, so it needs to be parsed into a format the browser can interpret\n  The document is parsed into a parsed node tree, whose syntax is very standardized\n  The node tree is then rendered as a web page, which can be viewed in the browser window\n  The node tree is actually the Document Object Model (DOM), which also provides an interface for interacting with elements in the tree\n  In short, what you see when looking at the page source of a web page in your browser is just that: the source code of the page. It isn\u0026rsquo;t the final product, since the source code is parsed into a format which can be better understood by browsers.\n  If you\u0026rsquo;ve messed around with a tag management system or with JavaScript, chances are you already know all this. The DOM is both a representation of on-page elements and an interface (more on this in the next chapter) which describes a standard for interacting with these elements. It\u0026rsquo;s called the Document Object Model because the HTML elements you lovingly create in your source code are represented as objects, or nodes, or branches of a tree, in a top-level object called document. So when you tag a section of text as a DIV element in HTML, you are actually creating a new node with tag name DIV into the document hierarchy as a new element that can be interacted with.\nSo the DOM and your HTML document are related, in that they share the same elements, but they are not the same thing. The DOM is the browser\u0026rsquo;s interpretation of the HTML document. The fact that it\u0026rsquo;s standardized makes sure that all your sloppiness in HTML markup is glossed over in a perfectly structured DOM.\nIf you want to take a look at what the document object looks like for any given page, just open a JavaScript console, type document and press enter.\n  Almost all JavaScript that interacts with a page (such as analytics tags) utilizes the DOM. This is because representing a document with a collection of objects allows for scripting languages such as JavaScript to mimic the syntax of more traditional, object-oriented programming languages. Also, using a standardized set of objects and functions in the DOM normalizes the potentially chaotic and horrible markup that many, many web pages out there display.\nThe thing about the DOM is that if you understand its basic functions and if you read up on the standard, you should be inspired to create better markup as well. If the ID attribute was just a CSS selector to you before, after realizing what a crucial part it plays in the unique identification of a single DOM node, you\u0026rsquo;ll be more inclined (hopefully) to utilize it in the future in all relevant markup situations.\nThe DOM is a huge standard and there\u0026rsquo;s lots to learn. The best way to learn is to inspect objects in the model by utilizing the JavaScript console. Reading up on GTM articles is a good way to brush up your skills as well, since almost all GTM customizations have to do with manipulating the DOM in some way.\nThere\u0026rsquo;s also the Browser Object Model (BOM), which is a de facto standard for interacting with the browser window. The BOM houses the window object, which, in turn, contains all the global variables and functions of the page AND the document object. So when you type document in the JavaScript console, you are actually accessing the window.document object.\nConfusing? Perhaps, but in the end it\u0026rsquo;s all quite logical. The DOM is a necessary interface between the client and the HTML document, because so much of today\u0026rsquo;s web development revolves around dynamic and often subtle modification of the web pages. That\u0026rsquo;s how you get your animations, transitions, event handlers and so forth to work.\nFURTHER READING:\n  Document Object Model description (MDN)\n  W3Schools HTML DOM guide\n  JavaScript Window - the Browser Object Model (W3Schools)\n  2. The DOM API The DOM API is actually the DOM itself (remember, it\u0026rsquo;s a standard for interacting with the document), but for clarity\u0026rsquo;s sake I use the term API here (application programming interface). Interacting with the document is done via the methods and objects exposed by the API.\nWhen using JavaScript (as most often is the case) to interact with the DOM, all calls should naturally be in the context of a \u0026lt;script\u0026gt; element. This means that you either fetch the JavaScript in the declaration itself, or you add the code inline. Certain attributes such as \u0026ldquo;onclick\u0026rdquo; allow you to add JavaScript directly to the context of a HTML element without using \u0026lt;script\u0026gt;, but the most common instance is having the \u0026lt;script\u0026gt; tag in place.\nThere are many functions and objects you can access via the DOM, so I won\u0026rsquo;t go into too much detail here. However, I will add an example of what DOM manipulation means in terms of HTML vs. JavaScript. Here\u0026rsquo;s a typical HTML passage in a normal web page:\n\u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;header\u0026#34;\u0026gt;Why Four Strings Is Enough\u0026lt;/span\u0026gt; \u0026lt;p class=\u0026#34;paragraph\u0026#34;\u0026gt;Many dismiss the ukulele because it only has four strings. However, it has more strings than a typical balalaika.\u0026lt;/p\u0026gt; \u0026lt;div id=\u0026#34;request-access\u0026#34;\u0026gt; \u0026lt;p class=\u0026#34;paragraph\u0026#34;\u0026gt;Request access to ukulele vault by sending mail to \u0026lt;span class=\u0026#34;email\u0026#34;\u0026gt;uku@lele.com\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; So there are some unique elements where I utilize the ID attribute, and some elements which are reused, represented by the CLASS attribute. It\u0026rsquo;s important to remember that if you use ID, you should only ever have one such element on the page. Otherwise you will make life difficult for anyone wishing to manipulate the element using the DOM API (plus it won\u0026rsquo;t be valid HTML).\nHere are some examples of JavaScript functions I can use to interact with the document:\nvar content = document.getElementById(\u0026#34;content\u0026#34;);  This stores the whole #content node with all its children into the JavaScript variable content. I can then use other functions of the DOM to manipulate the node further.\ncontent.setAttribute(\u0026#34;id\u0026#34;, \u0026#34;main-content\u0026#34;);  Changes the ID attribute of the #content DIV to #main-content. Here I utilize the JavaScript object where I stored the entire #content node just before.\nvar classArray = document.getElementsByClassName(\u0026#34;paragraph\u0026#34;);  Stores all elements with CLASS .paragraph in an Array object referred to as classArray.\nconsole.log(classArray[0].innerHTML);  Writes the entire HTML content of the first P element with class .paragraph into the console.\nalert(document.getElementById(\u0026#34;request-access\u0026#34;).children.length);  Pops up a dialog box with the number of child nodes for the HTML element with ID #request-access. In this case, the alert will contain the number 1.\nThere are many, many ways to interact with the document, its objects and functions using the interface of the DOM. You should be pretty comfortable with these especially if you want to do some advanced tag management and web analytics tweaking.\nFURTHER READING:\n Functions and properties of individual elements in the document (MDN)  3. JavaScript injection + GTM Now that you understand perfectly how the DOM works, you\u0026rsquo;re ready for the next big thing: code injection.\nThe whole premise of a JavaScript tag management system revolves around code injection. That\u0026rsquo;s why you\u0026rsquo;re fine with just adding the container snippet into your source code when installing GTM. By adding this seemingly harmless piece of code into your source code, you\u0026rsquo;re actually giving permission to the gtm.js library to inject more and more code, elements, and functions into your DOM.\nAnd what is this injected code? Your tags and macros, of course!\nLet\u0026rsquo;s see how injection works with a simple example of adding a new \u0026lt;script\u0026gt; node into the HEAD of the document:\nvar s = document.createElement(\u0026#34;script\u0026#34;); s.innerHTML = \u0026#34;alert(\u0026#39;Injection successful\u0026#39;);\u0026#34;; document.head.appendChild(s);    Line 1: A new SCRIPT element is created, but it isn\u0026rsquo;t attached anywhere yet\n  Line 2: Some code is added into the content of the new tag\n  Line 3: The new element is added as the last child of the document HEAD\n  If you copy-paste this code into a JavaScript console, you should see the pop-up dialog with \u0026ldquo;Injection successful\u0026rdquo;.\nIn this code you utilize the DOM with createElement() and appendChild() functions, the innerHTML property, and the document and document.head elements in the tree. If you now look at your elements (not the source code) by opening the developer tools of your favorite browser, you should see this new element as the last child of the HEAD.\n  All the Custom HTML tags you create via GTM are added to you page in a similar fashion. When you visit a page where such a tag is set to fire, the script is injected the moment the firing rule triggers. Any code in the tag is run there and then.\nThis also highlights the dangers of using a tag management solution to fix / hack errors in your web page. Code injection is brutal in the sense that any code within is run verbatim. There are no default checks in place to prevent your code from messing with global objects or your carefully crafted content. Since the DOM API opens all elements in the tree for manipulation, it\u0026rsquo;s very easy to create AND destroy something beautiful.\nSo be careful.\nFURTHER READING:\n  The Container Snippet: GTM Secrets Revealed\n  document.createElement() with examples (MDN)\n  4. Handling false This was all too easy so far, so I\u0026rsquo;ll leave you with something truly mind-boggling.\nI\u0026rsquo;ve written about this phenomenon before, and I still think its key to understanding how GTM and JavaScript work together.\nFirst of all, I apologize in advance. JavaScript\u0026rsquo;s treatment of \u0026ldquo;falsy\u0026rdquo; values such as undefined, null, and false is notoriously difficult to understand. Not only is it difficult to understand, it\u0026rsquo;s even more difficult to teach in an understandable manner.\nWhen talking about \u0026ldquo;truthy\u0026rdquo; and \u0026ldquo;falsy\u0026rdquo; values, we\u0026rsquo;re not talking strictly about values which are either true or false. Rather, we mean values which are coerced to either true or false in a Boolean context. Boolean is a type which holds either true or false as its value, and a context is Boolean if some statement hinges on whether the expression evaluates to either true or false.\nAn if-clause is probably the most used Boolean context. The following piece of code is a prototypical example of type coercion:\nif(t) { ...do something if t evaluates to true... } else if (!t) { ...do something if t evaluates to false... }  The if-clause is run if t is true, but it doesn\u0026rsquo;t mean the value of t needs to be true. Rather, the value of t needs to be \u0026ldquo;truthy\u0026rdquo;. The else-clause is run if the value is not \u0026ldquo;truthy\u0026rdquo; but \u0026ldquo;falsy\u0026rdquo; (oh my those are stupid terms).\nSo in this context, t is coerced to Boolean values. It\u0026rsquo;s easier to go through the values which evaluate to false, since there are only six of them.\nundefined\nnull\nfalse\n0\n\u0026quot;\u0026rdquo;\nNaN\nSo if a variable holds any of these values, it would evaluate to false in a Boolean context.\nStill with me? Good. For GTM, we are particularly interested in the values undefined and null.\nundefined is a value (and a type, actually) reserved for variables which have either been declared but have no type assigned to them (via value assignment), OR have not been declared at all. This is very relevant for JavaScript, since often and especially with dynamic web pages many variables receive a value and type only after a specific user interaction (such as a click). If a function is executed before such an interaction and without the check for undefined, the function execution will fail in an ugly JS error.\nThe two simplest ways to safeguard your variable execution are:\nif(!t) { alert(\u0026#34;t is falsy and perhaps undefined!\u0026#34;); } if(typeof(t) === undefined) { alert(\u0026#34;t is undefined or has not been declared!\u0026#34;); }  The difference is that using the logical not operator (!) is recommended when you know the variable has been declared but are not certain if it has a value assigned yet, whereas the typeof() operator should be used when you are not certain if the variable has been declared at all.\nThe logical not operator is literally \u0026ldquo;not\u0026rdquo;, so if(!t) { ... } is read as if (t is false is true) { \u0026hellip;run this code\u0026hellip; }. Thus, the if-clause is only run if t is one of the \u0026ldquo;falsy\u0026rdquo; values listed in the beginning of this chapter.\nI can see your head spinning - again, I apologize!\nThe thing is, you shouldn\u0026rsquo;t really use typeof(var)===undefined, since you should always be able to see whether a variable has been declared either by looking at the immediate local function context or by looking for the variable in the global object.\nYes, let me explain this as well.\nJavaScript has static scope. This means that variables are either always global or scoped to the function they are wrapped in. If a variable is scoped to a function and if you\u0026rsquo;ve observed best practices in your programming, you should quickly see whether or not a variable has been declared by the time your code is run.\nvar globalVar; // global variable function t() { var localVar; // local variable  secondVar; // global but bad code } alert(localVar); // error since trying to access local variable outside function  Another thing to note is that if a variable is scoped globally, it will be a property of the global window object. Thus, you can use the logical not operator to check for its absence:\nif(!window.t) { something... } // run something if t is not a global variable ... if(!t) { something... } // referenceError if t is not a global variable  When working with GTM and GA, a very important thing to understand is how the DOM API works with elements that are not found on the page. You see, a function such as getElementById() or getElementsByTagName() will always return an object. An object, if you remember, was not among the \u0026ldquo;falsy\u0026rdquo; values listed in the beginning. However, this object will have the special value null if the actual HTML element is not found on the page.\nIf you stretch your memory again, null was among the values that are \u0026ldquo;falsy\u0026rdquo;. Thus:\nvar t = document.getElementById(\u0026#34;test\u0026#34;), // \u0026#34;test\u0026#34; exists, will return the DOM element  y = document.getElementById(\u0026#34;testy\u0026#34;); // \u0026#34;testy\u0026#34; does not exist, will return object with value null if(typeof(\u0026#34;t\u0026#34;) === undefined) { // does nothing  alert(\u0026#34;t is undefined\u0026#34;); } if(!t) { // does nothing  alert(\u0026#34;t resolves to false\u0026#34;); } if(t===null) { // does nothing  alert(\u0026#34;t is null\u0026#34;); } if(typeof(\u0026#34;y\u0026#34;) === undefined) { // does nothing  alert(\u0026#34;y is undefined\u0026#34;); } if(!y) { // will alert the following message  alert(\u0026#34;y resolves to false\u0026#34;); } if(y===null) { // will alert the following message  alert(\u0026#34;y is null\u0026#34;); }  So the tips of this chapter are:\n**1.**Use the global window object and the logical not operator as a fallback in case a variable hasn\u0026rsquo;t been declared in global scope.\n2. Read through your code to see if a variable has been declared in local (function) scope OR if you\u0026rsquo;re lazy, use the fallback if(typeof(variable)===undefined) { do something; }.\n3. Either use the logical not operator with a DOM element, e.g. if(!someDOMElement), OR check for null, e.g. if(someDOMElement===null), to fall back in case a DOM element you are referencing is not found in the DOM.\nYou don\u0026rsquo;t want to write a long block of code only to see it miserably explode in JavaScript error mayhem thanks to race conditions or faulty declarations.\nFURTHER READING:\n  JavaScript logical operators (MDN)\n  JavaScript undefined (W3Schools)\n  Truthy and falsy values in JavaScript\n  5. Conclusions I\u0026rsquo;m a huge, HUGE fan of JavaScript. I love it so much I\u0026rsquo;m seriously considering naming my first-born with initials that spell JS (Jebediah Simo, for example). I also think that anyone wanting to extend the capabilities of Google Analytics, and DEFINITELY anyone working with Google Tag Manager, should understand at least the basics.\nThere\u0026rsquo;s a lot to learn, and the splintered mess of cross-browser issues doesn\u0026rsquo;t make it any easier. However, lots of ridiculously smart engineers are working on the standard, so if things go smoothly, we should see some pretty cool and facilitating additions in the next release of the language.\nRemember, an excellent place for advice on JavaScript, especially related to GA and GTM, is Google+. Be sure to follow the communities, and drop a line if you need any assistance with your code.\nGoogle Analytics community in Google+\nGoogle Tag Manager community in Google+\n"
},
{
	"uri": "https://www.simoahava.com/tags/extension/",
	"title": "extension",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/internalize-google-analytics-v1-0/",
	"title": "Internalize for Google Analytics v1.0",
	"tags": ["chrome", "extension", "google analytics", "JavaScript"],
	"description": "Introducing the Internalize Google Chrome extension. You can use it to set your traffic as internal, which is then useful for filters in Google Analytics.",
	"content": "I created a new Chrome Extension: Internalize for Google Analytics. This is its very first version. It only works on websites with Universal Analytics.\nClick here to download Internalize for Google Analytics v1.0\nThe idea is that with the extension you can push a custom dimension value to your currently active session. You can then use a profile filter in GA to block traffic with this custom dimension value. It\u0026rsquo;s useful when blocking internal traffic with more traditional means (IP address or various GTM workarounds) won\u0026rsquo;t work.\nHow to use it   Create new session-scoped custom dimension in GA property settings\n  Make note of the index number of this new dimension\n  Go to the website whose traffic is being tracked to the property\n  Click the extension to open a pop-up\n  Add the index number to its designated place in the pop-up\n  Click the button \u0026ldquo;Enable Internalize\u0026rdquo;\n  This sends a non-interaction event to the active session. The Event Category is \u0026ldquo;Internalize\u0026rdquo; and the Event Action is \u0026ldquo;Enable\u0026rdquo;.\n  A custom dimension value \u0026ldquo;true\u0026rdquo; is sent with this event, and it is this value that you should filter in your profile settings.\n  The screenshot above assumes that the name of the new custom dimension is \u0026ldquo;Internal\u0026rdquo;.\nHow it works The extension first checks if Universal Analytics is implemented on the page. If it is, the extension icon turns blue to indicate that it\u0026rsquo;s ready to operate.\nOnce you add the index number and click \u0026ldquo;Enable Internalize\u0026rdquo;, the XMLHttpRequest() object is used to send a data package to Google Analytics servers. This payload contains the following:\n  The tracker ID of the first \u0026lsquo;ga\u0026rsquo; object found on page\n  The client ID of the first \u0026lsquo;ga\u0026rsquo; object found on page\n  The event data with the custom dimension\n  A cookie with a 30 minute expiration is written as well to prevent multiple pushes in a short period of time.\nUpcoming features There are some caveats right now. First of all, you can send the event to ANY Universal Analytics website, because it accesses the \u0026lsquo;ga\u0026rsquo; object found on the page. This is obviously a problem, and even though there\u0026rsquo;s not too much damage you can cause with a clickable browser extension (which sends non-interaction data), it\u0026rsquo;s still annoying to see extra hits on your site.\nAlso, it only works with the first \u0026lsquo;ga\u0026rsquo; object found on the page, so if you have multiple Universal Analytics properties tracked on a single page, this will only work on one of them.\nSo here\u0026rsquo;s a feature list I\u0026rsquo;ve drawn up for the near future:\n  Add an Options panel\n  Choose domains for which the extension will only work (now it works for all)\n  Choose a tracker ID for which the extension will only work (it will look for this tracker ID from the \u0026lsquo;ga\u0026rsquo; object)\n  Choose whether just a cookie is set or whether the Measurement Protocol is used as well\n  Add support for multiple tracker IDs\n  Working with internal traffic is a pain, and I hope this extension helps some of you with unwanted hits.\nClick here to download Internalize for Google Analytics v1.0\n"
},
{
	"uri": "https://www.simoahava.com/analytics/fix-ga-site-search-google-tag-manager/",
	"title": "Fix GA Site Search With Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "JavaScript", "site search"],
	"description": "How to fix Google Analytics site search tracking on your website, if it has features that make it difficult to track internal site search via Google Tag Manager.",
	"content": "Analyzing what people write in your site search field is pretty much one of the smartest things you can do for your website tracking. If certain terms pop up over and over again in internal search reports, it means that your site is not providing the answers people are looking for, meaning you have an excellent opportunity to provide supply for the demand!\n  However, not all site search applications are trackable out-of-the-box. In this post, I go over three scenarios, and I provide three solutions for tackling internal site search problems in Google Tag Manager.\n1) No query parameter in URL\nGoogle Analytics\u0026rsquo; site search settings require a query parameter in the URL, e.g. https://www.simoahava.com/search?q=analytics. Often, however, the site search tool does not apply a query parameter to the URL. Rather, the keyword might be as a typical URL folder, like https://www.simoahava.com/search/analytics.\nThis is pretty commonplace. You might have seen this phenomenon in Drupal websites. It\u0026rsquo;s easy to fix with a URL macro and the page field in your pageview tag.\n2) No search term in URL\nThis is a bit more difficult, since you can\u0026rsquo;t use the URL anymore. This means that the URL stays the same (or has some generic search path), and you need to look into on-page elements for information on what the user searched for. Some DOM scraping and the page field should do the trick.\n3) Nothing on the page to use\nThis is the most difficult scenario to handle, since there\u0026rsquo;s nothing on the page for you to latch on to. Perhaps the search term isn\u0026rsquo;t iterated (stupid), or it\u0026rsquo;s not contained in a DOM element (stupid, stupid). This time, we\u0026rsquo;ll try to get the value of the search field in the form submit event, and pass that as a virtual pageview.\n1) No query parameter in URL   I see this pretty often. You have a search tool but it doesn\u0026rsquo;t utilize a query parameter for the search term. Rather, it displays the keyword in a virtual folder in the URI. Nothing wrong with that, though every CMS should let you choose how the search terms are displayed in the URL.\nThis is what you\u0026rsquo;re going to do:\n  In your ordinary pageview tag, you click open the Fields to set setting, and you add a new field with page as the field name. Field value would be the reference to a Custom JavaScript Macro {{search path}}.\n  This macro will look for the search term in the URL\n  If it finds a search term, it will return the modified document path with the search term as a query parameter\n  If no search term is found, the macro returns false, meaning the document path will not be modified, and the pageview will fire normally\n  That\u0026rsquo;s the beauty of GTM, again. Since the macro returns false for non-search pageviews, you\u0026rsquo;ll just need the one, single, normal, ordinary pageview tag to cover all the searches as well. No need for extra tags, woohoo!\nSo, let\u0026rsquo;s say that the site search term is displayed like it\u0026rsquo;s displayed on www.drupal.org: /search/site/keyword. The {{search path}} macro should first check if the URI starts with /search/site/, and if it does, it should return whatever comes after, properly modified into a proper path.\nHere\u0026rsquo;s the code for the {{search path}} macro:\nfunction() { var regex = /^\\/search\\/site\\/(.*)/; if(regex.test({{url path}})) { return \u0026#34;/search/site?q=\u0026#34; + regex.exec({{url path}})[1]; } return; }  The regular expression looks at the {{url path}} macro (which stores the current URI of the page), and if the path starts with /search/site/, it returns a modified version of the URI with the search term appended as the value of the query parameter q. Change this parameter to reflect whatever you want to have in your Google Analytics view\u0026rsquo;s site search settings.\nFinally, just edit your pageview tag, add a new field to Fields to set, set page as the field name, and add your macro as its value. That\u0026rsquo;s all there is to it!\n  Don\u0026rsquo;t forget to do some Preview \u0026amp; Debugging, and also check with a debugger like WASP to see if the path gets sent correctly for search terms (and correctly for all other pages as well!).\n  2) No search term in URL OK, this one is a bit trickier. In this case, there\u0026rsquo;s no search term in the URL for you to use. This solution requires that the search term is explicitly presented in an HTML element we can grab onto. The best way is to have it in a span or div with a unique ID attribute. Since it\u0026rsquo;s the best way, it\u0026rsquo;s the one I\u0026rsquo;ll be using in this example, with a \u0026lt;span\u0026gt; element as the wrapper. Feel free to get creative with the DOM if your search term is hidden deeper in the page.\n  For this to work, this is what happens:\n  In your ordinary pageview tag, you click open the Fields to set setting, and you add a new field with page as the field name. Field value would be the reference to a Custom JavaScript Macro {{search path}}\n  This macro will look for the search term in the HTML element with the ID you specify, using a new DOM Element Macro {{search term}}\n  If it finds a search term, the {{search path}} macro will return the modified document path with the search term as a query parameter\n  If no search term is found, the macro returns false, meaning the Document Path will not be modified, and the pageview will fire normally\n  First, let\u0026rsquo;s create the DOM Element Macro {{search term}}.\n  Replace the Element ID field with whatever your page is using.\nNext, we\u0026rsquo;ll need to edit the Custom JavaScript Macro {{search path}}:\nfunction() { if({{search term}} \u0026amp;\u0026amp; {{search term}} !== \u0026#39;null\u0026#39;) { return \u0026#34;/search/site?q=\u0026#34; + {{search term}}; } return; }  So first we check if an element with the ID you specify exists. If it does, the macro returns \u0026quot;/search/site?q=\u0026lt;keyword\u0026gt;\u0026quot; as the document path, same as in the previous example. If no such element is found, the macro returns false, and the original document path (i.e. the URI of the page the visitor is on) is sent.\nThe final step is the same as before: edit your pageview tag, add a new field to the Fields to set, set the field name to page, and add this {{search path}} macro as the field value.\nOh, and don\u0026rsquo;t forget to preview, debug, and test, test, test.\n3) Nothing on the page to use This is bleak, but it happens. There\u0026rsquo;s absolutely nothing in the document object model that you can use to grab the search term. The URL is blank, there\u0026rsquo;s no search term printed on the page, no fairies whispering in your ear, NOTHING. I know, it sucks. Fire your developer. Just kidding. No, seriously, fire them. Yeah, I\u0026rsquo;m just kidding.\nI\u0026rsquo;ve chosen to fix this using form tracking. It basically requires that your search field has some unique identifier you can grab hold of in the submit event. I hope you have a proper ID for the field (or at least a CLASS or NAME!). I\u0026rsquo;m using ID here, again, but feel free to modify the script to get the value of the page.\nThe thing is, you\u0026rsquo;ll need to send a virtual pageview with the form submission, and it will have the search term in the Document Path as in the previous examples. However, if you send a new pageview with the search, you\u0026rsquo;ll be double-counting your search views, since you\u0026rsquo;ll be sending a pageview for when the search page loads as well! You don\u0026rsquo;t want that. That\u0026rsquo;s why this case is a bit more complex, as you\u0026rsquo;ll be using a browser cookie to carry the search term into the search page pageview, so you can then use that as the document path.\nHere\u0026rsquo;s what\u0026rsquo;s happening:\n  Have a Form Submit Listener tag firing on all pages\n  Create a new Custom HTML Tag which fires upon {{event}} equals gtm.formSubmit, i.e. when a form is submitted\n  In this tag verify that a search was made, and retrieve the search term using a DOM Element Macro {{search field value}}\n  In the tag, save this search term in a new browser cookie named \u0026ldquo;keyword\u0026rdquo;\n  Create a new 1st Party Cookie Macro {{cookie search term}}, which retrieves this cookie value\n  In the {{search path}} Custom JavaScript Macro, check if cookie exists; if it does, use that as the virtual page path\n  Utilize {{search path}} as the value of a new field in the Fields to set settings of the Tag. Set the field name to page and the macro as the field value.\n  Create a new Custom JavaScript Macro {{searchCallback}}, which deletes the cookie in the callback function of the pageview tag\n  PHEW!\nFirst, the Form Submit Listener. This is easy, just create a new tag of type Form Submit Listener, and have it fire on all pages.\n  After that, create the DOM Element Macro {{search field value}}. Be sure to change the Element ID setting to the ID of the form field your webpage uses in site search.\n  Next, create a new Custom HTML Tag which fires upon {{event}} equals gtm.formSubmit. Have the following code within:\n\u0026lt;script\u0026gt; if({{search field value}} \u0026amp;\u0026amp; {{search field value}} !== \u0026#39;null\u0026#39;) { document.cookie = \u0026#34;keyword=\u0026#34;+{{search field value}}+\u0026#34;;path=/\u0026#34;; } \u0026lt;/script\u0026gt; Here we look for an HTML element with the ID \u0026ldquo;search-field\u0026rdquo;, and with a VALUE attribute. If this is found, then the content of this attribute is set into a new cookie named \u0026ldquo;keyword\u0026rdquo;. This is, in fact, whatever was typed into the search field at the time of form submission.\n(NOTE! If there are multiple forms on a page, you might want to enhance this tag with a check that it was actually the search form that was submitted. It\u0026rsquo;s possible that another form is submitted with text in the search field, and this script would then falsely interpret this event as a submission of the site search form.)\nNext, create a new 1st Party Cookie Macro, which retrieves the value from the \u0026ldquo;keyword\u0026rdquo; cookie. Call this {{cookie search term}}.\n  Next, modify the Custom JavaScript Macro {{search path}} to look like this:\nfunction() { if({{cookie search term}}) { return \u0026#34;/search/site?q=\u0026#34; + {{cookie search term}}; } return; }  So, first we check if the cookie exists. If it does, then its value is returned as a properly formatted Document Path for the pageview tag.\nFinally, create a new Custom JavaScript Macro {{searchCallback}}, which looks like this:\nfunction() { if({{cookie search term}}) { return function() { document.cookie = \u0026#34;keyword=;path=/;expires=Thu, 01-Jan-70 00:00:01 GMT\u0026#34;; } } return; }  This looks for the \u0026ldquo;keyword\u0026rdquo; cookie, and if one is found, then the cookie is deleted. This is extremely important, since you don\u0026rsquo;t want to keep on sending the virtual pageview with every single page load! Cookies are deleted by setting their expiration date to the past.\nPut these all together in your pageview tag, utilizing the Fields to set with both hitCallback and page populated accordingly.\n  Why have the cookie deletion in the hitCallback and not in the macro which returns the document path? Well, two reasons:\n  A macro shouldn\u0026rsquo;t be used to set anything. The callback macro is obviously an exception, since it RETURNS a function which sets something.\n  There\u0026rsquo;s some weird race condition in play, and if you refer to the 1st Party Cookie macro in the same script which also deletes the cookie, the macro returns an undefined. I\u0026rsquo;ll have to research this phenomenon a bit more.\n  This was way more difficult than the other methods, and let\u0026rsquo;s face it: If you need to resort to the form submission, you should probably flog your developer first. Make them create a site search engine that actually works with analytics, since it really is one of the best things you can measure.\nConclusions I was long overdue a traditional tutorial post, so I hope you\u0026rsquo;ve enjoyed this one. I\u0026rsquo;m still working on the JavaScript for GTM: Part 2 (check Part 1 here), but now I\u0026rsquo;m also dreaming about the mini-vacation that starts tomorrow.\nHowever, if this post raises any questions, if you have other ideas for tracking site search, or if you have a problem you need help with, feel free to drop a line below in the comments! Nothing like a JavaScript challenge to make summer more fun.\nHave a great, warm, relaxing summer, my friends! Live life to the fullest, and dream of JavaScript and all things wonderful.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/javascript-101-gtm-part-1/",
	"title": "JavaScript 101 For GTM: Part 1",
	"tags": ["Google Tag Manager", "Guide", "JavaScript"],
	"description": "Part one of a two-parter, where I introduce some key JavaScript concepts for successful Google Tag Manager management and use.",
	"content": "Here\u0026rsquo;s the link to part 2 of this JavaScript guide.\nThe thing about Google Tag Manager, or any JavaScript tag manager for that matter, is that there\u0026rsquo;s JavaScript involved. In fact, the tool itself is just a JavaScript library with some additional bells and whistles (such as a management UI). This means that to make the best of it, some knowledge of JavaScript is warranted, and that\u0026rsquo;s the point of this post.\nThis will not be a how-to or JavaScript for dummies, but rather a description of how JavaScript should be managed within GTM. However, I have added some of my favorite JavaScript learning resources to the mix, if you want to start from the beginning. JavaScript is a pretty easy programming language to learn, partly because it\u0026rsquo;s been \u0026ldquo;dumbed down\u0026rdquo; and standardized so much over the years. The biggest difficulty with JavaScript, in my opinion, has nothing to do with the language itself. The biggest problem, by far, is how JavaScript is interpreted by the browsers out there. At its best, the same piece of code works perfectly across all browsers in existence. At its worst, you\u0026rsquo;ll have to add complex cross-browser fallbacks to make sure your code runs even on certain modern browsers.\nThe good thing about using GTM, or any library, is that the library itself usually takes care of cross-browser concerns for you. With GTM, however, this is limited to the tag and macro templates that you use. When you start adding custom JavaScript yourself, it\u0026rsquo;s up to you to be wary of cross-browser support for your scripts.\nQuirksmode.org is a pretty awesome resource if you want to find out more about browser compatibility.\nWithout further ado, let\u0026rsquo;s go the guide:\n  Favorite resources\n  JavaScript in Google Tag Manager\n  The dataLayer structure\n  Interacting with dataLayer\n  Custom HTML tag\n  Custom JavaScript macro\n  This is, again, just the first of a two-part series. I\u0026rsquo;m sorry, I\u0026rsquo;m just lazy. The sun is finally shining here, I\u0026rsquo;m looking at the garden, and for some reason the notion of outdoors, smell of freshly cut grass and the sweet liberation only a can of cold beer can provide are edging their way to the top of the \u0026ldquo;things I\u0026rsquo;d rather be doing than blogging\u0026rdquo; list, which is a very, VERY short list indeed.\n1. Favorite resources Here are some resources to get you going on your JavaScript learning trail:\nWEBSITES:\nCodecademy - Interactive learning portal for a number of different programming disciplines. Their JavaScript track is excellent to get you started with the language.\nCode School - Another training portal. Their JavaScript track is good as well.\nw3schools.com - Better as a reference than as a learning resource, but lots of good content nonetheless.\nMDN Web API Interfaces - Reference tool for all the web APIs you\u0026rsquo;ll ever need.\nBOOKS:\nDouglas Crockford - JavaScript: The Good Parts - One of the best books on JavaScript best practices out there (intermediate to advanced).\nNicholas Jakas - Professional JavaScript for Web Developers - My favorite JavaScript book. The most complete guide out there.\nMarijn Haverbeke - Eloquent JavaScript - Excellent book on JavaScript, with a free digital edition!\nBLOGS:\nDailyJS - Daily tips on JavaScript.\nDavid Walsh Blog - Lots of stuff on web development, with many in-depth articles on JavaScript.\nJoe Zim\u0026rsquo;s JavaScript Blog - Actionable tips and guides for JavaScript and related frameworks.\nThese are my favorite resources. If you have more suggestions, let me know via e-mail or in the blog comments.\n2. JavaScript in Google Tag Manager In almost all of my Google Tag Manager posts, I use JavaScript in one way or another to get the task done. That\u0026rsquo;s because JavaScript is THE language of Google Tag Manager. The library itself is a JavaScript library, and all the bells and whistles of the UI are meant to control the type of JavaScript that the tool injects on the web page.\nHowever, you can\u0026rsquo;t just go around shooting JS code wherever you like. In the GTM UI, JavaScript can only be added in the context of a script. Currently, there are two places which accept JavaScript code raw and unfiltered: the Custom HTML Tag and the Custom JavaScript Macro. So don\u0026rsquo;t try to write JavaScript in other tag fields, it won\u0026rsquo;t work.\n  So if you want to perform custom JavaScript functions, you will either need to create a Custom HTML Tag which executes some code, or if you want to use JavaScript to access DOM elements, for example, you might want to create a Custom JavaScript macro. We\u0026rsquo;ll get to these soon, don\u0026rsquo;t worry.\nFURTHER READING:\n Google Tag Manager Developer Guide  3. The dataLayer structure A JavaScript tag management solution is based on code injection. This means that the code it executes on your site is alien to the page template. The only thing the page template originally has is the container snippet. This snippet acts as the invitation for GTM to perform its magic on your page.\nBecause GTM injects the code it wants to execute, it\u0026rsquo;s vital that it uses its own data structure. If there was no proprietary data model, all the variables GTM would be accessing / modifying would run the risk of causing conflicts with other libraries. For instance, say you have a global variable pageHeader created by your developers, and it stores the main header of your page template. Then GTM comes along and uses pageHeader to manipulate the document title that is pushed to Google Analytics. Because it accesses the same global variable that you use to store on-page information, the risk that GTM would break your page template becomes a horrible reality.\nWith dataLayer, successful (read: smart) use of GTM is restricted to only those variables that are exposed within the dataLayer structure. Thus the only risk you run is having another library which ALSO uses dataLayer (spelled exactly like that). The better known that the dataLayer standard becomes, the less of a risk this will be, since other libraries will know to avoid using this name.\ndataLayer is a JavaScript object of the type Array. An Array is a special type of object. The difference is that in JavaScript, objects have properties, whereas Arrays have members. An Array member can be an object.\nObject properties are defined with specific notation, e.g.:\nvar sampleObject = {\u0026#39;First-name\u0026#39;: \u0026#39;Simo\u0026#39;}; alert(sampleObject[\u0026#39;First-name\u0026#39;]); alert(sampleObject.First-name); // Error!  So this is an object, not an Array. If you want to get the terminology straight, in the first line I created an object using object literal notation. That\u0026rsquo;s the curly bracket thingamajig there. In the second line, I retrieved the \u0026lsquo;First-name\u0026rsquo; property of the object by using square bracket notation. You could also use dot notation to access object properties (third line), but with \u0026lsquo;First-name\u0026rsquo; that would cause a NaN (Not a Number) error, because the hyphen in the name is interpreted as a minus sign. Thus JavaScript in its infinite wisdom is misguided enough to think that sampleObject.First returns a number of which another number name is subtracted. But never mind, this was just a detour.\nThe thing to understand about dataLayer is that an Array behaves a bit differently than your run-of-the-mill JavaScript object. First of all, an Array is like a table or queue, however you want to look at it. You can add multiple members to it, and any one of these members can be of any JavaScript type, such as objects.\nIn GTM, dataLayer is (almost) always interpreted as an Array of objects. Thus you can have the sampleObject I created above as a member of dataLayer:\nvar dataLayer = [{\u0026#39;First-name\u0026#39;: \u0026#39;Simo\u0026#39;}];  As you can see, it doesn\u0026rsquo;t matter any more that the object was called \u0026lsquo;sampleObject\u0026rsquo;, since from this moment on it will only be referenced to using its index number in the Array. Index numbers start from 0, so if this is the first object in the array, it\u0026rsquo;s reference is dataLayer[0]. The next object that is added to the Array would have index number 1, and so on.\nYou can access object properties in an Array using the same notation as you just learned of in the beginning of this chapter. However, first you need to know which index the object whose property you want to access resides in.\nSo, if you\u0026rsquo;re still following me, to access the \u0026lsquo;First-name\u0026rsquo; property of the FIRST dataLayer index, you\u0026rsquo;d need to use something like this:\nalert(dataLayer[0][\u0026#39;First-name\u0026#39;]);  This would pop up a dialog with \u0026ldquo;Simo\u0026rdquo;. However, in GTM you don\u0026rsquo;t need to worry about dataLayer index numbers, since you can just use the Data Layer Variable Macro. I\u0026rsquo;ll have more about interacting with dataLayer in the next chapter.\nFURTHER READING:\n  w3schools: JavaScript Arrays\n  MDN: Working with JavaScript objects\n  4. Interacting with dataLayer In GTM, you basically interact with dataLayer in three different ways:\n  First, you define dataLayer as an Array\n  Next, you push objects and properties into dataLayer\n  Finally, you access these properties using a Data Layer Variable Macro\n  Let\u0026rsquo;s go through these step-by-step.\nYou define dataLayer by declaring the variable and setting its type to Array. The most common way to do this is to use literal notation:\nvar dataLayer = [];  This tells your page that a new variable called dataLayer has now been declared, and the square brackets define its type as Array. After this, you can start pushing stuff into the structure, or if you have some properties that you already want to push, you can just add them to the declaration:\nvar dataLayer = [{ \u0026#39;pageTitle\u0026#39;: \u0026#39;Homepage\u0026#39;, \u0026#39;visitorType\u0026#39;: \u0026#39;Loyal customer\u0026#39;, \u0026#39;loggedIn\u0026#39;: true }];  So here I create dataLayer as an Array (that\u0026rsquo;s the square brackets) with a single object (that\u0026rsquo;s the curly brackets) with three properties (that\u0026rsquo;s the \u0026lsquo;key\u0026rsquo;: \u0026lsquo;value\u0026rsquo;,) stuff. As you can see, there\u0026rsquo;s no comma after the last property. Also, true does not have single quotes around it, since I\u0026rsquo;m storing a value of Boolean type (true/false) and not a string.\nThe thing about the dataLayer declaration is that it should only ever be done once. You see, every time you redeclare a variable, its previous reference is overwritten! You have to be really careful about this, since if your developers accidentally add the dataLayer declaration twice on your page, its previous properties will be overwritten by the second declaration. That\u0026rsquo;s why its safest to declare the structure once and then just consistently use push() for all modifications later.\nThe push()-method is what all Arrays come equipped with. Using this method adds an object to the end of the Array, creating a new index for it automatically. Thus nothing is overwritten. You can have a dozen objects with exactly the same properties in dataLayer, and you\u0026rsquo;d get no errors. Note that GTM\u0026rsquo;s Data Layer Variable Macro has a way of navigating around this issue (see below).\nTo push a new object into dataLayer, the following notation should be used:\ndataLayer.push({\u0026#39;loggedIn\u0026#39;: false});  This just pushed a new object into dataLayer with a single property: \u0026lsquo;loggedIn\u0026rsquo; with the value false. So I can easily have dataLayer with two objects, both with property \u0026lsquo;loggedIn\u0026rsquo; being something different. That doesn\u0026rsquo;t matter, since they\u0026rsquo;re in separate objects.\n  Trying to push the same property twice into the SAME object would not result in an error, either. However, only the last property would remain in the object.\nThe thing about push() is that it won\u0026rsquo;t work if dataLayer hasn\u0026rsquo;t been declared as an Array. That\u0026rsquo;s why in your page template the following is a good way of prefixing the dataLayer.push() call:\nwindow.dataLayer = window.dataLayer || []; dataLayer.push(...);  The first line checks if dataLayer has already been declared. If it has, all is well, but if it hasn\u0026rsquo;t, it is then and there declared as an Array. This basically makes it so that the push() method will almost always work. The only way it won\u0026rsquo;t work is if dataLayer HAS been declared but as something other than an Array (pretty rare situation).\nNote that when working in GTM (Custom HTML and Custom JavaScript), you won\u0026rsquo;t have to add this check to your code, since the GTM container snippet will always declare dataLayer for you if you haven\u0026rsquo;t done so explicitly in the page template.\nFinally, the way that the Data Layer Variable Macro works is that when you give it a property whose value it must return, it starts going through the dataLayer properties one by one, starting from the newest. If it finds a property with the name you gave, it returns its value and stops traversing dataLayer. So if you have two objects in dataLayer with the property \u0026lsquo;loggedIn\u0026rsquo;, only the most recent property value will be returned by the Data Layer Variable Macro.\nIn layman\u0026rsquo;s terms, this means that for Google Tag Manager, pushing an object into dataLayer with a property that already exists in dataLayer overwrites the previous property value with the new one. Of course, it doesn\u0026rsquo;t really overwrite anything, but the Macro is only interested in the latest value.\nThere is a quirky exception to this, and that is if you push an Array into dataLayer.\nIf you then push another Array with the same name, GTM will only \u0026lsquo;overwrite\u0026rsquo; those Array indices that are shared by the two Arrays, but if the first Array had more indices, then those will remain for GTM to use. Allow me to demonstrate:\ndataLayer.push({\u0026#39;products\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]}); ... dataLayer.push({\u0026#39;products\u0026#39;: [\u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;]});  In the first line, I push an Array with four members (a, b, c, d). A bit later on, I push another Array with the same name, but this time with just two members. If GTM would be consistent, the second time around the Data Layer Variable Macro would have just two members in the \u0026lsquo;products\u0026rsquo; Array, but right now GTM works so that it just overwrites the first two indices in the Array. Thus the Macro would actually see \u0026lsquo;products\u0026rsquo;: [\u0026lsquo;e\u0026rsquo;, \u0026lsquo;f\u0026rsquo;, \u0026lsquo;c\u0026rsquo;, \u0026rsquo;d\u0026rsquo;]. This is a bit buggy to me, as to be consistent the second Array push should make it so that the Macro only accesses the values in the second Array.\nPhew, this was a lot of information, I hope you\u0026rsquo;re still with me here.\n5. Custom HTML Tag A Custom HTML Tag allows you to inject HTML markup in the site. This means that if you want to run custom JavaScript in a Custom HTML Tag, you need to create the script context for the code. In HTML, you establish a script context with the \u0026lt;script\u0026gt; element:\n\u0026lt;div id=\u0026#34;html_markup_here\u0026#34;\u0026gt;This is still just normal HTML markup\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; alert(\u0026#34;And this is JavaScript!\u0026#34;); \u0026lt;/script\u0026gt; If you try to run JavaScript without the script block, you\u0026rsquo;ll run into some serious validation errors.\nNote that you don\u0026rsquo;t need the legacy type=\u0026quot;text/javascript\u0026rdquo; attribute in your \u0026lt;script\u0026gt; tags, unless you\u0026rsquo;re still using HTML4 (gasp) and you want your code to validate. All modern browsers can do the math and make the logical conclusion that a SCRIPT element with JavaScript code inside should be interpreted as, surprise surprise, JavaScript.\n6. Custom JavaScript Macro In Google Tag Manager, a macro is designed to return a value when it is called. That\u0026rsquo;s why you have all those cool out-of-the-box macros out there, which each return some amazing piece of information about the site, the page, or the visitor.\nIf you haven\u0026rsquo;t already, be sure to read my macro guide, as it sheds more light on this topic.\nWith a Custom JavaScript Macro, it\u0026rsquo;s function (pun intended) is also to return a value. If there\u0026rsquo;s no return statement or if the macro isn\u0026rsquo;t wrapped in a function (only functions can return values), you\u0026rsquo;ll run into errors in GTM.\nSo here are the things to remember:\n1. Always wrap your Custom JavaScript macro in an anonymous function.\n2. Always return a value (any value) with a Custom JavaScript macro\nAn anonymous function is a function without a name. Why is this necessary? Well, you don\u0026rsquo;t create a function in a macro so that you call it on a whim using its name. No, the sole reason for a JS macro\u0026rsquo;s existence is that it\u0026rsquo;s called and its value is resolved at the same time, using the {{macro}} syntax of GTM. This is, again, one of the ways GTM ensures that global variables are not being messed about with. If you could name your functions, you might be using a name reserved for some other library. This would potentially have nightmarish results.\nAnd why return a value? Well, to reiterate, a macro\u0026rsquo;s function is to return a value. When you call a JS macro, it resolves some operation right then and there with all the data available at the moment it was called. It then returns a value based on this operation. What would be the function of using the macro syntax {{macro}} in your fields if not to retrieve a value? Stop asking silly questions.\nAn example of a proper Custom JavaScript macro:\nfunction() { return window.location.href.toLowerCase(); }  Here you have an anonymous function declared on the first line, a return statement with the operation on the second line, and the closing of the function block on the third line.\nWhen this macro is called, it returns the URL of the current page transformed into lower case. This is a prototypical GTM Custom JavaScript Macro. It\u0026rsquo;s just an anonymous function which does not accept parameters. Thus its sole objective is to return a value, based on some operation on some other value that can be found in the document object model (more on this later). Remember that you can access other macros in your Custom JavaScript Macros, so you can do a lot of pretty complex stuff with Custom JavaScript macros. For example, the following macro returns the first folder in the URL path (e.g. \u0026ldquo;simo\u0026rdquo; in /simo/articles/analytics):\nfunction() { return {{url path}}.split(\u0026#34;/\u0026#34;)[1]; }  A Custom JavaScript Macro is probably the most versatile tool you have available in your GTM toolbox, and you should definitely check out my other Google Tag Manager posts for creative ways to use them.\nConclusions Phew, that was a lot information there. I\u0026rsquo;m going to have to split this into a two-parter, just to keep me motivated :)\nI hope you take a look at the JavaScript resources I wrote of in the first chapter. Even if you\u0026rsquo;re well versed in the craft of client-side programming, most of the time its good to refresh your knowledge with some \u0026ldquo;basic stuff\u0026rdquo;.\nThe thing with GTM is that in addition to functioning like your average JavaScript library, it comes along with certain proprietary features that require you to adapt your JavaScript skills somewhat. The purpose of this (and the following article) are just that: make you knowledgeable of what GTM requires from you and the code you create.\nHere\u0026rsquo;s the link to part 2 of this JavaScript guide.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/fun-google-tag-manager-part-2/",
	"title": "Fun With Google Tag Manager: Part 2",
	"tags": ["Google Tag Manager", "Guide", "Tips"],
	"description": "Second part of a two-part JavaScript post, going over fun things you can do in Google Tag Manager.",
	"content": "Apologies for leaving you hanging. It\u0026rsquo;s now almost three weeks since I published the first part of this post, and I\u0026rsquo;m sure you\u0026rsquo;ve been holding your breath ever since.\nThere\u0026rsquo;s been a lot going on since the last post. First, my favorite sports team in the world, San Antonio Spurs, won their fifth NBA championship from the defending champs, Miami Heat. Next, my wife and I moved to our new house, and we\u0026rsquo;ve been remodeling ever since. It\u0026rsquo;s been hectic with work and life getting mixed up in a potpourri of saw dust and web analytics, but now it\u0026rsquo;s time to return back to the wonderful world of Google Tag Manager!\nI promised you four new tips in this one, but I\u0026rsquo;m going to top myself and give you six gems from the world of tag management. Here\u0026rsquo;s what you\u0026rsquo;ll get:\n  dataLayer declaration and push\n  Using eventCallback in dataLayer.push()\n  Stopping a Timer Listener\n  Naming and version control conventions\n  Fire a single tag multiple times\n  {{true}} and {{false}}\n  dataLayer declaration and push Google Tag Manager uses dataLayer as its own proprietary data model. dataLayer is a JavaScript object of Array type. This means that it has some methods unique to Arrays, such as push().\n(See my comment to this post here for an overview of how to access Array objects vs. object properties.)\nThe thing is, you can\u0026rsquo;t use dataLayer.push() if you haven\u0026rsquo;t declared dataLayer as an array first. The repercussions of this are that if you try to use dataLayer.push() before dataLayer has been declared as an Array, or if it\u0026rsquo;s been declared as something different (such as a normal object or string), you\u0026rsquo;ll get some errors:\n  So here\u0026rsquo;s the first tip:\nYou need to declare dataLayer as an Array before you can use its push() method.\nTo turn dataLayer into an array, you need the following code before the container snippet:\nvar dataLayer = [];  If you don\u0026rsquo;t declare it with this explicit statement, then GTM will declare dataLayer for you. This is very important, since it ensures that you will always have dataLayer available for your tags.\nThe thing with JavaScript objects and primitives (numbers, strings, etc.) is that if you redefine a JavaScript entity, its previous state will be overwritten! JavaScript entities are dynamic, so you can redefine a string as an object, an Array as a number, and so forth without any difficulty. There\u0026rsquo;s also no way of protecting a global variable such as dataLayer from being redefined as a different type, so precautions must be taken so that you don\u0026rsquo;t mess things up.\nWhy is this important? Well, if you have more than one dataLayer declaration on your page template OR in your GTM tags, you will erase all previous information in the structure! Oops. Here\u0026rsquo;s an example:\nvar dataLayer = [{\u0026#39;pageCategory\u0026#39;: \u0026#39;home\u0026#39;, \u0026#39;logged-in\u0026#39;: \u0026#39;true\u0026#39;}]; ... var dataLayer = [{\u0026#39;event\u0026#39;: \u0026#39;allDone\u0026#39;}];  After the first line of code, dataLayer has a single object with two properties: pageCategory and logged-in. A little on further down the code is the second declaration. Now, after this code is run, dataLayer will still only have one object, but with just the event property! Can you see how harmful this is? None of the previous information is recoverable after this.\nThe safest way to prevent this is to preface the ONLY dataLayer declaration on the page with the following line:\nwindow.dataLayer = window.dataLayer || [];  And then only use dataLayer.push() to add objects to the Array. This line ensures that if dataLayer has already been declared, it won\u0026rsquo;t be overwritten. It\u0026rsquo;s not foolproof, since it doesn\u0026rsquo;t test whether the global variable dataLayer is of type Array or not, but I think it\u0026rsquo;s good enough. If you have another JavaScript library which HAPPENS to use a variable called dataLayer as well, you might be in trouble. But this is what testing and debugging is for.\nSo here\u0026rsquo;s my final tip for this section:\nDeclare dataLayer JUST ONCE and use dataLayer.push() for all interactions afterwards.\nIn your GTM tags you can just use dataLayer.push(), and you don\u0026rsquo;t need to check whether dataLayer exists or not, since GTM declares it for you in the container snippet. But when working on the page template, you need to be extra careful not to overwrite the Array.\nUsing eventCallback in dataLayer.push() You can use the eventCallback dataLayer variable in the push() method to execute code after all dependent tags have fired. I know, that was a mouthful but it\u0026rsquo;s really simple. Take this example:\ndataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;fireEvent\u0026#39;, \u0026#39;eventCallback\u0026#39;: function() { console.log(\u0026#34;Event has fired\u0026#34;); } });  Well, it\u0026rsquo;s a pretty lousy example, but it does what it\u0026rsquo;s meant to. After all tags that fire upon {{event}} equals fireEvent have completed, the eventCallback function will execute and you should see the text in the JavaScript console.\neventCallback is, in essence, a shorthand for the proprietary hitCallback feature that you can invoke in your Google Analytics tags.\nYou can use this for a quick callback, such as waiting for all tags to fire before a link redirects or pushing a second event as the callback to impose a rudimentary tag firing order.\nStopping a Timer Listener You know of the Timer Listener, right? You can use it to have an event push every X milliseconds. You can limit the number of times the timer fires, or you can just let it fire from here to eternity.\n  The thing is, you might want to halt the listener before the limit is reached. For example, if you\u0026rsquo;re using the Timer Listener to send pulses reflecting the time the user has spent digesting content, for example, you might want to halt it once the reader reaches the end of the page to keep your events from being cluttered with timer hits.\nTo halt a timer, you need to use the window.clearInterval() method. It requires a parameter, namely the ID of the timer that is firing.\nGTM\u0026rsquo;s timer listener is your run-of-the-mill JavaScript setInterval() timer, and its ID is stored in, what else, a dataLayer variable called gtm.timerId. So to halt the timer, you need to send the content of gtm.timerId as the parameter of window.clearInterval(). For this to work, you\u0026rsquo;ll need to create a new Data Layer Variable Macro which refers to gtm.timerId:\n  After this, whenever you want to halt a timer, all you have to do is have the following line of JavaScript in your Custom HTML tag or your Custom JavaScript macro:\nwindow.clearInterval({{timer id}});  This will halt the most recently fired timer. So if you have multiple timers, you might have to edit the code somewhat to find the proper timer (easiest way to do this is to use {{event}} equals your_timer_event as one of the rule conditions of the tag that stops the timer).\nNaming and version control conventions GTM doesn\u0026rsquo;t exactly shine when it comes to managing a bucketload of tags, macros, and rules (TMRs). Because of this, it\u0026rsquo;s really important to adopt a good naming convention so that you can make sense of what you have in your container.\nFor alternative approaches (more explicit naming to satisfy the in-tool search), check Doug Hall\u0026rsquo;s original, excellent post on online-behavior. A more recent post by Justin Goodman covers similar ground.\nI use a very simple naming convention in all my TMRs. It scales pretty nicely, as it works in large enterprise implementations as well as in smaller containers. Also, the most important thing is that it\u0026rsquo;s readable.\nHere\u0026rsquo;s what I use.\nTAGS\nTag type - purpose\nHTML - Weather API Call\nHTML - Social Share Tracking\nUA - Page View Tracking\nListener - Link Click\nListener - Click\nHaving the tag type is pretty redundant since you can see that in the second column of the tag list. However, I enjoy that it makes the list look a bit less cluttered.\n  RULES\nKey macro - trigger\nEvent - gtm.js\nEvent - Outbound Link Click\nURL - Thank you page\nThe key macro is what drives the rule. Most often its an {{event}}, but every now and then you might have the same event across many rules, and then you\u0026rsquo;ll need to use some other macro to express the difference between the two rules. An example of this is the URL rule above. It\u0026rsquo;s underlying {{event}} is gtm.js, so I have to use the URL condition to make it different from all the other gtm.js rules.\nMACROS\nWhat is returned\nRandom Number\nDebug Mode Status\nElement URL\nURL Hostname\nWith macros, we\u0026rsquo;re only interested in what they return, since that\u0026rsquo;s what macros are used for. So in the macro name, it\u0026rsquo;s important to express just what is returned when the macro is called.\nAs for version control, my tip is this:\nName your versions AND add notes!\n  That\u0026rsquo;s it. It\u0026rsquo;s so important to annotate your container versions with information what was updated and what changed. This is the only way to stay on top of your game, especially if you\u0026rsquo;re dealing with a huge version history. Be pedantic about this and make sure it\u0026rsquo;s part of your quality assurance process for tag management!\nFire a single tag multiple times Here\u0026rsquo;s a common problem: You have multiple trackers firing on the same domain, but you want to use just one tag for them. Well, currently GTM doesn\u0026rsquo;t have a switch with which you can fire the same tag multiple times with different parameters, so you\u0026rsquo;d need to create a separate tag for each tracker.\nHowever, there\u0026rsquo;s a workaround. You can use hitCallback, the Lookup Table Macro and a global JavaScript variable to fire your tag multiple times with different values in the fields!\n(Note: GTM\u0026rsquo;s Debug mode doesn\u0026rsquo;t let you push items into dataLayer, even if you\u0026rsquo;re doing it from a returned function (which is totally valid). Unfortunately this means that you won\u0026rsquo;t be able to test this solution in the Debug mode, and you\u0026rsquo;ll have to publish the container to test it.)\nHere\u0026rsquo;s what you need:\nJAVASCRIPT VARIABLE MACRO {{tag callback counter}}:\n  LOOKUP TABLE MACRO {{tracker name}}:\n  HITCALLBACK MACRO {{tag callback function}}:\n  So, before we check how the tag is set up, let\u0026rsquo;s go over these macros. The {{tag callback counter}} is a JavaScript variable macro which refers to the global JS variable you set up later in {{tag callback function}}. The idea here is that the {{tracker name}} Lookup Table macro returns a different string, depending on what the counter is. Every time the callback is executed, the counter goes up by 1.\nThe key is to use your first tracker as the default value of the Lookup Table macro. This is because when the tag first fires (on {{event}} equals gtm.js), the JavaScript variable that {{tag callback counter}} refers to will not exist yet (since it\u0026rsquo;s defined in the CALLBACK of this tag).\nWhen the callback is executed, a GTM event is pushed into dataLayer and the counter value is increased by one. You define the maximum number of times the tag should fire with the maxRepeat variable in the callback function. Just remember to have a row for each iteration in the Lookup Table, otherwise the tracker in the Default Value field of the Lookup Table will be returned for each iteration that doesn\u0026rsquo;t have a row in the table!\nIn your tag, add the following in the Fields to Set:\n  This will ensure that the callback function is executed every time the tag has fired.\nNow that you have your Lookup Table, you can use it in the Tracker Name field of your tag:\n  Using the macro here makes it so that with every iteration of the tag, a different value is set as the Tracker Name, thanks to the Lookup Table and the counter that increases by 1 with every pass.\nFinally, you need to add TWO rules to the tag. That\u0026rsquo;s two rules, not two conditions. The rules are\n{{event}} equals gtm.js\nand\n{{event}} equals tagCallback\n  Having two rules means that the tag will fire when and if either one is matched. So first it will fire upon {{event}} equals gtm.js, since that\u0026rsquo;s the event that is pushed when GTM is first loaded. Next, it will fire every time {{event}} equals tagCallback, so twice in my example. Thus, the tag fires three times with three different tracker names.\nYou could use this also if you have different properties you want to track to. Just replace the tracker names with UA-XXXXX-Y codes and you\u0026rsquo;re set!\nHere\u0026rsquo;s a quick recap of what happens:\n  Tag is fired upon {{event}} equals gtm.js. Since {{tag callback counter}} can\u0026rsquo;t find the global variable, the Lookup Table macro {{tracker name}} returns the default value, which is localTrackerA\n  When the tag has fired, the counter is created with value 0, and the tagCallback event is pushed into dataLayer\n  tagCallback triggers the tag again, and this time the Lookup Table finds the counter with value 0, and returns localTrackerB\n  When the tag has fired, the counter number is increased to 1, and the tagCallback event is pushed into dataLayer\n  tagCallback triggers the tag for the third time, and again the Lookup Table finds the counter with value 1, thus rollupTracker is returned\n  The hitCallback is fired again, but this time the maximum number of repeats is reached and the function just returns an undefined value. Thus, no callback event is pushed into dataLayer, and the tag does not fire again.\n  I think this is an amazing feat of strength from dynamic macros and the callback function. This should make any enterprise setup much, MUCH leaner.\n{{true}} and {{false}} This one\u0026rsquo;s a quicky but pretty important. Create two Custom JavaScript macros:\n{{true}}\nfunction() { return true; }  {{false}}\nfunction() { return false; }  Why, you ask? Well, let\u0026rsquo;s say you want to set the Anonymize IP setting to true for one domain and false for two other domains (this idea came from Carmen Mardiros, by the way!). You want to use a Lookup Table Macro to achieve this, but when your Lookup Table aligns domains with the return value of \u0026ldquo;true\u0026rdquo; and \u0026ldquo;false\u0026rdquo;, you notice that things don\u0026rsquo;t work as you\u0026rsquo;d like them to. The problem is that if you add text to a field in GTM, it is always cast to string type. Anonymize IP requires Boolean type, otherwise it fails.\nTo circumvent this problem, instead of typing \u0026ldquo;true\u0026rdquo; and \u0026ldquo;false\u0026rdquo; into GTM\u0026rsquo;s fields, use the macros you just created. They return true and false in proper Boolean type! This way you will be able to pass the correct value types to the settings which require Boolean values.\nNaturally, this should be fixed by adding Boolean true and false as possible values in all GTM fields.\nConclusions Well there you have it! Another round of GTM magic. I can\u0026rsquo;t wait to experiment more and uncover even more cool stuff to share with you. Be sure to join our amazing Google+ community which has served as inspiration for most of these tips.\nAlso, if you happen to be in any of the following upcoming conferences, come say hi to me:\n  Google Analytics conference in Stockholm (August 2014)\n  Web Analytics Wednesday in Copenhagen (September 2014)\n  Conversion Conference in London (October 2014)\n  eMetrics in London (October 2014)\n  Marketing Festival in Brno (October-November 2014)\n  "
},
{
	"uri": "https://www.simoahava.com/analytics/fun-with-google-tag-manager-part-1/",
	"title": "Fun With Google Tag Manager: Part 1",
	"tags": ["Google Tag Manager", "Guide", "macros"],
	"description": "First part of the two-part Google Tag Manager post, where we go over fun things you can do with JavaScript.",
	"content": "It\u0026rsquo;s time to dig into my tip library for some pretty cool things you can do with Google Tag Manager to enhance your site tagging. Some of these are macro magic through and through, some of these are best practices, and some of these are things that will make your life easier while managing a tag management solution.\n  I\u0026rsquo;ve split this post into two parts to make it more Hobbit and less Lord Of the Rings length-wise.\nHere\u0026rsquo;s what you\u0026rsquo;ll find within this first part:\n  Unbind jQuery\n  Undefined fields are not sent to GA\n  Track copy event\n  Errors as events\n  Keep your eyes peeled for the second part.\nI know, the anticipation is probably killing you.\n1. Unbind jQuery I\u0026rsquo;m not a huge fan of jQuery. I mean, it\u0026rsquo;s definitely useful, since it simplifies a lot of the difficult and error-prone syntax that complex JavaScript entails. At the same time, I enjoy complex challenges, and learning the hard way has definitely made me a better developer.\nIn my experience, and this is definitely a very marginal view, jQuery introduces a certain degree of, erm, sloppiness. I know the \u0026ldquo;it\u0026rsquo;s too easy\u0026rdquo; argument is infantile, but in some cases the lack of transparency to what\u0026rsquo;s actually happening in the code can invite some pretty poor development work. In a way, jQuery is more about reading a manual than about actually creating code, and that sucks. Especially with event handlers, some development decisions are clearly influenced by how easy it is to bind the handlers to elements.\nFor Google Tag Manager, this is all too familiar. An innocuous return false; or e.stopPropagation() will prevent your GTM listeners from ever retrieving the DOM event. Often the only reason propagation is cancelled like this is the fact that the code was copy-pasted from some Stack Overflow discussion completely unrelated to your website\u0026rsquo;s markup.\nOK, sorry for the rant. Here\u0026rsquo;s what I suggest. Whenever your GTM listener doesn\u0026rsquo;t fire when it should, open the JavaScript console and type the following line of code there\u0026hellip;\nif(typeof(jQuery)!==\u0026#39;undefined\u0026#39;){jQuery(\u0026#39;body\u0026#39;).find(\u0026#39;*\u0026#39;).off();}  \u0026hellip;and press enter. Then try clicking or form submitting again.\nWhat this simple line does is it unbinds all jQuery event handlers from all HTML elements in the document body. If your GTM listener works after this, it means that there\u0026rsquo;s some interfering jQuery script. The next step is to find the problematic script, and ask your developer to fix it so that propagation isn\u0026rsquo;t prevented.\n(Be sure to check out my Chrome extension, where this feature is handily one button click away!).\nOr you can choose the hacker route. If you find the interfering script, and if you find the function that\u0026rsquo;s grabbing your clicks or submits and preventing propagation, you can use a Custom HTML tag to fix it.\nNOTE! This is a hack. Be sure to test, test and TEST before publishing a container where you unbind jQuery. Also, it wouldn\u0026rsquo;t hurt to consult with someone who has an understanding of just what event handlers are used on the site.\nLet\u0026rsquo;s say you have a Back To Top button which is not sending clicks to GTM\u0026rsquo;s listeners. You find the code in one of the page assets, and it looks something like this:\njQuery(\u0026#39;.top-of-page, a[href=\u0026#34;#top\u0026#34;]\u0026#39;).click(function () { jQuery(\u0026#39;html, body\u0026#39;).animate({ scrollTop: 0 }, 400); return false; });  As you can see, the dreaded return false; is there. That\u0026rsquo;s what\u0026rsquo;s stopping the events from climbing up the DOM tree.\nTo fix this, create a new Custom HTML tag with the following code within:\n\u0026lt;script\u0026gt; jQuery(\u0026#39;.top-of-page, a[href=\u0026#34;#top\u0026#34;]\u0026#39;).off(\u0026#39;click\u0026#39;); jQuery(\u0026#39;.top-of-page, a[href=\u0026#34;#top\u0026#34;]\u0026#39;).click(function(e) { e.preventDefault(); jQuery(\u0026#39;html, body\u0026#39;).animate({ scrollTop: 0 }, 400); }); \u0026lt;/script\u0026gt; If jQuery and the problematic script are loaded in the \u0026lt;head\u0026gt; of your template, you can have this tag fire on {{url}} matches RegEx .*. If they\u0026rsquo;re loaded elsewhere (like in the footer of your page), you might need some other event rule such as {{event}} equals gtm.load.\nOn the first line, I unbind the original click handler. Then, I add e as an argument of the new click function. This e contains the event object, and I can then use its preventDefault() method to stop the original action of the click without stopping its propagation. Finally, I remove the return false; from the end of the function.\nNaturally, you\u0026rsquo;ll need to test, test, test and then test some more whether this hack breaks down your entire site or not.\nOf course, it\u0026rsquo;s not just jQuery that might interfere with your listeners. It might be vanilla JavaScript or some other library. This chapter focuses on jQuery simply because it\u0026rsquo;s so widely spread that most of the time it really is the source of the problem.\n2. Undefined fields are not sent to GA Here\u0026rsquo;s a fun fact about the Google Analytics API: if a field such as Custom Dimension is undefined when the GA endpoint is called, the Custom Dimension is left out of the payload. How is this fun? Well, it introduces an opportunity to make your tags a bit more dynamic.\nIt would be nice to get a list of all the fields that behave this way, and also how they behave with other return values such as false or 0.\nSo, let\u0026rsquo;s say you have a Custom Dimension such as blog author that you want to send with your page view tags. Naturally, you only set the blog author on blog pages. You don\u0026rsquo;t want to send a dummy value like \u0026ldquo;empty\u0026rdquo; or \u0026ldquo;n/a\u0026rdquo; on other pages on your site, so you\u0026rsquo;re absolutely certain that you need two tags: one when the blog author is populated (and can be sent as a Custom Dimension), and one on all the other pages (where the Custom Dimension will be left out of the tag).\n  However, thanks to how the GA API works, you only need one tag. Just make sure that the macro which returns the Custom Dimension value resolves to undefined type when the dimension doesn\u0026rsquo;t have a value. There are a bunch of ways you can do this:\nfunction() { var t; var y = \u0026#34;\u0026#34;; return t; // returns undefined type  return; // returns undefined type  return undefined; // returns undefined type  return false; // returns boolean type  return y; // returns string type  return x; // throws ReferenceError }  As you can see, returning false, an empty string, or a variable which hasn\u0026rsquo;t been declared will not do.\nThe cool thing about GTM is that if you have a Data Layer Variable Macro which doesn\u0026rsquo;t resolve, it will be automatically set to undefined type, meaning a Custom Dimension which refers to this macro will not get sent. So the best way to navigate through this particular scenario is to have blog-author declared in the dataLayer declaration before the container snippet on the page template. If the page isn\u0026rsquo;t a blog page, then this declaration can just be left out.\n\u0026lt;script\u0026gt; var dataLayer = [{\u0026#39;blog-author\u0026#39;: \u0026#39;Simo Ahava\u0026#39;}]; // Blog author declared, dimension will get sent \u0026lt;/script\u0026gt; ... \u0026lt;script\u0026gt; var dataLayer = []; // Blog author not declared, dimension will not get sent \u0026lt;/script\u0026gt; Then, when you create a Data Layer Variable Macro, on pages with blog-author in the array, it will fire the Custom Dimension with the blog author\u0026rsquo;s name as the value. On all other pages the dimension will not get sent.\nOnce you familiarize yourself with JavaScript and understand all the different ways how GTM and JavaScript resolve variables to undefined type, you can get really creative with this feature, making your tags even more leaner.\n3. Track copy event Ever wondered how often people copy text using CTRL+C or some other means? Well, wonder no more, because now you can track it as an event!\nCreate this simple Custom HTML tag:\n\u0026lt;script\u0026gt; var c = document.getElementById(\u0026#34;entry-content\u0026#34;); if(typeof(c)!==\u0026#39;undefined\u0026#39;) { c.addEventListener(\u0026#39;copy\u0026#39;, function(evt) { dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;copy\u0026#39;}); }); } \u0026lt;/script\u0026gt; This is a very simple script that pushes the dataLayer event \u0026lsquo;copy\u0026rsquo; whenever someone copies something on your page within the HTML element with ID entry-content (e.g. \u0026lt;div id=\u0026quot;entry-content\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;).\nBe sure to change the element whose content you want to track with the event listener to match the markup on your website.\nYou can make it more versatile by exploring the contents of the evt argument, which is the event object. Its target property contains the HTML element which was the target of the event. Here are some ideas:\n... dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;copy\u0026#39;, \u0026#39;copy-id\u0026#39;: evt.target.id, \u0026#39;copy-content\u0026#39;: evt.target.innerHTML}); ...  And so on. Note that evt.target stores the element where the copy event started. So if the copy action takes place over multiple paragraphs, headers, or spans, for example, you\u0026rsquo;ll only be able to observe the properties of the first element targeted by the action.\n4. Errors as events Google Tag Manager has this cool thing called the JavaScript Error Listener. This fires whenever an unchecked exception occurs on the site. For example, consider the following:\nvar myText = \u0026#34;Hello!\u0026#34;; alert(myTxt); // Unchecked ReferenceError, will fire GTM\u0026#39;s error listener  // Exception caught, will not fire GTM\u0026#39;s error listener try { alert(myTxt); } catch(e) { console.log(e.message); }  With GTM\u0026rsquo;s error listener, you can send each uncaught error as an event to Google Analytics. You can then analyze these events to find problems with your scripts. If you have a huge site, you might get hundreds of these errors, so you\u0026rsquo;ll need to adjust the code to perhaps only listen for certain pages or errors.\n  I\u0026rsquo;m hoping at some point we can have the listener fire only for errors propagated by GTM\u0026rsquo;s tags and macros. This way you can debug your GTM installation without having to worry about all the other errors on your website (even though you definitely should!).\nTo get the listener up and running, do the following:\n  Create a new JavaScript Error Listener tag\n  Have it fire on all pages (or some other rule of your choice)\n  Create a new rule Event - gtm.pageError, where {{event}} equals gtm.pageError\n  Create three new Data Layer Variable macros:\n  1. {{Error - Message}}, where variable name is gtm.errorMessage 2. {{Error - Line}}, where variable name is gtm.errorLineNumber 3. {{Error - URL}}, where variable name is gtm.errorUrl   Create a new Event tag and configure it to your liking\n  Make sure the Event tag fires on the rule you created in (3)\n    Remember to set the Non-Interaction Hit parameter of the Event tag to true. You don\u0026rsquo;t want to kill your bounce rate with errors on your site!\nIf you\u0026rsquo;re seeing a lot of Script error. hits, it means that the errors occur in JavaScript resources loaded from other domains. Due to browser / cross-domain security policies the actual error name and message are obfuscated.\nConclusions The second part of this post is on its way. I didn\u0026rsquo;t want to puke too much text into a simple tips \u0026amp; tricks post, but this one is 1800+ words already, so\u0026hellip; :) Topics in the next post are:\n  dataLayer declaration vs. dataLayer.push()\n  How to stop a GTM timer listener\n  Adopting a proper naming convention\n  Annotating container versions with names and notes\n  Anyway, I love nothing better than to explore the versatility of Google Tag Manager. There\u0026rsquo;s so much you can do with JavaScript, DOM API, and the Google Tag Manager library!\nAll that\u0026rsquo;s required is a bit of know-how with JavaScript and the Document Object Model, a lot of creativity, and a lot of testing, previewing and debugging.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/gtm-sonar-v1-2/",
	"title": "Google Tag Manager Sonar v1.2",
	"tags": ["chrome", "Google Tag Manager", "gtm sonar"],
	"description": "Introducing the GTM Sonar chrome extension, which lets you debug Google Tag Manager&#39;s event tracking.",
	"content": "That\u0026rsquo;s right, I changed the name! Huge thanks to Paul Gailey for the inspiration.\nGet the latest version of GTM Sonar here.\nJust a minor update this time. I added some informative text to the pop-up, along with an error screen if something goes wrong.\n  Another change is that now when an element is added to debugDL, a counter on the Browser Action icon will start climbing, representing the number of objects in the array. Cool!\nSo, my to-do list is really short now. The only thing I have in mind right now is boring stuff like refactoring the code, improving error handling and the such. If you have ideas for the extension, let me know!\nHere\u0026rsquo;s the updated version history:\nv1.2 - June 1, 2014\n  Added a badge icon which increases in count every time an event is pushed into debugDL\n  Improved the pop-up instructions\n  If the extension doesn\u0026rsquo;t work, an error message will be displayed in the pop-up\n  Changed the name of the extension to GTM Sonar (thanks Paul Gailey for the idea!)\n  v1.1 - May 26, 2014\n  Added a popup to control debugger functions\n  Added a switch to kill all jQuery binds\n  Added the choice of Click Listener, Link Click Listener and Form Submit Listener\n  v1.0 - May 23, 2014\n  First release version\n  Clicking the Browser Action halts all click events and sets up the debugDL object\n  Clicking anywhere on the page stores the gtm.click event in debugDL\n  Clicking the Browser Action again removes all scripts and event handlers injected by this extension\n  "
},
{
	"uri": "https://www.simoahava.com/tags/gtm-sonar/",
	"title": "gtm sonar",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/development/",
	"title": "development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/gtm-auto-event-listener-debugger-v1-1/",
	"title": "GTM Auto-Event Listener Debugger v1.1",
	"tags": ["chrome extension", "development", "Google Tag Manager"],
	"description": "Update to the GTM Sonar Chrome extension, which lets you debug Google Tag Manager&#39;s event tracking.",
	"content": "(Last updated June 2014: Read the latest post on the extension, GTM Sonar v1.2.)\nI updated my Chrome Extension, GTM Auto-Event Listener Debugger v1.1. I released the first version a couple of days ago. The extension can be used to debug Google Tag Manager\u0026rsquo;s auto-event tracking and its compatibility with web page markup.\nDownload the latest version here.\nI did some major changes, and here\u0026rsquo;s the rundown.\n  I transferred all debugger actions into a pop-up, which opens when you click the Browser Action. The debugger is still tab-specific, so it can have different states in different tabs. Also, The Browser Action still turns green when the debugger is enabled for current tab.\n  I added a radio button list for you to choose the listener type from. I didn\u0026rsquo;t want to enable multiple listeners at once, because halting the click event had implications for form submits as well. The three listeners are now:\n  - Click Listener : Halts default action of all clicks on page, and upon clicking on the page, pushes gtm.click and other data into `debugDL` - Link Click Listener : Halts default action of all clicks on page, and if clicked element is a link, pushes gtm.linkClick and other data into `debugDL` - Form Submit Listener : Halts default action of all forms on page, and upon submitting a form, pushes gtm.formSubmit and other data into `debugDL`   I added a switch with which you can kill all jQuery binds on the page. The switch is only available if jQuery is found on the page. When you click the switch, all jQuery handlers are cancelled. The idea behind this is that by killing the event handlers, you can verify if jQuery is preventing the listeners from working. I did this simply because more often than not this has been the problem. The switch can be used just once. To get the event handlers working again, you need to reload the page.\n  Basic functionality is still as it was, though I\u0026rsquo;ve made some improvements here and there. It still needs quite a bit of refactoring, as I\u0026rsquo;m pretty sure my JavaScript is just too complex. The visuals could be improved as well, but I\u0026rsquo;d rather have the basic stuff work first and only then start twiddling with the decorations.\n  Screenshots         Information about the version upgrade Be sure to download the extension here, and read the release notes. I\u0026rsquo;ve added a short bit on version history into the release notes (along with more detailed instructions), so you can now follow the development cicle as well.\nThe to-do list now contains the following items (thanks for your feedback!):\n  Improve error handling\n  Show badge icon which represents number of objects in debugDL\n  Show a tooltip or some other visual signal when an event is successfully pushed into debugDL\n  Let me know if you have any improvements in mind.\nHalting the default action of clicks and submissions will not be enough on some pages. With complex sites, there\u0026rsquo;s usually a lot of other JavaScript interfering with this simple script, so no matter what you try to do, the clicks and submits still redirect you. In those cases, the only thing I can suggest for you to do is try to hit ESC or click the STOP button in your browser as soon as you\u0026rsquo;ve performed the action. This should be enough to cancel the action but still enable the data to be pushed into debugDL.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/debugging-auto-event-tracking-gtm/",
	"title": "Debugging Auto-Event Tracking in GTM",
	"tags": ["chrome extension", "development", "Google Tag Manager"],
	"description": "Introducing the GTM Sonar Google Chrome extension, which you can use to debug Google Tag Manager&#39;s event tracking.",
	"content": "(Last updated June 2014: Read the latest post on the extension, GTM Sonar v1.2.)\nMany of the Google Tag Manager articles on this blog could be considered hacks, in that they extend the out-of-the-box features of GTM in ways that will surely not be officially supported by Google. The crux of the problem is that lots of folks are taken by surprise when GTM refuses to work properly on their site, or when they have trouble tracking key elements on the page template.\nAuto-event tracking is a very common denominator when problems with GTM arise. It\u0026rsquo;s a wonderful feature that lets you automatically tag and track actions on the webpage using event listeners, but there are many issues with conflicting JavaScript and shoddy markup that might prevent auto-event tracking from working altogether.\nTo help you in establishing the current state of your template, I give you the GTM Auto-Event Listener Debugger Chrome Extension (name under construction). It should be a welcome tool for you if you want to debug your page template and your on-page elements with automatic click tracking in mind.\n  First things first: download the GTM Auto-Event Listener Debugger here.\nDisclaimer Markup woes should always be resolved with site developers first and a licensed therapist second. Most of the time it\u0026rsquo;s just a question of whether best practices have been observed, such as identifying unique elements with IDs, grouping similar styles together with classes, or having a consistent layout hierarchy. These are all issues that should be fixed in the CMS (or the template), because they facilitate development work on the website in the future as well.\nHowever, there are many reasons why a rapport with the developers can\u0026rsquo;t be established. You might be a hired hand with only GTM and web analytics as your domain. The developers might know squat about web development. The organization might insist upon rigid processes and bureaucracy to get anything done, and that just won\u0026rsquo;t sit with your vision of agile analytics.\nWhatever the case, sometimes you just have to hack your way through the shortcomings of your CMS or your page template. If nothing else, at least you\u0026rsquo;ll have a proof-of-concept in your hands to deliver to the developers.\nInstructions To use the debugger, first make sure you\u0026rsquo;ve enabled it in Tools -\u0026gt; Extensions of your Chrome browser. If it\u0026rsquo;s enabled, you should see a red target icon on your extension pane.\n  The actual steps of using the debugger are simple:\n  Browse to a web page you want to debug\n  Click the red target (it should turn green to signal that the debugger is running)\n  Click anywhere on the page\n  Open JavaScript Console in Chrome, type debugDL, and press Enter\n    When you switch the debugger on, it does two things: 1) it halts the default action of all clicks on the page, 2) when you click an element, it stores its details in the debugDL array in identical format to the GTM listeners.\nWhy prevent default action of clicks? Simple, for debugging reasons it\u0026rsquo;s better if you stay on the same page for testing stuff. Halting the default action prevents links from working. Because debugDL is an object in the current document, it will persist only for the page you\u0026rsquo;re on (similar to how dataLayer works).\nAnd why store the object in debugDL and not dataLayer, for example? Well, I wanted this debugger to not screw up with any existing frameworks, so you can play around freely. Sure, someone else might have come up with the name \u0026ldquo;debugDL\u0026rdquo; in other libraries or extensions, but I\u0026rsquo;ll take my chances.\nWhen you turn the debugger off (by clicking the target again), the default action of click events is returned. The debugDL array is not erased, however, and you can continue to view its contents for as long as you\u0026rsquo;re on the page.\nThe debugging part begins when you start going through debugDL. Every click is registered in identical format to what you\u0026rsquo;d get if you had a GTM Click Listener active.\nWhat\u0026rsquo;s the point? The idea is to test your page template if it works with GTM listeners.\nOr maybe you want to plan ahead and choose the properties of the clicked element (or its parents or siblings) you want to access in your macros.\nOr maybe you want to see if the click events are propagating to the GTM listeners, which operate on the top document node.\nMy favorite reason is this: I do a LOT of debugging for people, and I\u0026rsquo;m not always given access to their preview / debug container. So there\u0026rsquo;s actually no listener firing on the page, and I have to guess by looking at markup and by reverse engineering their JavaScript to identify why click listeners might not work.\nWith this extension, I can debug the site without needing an actual GTM click listener at all.\nTechnical stuff If you know your Chrome Extensions, you\u0026rsquo;ll know that they operate in an isolated environment. What this means in practice is that you can\u0026rsquo;t access variables created by the page in the extension.\nThis has implications for this listener as well, because I want to store data in the window.debugDL array.\nTo circumvent this problem, when you turn the debugger on, it injects a script block into your page template, which creates the object. This script block also primes the click listener on the document node. When you then click anywhere on the page, another injection is done in the form of a push into debugDL.\nInjecting stuff sucks, but it\u0026rsquo;s the only way (as far as I know) to modify variables in the window object through the extension.\nWhen you turn the debugger off, it removes the event listener and all custom script blocks created by the extension.\nNote also that the debugger actions are unique to whichever tab the debugger was activated in. This means that you can have multiple tabs open, each with their own instances of the debugger running or not. Cool, huh?\nWhat\u0026rsquo;s next? Since this is my first foray into Chrome Extensions, there\u0026rsquo;s a lot of fine-tuning to be done. Here\u0026rsquo;s a list of stuff I\u0026rsquo;ve thought about doing in the near future. I\u0026rsquo;ve also requested beta-testing from the wonderful people in the Google+ GTM community, so I hope I\u0026rsquo;ll have more features on the drawing board soon!\n  A toggle to unbind all jQuery click handlers on the site – to test if interfering jQuery is the reason your clicks aren\u0026rsquo;t being registered by auto-event listeners\n  Add a Link Click Listener function as well – to only register clicks on link elements\n  Add a Form Submit Listener function as well – since around 75 % of all listener woes have to do with form submissions, this is really a no-brainer\n  Add other event handlers, such as mouseover, keydown, change – to demonstrate the power of automatically tracking all user-based actions\n  Make it prettier and flashier\n  Think of a better name for the extension\n  "
},
{
	"uri": "https://www.simoahava.com/personal/year-in-review-2013-2014/",
	"title": "Year In Review 2013-2014",
	"tags": ["personal"],
	"description": "Overview of years 2013-2014 in the life of Simo Ahava and this blog.",
	"content": "\u0026hellip;or How My Organic Traffic Went Through The Roof.\nIt\u0026rsquo;s been one of the craziest 365 days in my life, and I thought it would be apt to write a bit about all the stuff that\u0026rsquo;s taken place. I haven\u0026rsquo;t really been talking about myself in my guides and other previous posts, so please indulge me, for once!\nThe beginning A year ago I had just quit my job at my previous employer, Innofactor Plc. When I quit, I\u0026rsquo;d been working as a product owner for Microsoft SharePoint CMS (content management system) components, and I wasn\u0026rsquo;t really seeing any future in it. To be honest, I still don\u0026rsquo;t think \u0026ldquo;Microsoft SharePoint\u0026rdquo; and \u0026ldquo;CMS\u0026rdquo; should ever be uttered in the same sentence, but Innofactor did do some pretty cool stuff that made the relationship between the two a bit smoother.\nAnyway, a good friend of mine called me and told me that there was an SEO job open at the digital marketing agency he was working at. Well, I jumped at the chance because\n  He\u0026rsquo;s a great guy and I wouldn\u0026rsquo;t mind working with him\n  The job would involve more interaction with clients (which I missed)\n  SEO was something I\u0026rsquo;d been doing as a freelancer for a long time, and I didn\u0026rsquo;t mind making it my profession\n  So now I found myself working at NetBooster, a pan-European digital marketing agency. Our Helsinki office is small, but we have a dedicated crew doing their absolute best to deliver more value to our clients.\nAbout a week before my work started, I created this blog at www.simoahava.com. I wanted a channel through which I could write about my experiences in digital marketing.\nMy first posts sucked. They really did.\nThe problem with SEO I very soon noticed a number of problems with SEO, especially for someone working at a digital agency. First of all, no matter how hard we try to be on top of things, reading up on the latest trends, algorithm changes, penalties, recoveries, exposés, etc. it will always be nigh impossible to deliver this knowledge to our clients. Part of the problem is that especially in Finland there\u0026rsquo;s still a lot of \u0026ldquo;old-school\u0026rdquo; SEO being shipped around, where shady agencies are delivering black hat links, 200€-per-month maintenance packages (what the hell can you do with 200€ per month???), and promising stuff to clients that will never bring actual profit or ROI to them.\nTrying to promote content-based strategies instead of mindless ranking tactics to companies still deeply entrenched in the latter is very difficult. It\u0026rsquo;s obvious SEO can\u0026rsquo;t be confined to a silo in today\u0026rsquo;s multi- and omni-channel marketing world, but for some reason we still get heaps and heaps of RFPs where this is exactly what the client wants.\nSEO is really, really, REALLY easy. Just create good quality content regularly, and market it naturally. Stop laughing, it\u0026rsquo;s the truth! If you don\u0026rsquo;t believe me just read Jeff Sauer\u0026rsquo;s excellent post on his SEO views.\nJust for a point of comparison, here\u0026rsquo;s what my organic traffic looks like from day 1 of the blog to today:\n  That\u0026rsquo;s from 0 to almost 7,000 monthly sessions from organic sources. I know, it\u0026rsquo;s not HUGE but I must remind you of the following things:\n  I have never ever built a single link for my blog\n  I have never ever spent a single second on keyword research for my posts\n  A blog focusing mainly on Google Tag Manager is pretty niche\n  I do not promote my posts in social media\n  OK, I\u0026rsquo;ll have to clarify the last bit. I do \u0026ldquo;promote\u0026rdquo; them, in that I use them to start a discussion in Google+. I don\u0026rsquo;t do the promote-once-in-the-morning-then-again-in-the-afternoon-then-again-in-a-week stuff because I\u0026rsquo;m just not comfortable in promoting my own posts. I let others do that for me.\nSo SEO is really simple. I get a lot of content ideas from my blog comments, which is why, I guess, they always seem to answer some search intent.\nDigital analytics If you\u0026rsquo;ve been reading my blog, you\u0026rsquo;ll know it\u0026rsquo;s mostly about web analytics. Google Analytics, to be more precise.\nI don\u0026rsquo;t hide the fact that I think data is the only stable thing in the turbulent world of marketing. Or rather, the only stable strategy in a business where strategies pop up like zits after a weekend of booze and chocolate.\nThe thing I learned during this year of intense interaction with clients in workshops, seminars, consultation projects, and so forth is that there\u0026rsquo;s a LOT of untapped opportunity in how data is utilized.\nBecause I\u0026rsquo;m very developer-minded, the first opportunity is in data collection. It\u0026rsquo;s not enough to just pull page view hits and then observe changes in standard metrics. Remember, Google Analytics is a tool for millions of businesses, and these standard configurations must cater to all these needs and not just the needs of your lumber yard or clothing store.\nIf you want to dig into how data collection can be optimized, I suggest you take a look at what people (including myself) have been writing about tag management.\nHowever, I really enjoy reading stuff by Googlers such as Avinash, Justin, and Daniel, because they get it right.\nThey know it\u0026rsquo;s not just about data, but rather the insights derived thereof and the people, the analysts, doing the interpretive work.\nA smart data collection method will only get you so far. A smart data analyst will take your business to the stars.\nThe people, the wonderful people Another reason this has been a great year is because I think moving to the world of digital marketing and especially digital analytics has been my professional break-through. I\u0026rsquo;ve met so many wonderful people over the months that I can\u0026rsquo;t even keep count.\nMy blog and my activity in Google+ (I love, LOVE that platform!) have brought along all the following awesomeness:\n  More conference invitations than I can keep up with\n  Guest post requests (still haven\u0026rsquo;t had time to do any)\n  Podcast interviews\n  The culmination of my developer ambitions (secret is revealed at the end of this post)\n  And many other things.\nI\u0026rsquo;ve never ever seen a developer / professional community as vibrant as the ones surrounding Google Analytics and Google Tag Manager. I think it\u0026rsquo;s awesome that the Google engineers and product managers themselves work with us in these communities, awaiting input and feature (and bug) requests, and contributing to the discussions.\nThe conferences have been awesome, and I have to single out two which should be on the list of \u0026ldquo;must-visit\u0026rdquo; for all analysts and marketers out there:\nSuperweek (Hungary) - Situated on top of a mountain, with rolling fog and clouds enveloping the hotel grounds, this conference is set in a very The-Shining-esque milieu. Nevertheless, it\u0026rsquo;s still one of the top conferences in digital today. It\u0026rsquo;s going to be organized in January 2015, make sure you don\u0026rsquo;t miss it.\nMeasureCamp (London) - A true unconference for beginners, experts, and enthusiasts in digital analytics. The atmosphere is amazing, and the sessions are truly inspiring (even if they\u0026rsquo;re ad hoc or in a game show format).\nI\u0026rsquo;m working hard to get a strong web analytics / digital marketing community in Finland as well, but this requires time and effort that I can\u0026rsquo;t spare alone. Luckily we\u0026rsquo;ve started doing a lot of little networking events here in Finland, and maybe some day we can boast our own eMetrics, SMX, or Superweek.\nPersonal stuff Personally, it\u0026rsquo;s also been one of the most amazing and massive years of my life.\n  I got married in August to the most wonderful, beautiful, amazing woman in the world. She\u0026rsquo;s my best friend and she means the world to me. Someone who can put up with my all-around geekiness should deserve a medal. We also had a really fun honeymoon doing a crazy road trip in the United States. That\u0026rsquo;s 2 weeks and 3,000 miles of driving, with a loop from New York City - New Orleans - New York City.\nWe also just bought our dream house, and I can\u0026rsquo;t wait for us to move in.\nOn a sadder note, my grandmother of 98 years (!) passed away late April, and this made the very over-worked month of May even more difficult to endure.\nLife has its ups and downs, I guess. Luckily, this past year has been almost overwhelmingly full of positive twists and turns, so I consider myself very fortunate.\nMeet the newest Google Developer Expert I was just added to the Google Developer Expert program as one of the fewer than hundred (at the time of writing) individuals and one of the just four Google Analytics experts, whose job is to evangelize, educate, write, and develop Google\u0026rsquo;s products from a developer perspective. It doesn\u0026rsquo;t make me a Google employee, but it is a sign of appreciation from Google for the work I\u0026rsquo;ve put into making Google Tag Manager more accessible to developers and marketers alike.\nThis is a huge thing for me, since I\u0026rsquo;ve never considered myself a developer. I guess that must change now :)\nThanks to the GDE team at Google for accepting me to the program, and special thanks to Mr. Daniel Waisberg for supporting me throughout the process and Mr. Nico Miceli for the great chat we had around GDE stuff in my first interview.\nThanks also to everyone reading my blog posts, and thanks to the Google Tag Manager team for making such a fun product to write about!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/simple-split-test-google-tag-manager/",
	"title": "Simple Split Test With Google Tag Manager",
	"tags": ["custom html", "Google Tag Manager", "Guide", "macros"],
	"description": "How to write a simple split A/B test using only Google Tag Manager for handling the test, and automatically collecting the data to Google Analytics.",
	"content": "Over the last couple of posts I\u0026rsquo;ve mainly been doing proof-of-concept (POC) tests with Google Tag Manager. The great thing about a POC is that you don\u0026rsquo;t really need to have any viable results or insight-driving technological innovations. The point is to showcase some feature of the platform on which the experiment was conducted.\nIn this post, I\u0026rsquo;ll take a care-free step into the world of POCs again. My goal is to do a simple split test in order to identify which variant of a landing page (or key element thereof) produces the most conversions.\nThis isn\u0026rsquo;t a replacement for any of the REAL A/B testing tools out there. Rather, this is a way for me to showcase yet another way to utilize the wonderful complexity of GTM\u0026rsquo;s JavaScript wizardry.\nAnyway, this is what you\u0026rsquo;ll (hopefully) take with you after reading this post:\n  How to modify a DOM element\n  How to set a cookie\n  How to show a different variation to 50 % of users\n  Combine these and you have a split test in the making!\nThe end result This is how it\u0026rsquo;s going to go down. Exactly 50% of people viewing the experiment page will see the following button:\n  This is also the control variant. It\u0026rsquo;s the one I\u0026rsquo;ve used for years and years to make millions upon millions to my faithful followers (just kidding). However, I want to see if a more nonchalant approach to click baiting would work (and I want to appeal to the younger, less critical demographic), so this is the test variant shown to the other 50%:\n  So I\u0026rsquo;ll be measuring just how many clicks each of these buttons collect. In the end, I should see it in my reports both as events and as a Custom Dimension. Here\u0026rsquo;s what a goal conversion report would look like for the two variations.\n  It\u0026rsquo;s not exactly hard science, but it got the job done. Surprisingly, the control variant was stronger, producing more conversions (who would\u0026rsquo;ve guessed?!).\nSetting up GTM For this experiment, you\u0026rsquo;ll need the following:\n  1st Party Cookie Macro {{Split Variation Cookie}}\n  Random Number Macro {{Random Number}}\n  Custom HTML Tag\n  Custom Dimension\n  Universal Analytics Event Tag\n  1st Party Cookie Macro You\u0026rsquo;ll need a 1st party cookie to make sure the same user sees the same variant every time s/he visits the page. So when the variant is first assigned to the user, this cookie is also written. The cookie will return the value of the variant, which is important for the Event Tag and Custom Dimension as well.\n  Create a new 1st Party Cookie Macro\n  Name it Split Variation Cookie\n  Set Cookie Name to splitVar\n  So something like this:\n  This macro will return the value of cookie splitVar which, as we will soon learn, stores the variant the user should see during the experiment.\nRandom Number Macro I\u0026rsquo;ll use the Random Number Macro to sample 50 % of my visitors. It\u0026rsquo;s a nice trick, and I first saw it on Dan Russell\u0026rsquo;s blog.\n  Create a new Random Number Macro\n  Name it Random Number\n  Here we go:\n  So every time this macro is called, it returns a value between 0 and 2147483647. Can you see how useful this is for sampling? If I check if {{Random Number}} \u0026lt; 0.5*2147483647, it should return true 50 % of the time.\nCustom HTML Tag This is where the magic happens. You\u0026rsquo;ll need a Custom HTML Tag to perform a wide variety of things: modify the DOM element, identify the user\u0026rsquo;s preferred variant, set and read a cookie, etc\u0026hellip;\nLet\u0026rsquo;s start with the code itself. This is what the tag should have. Remember to have it fire on all pages on which the experiment is run.\n\u0026lt;script\u0026gt; function testVariant() { controlElement.style.backgroundColor=\u0026#34;#000099\u0026#34;; controlElement.innerHTML=\u0026#34;**Click here if you want, whatever**\u0026#34;; return variantTwo; } var controlElement = document.getElementById(\u0026#34;call-to-action\u0026#34;), // Set ID of control element  variantOne = \u0026#34;clickDollars\u0026#34;, // Variation 1 name  variantTwo = \u0026#34;clickWhatever\u0026#34;, // Variation 2 name  variant = variantOne, randomNumSample = 1073741824; if(!{{Split Variation Cookie}} \u0026amp;\u0026amp; controlElement) { // If cookie isn\u0026#39;t set run code  if({{Random Number}} \u0026lt; randomNumSample) { variant = testVariant(); // For 50 % of hits, fire Variation 2  } var d = new Date(); // Create cookie  d.setTime(d.getTime()+1000*60*60*24*730); var expires = \u0026#34;expires=\u0026#34;+d.toGMTString(); document.cookie = \u0026#34;splitVar=\u0026#34;+variant+\u0026#34;; \u0026#34;+expires+\u0026#34;; path=/\u0026#34;; } else if({{Split Variation Cookie}} == variantTwo \u0026amp;\u0026amp; controlElement) { // If user has only seen Variation 2  variant = testVariant(); } \u0026lt;/script\u0026gt; Let\u0026rsquo;s go over this code.\nLines 2–6 contain the code which modifies a pre-set DOM element. Thus this function operates the actual split test, showing a different variation of the landing page for 50 % of users. This function returns the name of variation 2.\nLines 8–12 set up some variables for this script to work. Change call-to-action in getElementById(\u0026ldquo;call-to-action\u0026rdquo;); to match the ID attribute of the HTML element you want to dabble with on the page. Change string values for variationOne and variationTwo to match how you want to name or two variants. These values will be used in your GA reports. Finally, the variable randomNumSample contains a numerical value exactly 50 % of the maximum the Random Number Macro can return.\nLines 14–21 first test if you\u0026rsquo;ve already been assigned a variation by checking your cookies. If no cookie is found, you\u0026rsquo;re a new user, and a variation must be assigned to you. In this case, the script randomly assigns you either the control variant (the default, \u0026ldquo;clickDollars\u0026rdquo;) or the test variation (\u0026ldquo;clickWhatever\u0026rdquo;). Next, it writes this information in a cookie which will stay with you for a long time.\nLines 22–24 are executed if the cookie is found. In this case, if the variant that was assigned to you was the test variation, the DOM altering function from the beginning (see lines 2–6) is run again to make sure you always see the same variation of the landing page. If the variant that was assigned to you was the control variation, nothing happens, since that\u0026rsquo;s the default version of the landing page.\nSome things to note. First of all, the cookie is set for two years, so it mimics the _ga cookie. The point is that the user should always see the same variation (provided they\u0026rsquo;re using the same browser). For some split tests this isn\u0026rsquo;t necessary, so you can change the code to forgo the cookie check. Also, all the functions test for undefined values (both of the cookie and the control element), so I didn\u0026rsquo;t see the need to add unnecessary try\u0026hellip;catch blocks or anything else.\nPro tip - Make use of the rarely utilized custom firing schedule for this tag to make sure your experiment only runs a certain time:\n  Custom Dimension You\u0026rsquo;ll need to create a new user-scope Custom Dimension, so that you\u0026rsquo;ll have one additional segment to play around with in your data.\n  Go to Google Analytics admin, and choose the property you\u0026rsquo;ll be tracking these hits to\n  Choose Custom Definitions \u0026gt; Custom Dimensions, and create a new user-scope CD\n  Give it a descriptive name (I named mine just Test variation)\n  Make note of dimension index\n  So something like this:\n  You could also use a session-scope or even a hit-scope dimension, depending on the scope of your test. However, even though I\u0026rsquo;m not a professional CRO practitioner (far from it!), I see the benefit of observing interactions on a multi-visit, user-level rather than the relatively tiny realm of a single visit or hit. However, some web design questions can probably be answered on a more limited scope, I guess.\nUniversal Analytics Event Tag The last bit you\u0026rsquo;ll need is some event that\u0026rsquo;s firing and sending your data to Google Analytics. Because I\u0026rsquo;m observing a button (that I\u0026rsquo;ve encased in an \u0026lt;a href...\u0026gt; element), I\u0026rsquo;m also using a Link Click Listener. If that says nothing to you, be sure to check out my guide on auto-event tracking.\nSo anyway, I\u0026rsquo;ve got a Link Click Listener firing on the test page. What I want to know is just what variant the user was seeing when they clicked the button. This is how I\u0026rsquo;ll count my conversions and determine which variant was the winner. I\u0026rsquo;ll also send the Custom Dimension with the event, so that the user is properly annotated with the variant name. I\u0026rsquo;m sending the CD with the event simply because it\u0026rsquo;s convenient for the scope of this article. You should definitely send it with the pageview, so that you can segment your visitors properly (e.g. out of people who saw the control variant, how many clicked the button; out of people who saw the test variant, how many clicked the button).\nMy Event Tag looks like this:\n  As you can see, I\u0026rsquo;m using the cookie value (\u0026ldquo;clickDollars\u0026rdquo; or \u0026ldquo;clickWhatever\u0026rdquo;) to annotate the event and the Custom Dimension! What a nice way of saving time and resources. So when someone clicks on the default variation, for example, the Event Tag will send something like this to GA servers:\nEvent Category: Call-to-action\nEvent Action: clickDollars\nEvent Label: /call-to-action-page/subscribe/\nDimension 1: clickDollars\nRemember to change the index number of the dimension to match the CD you created in the previous chapter!\nThe firing rule I\u0026rsquo;m using is simply:\n{{event}} equals gtm.linkClick\n{{element id}} equals call-to-action-link\nThis rule will thus only fire when a click occurs on an element with ID \u0026ldquo;call-to-action-link\u0026rdquo;. Conveniently, this just happens to be my control element.\nConclusions Well here it is. A simple split test done solely with GTM and some JavaScript magic. Now let me reiterate: this is hardly the best way to do split testing (we\u0026rsquo;re still waiting for Content Experiments to make their way to GTM + Universal Analytics..) but it does showcase the power and might of a tag management system. I am, after all, directly manipulating an on-page element without touching server-side or front-end code! That\u0026rsquo;s amazing\u0026hellip;and scary.\nBefore you start fiddling with your DOM (wow, that sounded dirty), make sure you know just what you\u0026rsquo;re doing and to which element. This is crucial, because you don\u0026rsquo;t want to accidentally set off a full-scale website redesign without actually, you know, intending to.\nNote also that because you\u0026rsquo;re changing the element in an asynchronously loading tag, it\u0026rsquo;s more than possible that the user will see the control variant before the script overwrites it with the test variant. This isn\u0026rsquo;t a good thing, as if the control group realizes they\u0026rsquo;re participating in an experiment, it will hurt the reliability of the results.\nThere are a couple of nice things to snag from this to your other scripts. Make use of the set-cookie function I wrote in the Custom HTML tag. It\u0026rsquo;s a versatile solution, and I\u0026rsquo;ve actually used it before as well. The random number sampling is simply genius, and I love how easy it is to operate!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/send-mail-upon-google-tag-manager-event/",
	"title": "Send Mail Upon Google Tag Manager Event",
	"tags": ["Google Tag Manager", "Guide", "JavaScript", "php"],
	"description": "Add a simple send mail PHP script to your Google Tag Manager tags, sending email whenever an event occurs.",
	"content": "Let\u0026rsquo;s say you want to set up a rudimentary email alert system in your Google Tag Manager implementation. Say, for example, you want to receive an email every time an uncaught error occurs on your website. It\u0026rsquo;s not a very good use case, since a large website can spawn hundreds of uncaught exceptions in a short period of time, but let\u0026rsquo;s just pretend for now.\nIf you know your JavaScript, you\u0026rsquo;ll know that you can\u0026rsquo;t send mail using client-side code. That\u0026rsquo;s a browser security thing, and I completely understand it. However, a vast majority of websites are created using a CMS which might provide some mail function or hook for you to use. And even if you don\u0026rsquo;t have a cooperative web server (or there\u0026rsquo;s no way you can persuade your developers to help), you could use any one of the online email APIs that are out there.\nIn this post, I\u0026rsquo;ll show two easy ways of getting your mail sent: using the mail() function in PHP, and using Mandrill\u0026rsquo;s API. My use case will be simple: every time an uncaught exception occurs on the website, an email is sent to my address with details about the error. I\u0026rsquo;ll be using the JavaScript Error Listener to catch the error and push it into the mailer.\nWord of warning: The examples in this post are very rudimentary and serve simply to show you what the basics are of sending your emails using GTM. Before you start coding a mailer system of your own, make sure you familiarize yourself with the documentation so that you don\u0026rsquo;t accidentally or unintentionally open a door for mail spammers.\nBefore you start For this to work, you\u0026rsquo;ll need the GTM JavaScript Error Listener up and running, with a couple of macros ready to collect data on the uncaught errors.\nSo first, set up the error listener with a firing rule of your choice. I have it running on all pages, but if you want to monitor just a specific page, feel free to change the rule.\n  Next, you\u0026rsquo;ll need three macros: one for the error message, one for the faulty URL, and one for the line number where the faulty code was found.\n{{Error - MSG}}\nMacro Type: Data Layer Variable\nData Layer Variable Name: gtm.errorMessage\n{{Error - URL}}\nMacro Type: Data Layer Variable\nData Layer Variable Name: gtm.errorUrl\n(NOTE: At the time of writing, this macro will return an empty string. However, I\u0026rsquo;m sure it will be fixed soon. Until then, you can use the default {{url}} macro instead.)\n{{Error - LINE}}\nMacro Type: Data Layer Variable\nData Layer Variable Name: gtm.errorLineNumber\n  And then you\u0026rsquo;re good to go.\nSend mail with PHP If your site has been built with PHP, you can use the built-in mail() function to send your emails. (If your site runs on WordPress, you should also take a look at the wp_mail() function which is a bit more robust!)\nFor this method to work, you\u0026rsquo;ll need two things:\n  A Custom HTML Tag in your GTM setup to send the data payload to the PHP script\n  A PHP script which processes the data and sends it\n  The Custom HTML Tag below uses jQuery to send the POST request, so you\u0026rsquo;ll need to make sure that jQuery is loaded before the Custom HTML is run. I\u0026rsquo;m not a huge fan of jQuery or really any JS framework stacked on top of GTM, but for making POST requests using AJAX, it\u0026rsquo;s a huge help.\nSo anyway, here\u0026rsquo;s what the Custom HTML Tag looks like:\n\u0026lt;script\u0026gt; var data = { errorUrl: {{Error - URL}}, errorLine: {{Error - LINE}}, errorMsg: {{Error - MSG}} }; jQuery.ajax({ type: \u0026#34;POST\u0026#34;, url: \u0026#34;/wp-content/uploads/email.php\u0026#34;, data: data }); \u0026lt;/script\u0026gt; Like I said, it\u0026rsquo;s really bare-bones. You should add some validation code, maybe utilize the success, error and complete callbacks and so forth.\nThe first bit is about building the payload. I\u0026rsquo;m doing that by creating an object literal, which is populated by the values retrieved by the error macros created earlier. So when an error occurs, the object data might have the following key-value pairs:\nerrorUrl: https://www.simoahava.com/\nerrorLine: \u0026ldquo;3\u0026rdquo;\nerrorMsg: \u0026ldquo;Uncaught ReferenceError: myFunction is not defined\u0026rdquo;\nThe next bit is about using jQuery\u0026rsquo;s ajax() function to send the data with a POST request to the custom PHP script. You\u0026rsquo;ll need to specify the location of the script for this to work.\nI wrote a very simple mail script, which you can find below. Save this as email.php and store it in a location you can refer to in the POST request.\n\u0026lt;?php if($_POST){ $message = \u0026#39;Uncaught exception on page \u0026#39; . $_POST[\u0026#39;errorUrl\u0026#39;] . \u0026#39;, line \u0026#39; . $_POST[\u0026#39;errorLine\u0026#39;] . \u0026#39;: \u0026#39; . $_POST[\u0026#39;errorMsg\u0026#39;]; mail(\u0026#34;XYZ@XYZ.com\u0026#34;, \u0026#34;[GTM simoahava.com] Uncaught error\u0026#34;, $message); } ?\u0026gt;  Again, very simple. The PHP basically waits for a POST request, after which it populates a variable $message with the payload data. Next, it utilizes the mail() function to send mail to my email with the subject [GTM simoahava.com] Uncaught error, and the error data in the message body.\nTa-da! You\u0026rsquo;ve just created your email alert system using some simple jQuery and PHP. Here\u0026rsquo;s what a sample email looks like:\n  Send mail with the Mandrill API Mandrill is an online mailing system, which provides a pretty flexible framework for sending mail using their JSON API. I chose it simply because it\u0026rsquo;s free (you have to pay if you send more than 12,000 mails per month using Mandrill), and because it looks pretty simple to set up.\nTo get things up and running, you\u0026rsquo;ll need two things:\n  Mandrill API key\n  A Custom HTML Tag to send the payload data to the API\n  To get the API key, browse to http://mandrill.com/signup/, and sign up for access. Once you log in for the first time, you should click on the \u0026ldquo;Get API Keys\u0026rdquo; button.\n  In the next screen, choose the \u0026ldquo;+ New API Key\u0026rdquo; to generate a new API key for you. You should immediately see your newly generated key in the list below.\n  And that\u0026rsquo;s it for setting up your Mandrill account. What you need to do next is create the Custom HTML Tag which negotiates with the Mandrill API to send your mail. Here\u0026rsquo;s what this tag should look like:\n\u0026lt;script\u0026gt; var data = { \u0026#34;key\u0026#34;: \u0026#34;IVrVuUMfhLFbY97Rjjp1ug\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Uncaught exception on page \u0026#34; + {{Error - URL}} + \u0026#34;, line \u0026#34; + {{Error - LINE}} + \u0026#34;: \u0026#34; + {{Error - MSG}}, \u0026#34;subject\u0026#34;: \u0026#34;[GTM simoahava.com] Uncaught error\u0026#34;, \u0026#34;from_email\u0026#34;: \u0026#34;simo@simoahava.com\u0026#34;OB, \u0026#34;from_name\u0026#34;: \u0026#34;GTM Simo\u0026#34;, \u0026#34;to\u0026#34;: [ { \u0026#34;email\u0026#34;: \u0026#34;simo@simoahava.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Simo Ahava\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;to\u0026#34; } ] }, \u0026#34;async\u0026#34;: true }; jQuery.ajax({ type: \u0026#34;POST\u0026#34;, url: \u0026#34;https://mandrillapp.com/api/1.0/messages/send.json\u0026#34;, dataType: \u0026#34;json\u0026#34;, data: data }); \u0026lt;/script\u0026gt; As you can see, you need to add a bit more information into the payload object to satisfy the requirements of the Mandrill API.\nThe code itself is pretty similar to when we sent the POST request to the PHP script, with the notable exception of having to declare the dataType parameter explicitly as \u0026ldquo;json\u0026rdquo;.\nWhen the mail is sent successfully, you should again see a pretty hideous email in your inbox. By the way, take a look at Mandrill\u0026rsquo;s reports. You\u0026rsquo;ll see some pretty interesting data on your API use, so if you want to build this into a whole email-system-thing, you can use Mandrill\u0026rsquo;s powerful features to diagnose your mail traffic.\n  Conclusions Sending mail via Google Tag Manager doesn\u0026rsquo;t happen with just a click of a button. Because browser security prevents you from sending email directly with JavaScript (i.e. GTM), you\u0026rsquo;ll need to access either server-side methods or some external API. This post provided very rudimentary examples of both scenarios.\nIf you want to build on this idea, be sure to make your code as waterproof as possible. Hijacking someone\u0026rsquo;s mail script is a really easy way to cause a lot of damage in the form of spam. Also, I don\u0026rsquo;t like the fact that you have to provide the Mandrill API key \u0026ldquo;in the open\u0026rdquo;, so you might want to look at Mandrill\u0026rsquo;s PHP client as a way to use Mandrill\u0026rsquo;s API to send the mail server-side.\nAnd before you try anything, the API key I created was just for testing purposes, it no longer exists.\nAnyway, perhaps at some point GTM will support sending mail via GTM\u0026rsquo;s own protocols, which will make this post nice and obsolete, but until then: have fun!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-dom-listener/",
	"title": "Google Tag Manager: DOM Listener",
	"tags": ["extension", "Google Tag Manager", "Guide", "JavaScript"],
	"description": "Create a DOM listener using Google Tag Manager. This can be used to detect dynamic changes in the page layout.",
	"content": "In this post, I\u0026rsquo;ll walk you through a tutorial on how to create a Google Tag Manager extension. This extension will be a new listener, whose sole job is to listen for changes in the HTML and CSS markup on the page.\nThe reason I\u0026rsquo;m teaching you how to create a DOM listener is simple. Ever so often I come across people asking for help, but they are unable to touch on-page code. Usually the problem is magnified with form handlers, since the developers might have installed some custom jQuery form manager, for instance, which simply refuses to cooperate with GTM\u0026rsquo;s form listeners. That is why you might want to fire a GTM event when a certain message pops up on the screen, for example.\nWith a DOM listener, you can fire a GTM event every time something changes on the page. Well, not every time. Actually, in this example you\u0026rsquo;re restricted to element insertion and attribute changes. A working use case might be form validation: if you want to track invalid forms, maybe by sending the content of the validation error message with an event, you might just as well create a DOM listener. This listener will then trigger when an error message appears on the page.\nDISCLAIMER: To be truthful, I feel quite strongly about using hacks such as this to fix faulty markup or an otherwise shoddy tag implementation. The main idea behind this post is to introduce a feature of JavaScript which can alleviate some of your tag management pains. However, if you find that you need hacks such as the DOM listener to circumvent development problems on your website, I would strongly suggest that you take this up with your developers and try to come up with a solution which works with GTM\u0026rsquo;s standard features.\nThe premise To create the DOM listener, I will leverage an API known as MutationObserver. This little piece of magic will create an observer object, which triggers every time a mutation takes place. This mutation can come in different sizes and shapes, but for the purposes of this guide, I will listen for two kinds of mutations, for two kinds of use cases:\n  Node insertion - when a new \u0026lt;span class=\u0026quot;error\u0026quot;\u0026gt; is injected in the DOM\n  CSS style change - when a previously hidden \u0026lt;span class=\u0026quot;error\u0026quot;\u0026gt; is displayed\n  So the first use case is when your form injects a new SPAN element into the DOM upon an error. The script will check if the injected node is really the error message, and if it is, it pushes a dataLayer event with the content of the SPAN (the message itself).\nThe second use case is when an invalid form causes a hidden SPAN to appear, with the error message within.\nListening for node insertion is a bit different than listening to an attribute change. A node insertion listener can be primed on any node on the DOM, meaning you have much more to work with in terms of flexibility. Listening for attribute changes requires you to pinpoint exactly which node you want to observe, and the attribute change will be reported for that node only.\nPreparations Here are the ingredients:\n  A page where a \u0026lt;span class=\u0026quot;error\u0026quot;\u0026gt; is either inserted or revealed with a CSS style change\n  Custom HTML tag(s)\n  My test page code looks like this (I combined both use cases into one here):\n\u0026lt;script\u0026gt; function addHTML() { var myDiv = document.getElementById(\u0026#34;contents\u0026#34;); var newSpan = document.createElement(\u0026#34;span\u0026#34;); newSpan.className = \u0026#34;error\u0026#34;; newSpan.innerHTML = \u0026#34;This form contained errors\u0026#34;; myDiv.appendChild(newSpan); } function changeCSS() { var mySpan = document.getElementsByClassName(\u0026#34;error\u0026#34;)[0]; mySpan.style.display = \u0026#34;block\u0026#34;; } \u0026lt;/script\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; onClick=\u0026#34;addHTML();\u0026#34;\u0026gt;Add span with .innerHTML\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; onClick=\u0026#34;changeCSS();\u0026#34;\u0026gt;Add span with CSS\u0026lt;/a\u0026gt; \u0026lt;div id=\u0026#34;contents\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;error\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt;This form contained errors\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; Let\u0026rsquo;s just quickly go over this page.\nFirst, you have two functions. The function addHTML() will insert the following in the DIV#contents below:\n\u0026lt;span class=\u0026quot;error\u0026quot;\u0026gt;This form contained errors\u0026lt;/span\u0026gt;\nThe insertion is done upon clicking a link whose text is \u0026ldquo;Add span with .innerHTML\u0026rdquo;.\nIn the second function, the SPAN.error is already in the DOM, but it\u0026rsquo;s been hidden with a display: none CSS command. When the link labelled \u0026ldquo;Add span with CSS\u0026rdquo; is clicked, this style directive will be changed to display: block.\n  This is just my test page. Obviously you\u0026rsquo;ll need to navigate around your current implementation to make things work.\nFinally, you\u0026rsquo;ll need your custom HTML magic. The next two chapters will go over the tags and the code you\u0026rsquo;ll need to write for them.\nCase 1: node insertion In this use case, a new node (\u0026lt;span class=\u0026quot;error\u0026quot;\u0026gt;) is inserted into a DIV on the page. The markup on the page looks something like this:\n\u0026lt;script\u0026gt; function addHTML() { var myDiv = document.getElementById(\u0026#34;contents\u0026#34;); var newSpan = document.createElement(\u0026#34;span\u0026#34;); newSpan.className = \u0026#34;error\u0026#34;; newSpan.innerHTML = \u0026#34;This form contained errors\u0026#34;; myDiv.appendChild(newSpan); } \u0026lt;/script\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; onClick=\u0026#34;addHTML();\u0026#34;\u0026gt;Add span with .innerHTML\u0026lt;/a\u0026gt; \u0026lt;div id=\u0026#34;contents\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; So as you can see, there\u0026rsquo;s an empty DIV (#contents), which is then appended with a new SPAN, created by the function addHTML() which, in turn, waits for a click event on the link on the page.\nNow, the next step is to create the listener itself. You\u0026rsquo;ll need to use the MutationObserver API to listen for any node which is inserted into the observed target. The node itself can be on any level in the DOM hierarchy, but I chose the DIV#contents to keep things simple. When a node is inserted, a dataLayer push is done with a new GTM event and the text content of the SPAN.\nHere\u0026rsquo;s what you Custom HTML Tag should look like:\n\u0026lt;script\u0026gt; var contentDiv = document.querySelector(\u0026#34;#contents\u0026#34;); var MutationObserver = window.MutationObserver || window.WebKitMutationObserver; var observer = new MutationObserver(function(mutations) { mutations.forEach(function(mutation) { if(mutation.type===\u0026#34;childList\u0026#34; \u0026amp;\u0026amp; mutation.addedNodes[0].className===\u0026#34;error\u0026#34;) { dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;newErrorSpan\u0026#39;, \u0026#39;spanErrorMessage\u0026#39;: mutation.addedNodes[0].innerHTML}); observer.disconnect(); } }); }); var config = { attributes: true, childList: true, characterData: true } observer.observe(contentDiv, config); \u0026lt;/script\u0026gt; OK, OK, lots of stuff going on here. Let\u0026rsquo;s go through the script and see what it does. First, you create a reference to the DIV you will be listening to. I\u0026rsquo;m using the querySelector() function, because it\u0026rsquo;s a nice way to combine CSS selectors and JavaScript.\nNext, you create the MutationObserver itself by first tackling a known cross-browser issue with WebKit browsers. The observer listens for mutations of type childList (a new child node is inserted) and checks if the first added node has CSS class \u0026ldquo;error\u0026rdquo;. You\u0026rsquo;ll have to modify this code if the SPAN with the error is not the first node that your script inserts into the DOM.\nIf such a mutation is detected, a GTM event is pushed into dataLayer (newErrorSpan), and the error message contents are sent as a data layer variable as well. Note that I use innerHTML to get the contents of the SPAN. If your SPAN has HTML formatting within, you might want to use innerText instead.\nThe disconnection means that after this particular mutation takes place, no further mutations are listened for. So if someone keeps on pushing the \u0026ldquo;submit\u0026rdquo; button, the observer will shut down after the first error. You might want to remove this line if you want to track ALL the potential errors your visitor propagates on the form.\nLastly, I create a configuration for the MutationObserver and prime it on the DIV.\nAnd that\u0026rsquo;s it for node insertion. If you set this new listener to fire on pages where the SPAN with class \u0026ldquo;error\u0026rdquo; is created in a DIV with ID \u0026ldquo;contents\u0026rdquo;, a dataLayer.push() will take place every time the SPAN is inserted into the DOM. Try it for yourself!\n  Case 2: CSS change In this case, you have a hidden SPAN with the error message, which is then revealed when the form validation fails. Here\u0026rsquo;s what the HTML might look like:\n\u0026lt;script\u0026gt; function changeCSS() { var mySpan = document.querySelector(\u0026#34;.error\u0026#34;); mySpan.style.display = \u0026#34;block\u0026#34;; } \u0026lt;/script\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; onClick=\u0026#34;changeCSS();\u0026#34;\u0026gt;Add span with CSS\u0026lt;/a\u0026gt; \u0026lt;div id=\u0026#34;contents\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;error\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt;This form contained errors\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; So I have a simple SPAN within a DIV with the error message. This is initially set to display: none, but when the link is clicked, the display status is changed to block.\nAs for your Custom HTML Tag, you\u0026rsquo;ll need something like this:\n\u0026lt;script\u0026gt; var spanError = document.querySelector(\u0026#39;.error\u0026#39;); var MutationObserver = window.MutationObserver || window.WebKitMutationObserver; var observer = new MutationObserver(function(mutations) { mutations.forEach(function(mutation) { if (mutation.type===\u0026#34;attributes\u0026#34; \u0026amp;\u0026amp; mutation.target.style.display===\u0026#34;block\u0026#34;) { dataLayer.push({\u0026#39;error\u0026#39;: \u0026#39;modErrorSpan\u0026#39;, \u0026#39;spanErrorMessage\u0026#39;: mutation.target.innerHTML}); observer.disconnect(); } }); }); var config = { attributes: true, childList: true, characterData: true } observer.observe(spanError, config); \u0026lt;/script\u0026gt; It\u0026rsquo;s pretty similar to the previous one but with one or two small changes. First, you\u0026rsquo;re not listening to the DIV, you\u0026rsquo;re listening to the actual node you know will be the target of the style change. This is important, and it means that you have to know exactly what the target will be when creating this script.\nNext, in the observer itself, you\u0026rsquo;ll need to specify just what the style change was. I used simply a change from display: none to display: block, but you might have something different in your code. So don\u0026rsquo;t forget to change the content of the if-clause to match what the new style is.\nThe benefit here is that you\u0026rsquo;re listening to just one single node, so there\u0026rsquo;s a performance boost. I\u0026rsquo;ve got the observer.disconnect(); again, but you might want to remove that if you want to send events perpetually for each invalid click of the submit button.\nDon\u0026rsquo;t forget to test.\n  Conclusions This might seem like a cool hack to you. After all, you\u0026rsquo;re listening for changes on the page without actually any page refresh happening! Also, you\u0026rsquo;re extending GTM\u0026rsquo;s default listeners so you\u0026rsquo;re kind of like a Google engineer, right?\nWell, remember what I said in the disclaimer of this text. This is a hack, a circumvention, a band-aid, designed to overcome problems with your markup or your JavaScript. Having a custom form handler which doesn\u0026rsquo;t propagate a proper form submit event (which is required by GTM\u0026rsquo;s default form submit listener) is a bit suspect and reeks of bad practices. So before you resort to this DOM listener, make sure you exhaust all other, more orthodox possibilities with your developers.\nThen again, you might not need this to overcome any development problems, but rather to complement your current tag setup. In that case, go crazy! It\u0026rsquo;s an excellent way to add more flexibility to your tags. Do note the cross-browser support, however. Support is not comprehensive enough to warrant using this listener as a firing rule for some business-critical tag.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/accuracy-test-gtm-default-events/",
	"title": "Accuracy Test Of GTM Default Events",
	"tags": ["firing rules", "Google Tag Manager", "test"],
	"description": "A test to check how long it typically takes for DOM Ready and Window Loaded triggers to fire after GTM has loaded. This test was run on my site.",
	"content": "If you know your Google Tag Manager, you know that GTM pushes three data layer events into the queue when any page with the container snippet is rendered. Each of these three events signals a specific stage in the page load process. Here are the events (be sure to read my guide on GTM rules to understand further what these events do):\n  gtm.js - This is pushed into the data layer as soon as GTM is initialized and the container is loaded. This is also the default event for all rules without an explicit event macro rule as a condition. Basically, if you want something to happen at the earliest possible moment, you need to have {{event}} equals gtm.js as the rule\n  gtm.dom - When the DOM has been populated with on-page elements, this event is pushed into the data layer. If you have HTML elements or dependent JavaScript snippets loaded at the very bottom of the page template, having your tag fire upon {{event}} equals gtm.dom will ensure that these latecomers can be used in your tags\n  gtm.load - Once the window has finished loading, along with all images, scripts, and other assets, gtm.load is pushed into the data layer. If you have scripts or DOM elements that take a long while to load, and you want to be 100 % sure that they have loaded before your tags fire, using {{event}} equals gtm.load as a firing rule for your tag might be wise\n    Now, having said that, I wanted to test just how accurate gtm.dom and gtm.load are as trigger events. If I were to have my most important tag, GA page tracking, fire upon either one, just how many hits will I miss compared to the default {{url}} matches RegEx .* rule?\nI know there will be some losses in accuracy, because any delay in firing a tag increases the risk of the person viewing the page clicking a link or closing the browser before the tag has had a chance to fire. But just how much data is actually lost?\nResults in brief: If you don\u0026rsquo;t want to go through the rest of the article and are just interested in results, here\u0026rsquo;s what I found. Using gtm.dom as the trigger is almost as reliable as using gtm.js. With gtm.load, you\u0026rsquo;ll see far more missed hits, but it might still be within an error margin you find acceptable. However, it is important to remember that the actual results will vary depending on your DOM and page load times. If you have a complex page template with a lot of dynamically created content, huge images, lots of external assets, etc., you\u0026rsquo;ll see a higher error rate than with my humble blog.\nThe premise Here\u0026rsquo;s how I set up the test:\n  I used my own blog as the guinea pig. I wanted an actual \u0026ldquo;live\u0026rdquo; environment to test with, and my blog is a pretty good example of a standard GTM setup\n  For exactly 28 days, I had two non-interaction events firing: one upon {{event}} equals gtm.dom and one upon {{event}} equals gtm.load\n  After 28 days, I could compare the number of events to page views to get the number of hits I\u0026rsquo;d miss if I chose gtm.dom or gtm.load over the default gtm.js\n  The test time was from the beginning of Wednesday, 12 March 2014 to the end of Tuesday, 8 April 2014.\nSome details about my setup:\n  (By the way, I\u0026rsquo;m renaming my TMRs (tags, macros, rules) at some convenient point in the near future, so don\u0026rsquo;t read too much into my current naming schema.)\nThere\u0026rsquo;s a \u0026ldquo;Dwell and scroll\u0026rdquo; tag, which starts to work its magic upon gtm.dom. Basically, it waits 30 seconds, looks for a scroll action by the visitor, and if both the timeout and a scroll have taken place, it sends a bounce-rate-killing event to GA.\nThere\u0026rsquo;s also a tag for my weather script. This is pretty expensive in terms of performance, since it makes two external API calls. However, it only fires during the first page view of a session, and it initiates with gtm.js. The more expensive weather API call is also done asynchronously.\nFinally, there\u0026rsquo;s my page load time script, set to fire on gtm.load, some event pushes and my listeners.\nMy tag setup is really lightweight. There shouldn\u0026rsquo;t be any major reason why my tags would cause gtm.dom or gtm.load to be delayed, unless the weather scripts starts to timeout in the external resource calls.\nIn my GA account, I don\u0026rsquo;t filter out my own hits; actually, I don\u0026rsquo;t have a single filter on my blog profile. I know, you probably have a big, nasty look of disgust on your face right now. But you know what, I never thought I\u0026rsquo;d get enough traffic to care, and now that I do, I still don\u0026rsquo;t really care. Furthermore, I find it difficult to move to a new, filtered profile, since I don\u0026rsquo;t have any historical data. OK. Stop chucking that lettuce at me. I\u0026rsquo;ll go and create a filtered profile right now!\nThe results - page views Here\u0026rsquo;s what I found out:\n  Total page views: 12,167\n  Total gtm.dom events: 12,115 (-52, 99,6 %)\n  Total gtm.load events: 11,945 (-222, 98,2 %)\n  Well, that\u0026rsquo;s pretty good! Based on this result, I wouldn\u0026rsquo;t hesitate to recommend you to use {{event}} equals gtm.dom if you have even the slightest concern that some vital data in the DOM is required in your tags. Also, gtm.load does pretty well, though I do believe that a near 2 percent error rate might be too much for some large eCommerce sites. My site is very lightweight, so a more complex and flashy site with a significantly longer average page load time will surely have more missed gtm.load hits.\nHowever, I had to probe further. If you remember, I had a couple of other events firing on every page view as well. Because of this, I\u0026rsquo;d like to take a look at visits to see if there\u0026rsquo;s some discrepancies between page views sent and visits recorded.\nThe results - visits I performed this analysis by segmenting out visits without a single gtm.dom or gtm.load test event. Here\u0026rsquo;s what I found:\n  Hold on\u0026hellip; what?\nAlmost 4 percent of all visits occurred without a single gtm.dom or gtm.load test event. So, I must have visits without a single page view, because the number of visits without these GTM events exceeds the number of pageviews without them. And yes, this confirms my suspicions:\n  So here\u0026rsquo;s the deal: I have a bunch of visits without a single gtm.dom or gtm.load event being fired, and almost 85 % of these visits don\u0026rsquo;t have a landing page, i.e. a single page view hasn\u0026rsquo;t been sent.\nInteresting.\nWell, when I look at the event catalog for these \u0026ldquo;ghost visits\u0026rdquo;, I see a bunch of my adjusted bounce rate events and my weather events.\n  The interesting thing (not visible in these tables) is that my adjusted bounce rate event actually has more total events than unique events, which would mean that these visits had multiple page loads which didn\u0026rsquo;t send an actual page view to Google Analytics! How screwed up is that?\nAlso, because my weather script did fire on a number of occasions, and still my test events weren\u0026rsquo;t pushed, I\u0026rsquo;ll have to believe that something interfered with my test events. Remember, my \u0026ldquo;NoBounce\u0026rdquo; event waits 30 seconds before firing a hit AND it waits for gtm.dom before initializing. This couldn\u0026rsquo;t be just a case of gtm.dom and gtm.load not being pushed into the data layer. This was clearly a case of my test scripts just refusing to fire!\nRemember also, I don\u0026rsquo;t have any filters on my profile, so I\u0026rsquo;m not filtering out page views and just seeing the events. Just over 3 percent of all my visits are completely page-view-less!\nThis is weird, but I\u0026rsquo;ll just chalk it up to an error margin associated with increased granularity in measurement. I know I shouldn\u0026rsquo;t be picking on micro-level phenomena such as this, but it still makes me wonder. Are page-view-less visits thanks to some configuration I have in GTM, or should they be attributed to the visitor?\nBy the way, I looked through every single report in GA, and they didn\u0026rsquo;t reveal anything out of the ordinary. It would be interesting to pursue this further, but for the purposes of this test, this is all more just a fascinating detail than anything that you or I can learn from.\nConclusions Apart from the weirdness with the page-view-less visits, I\u0026rsquo;m still comfortable in recommending using {{event}} equals gtm.dom for all your tags. If you want to use gtm.load as the trigger, you\u0026rsquo;ll have to be aware that you will lose a lot more hits, even if the rate is still around just 2 percent. But that\u0026rsquo;s just with my lightweight setup.\nWhether or not race conditions had anything to do with missed hits, since my adjusted bounce rate script also fires on gtm.dom, I don\u0026rsquo;t know. A huge site with dozens of tags all firing on the same triggers might exhibit more variation in how accurate gtm.dom and gtm.load are as firing rules.\nTo play it safe, I still recommend having all your critical, independent tags firing as early as possible, i.e. after gtm.js has been pushed into the data layer. However, there\u0026rsquo;s no reason not not to use gtm.dom and gtm.load as trigger rules as well. You\u0026rsquo;ll just have to be aware that you might be missing some hits.\n"
},
{
	"uri": "https://www.simoahava.com/tags/firing-rules/",
	"title": "firing rules",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/test/",
	"title": "test",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/form-tracking-google-tag-manager/",
	"title": "Advanced Form Tracking In Google Tag Manager",
	"tags": ["forms", "Google Tag Manager", "Guide", "macros"],
	"description": "How to track web forms using Google Tag Manager.",
	"content": "There is a new, updated version of this article for the new version of Google Tag Manager. I strongly suggest you read that as well!\nI really enjoy the ad hoc Q\u0026amp;A sessions my blog posts have inspired. I haven\u0026rsquo;t said this enough, but I am really, REALLY grateful to people who take their time to comment on my posts, even if it\u0026rsquo;s just say a quick \u0026ldquo;Hi!\u0026rdquo;. The main reason I enjoy getting blog comments is because they often turn into blog posts. Seriously, if you\u0026rsquo;re a blogger suffering from writer\u0026rsquo;s block, take a look at your comments. There\u0026rsquo;s a wealth of content ideas right there.\nI\u0026rsquo;m often asked about form tracking and Google Tag Manager. This is a recurring theme in the Google+ GTM community as well. Part of me (the pretentious part) wants to credit this to the fact that I neglected to add a section for the Form Submit Listener tag in my auto-event tracking post. This is when the rational me kicks in and says that it\u0026rsquo;s actually because tracking forms is pretty darn difficult. There are so many things that can go wrong, so many variables to look at, so many different things to track, so many different forms on a single page, so many different technologies creating the forms, so many\u0026hellip; you get the drift.\nSo this guide is meant to cover some of the more advanced use cases for form tracking in GTM, while still providing relevant examples of GTM macros and rules to make the tips actionable.\n1. Form tracking: the basics To track a form in Google Tag Manager, your best bet is to create a Form Submit Listener tag. This will then listen for form submit events (provided that they aren\u0026rsquo;t prevented by conflicting scripts), and push an {{event}} equals gtm.formSubmit event into the data layer for you to work with.\nIf you\u0026rsquo;re feeling a little confused, you should first read up on the basics: auto-event tracking, macros, and rules. Justin Cutroni has an excellent post on auto-event tracking, and it covers form tracking basics as well.\nSo let\u0026rsquo;s get started. Before we do anything, let\u0026rsquo;s create the Form Submit Listener tag.\n  Create a new tag of type Form Submit Listener\n  Set the firing rule to fire on all pages\n    Now, you have some room to improvise here. First of all, it isn\u0026rsquo;t at all necessary to have the tag fire on all pages. If you want to optimize things, you could only have it fire on pages with forms.\nAlso, as you can see there are two checkboxes that you can check, if you so choose:\n  Wait for tags - This introduces a timeout of X milliseconds (2000 by default) to the submit event. This is to allow all tags that use the form submit event as a trigger to fire. If the timeout elapses before the dependent tags have fired, the form submit event will complete, and there\u0026rsquo;s a risk that you\u0026rsquo;ll miss some hits. Still, I do not recommend that you increase this timeout, because if your dependent tags don\u0026rsquo;t fire within 2 seconds, the fault is with them, not with GTM or your form.\n  Check for validation - Basically, if the form doesn\u0026rsquo;t validate (i.e. the submission event is interrupted by a validation function), the tag will not fire. This would be smart to check, unless you know there are event propagation issues involved, such as when a custom submit handler is used.\n  The Form Submit Listener waits for a form submit event on the page. Once a form is successfully submitted, a GTM data layer event gtm.formSubmit is pushed into the data layer. If you have the \u0026ldquo;Wait for tags\u0026rdquo; option checked, any tags using this event as a trigger will be first fired (if they fire within the delay time specified), and only then will the submission be completed. After this, you can use the rule {{event}} equals gtm.formSubmit for any tags that should be fired when a form is submitted.\nSo this is the starting point for our journey. A lonely form submit listener, its meager existence reduced to waiting for a form to submit, ready to fulfill the duties it was assigned by a sadistic GTM admin.\n2. Useful macros As always, a sleek and functional GTM setup is dependent on how you use macros. For the purposes of this post, here\u0026rsquo;s a bunch of macros you\u0026rsquo;ll find useful.\n{{element}}\nMacro Type: Auto-Event Variable\nVariable Type: Element\nDescription: Captures the submitted form object\n{{element id}}\nMacro Type: Auto-Event Variable\nVariable Type: Element ID\nDescription: Captures the ID attribute value of the submitted from\n{{element url}}\nMacro Type: Auto-Event Variable\nVariable Type: Element URL\nDescription: Captures the ACTION attribute value of the submitted form\n{{field value}}\nMacro Type: Custom JavaScript\nJavaScript:\nfunction() { var inputField = document.getElementById(\u0026#34;myFormField\u0026#34;); return inputField.value || \u0026#34;\u0026#34;; }  Description: Returns the value of the element whose ID is myFormField (change this to your liking)\n{{selected item}}\nMacro Type: Custom JavaScript\nJavaScript:\nfunction() { var selectId = document.getElementById(\u0026#34;mySelectList\u0026#34;); try { var options = selectId.options; for (var i = 0;i \u0026lt; options.length;i++){ if(options[i].selected) { return options[i].value; } } } catch(e) {} return \u0026#34;\u0026#34;; }  Description: Returns the VALUE of the selected item in the select list whose ID you specify with the variable selectId\n{{checked button}}\nMacro Type: Custom JavaScript\nJavaScript:\nfunction() { var radioName = \u0026#34;myRadioButtons\u0026#34;; try { var buttons = document.getElementsByName(radioName); for (var i = 0;i \u0026lt; buttons.length;i++){ if(buttons[i].checked) { return buttons[i].value; } } } catch(e) {} return \u0026#34;\u0026#34;; }  Description: Returns the VALUE of the checked element out of the radio buttons whose NAME attribute is specified in the variable radioName\n{{string of checked buttons}}\nMacro Type: Custom JavaScript\nJavaScript:\nfunction() { var inputs = document.getElementsByTagName(\u0026#34;input\u0026#34;), selectedRadios = \u0026#34;\u0026#34;; for (var i = 0;i \u0026lt; inputs.length;i++) { if(inputs[i].type===\u0026#34;radio\u0026#34; \u0026amp;\u0026amp; inputs[i].checked) { selectedRadios += inputs[i].value +\u0026#34; \u0026#34;; } } return selectedRadios.replace(/\\s$/,\u0026#34;\u0026#34;); }  Description: Returns a concatenated string of all the values of checked radio buttons on the page\n{{array of checked buttons}}\nMacro Type: Custom JavaScript\nJavaScript:\nfunction() { var inputs = document.getElementsByTagName(\u0026#34;input\u0026#34;), selectedRadios = []; for (var i = 0;i \u0026lt; inputs.length;i++) { if(inputs[i].type===\u0026#34;radio\u0026#34; \u0026amp;\u0026amp; inputs[i].checked) { selectedRadios.push(inputs[i].value); } } return selectedRadios; }  Description: Returns an array of all the values of checked radio buttons on the page\nYou can apply the previous two macros to checkboxes as well by changing inputs[i].type===\u0026quot;radio\u0026rdquo; to inputs[i].type===\u0026quot;checkbox\u0026rdquo;.\n3. Capture field value Often you have a situation where you want to send the value of some field with your Google Analytics hit. Perhaps you want to segment your events between people who submitted value1 and those who submitted value2. Or you might want to use a specific field value as a firing or blocking rule for your other tags.\n  Whatever the case, here\u0026rsquo;s how you retrieve the value from a specific field in your form. Remember, you are not allowed to send personally identifiable information such as names, e-mail addresses or phone numbers to Google Analytics!.\nThe easy method:\nThe easy way to do it is to create the {{field value}} macro I introduced in the previous chapter. This requires that the field for which you want to retrieve the value has an ID attribute, e.g. \u0026lt;input type=\u0026quot;text\u0026quot; id=\u0026quot;lunch\u0026quot;\u0026gt;.\nYou\u0026rsquo;d then add this macro to your Event tag (or rule or pageview tag or similar), and when the tag fires upon {{event}} equals gtm.formSubmit, the macro will return the value of the form element whose ID you specified earlier.\nThe PRO method:\nIf you have many forms, each with a unique field (with a unique ID) that you want to track, you don\u0026rsquo;t need to create a separate macro for each form. Just change the variable declaration in the macro to var inputField = document.getElementById({{field ID lookup}});. Next, create a new Lookup Table macro, where you return the ID of the field whose value you want to retrieve, depending on if the user is on a specific page, if the form has a specific ID or something similar. Here\u0026rsquo;s an example, where the field ID depends on the form element ID:\n  If you don\u0026rsquo;t have an ID attribute to access, you\u0026rsquo;ll need to get creative. Usually this also means that you\u0026rsquo;ll need to resort to error-prone scripts, so I really suggest that you get your developers to introduce ID attributes to your fields. However, here are some ideas (these replace the variable declaration in the original {{field value}} macro). Note that the [0] after the functions refers to the first index of the array. So if you want the second input element, you\u0026rsquo;d use [1], for the third you\u0026rsquo;d use [2] and so on.\nGet first input element of submitted form:\nvar inputField = {{element}}.getElementsByTagName(\u0026quot;input\u0026quot;)[0];\nGet first input element with \u0026ldquo;myInput\u0026rdquo; as the NAME attribute (\u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;myInput\u0026quot;\u0026gt;):\nvar inputField = document.getElementsByName(\u0026quot;myInput\u0026quot;)[0];\nGet first input element with \u0026ldquo;myInput\u0026rdquo; as the CLASS attribute (\u0026lt;input type=\u0026quot;text\u0026quot; class=\u0026quot;myInput\u0026quot;\u0026gt;):\nvar inputField = document.getElementsByClassName(\u0026quot;myInput\u0026quot;)[0];\nAnd so on. Feel free to get creative with JavaScript.\n4. Capture drop-down list value To get the selected value of a drop-down list (SELECT element), you\u0026rsquo;ll need to use a macro which goes through all the OPTION elements in your SELECT list, and returns the one that is selected. The macro used here was introduced earlier in this post ({{selected item}}).\n  The easy method:\nThe easy method, again, utilizes the ID attribute of the SELECT element. For example, you might have a list such as:\n\u0026lt;select id=\u0026#34;myList\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;home\u0026#34;\u0026gt;Home\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;work\u0026#34;\u0026gt;Work\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; You\u0026rsquo;d change the variable declaration in the {{selected item}} macro to var selectId = document.getElementById(\u0026ldquo;myList\u0026rdquo;);. Next, the script goes through all the OPTION elements within this SELECT structure, and it returns the value of the option that was selected.\nThe PRO method:\nNaturally, you can apply the same Lookup Table magic as with the field value method above. If your SELECT element doesn\u0026rsquo;t have an ID attribute, you\u0026rsquo;ll need to get creative. Here are some ideas for the variable declaration.\nAccess the first SELECT list on the page:\nvar selectId = document.getElementsByTagName(\u0026quot;select\u0026quot;)[0];\nAccess the first SELECT list of the submitted form:\nvar selectId = {{element}}.getElementsByTagName(\u0026quot;select\u0026quot;)[0];\n5. Capture selected radio button value(s) A very solid case for value retrieval is when you have a choice of radio buttons. For example, in the example below, you might want to fire a different tag (with different custom dimensions) depending on whether the user chose Home or Health insurance.\n  The trick with the radio button element is a consistent (and unique) use of the NAME attribute. If you specify each group of radio buttons with their own, unique NAME attribute, this script will work beautifully.\nThe easy method:\nUse the {{checked button}} from before, and substitute \u0026ldquo;myRadioButtons\u0026rdquo; with the NAME attribute value of the group of radio buttons you want to analyze.\nThis script goes through all the radio buttons with the given NAME attribute and returns the value of the button that is checked. As you can see, this will only work if your button groups have unique NAME attributes.\nThe PRO method:\nSometimes you have several radio button groups on your form, and you want to send or process the selected buttons from each of these in a single tag. Here\u0026rsquo;s how:\nConcatenate a string of all the selected radio button values (Custom JavaScript macro)\nfunction() { var inputs = document.getElementsByTagName(\u0026#34;input\u0026#34;), selectedRadios = \u0026#34;\u0026#34;; for (var i = 0;i \u0026lt; inputs.length;i++) { if(inputs[i].type===\u0026#34;radio\u0026#34; \u0026amp;\u0026amp; inputs[i].checked) { selectedRadios += inputs[i].value +\u0026#34; \u0026#34;; } } return selectedRadios.replace(/\\s$/,\u0026#34;\u0026#34;); }  This script goes through ALL the radio buttons on the form, and returns a concatenated string of the checked button values, e.g. \u0026ldquo;male 30 home\u0026rdquo; for three radio button groups.\nSave the values in an object for later processing (Custom JavaScript macro)\nfunction() { var inputs = document.getElementsByTagName(\u0026#34;input\u0026#34;), selectedRadios = []; for (var i = 0;i \u0026lt; inputs.length;i++) { if(inputs[i].type===\u0026#34;radio\u0026#34; \u0026amp;\u0026amp; inputs[i].checked) { selectedRadios.push(inputs[i].value); } } return selectedRadios; }  This returns an array of values, grabbed from each checked radio button found on the page. You can then process this array in another macro (or this same one), or do other magic to it. I prefer this over the concatenated string method, because you have more to work with when you utilize JavaScript array structures.\n6. Capture selected checkbox value(s) The trick with checkboxes is that you almost always have multiple, checked values. This means that if you want to send information in your event on what boxes were checked, you\u0026rsquo;ll always have to resort to methods such as those introduced above for the radio button (the PRO method). So to retrieve the values of your checked checkboxes, use the script(s) from above, but instead of\nif(inputs[i].type===\u0026quot;radio\u0026quot;...)\nyou\u0026rsquo;ll need to use\nif(inputs[i].type===\u0026quot;checkbox\u0026quot;...)\nto access your checkboxes.\nThe other method:\nHow about if you just want to make a value lookup? For example, if you wanted to fire a tag if the form includes the opt-in for spam\u0026hellip; I mean the occasional promotional e-mail?\n  In this case, you\u0026rsquo;d create a Custom JavaScript macro that looks like this:\nfunction() { var checkbox = document.getElementById(\u0026#34;spamReq\u0026#34;); return checkbox.checked || false; }  This macro returns \u0026ldquo;true\u0026rdquo; if the checkbox is checked, and \u0026ldquo;false\u0026rdquo; if it isn\u0026rsquo;t or if the script encounters an error.\nThe PRO other method:\nWhat about if you don\u0026rsquo;t have an ID attribute to access (you fool)? Well, there are many ways you can identify the correct checkbox. You can look for the nth input element on a form (e.g. 15th element of the submitted form is the checkbox) with something like:\nvar checkbox = {{element}}.getElementsByTagName(\u0026#34;input\u0026#34;)[14];  If it\u0026rsquo;s the ONLY checkbox on the form, you could get its check status (true/false) with something like this:\nfunction() { var inputs = {{element}}.getElementsByTagName(\u0026#34;input\u0026#34;); for(var i = 0;i \u0026lt; inputs.length;i++) { if(inputs[i].type===\u0026#34;checkbox\u0026#34;) { return inputs[i].checked; } } return false; }  Note that you can also leverage the querySelector() method like this (browser support isn\u0026rsquo;t as good as with getElementsByTagName()):\nfunction() { var check = document.querySelector(\u0026#39;input[type=\u0026#34;checkbox\u0026#34;]\u0026#39;); return check ? check.checked : undefined; }  The ultimate desperation method would be to use the more flimsy properties of the DOM element, such as previousSibling.innerText || previousSibling.textContent to see what text directly preceded the checkbox (such as \u0026ldquo;Yes\u0026rdquo; in the example from before). However, I won\u0026rsquo;t tell you how to do that here because I don\u0026rsquo;t think it\u0026rsquo;s a very clean or robust method. You\u0026rsquo;ll run into a lot of trouble if you try to latch onto something as frail as text content.\n7. One form element - multiple forms (ASP.NET) Oh man, oh MAN, do I hate working with ASPX pages. Microsoft\u0026rsquo;s technology stack makes it often very frustrating to work with HTML pages, since the dynamically created pages depend largely on the quality of the master page template.\nOften you\u0026rsquo;ll come across a situation where the entire page is wrapped in a FORM element. This is the form wrapper of an ASPX page, and it\u0026rsquo;s used to create the dynamic content you see fully rendered in your browser. However, for GTM this is problematic. If there\u0026rsquo;s more than one form on the page, which is very often the case, you\u0026rsquo;ll find it hard to determine just which form was sent when a submit button is clicked. You see, gtm.formSubmit is fired when the form wrapper is sent, and the submissions could have been initiated from any one of the actual forms on the page. How will you ever know if the submission originated from the site search box, the contact form, or the newsletter subscription field?\n  The workarounds aren\u0026rsquo;t ideal, but they\u0026rsquo;ll have to do for now. At some point I\u0026rsquo;d like to have the option of going through the {{event}} macro\u0026rsquo;s history. Heck, I want to access historical versions of ALL data layer variables! Until then, I\u0026rsquo;ll have to resort to the methods outlined below.\nIdentify correct form using a click listener:\nThis method uses the click listener to identify which submit button was clicked, and then uses this information as a firing / blocking rule in the tag which reacts to the form submission. To help me here, I\u0026rsquo;m using information I collected in my GTM listener firing order test. There I noticed that in the event of race conditions, gtm.click is always activated before gtm.formSubmit.\nIn this example, I have two forms on the page: a site search box with submit button ID siteSearchButton, and a contact form with submit button ID contactSubmitButton.\nFirst, you\u0026rsquo;ll need to create a Custom HTML tag which is set to fire upon {{event}} equals gtm.click. Yes, you\u0026rsquo;ll need an active click listener. Add the following code within:\n\u0026lt;script\u0026gt; dataLayer.push({\u0026#39;clickedButton\u0026#39;: {{element id}}}); \u0026lt;/script\u0026gt; This pushes the ID of the clicked button into the data layer. So if someone does a site search, the pushed value would be siteSearchButton, and if someone sends a contact form, the pushed value would be contactSubmitButton.\nNext, I\u0026rsquo;ll need a Data Layer Variable macro to access this value. So create one:\n  Finally, in your event tag for the site search submission, you\u0026rsquo;ll need the following rule:\n{{event}} equals gtm.formSubmit\n{{Clicked Button ID}} equals siteSearchButton\nConversely, in your event tag for the contact form submission, you\u0026rsquo;ll need a rule like:\n{{event}} equals gtm.formSubmit\n{{Clicked Button ID}} equals contactSubmitButton\nThis works, but I\u0026rsquo;m not sure how reliable it is. There\u0026rsquo;s always the possibility that only the gtm.click tag fires or only the form submission tag fires (due to lag or whatever), but so far I haven\u0026rsquo;t really had trouble with this solution. The major downside is that it\u0026rsquo;s really ugly. You need an intermediate tag to handle your main tag firing, and that really sucks. I\u0026rsquo;d want to operate this whole event taxonomy using just the main tags, without having to resort to \u0026ldquo;helpers\u0026rdquo;.\nIdentify correct form using {{field value}}:\nAnother method I\u0026rsquo;ve used to circumvent the form wrapper problem is to check if a form field has text within. If it does, I can assume that the form that was submitted was the one with the text. I mean, you wouldn\u0026rsquo;t start filling one form, and then jump to another and send that one, would you? WOULD YOU? (Please say no.)\nSo here\u0026rsquo;s a situation where I have two forms on the page (with just one form wrapper). The first form is the site search box, and the search field has an ID of siteSearch. I also have a contact form on the page, and one of its fields requires an e-mail address, and its ID is email.\nI then have a Custom JavaScript macro (named {{submitted form}}) that looks like this:\nfunction() { var search = document.getElementById(\u0026#34;siteSearch\u0026#34;), contactEmail = document.getElementById(\u0026#34;email\u0026#34;); if(search \u0026amp;\u0026amp; search.value != \u0026#34;\u0026#34;) { return \u0026#34;search\u0026#34;; } else if (contactEmail \u0026amp;\u0026amp; contactEmail.value != \u0026#34;\u0026#34;) { return \u0026#34;contact\u0026#34;; } return \u0026#34;\u0026#34;; }  If there\u0026rsquo;s a value in the site search field, this script returns the string \u0026ldquo;search\u0026rdquo;. If there\u0026rsquo;s a value in the e-mail field of the contact form, this script returns the string \u0026ldquo;contact\u0026rdquo;. If neither is found, the script returns an empty string.\nFinally, I can create two rules. This rule fires a tag when the site search field has text:\n{{event}} equals gtm.formSubmit\n{{submitted form}} equals search\nThis rule fires a tag when the contact form e-mail address field has text:\n{{event}} equals gtm.formSubmit\n{{submitted form}} equals contact\nI don\u0026rsquo;t really like this method, since its pretty difficult to keep up-to-date, and there\u0026rsquo;s not a shred of robustness left in the implementation.\nTo conclude, let\u0026rsquo;s just say that for ASPX pages, it\u0026rsquo;s still pretty difficult to get reliable results with GTM\u0026rsquo;s form submit listener. \u0026ldquo;Luckily\u0026rdquo;, whenever I have to work with an ASPX website, the form handlers are the least of my problems.\n8. Conclusions In this post I outlined some advanced ways to use GTM\u0026rsquo;s form submit listener. Value retrieval is something that\u0026rsquo;s been asked around a lot, so I focused mainly on that aspect of data collection.\nI didn\u0026rsquo;t wander into the murky woods of custom submit handlers, jQuery validators, event propagation problems and such, because these problems are usually pretty restricted to a certain script, implementation, or page. Suffice to say, if you do use custom submit handlers, for example, you\u0026rsquo;ll probably have to get your developers to edit these scripts so that the form submit event has a chance to reach GTM\u0026rsquo;s listeners (see my post on event delegation problems).\nSometimes the form on your page isn\u0026rsquo;t a form at all (in its HTML sense), and it\u0026rsquo;s been created entirely with some JavaScript framework (or, gasp, Flash). In these situations, just having the form submit listener won\u0026rsquo;t work, because no actual submit event is every fired. In these cases you might have to talk to your developers about manually creating the gtm.formSubmit event push. For information on how to do this, check out Doug Hall\u0026rsquo;s excellent post on extending GTM\u0026rsquo;s auto-event tracking.\nDid you miss something in this post? Is there some aspect of form tracking that you\u0026rsquo;d want more information on? Drop me a line in the comments, and let\u0026rsquo;s see what we can come up with!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/macro-magic-google-tag-manager/",
	"title": "Macro Magic For Google Tag Manager",
	"tags": ["Google Tag Manager", "macros"],
	"description": "A bunch of Google Tag Manager macros that you might find useful when working with GTM.",
	"content": "(Last updated June 2014) This post is an attempt at a whole new level of interaction. These words will transcend the barriers of time and space, bridging together the physical world and its digital counterpart. You see, in an undisclosed number of hours after the publishing of this blog post, I will be talking at the MeasureCamp unconference on this very subject. Or, I hope I will. The whole unconference thing is somewhat confusing, and it involves lighting-fast reflexes and street smarts for slot selection; traits which I sadly lack.\n  However, if you DO see me speak at MeasureCamp, this post is meant as a companion to that talk. And if you do not (or did not, this transcendental approach is really confusing), perhaps this post will inspire you with some ideas for your own day-to-day Google Tag Manager use.\nIn this post, my fair friends, I\u0026rsquo;ve outlined a number of cool ways in which you can use macros to make your data collection woes a thing of the past. If you don\u0026rsquo;t know what macros are or how they operate, be sure to check my macro guide. Almost all of my Google Tag Manager posts in this blog revolve around macros in one way or another, so while you\u0026rsquo;re at it, check out some of my other GTM articles as well. And sorry for this shameless self-promotion, I can\u0026rsquo;t help it.\nIntroducing Macro Magic   So what is Macro Magic? Well, to put it simply, it\u0026rsquo;s what follows when you accept that macros, not tags, are what make GTM the time- and effort-saver that it is. Sometimes macros make your tag setups a lot sleeker and leaner, since the flexibility that they introduce to tags greatly reduces the need to capture every single variation of tracking in its own, unique tag. Sometimes they open doors to data collection methods you didn\u0026rsquo;t think were possible with traditional, on-page tracking. And sometimes they\u0026rsquo;re just little helper functions, which make operating GTM so much easier.\nThe examples below fall into these categories nicely, and I\u0026rsquo;m sure there\u0026rsquo;s something for everyone.\nBy the way, I am really, really looking forward to a Google Tag Manager which is accompanied by a resource library similar to Google Analytics\u0026rsquo; solution gallery. Just imagine how cool it would be to have a library of tags and macros you can just download and insert into your container? Well, as long as it\u0026rsquo;s just a utopian dream of mine, posts like this will probably be in demand.\nNote that not all of these are my own inventions. It would be ridiculous to claim intellectual property for some of the macros, but where prudent, I\u0026rsquo;ve added a link to the source of the macro. If you feel like I\u0026rsquo;ve stolen your ideas, please let me know and I\u0026rsquo;ll see if I can pass credit where it is due.\n1. Client time Macro Type: Custom JavaScript\nUse for: To get local time of client as opposed to the the time of the web server hosting the site.\nExample return value: 11\nInstructions and description:\n  Create new Custom JavaScript Macro\n  Add the following code:\n  function() { var now = new Date(); return now.getHours(); }  Well it\u0026rsquo;s easy to credit where I got the idea for this one. Google uses a version of this as an example of the Custom JavaScript Macro:\n  The idea behind this macro is that it returns the time (hours) of the client as opposed to the time of the website (i.e. server), which is what\u0026rsquo;s measured by default.\nSo if you\u0026rsquo;re interested in what the local time (in hours) is of the visitor, use this macro and store it as a custom dimension, for example.\n2. Random number for sampling Macro Type: Random Number\nUse for: To fire a tag only for every 10th visitor, for example\nExample return value: 647156\nInstructions and description:\n Create Random Number Macro  OK, so the macro isn\u0026rsquo;t difficult at all to create, nor is it anything magical (YET!). But this is a beautiful scenario for something as simple as the Random Number macro (which, I\u0026rsquo;m sure, is often very neglected). This idea comes directly from Dan Russell, and you can read about this in his original blog post\nSampling a percentage of your users with Google Tag Manager.\nThe sweetness is not in the macro but in the rule which you\u0026rsquo;ll use to fire your tags. If you want to fire a tag for every 10th visitor, you\u0026rsquo;d add a rule with the following condition:\n{{Random Number Macro}} ends with 3\nIf you want to fire a tag for every 100th visitor, you\u0026rsquo;d edit the rule so that it\u0026rsquo;s:\n{{Random Number Macro}} ends with 01\nIt\u0026rsquo;s so simple and beautiful. Because the Random Number macro returns a random number, you can assume that there\u0026rsquo;s roughly a 1/10 chance that the number will end in 3, and roughly a 1/100 chance that it will end in 01.\nHow\u0026rsquo;s that for pre-sampling your data before Google Analytics gets to work on it with its brutal thresholds?\n3. Return file extension of clicked link Macro Type: Custom JavaScript\nOther requirements: Macro for Auto-Event Variable of type Element (named {{element}} in this example)\nUse for: Rules, event fields\nExample return value: pdf\nInstructions and description:\n  Create Custom JavaScript Macro\n  Add following code:\n  function() { var ext = {{element}}.pathname.split(\u0026#34;.\u0026#34;); return ext.length \u0026gt; 1?ext.pop():\u0026#39;html\u0026#39;; }  This macro came from Stéphane Hamel, in another inspiring, \u0026ldquo;crowd-sourced\u0026rdquo; Google+ discussion. The idea is that when a Link Click Listener is activated, this macro will return the file extension of the clicked link, and if one doesn\u0026rsquo;t exist, it returns just html. So if the link was to http://www.mydomain.com/brochure.pdf, this macro will return pdf.\nYou can use this in your tags either as a rule or as a value for a specific field. For example, here\u0026rsquo;s an event tag which sends the extension of the clicked link as one of the parameters:\n  If the link were to http://www.mydomain.com/brochure.pdf, this tag would fire with:\nEvent Category \u0026ldquo;pdf link click\u0026rdquo;\nEvent Action \u0026ldquo;brochure.pdf\u0026rdquo; (more on this in the next chapter)\nEvent Label \u0026ldquo;url of page where brochure.pdf was downloaded from\u0026rdquo;\n4. Return file name of clicked link Macro Type: Custom JavaScript Other requirements: Macro for Auto-Event Variable of type Element (named {{element}} in this example)\nUse for: Event fields\nExample return value: brochure.pdf\nInstructions and description:\n  Create Custom JavaScript Macro\n  Add following code:\n  function() { var filepath = {{element}}.pathname.split(\u0026#34;/\u0026#34;); var filename = filepath.pop(); return filename.indexOf(\u0026#34;.\u0026#34;) \u0026gt; -1?filename:\u0026#39;n/a\u0026#39;; }  This macro is very similar to the previous one, except now it returns the whole file name. If the downloaded link is not an asset with a file extension, the script returns the ambiguous \u0026ldquo;n/a\u0026rdquo; string, which can be used in blocking rules, for example.\nUse this macro to make your link click event tags more dynamic. In the best case scenario, you\u0026rsquo;ll just need one event tag for all your link clicks, because variable value macros like this are what make your tags the dynamic vessels they should be.\n5. Check if clicked link is internal Macro Type: Custom JavaScript\nOther requirements: Macro for Auto-Event Variable of type Element URL (named {{element url}} in this example), macro for URL of type URL Hostname (named {{url hostname}} in this example)\nUse for: Firing rules\nExample return value: true\nInstructions and description:\n  Create Custom JavaScript macro\n  Add following code within:\n  function() { return {{element url}}.indexOf({{url hostname}}) \u0026gt; -1; }  This returns true if the clicked element URL contains the hostname of the current page, and false, if the clicked element URL contains some other hostname.\nYou can use this in your firing rules, e.g. fire an \u0026ldquo;Outbound link\u0026rdquo; event only when the clicked link is not internal:\n  6. Get clientID using _ga cookie Macro Type: 1st Party Cookie, Custom JavaScript\nOther requirements: Universal Analytics property\nUse for: Offline measurement, stitching client data\nExample return value: 475226310.1380715146\nInstructions and description:\n  Create 1st Party Cookie macro\n  Set cookie name to _ga\n  Create Custom JavaScript macro\n  Add following code within:\n  function() { try { var cookie = {{ga cookie}}.split(\u0026#34;.\u0026#34;); return cookie[2] + \u0026#34;.\u0026#34; + cookie[3]; } catch(e) { console.log(\u0026#34;No Universal Analytics cookie found\u0026#34;); } }  First you create a new 1st Party Cookie macro, which returns the value of the _ga cookie. It might be something like GA1.2.475226310.1380715146. Next, the Custom JavaScript macro parses this string, and returns the third (client ID) and fourth (timestamp) elements of the cookie value (475226310.1380715146), which combine to make the complete clientID of the user.\nYou can then add this as a session-level custom dimension, for example, to identify which clientID committed to what actions on your site during their session. You could then use the Measurement Protocol to work all sorts of offline-online data stitching magic with your database tools!\nNote that I use a very rudimentary try-catch block for error handling. Feel free to handle errors in any way you want (as long as you do handle them!).\n7. Get clientID using ga.getAll() Macro Type: Custom JavaScript\nOther requirements: Universal Analytics property\nUse for: Offline measurement, stitching client data\nExample return value: 475226310.1380715146\nInstructions and description:\n  Create Custom JavaScript macro\n  Add following code within:\n  function() { try { var tracker = ga.getAll()[0]; return tracker.get(\u0026#39;clientId\u0026#39;); } catch(e) { console.log(\u0026#34;Error fetching clientId\u0026#34;); } }  This is Google\u0026rsquo;s recommendation for fetching the client ID (rather than parsing the cookie). You basically access the ga object using the function getAll(). This would normally retrieve all named trackers, but you\u0026rsquo;re only interested in the first one (which is the one GTM sets). Finally, it uses the get method of the tracker object to retrieve the value of the property clientId, which is, coincidentally, the clientId.\nNOTE! This macro will not work if it fires before a page view is sent. The tracker is created during the page view call, so you\u0026rsquo;ll either have to use a callback function (see the next section) or have the tag which fetches the client ID fire upon {{event}} equals gtm.load.\n8. hitCallback with a Universal Analytics tag Macro Type: Custom JavaScript\nUse for: Imposing a tag firing order\nInstructions and description:\n  Create Custom JavaScript macro\n  Add following code within:\n  function() { return function(){dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;pageViewFired\u0026#39;});} }  The Universal Analytics tag doesn\u0026rsquo;t have a callback field that you can use (yet). So you\u0026rsquo;ll have to resort to the \u0026ldquo;Fields to set\u0026rdquo; option, and add the callback function manually.\nAnyway, the idea with hitCallback is to do something as soon as the tag has fired. In my example, I push an event called \u0026lsquo;pageViewFired\u0026rsquo;, which I can then use in some other tag to make sure it fires only after the page view has been sent. You\u0026rsquo;d need to add the macro to your tag like this:\n  The field to set is called hitCallback and its value should be the macro you just created.\nIf you look at the code, you\u0026rsquo;ll see a funny thing. You\u0026rsquo;re returning the function which does all the work. Why, you wonder? Simple. When the event tag is fired, it will go through all the fields that have been set for the tag, performing all the actions within. If you didn\u0026rsquo;t have the function returning another function, the callback would fire twice: first when the event tag\u0026rsquo;s firing begins, and again when the tag has completed (when the callback function is actually called).\nThis might be a bit difficult to fathom, but it\u0026rsquo;s just how GTM and JavaScript work. hitCallback is expecting a function, so the function in your callback tag has to return a function, because macros need to always return something.\nMessy. But it works!\n9. Return URL path + query string Macro Type: Custom JavaScript\nUse for: Working with the query string\nExample return value: /analytics/?internal=true\nInstructions and description:\n  Create Custom JavaScript macro\n  Add following code within:\n  function() { return location.pathname+location.search; }  This is a very simple macro. There\u0026rsquo;s no predefined macro for you to use if you want to return the url pathname with query parameters. This macro does just that.\n10. Property ID lookup with hostname Macro Type: Lookup Table\nOther requirements: Macro for URL of type URL Hostname (named {{url hostname}} in this example)\nUse for: To have the same tracking tag for different property IDs on different domains\nExample return value: UA-12345-1\nInstructions and description:\n  Create a Lookup Table macro\n  Add some tracking code as the Default Value (optional)\n  In the first column, choose {{url hostname}} as the macro that is evaluated\n  In the first column, add all the different hostnames you want to assign a property ID to\n  In the second column, add the respective property ID for each hostname\n  This is a very useful macro if you have multiple domains with different property IDs tracked by the same container. Whenever you have the same tags running on all the different domains, using a lookup like this means you don\u0026rsquo;t have to create a separate tag for each ID.\nThe macro should look like this:\n  This macro will return the correct property ID depending on which domain the user is browsing on.\n11. Track debug hits to different property ID Macro Type: Lookup Table, Debug Mode\nUse for: When using GTM\u0026rsquo;s debug mode, track your hits to a different property ID (e.g. test account)\nExample return value: UA-12345-1\nInstructions and description:\n  Create a Debug Mode macro (named {{debug mode}} in this example)\n  Create a Lookup Table macro\n  Add some tracking code as the Default Value (optional)\n  In the first column, choose {{debug mode}} as the macro that is evaluated\n  In the first column, add rows for true and false\n  In the second column, add the respective property ID for when debug mode is true and when it is false\n  This is very similar to the previous macro, but it uses the Debug Mode macro as well. Sometimes you\u0026rsquo;ll notice yourself doing so much debug work that you\u0026rsquo;ll want to track these hits on a different account altogether. This is a really simple way to do it.\n  The Debug Mode macro returns \u0026ldquo;true\u0026rdquo; if debug mode is on, and \u0026ldquo;false\u0026rdquo; if it\u0026rsquo;s off. Use these values to assign a different property ID as the return value of the Lookup Table macro.\n12. Get title attribute of document Macro Type: JavaScript Variable\nUse for: Retrieving and using the document title attribute\nExample return value: Macro Magic For Google Tag Manager - Simo Ahava\u0026rsquo;s Blog\nInstructions and description:\n  Create JavaScript Variable macro\n  Set field Global Variable Name to document.title\n  This macro retrieves the HTML title attribute of your document. A great use case for this is if you want to process tags on a 404 page differently. You\u0026rsquo;d then create a rule like this:\n  Another use case could be for content grouping. Use the title macro to group your content logically in your tags.\n13. Check if browser has cookies enabled Macro Type: JavaScript Variable\nUse for: Checking if user has cookies enabled, filter some bot traffic\nExample return value: true\nInstructions and description:\n  Create JavaScript Variable macro\n  Set field Global Variable Name to navigator.cookieEnabled\n  The navigator.cookieEnabled property returns a boolean value (true/false) determined by whether or not the browser allows websites to write cookies. You can use this in your rules to make sure tags only fire for browsers with cookies, or to filter non-cookie data to a different view.\nYou could also use this to block tags from firing for search engine bots, since there\u0026rsquo;s some consensus that a majority of them do not accept or use cookies (see this test for Googlebot from February 2013).\n14. A bunch of useful Auto-Event Variable extensions Macro Type: Data Layer Variable Use for: Making the most of auto-event tracking\nHere\u0026rsquo;s a list of extensions for your auto-event tracking. By creating Data Layer Variable macros, you can use them in your rules to make your event tracking and whatnot even more detailed. In the \u0026ldquo;What it does\u0026rdquo; sections, the element refers always to the element which was clicked / submitted and thus stored in dataLayer as gtm.element.\nData Layer Variable Name: gtm.element.nodeName\nExample return value: IMG\nWhat it does: Returns the tag name of the element (well, strictly node name but in most cases it\u0026rsquo;s the same thing)\nData Layer Variable Name: gtm.element.value\nExample return value: Simo Ahava\nWhat it does: Returns the value of the element. This is useful if you\u0026rsquo;re tracking input elements on your forms (with e.g. blur, focus, or change), and you want to send an event every time a form field has been filled.\nData Layer Variable Name: gtm.element.hash\nExample return value: #chapter1\nWhat it does: Returns the hash (if any) of the element href. So if the link was to /this-page/?internal=true#chapter1, gtm.element.hash would return #chapter1\nData Layer Variable Name: gtm.element.pathname\nExample return value: /this-page/\nWhat it does: Returns the path in the element href. If the link was to /this-page/?internal=true#chapter1, gtm.element.pathname would return /this-page/\nData Layer Variable Name: gtm.element.search\nExample return value: ?internal=true\nWhat it does: Returns the full query string of the element. If the link was to /this-page/?internal=true#chapter1, gtm.element.search would return ?internal=true\nData Layer Variable Name: gtm.element.parentElement\nExample return value: (object), extend further with some property of the parent element\nWhat it does: Returns the direct parent of the element, and you should extend this macro further to access its properties (e.g. gtm.element.parentElement.id returns the value stored in the ID attribute of the parent tag)\nData Layer Variable Name: gtm.element.firstChild\nExample return value: (object), extend further with some property of the child element\nWhat it does: Returns the first direct descendant of the element, and you should extend this macro further to access its properties (e.g. gtm.element.firstChild.className returns value stored in the CLASS attribute of the child tag)\nData Layer Variable Name: gtm.element.nextSibling\nExample return value: (object), extend further with some property of the sibling element\nWhat it does: Returns the next element in the DOM tree which is on the same hierarchical level as the element, and you should extend this macro further to access its properties (e.g. gtm.element.nextSibling.nodeName returns the tag name of the sibling tag)\nSo here\u0026rsquo;s a bunch of extensions for you to play with. There\u0026rsquo;s still a huge amount of stuff out there you can try, especially with forms, so I suggest that you set up a click listener on your site and play around with the DOM.\n15. Detect mobile browser Macro Type: Custom JavaScript\nUse for: Mobile-specific tracking\nExample return value: true\nInstructions and description:\n  Create Custom JavaScript macro\n  Copy-paste the code from the attached TXT file\n  I had to add the code in an attachment, because it\u0026rsquo;s such a long and ugly piece of RegEx that it makes my eyes water. Sorry about that.\n  It\u0026rsquo;s dirty, I know. This script checks the browser user agent string and returns true if it matches a mobile device user agent. If it doesn\u0026rsquo;t, the script returns false.\nThis script is provided by (with slight modifications to fit GTM infrastructure) detectmobilebrowsers.com.\nConclusions There\u0026rsquo;s so much you can do with macros. I truly believe that the goal of all GTM implementations should be to achieve the elusive compromise between complexity and eye candy. It\u0026rsquo;s important to keep a clean house when tag management is concerned, especially since Google Tag Manager doesn\u0026rsquo;t, as of yet, provide us with too much to work with in terms of taxonomies, hierarchies and categories.\nI wasn\u0026rsquo;t kidding when I wrote in the beginning how I really want to see a solution gallery for GTM. I appreciate the fact that it will make posts like this obsolete (unless you enjoy my banter or the educational twist I hopefully succeed in conveying). Nevertheless, being able to share, store, and search for the amazing tags and macros that our vibrant community has produced would be the best thing since sliced bread.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/container-snippet-gtm-secrets-revealed/",
	"title": "The Container Snippet: GTM Secrets Revealed",
	"tags": ["container snippet", "Google Tag Manager", "JavaScript"],
	"description": "Deciphering the Google Tag Manager JavaScript container snippet. This guide will show you row-by-row what the container snippet does.",
	"content": "First of all, I\u0026rsquo;m sorry for the wacky title. Sometimes I just want to amuse myself. Nevertheless, this post is about the Google Tag Manager container snippet. There\u0026rsquo;s nothing secretive about it, but I\u0026rsquo;m betting many people have no clue what the snippet really does. That\u0026rsquo;s the revelatory part.\nIf you\u0026rsquo;ve never wondered what the snippet does, then shame on you! Remember, you own your page template. It\u0026rsquo;s yours. Any code that you write there is your responsibility. You wouldn\u0026rsquo;t let a complete stranger come to your house and paint your walls without permission, would you? So why copy-paste some code that you have no idea what it does? Because Google told you to?\n  Well, you can trust Google (stop laughing!). There\u0026rsquo;s nothing nefarious about the container snippet. Regardless, I wrote this post to tell you what it does, so that you can sleep better at night. Or you might learn something new about JavaScript (never a bad thing).\nNote that this text should be understandable to JavaScript novices as well, but if you feel daunted at any time, you can just skip ahead to the last chapter. Within is a short recap of what the container snippet does.\nSo sit back (but lean forward), and enjoy another one of life\u0026rsquo;s great mysteries unravelled.\nThe container snippet You should know what the container snippet is by now. It\u0026rsquo;s the piece of code that Google Tag Manager requires that you add to your page template, right after the opening \u0026lt;body\u0026gt; tag. You know, this (from my site):\n  (I left out the \u0026lt;noscript\u0026gt; part, because this post is just about the JavaScript snippet.)\nLet\u0026rsquo;s face it, the snippet code looks gibberish. This is because the JavaScript (that\u0026rsquo;s right, it\u0026rsquo;s JavaScript) is minified. Minification is the process of truncating the script to the smallest possible size without losing any data. This means removing whitespace (spaces and line breaks), and reducing variable and function names to single- or double-character length. Minification is done because every single character adds to the page size, and thus to the page load time. The benefit with something as small as the container snippet is negligible, but it is still observed as a best practice.\nBecause I want this post to add some value, allow me to perform a little makeover to the snippet.\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; (function (w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ \u0026#39;gtm.start\u0026#39;: new Date().getTime(), event: \u0026#39;gtm.js\u0026#39; }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != \u0026#39;dataLayer\u0026#39; ? \u0026#39;\u0026amp;l=\u0026#39; + l : \u0026#39;\u0026#39;; j.async = true; j.src = \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39; + i + dl; f.parentNode.insertBefore(j, f); })(window, document, \u0026#39;script\u0026#39;, \u0026#39;dataLayer\u0026#39;, \u0026#39;GTM-P8XR\u0026#39;); \u0026lt;/script\u0026gt; That\u0026rsquo;s better! It\u0026rsquo;s the same code but with some basic formatting. Much easier to read and understand, right?\nPositioning the container snippet You might have wondered why Google so forcefully recommends that you place the container snippet just after the opening \u0026lt;body\u0026gt; tag. There are two good reasons to do so.\nFirst, if you add the \u0026lt;noscript/\u0026gt; tag as well (as you should), this should always be in the body of the document. The tag (which is shown for browsers without JavaScript enabled) contains an iFrame which loads the GTM library. If you add the \u0026lt;noscript/\u0026gt; tag into the head of your document, it can perform pretty wildly with some browsers. You could experiment with leaving the \u0026lt;noscript/\u0026gt; in the body, and placing the JavaScript in the , but I haven\u0026rsquo;t tested it and certainly don\u0026rsquo;t recommend it.\nThe second reason is simple: to maximize data collection, the snippet should be the first thing loaded when the body of the page is rendered. Because the library is loaded asynchronously, there\u0026rsquo;s no reason to delay it at all. So have it load as the first thing when the body is rendered, so that you don\u0026rsquo;t risk losing any data due to sluggish DOM elements delaying the page load.\nLines 2 and 15 We can skip lines 1 and 16, since they\u0026rsquo;re just the HTML SCRIPT tag, wrapping the JavaScript block.\n(function (w, d, s, l, i) { ... })(window, document, \u0026#39;script\u0026#39;, \u0026#39;dataLayer\u0026#39;, \u0026#39;GTM-P8XR\u0026#39;);  To understand the first line, we have to look at the last line as well. This is an example of a self-invoking / self-executing anonymous function, or immediately-invoked function expression. There\u0026rsquo;s some debate on the semantics of which term you should choose, but for this purpose it\u0026rsquo;s just that: semantics.\nBasically, line 2 of the snippet declares an anonymous function (it has no name) with five parameters. These parameters are served via the function call at the end of the function block, which has, as it should, five arguments.\nThe whole idea behind an immediately-invoked function expression is to call the function as soon as it has been declared. That\u0026rsquo;s what lines 2 and 15 of the snippet are about. Line 2 declares a function which is called in line 15, as soon as the function code has been parsed by your browser.\nThe arguments that are sent with the function call, and turned into parameters in the declaration (the parameters are in parentheses), are as follows:\n  window (w) - the window object represents the open window in the user\u0026rsquo;s browser and everything in it (the document object model, or DOM, for example)\n  document (d) - the document object is the top node in the DOM tree; it contains the entire HTML document\n  \u0026lsquo;script\u0026rsquo; (s) - this string is used to load the gtm.js library (more on this later)\n  \u0026lsquo;dataLayer\u0026rsquo; (l) - this string is used to give a name to the dataLayer object; you can rename the object used by GTM by changing this value\n  \u0026lsquo;GTM-P8XR\u0026rsquo; (i) - this is your container ID, which you\u0026rsquo;ll get from Google Tag Manager\n  Line 3 w[l] = w[l] || [];  If you remember what the parameters and arguments were, this line translates to:\nwindow[\u0026#39;dataLayer\u0026#39;] = window[\u0026#39;dataLayer\u0026#39;] || [];  Here, the function looks for an object named dataLayer (or something else if you renamed it in the function arguments) in the window object, which contains all the objects on the page. If it finds one, it leaves it be. If it doesn\u0026rsquo;t find one, it declares it as an empty array.\nThis is important, because sometimes you\u0026rsquo;ll see this explicit declaration of the dataLayer object in the page template before the container snippet:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; dataLayer = []; \u0026lt;/script\u0026gt; Explicit declaration of dataLayer is useful if you need some dataLayer variables to be available at the earliest possible moment in your GTM tags. For example, if your page view tag requires some dataLayer variable to exist (for a custom dimension, perhaps), it\u0026rsquo;s better to declare it in the page template before the container snippet:\n\u0026lt;script\u0026gt; dataLayer = [{\u0026#39;author\u0026#39;: \u0026#39;Simo Ahava\u0026#39;}]; \u0026lt;/script\u0026gt; This explicit declaration is what the code on line 3 of the container snippet is looking for. If the explicit declaration is found, it is left as it is. If it hasn\u0026rsquo;t been declared, the container snippet creates the dataLayer object for you.\nThis brings me to an important point.\nIf you do not declare the dataLayer object explicitly, do NOT declare it in your GTM tags or after the container snippet. The container creates the dataLayer array for you, and if you try to declare it later on, you\u0026rsquo;ll end up erasing the original dataLayer, created and used by GTM. After dataLayer has been created, your only interaction with it should be by using the push() method of the array (dataLayer.push({\u0026lsquo;property\u0026rsquo;: \u0026lsquo;value\u0026rsquo;});).\nLines 4 through 7 w[l].push({ \u0026#39;gtm.start\u0026#39;: new Date().getTime(), event: \u0026#39;gtm.js\u0026#39; });  This is a very important part of the container snippet functionality.\nWhen the code reaches this point, a dataLayer.push() call is done. Within the curly brackets is an object literal. Object literals are JavaScript objects, which contain any number of key-value (or property-value or variable-value) pairs, often in JSON (JavaScript Object Notation). These objects are wrapped in curly brackets, and pushed to the end of the dataLayer array.\nIf most of the previous paragraph is gibberish to you, I don\u0026rsquo;t blame you.\nTo put it simply, with a dataLayer.push(), you\u0026rsquo;re pushing an object with properties to the end of the dataLayer queue. These properties can then be accessed in GTM.\nThis first push, on lines 4 through 7 of the container snippet, contains an object with the following properties:\n  Key 1: \u0026lsquo;gtm.start\u0026rsquo; - Value 1: New Date.getTime()\n  Key 2: event - Value 2: \u0026lsquo;gtm.js\u0026rsquo;\n  So if you\u0026rsquo;ve ever wondered when tags with {{event}} equals gtm.js or {{url}} matches regex .* get fired, it\u0026rsquo;s at this point.\nThe gtm.start property receives the current time (in milliseconds since Jan 1, 1970) as its value. Brian Kuhn of Google explained that this is used for calculating gtm.js load time and the cache hit rate of the request, so don\u0026rsquo;t worry about it.\nLines 8 through 10 var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != \u0026#39;dataLayer\u0026#39; ? \u0026#39;\u0026amp;ampamp;l=\u0026#39; + l : \u0026#39;\u0026#39;;  Let\u0026rsquo;s open this up again, using the parameters and arguments we\u0026rsquo;re already familiar with:\nvar f = document.getElementsByTagName(\u0026#39;script\u0026#39;)[0], j = document.createElement(\u0026#39;script\u0026#39;), dl = \u0026#39;dataLayer\u0026#39; != \u0026#39;dataLayer\u0026#39; ? \u0026#39;\u0026amp;ampamp;l=\u0026#39; + \u0026#39;newDataLayer\u0026#39; : \u0026#39;\u0026#39;;  The first line stores the first SCRIPT element on the page in variable f.\nThe second line creates a new SCRIPT element, and stores it in variable j.\nThe third line does one of two possible things. If you haven\u0026rsquo;t touched the default name of the dataLayer object (i.e. \u0026lsquo;dataLayer\u0026rsquo;), just an empty string is stored in variable dl. However, if you\u0026rsquo;ve chosen to rename the dataLayer (e.g. \u0026lsquo;newDataLayer\u0026rsquo;), the string \u0026lsquo;\u0026amp;ampamp;l=newDataLayer\u0026rsquo; is stored in variable dl. \u0026amp;ampamp; is the same as the ampersand (\u0026amp;), but it has been encoded because some platforms can\u0026rsquo;t process non-encoded ASCII characters.\nThese variables will be used in the next lines.\nLines 11 through 13 j.async = true; j.src = \u0026#39;//www.googletagmanager.com/gtm.js?id=\u0026#39; + i + dl;  And let\u0026rsquo;s beautify this one as well:\nj.async = true; j.src = \u0026#39;//www.googletagmanager.com/gtm.js?id=GTM-P8XR\u0026#39;;  Remember, in the last chapter you just created a new SCRIPT element and stored it in variable j? In these lines you add some attributes to it. The container ID is retrieved from function parameter i, and it was originally passed as an argument in the self-invoking function call (you remember this!). Go look if you don\u0026rsquo;t believe me.\nNote that the last line might look different if you chose to rename your dataLayer. In that case, it would actually be the equivalent of this:\nj.src = \u0026#39;//www.googletagmanager.com/gtm.js?id=GTM-P8XR\u0026amp;l=newDataLayer\u0026#39;;  This means that if you renamed your dataLayer object, this new name is passed as the value of query parameter l with the request to load the gtm.js library from GTM servers.\nAnyway, the whole point of these lines is to make the SCRIPT element you stored into variable j look something like this:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34; async src=\u0026#34;//www.googletagmanager.com/gtm.js?id=GTM-P8XR\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; So basically you\u0026rsquo;ve just created a SCRIPT tag which loads the external gtm.js library with your container ID (and your renamed dataLayer, if you did so) as query parameters.\nLine 14 f.parentNode.insertBefore(j, f);  Remember a few chapters back when variable f was defined as the first SCRIPT tag of the document? This line does the following:\n  Look for the first SCRIPT tag on the page\n  Just before that, insert the new SCRIPT tag which loads gtm.js (from the previous chapter)\n  So the code on line 14 of the container snippet makes sure that the GTM loader is the first SCRIPT tag on the page.\nAnd there you go! Read ahead for a short summary of what just happened.\nConclusions Here\u0026rsquo;s a short recap of what the container snippet does:\n  Declares an anonymous function with five parameters (the name of the dataLayer object and your container ID, among others)\n  Creates the dataLayer array, if you didn\u0026rsquo;t do so explicitly before the container snippet\n  Pushes the first event, gtm.js, into dataLayer, along with the time the push was done\n  Creates a new SCRIPT tag, which loads the external gtm.js JavaScript library with your container ID (and custom dataLayer object name) as parameters\n  Adds this new SCRIPT to the page template, so that it\u0026rsquo;s the first SCRIPT tag on the page\n  After the function is declared, the function is automatically executed\n  In the end, it\u0026rsquo;s a pretty simple piece of code. All it actually does is create (or verify) the dataLayer object, push the first event (named \u0026lsquo;gtm.js\u0026rsquo;) into dataLayer, and load the external gtm.js library which processes your container.\nAnd that\u0026rsquo;s the story of the container snippet.\nNext up: reverse engineering the entire gtm.js library (JUST KIDDING!).\n"
},
{
	"uri": "https://www.simoahava.com/analytics/block-internal-traffic-gtm/",
	"title": "Block Internal Traffic With Google Tag Manager",
	"tags": ["api", "custom html", "Google Tag Manager", "internal traffic", "ip", "macros"],
	"description": "Guide and tips for how to block internal traffic from Google Analytics using Google Tag Manager.",
	"content": "You\u0026rsquo;ve probably come across a number of guides or posts talking about why it\u0026rsquo;s necessary to block so-called internal traffic from your web analytics reports. The reasons are pretty solid: internal traffic does not emulate normal visitor behavior, it rarely contributes to conversions (skewing up your conversion rate), it inflates page views, and it wreaks havoc on your granular, page-by-page data.\nInternal traffic is vaguely described as \u0026ldquo;your employees\u0026rdquo;, \u0026ldquo;people really close to your brand\u0026rdquo;, \u0026ldquo;your marketing department\u0026rdquo;, \u0026ldquo;your web editors\u0026rdquo;, and so on. Basically, it should be a term which covers traffic that does not adequately represent trending visitor behavior on your site. Most often, this is \u0026ldquo;internal\u0026rdquo;, in that it is traffic by people who generate the content. It can also be your proof-readers (wives, husbands, best friends), beta testers (wives, husbands, best friends), outreach marketers (wives, husb\u0026hellip; you get my drift).\nIn this post, I\u0026rsquo;ll introduce two methods to annotate this kind of internal traffic using (mostly) Google Tag Manager. The underlying premise is that your internal traffic comes from such a diverse number of sources that it\u0026rsquo;s impossible to filter it using Google Analytics\u0026rsquo; own filters. Another possibility is that you\u0026rsquo;ve decided to anonymize IP data sent to Google Analytics servers, which means that even if you\u0026rsquo;d just have a certain range of IPs, you couldn\u0026rsquo;t use Google Analytics\u0026rsquo; filters, since the last octet has been censored (more about this later).\nBy the way, I use the term annotate instead of block (except in the search engine friendly title, GUILTY!), because whether or not you actually want to block the data or just segment it is up to you. You could also follow Bounteous\u0026rsquo; excellent guide on preventing the data from being sent to Google Analytics in the first place, but I wanted to follow a more reconciliatory route, offering a way for you to still incorporate internal traffic in your reports, if you so choose. And this route is, of course, a custom dimension.\nThe premise I will introduce two methods: 1) Visit to URL, and 2) IP extraction. Both have their pros and cons.\nIf the people you want included in your internal traffic are constantly on the move, use a variety of devices on the road, or are hard to pin down just by using an IP address range, you should use the first method. This way you\u0026rsquo;ll also leave these people an out. You see, sometimes your employees are also your customers or normal site visitors, which means that their behavior should be included in your actual traffic reports.\nIP extraction requires that you have specific IP address ranges in mind which you want to block. This is best for traffic which originates from stationary places, like office buildings or office networks.\nWhichever you choose, you\u0026rsquo;ll need to set up a custom dimension first, which will store your data. It\u0026rsquo;s really simple, and Google has a great guide on how to get started.\nIf you want to provide your internal traffic an out, you should create a session-scope custom dimension. This way internal traffic is annotated separately from one session to the next.\n  If, however, you want the annotation to stick \u0026ldquo;for life\u0026rdquo;, you should create a user-scope custom dimension (user is the same as device). This way the annotation will persist from session to session, even if the IP extraction fails or the user doesn\u0026rsquo;t visit your designated \u0026ldquo;internal traffic\u0026rdquo; URL.\nMethod 1: Visit to URL I really like this method, since it\u0026rsquo;s completely client-side, it doesn\u0026rsquo;t require any custom functions or external API calls to retrieve the IP address, and it provides an out for your employees or users, if they wish to be treated as external traffic.\nOne thing that it does require, however, is that your internal traffic remembers to use the URL every single time they visit your site (unless they don\u0026rsquo;t want to be treated separately). Since you\u0026rsquo;re using a session-scope dimension here, it\u0026rsquo;s ok if the internal traffic URL is not the landing page of the visit. A session-scope custom dimension hit is applied retroactively to all hits in the session, so as long as the custom dimension is sent with a single hit during the session, it will suffice to annotate all hits in the session.\nI prefer to use a URL query parameter as the trigger that tags the visitor as internal traffic. This means that:\n  You need to make sure that your server accepts query parameters in URL addresses\n  You should canonicalize all pages to the parameter-less version, so that search engines won\u0026rsquo;t index your internal traffic page by accident (use \u0026lt;link rel=\u0026quot;canonical\u0026quot; href=\u0026quot;http://url-of-page-without-query-parameters\u0026quot;/\u0026gt;)\n  You should configure your Google Analytics view so that this query parameter is stripped from reports (because it doesn\u0026rsquo;t add any value content-wise)\n  For this example, I\u0026rsquo;ve chosen the query internal=true as the trigger. So if you navigate to the homepage, you\u0026rsquo;d need to use http://www.mydomain.com/?internal=true to be tagged as internal traffic.\nHere\u0026rsquo;s how you use the query parameter as a custom dimension in your tags.\n  Create a new macro (Macro Name: Internal URL, Macro Type: URL, Component Type: Query)\n  In the field Query Key, add internal\n    When called, this macro looks at your URL, tries to find the query parameter \u0026ldquo;internal\u0026rdquo;, and if it does, it returns its value (\u0026ldquo;true\u0026rdquo;). If no parameter is found, it returns an undefined which suits us just fine.\nNext, you\u0026rsquo;ll need to send this along with your page view tag.\n  Open your page tracking tag\n  Go to More Settings \u0026raquo; Custom Dimensions\n  Add the index number of your custom dimension in the appropriate slot\n  Add {{Internal URL}} into the Dimension field\n    And that\u0026rsquo;s it! The beauty of Google Tag Manager macros is that you don\u0026rsquo;t need a separate page view tag for undefined custom dimensions. You can just use this one macro. If the query parameter \u0026ldquo;internal\u0026rdquo; is found, its value (\u0026ldquo;true\u0026rdquo;) is sent as a custom dimension along with your page view hit. However, if no such parameter is found, no custom dimension is sent, so you don\u0026rsquo;t have to worry about overwriting your data with an empty custom dimension or something.\nFinally, check with a debugger like WASP that your custom dimension is actually being sent over.\nPro tip, use a URL rewrite If you or your internal traffic users find that using a URL query parameter is tedious, you should set up a 301 server-side redirection, and use either a vanity URL or a subdomain to handle the redirection. So if http://www.mydomain.com/?internal=true is just too difficult to manage, make it so that http://www.mydomain.com/office or office.mydomain.com redirect the user to the home page with the query parameter in place (or both!). If you have an Apache server, you could try something like this for the subdomain redirect (check with IT first!):\nRewriteEngine on RewriteCond %{HTTP_HOST} ^office\\.mydomain\\.com$ RewriteRule ^$ http://www.mydomain.com/?internal=true [R=301,L] Method 2: IP extraction A popular way to separate internal traffic from external traffic is to use the client\u0026rsquo;s IP address. The most common method is to use Google Analytics\u0026rsquo; standard features to filter out any IP addresses that are within a specified range. Because this is so \u0026ldquo;standard\u0026rdquo;, I won\u0026rsquo;t go into it (you can read the official words here).\nThis guide is for annotating internal traffic, not for filtering it, so you\u0026rsquo;ll be checking the client\u0026rsquo;s IP address against a specified range, and if there\u0026rsquo;s a match, you\u0026rsquo;ll send this information via custom dimension.\n(Pro tip: This is especially useful if you have IP anonymization set up. In Google Analytics, IP anonymization censors the last octet of the client\u0026rsquo;s IP (111.222.333.XXX) and sets it to 0 (111.222.333.0). This is what\u0026rsquo;s sent to Google Analytics servers, ensuring a higher level of security, if you don\u0026rsquo;t want the geeks at Mountain View looking at your visitors\u0026rsquo; IP addresses.)\nTo extract the client IP, you have a number of choices. The accuracy will vary greatly, since if the visitor is using a proxy, for example, it\u0026rsquo;s not really their IP that\u0026rsquo;s retrieved but the proxy\u0026rsquo;s. In this guide, I\u0026rsquo;ll introduce you to a simple PHP value retrieval, and also how to retrieve the IP with JavaScript using an external API call.\nRetrieve and process the IP with PHP This is one way to do it, and it should work if you have a site which is parsed with PHP (WordPress, Drupal, etc.). In your page template, before your container snippet, you create the dataLayer object with window.dataLayer = window.dataLayer || [];. Use this to enter the client IP into the dataLayer object as it is created. This way it will be ready when the container is set up and your page views are sent:\nwindow.dataLayer = window.dataLayer || []; window.dataLayer.push({ \u0026#34;ipaddress\u0026#34;: \u0026#34;\u0026lt;?php echo $IPADDRESS ?\u0026gt;\u0026#34; });  This will retrieve the IP address using the designated PHP variable, and it will store it in the data layer variable ipaddress.\nThis is a great way to extract the IP address, since you don\u0026rsquo;t have to wait for an external API to resolve the address. Here\u0026rsquo;s a more thorough guide on retrieving the IP with a PHP call.\nNext, create a Data Layer Variable Macro through which you can process the IP:\n  Create new macro (Macro Name: Retrieve IP, Macro Type: Data Layer Variable)\n  Set ipaddress as the Data Layer Variable Name\n  Set none as the Default Value\n    (Why the default value, you ask? Well I like dealing with declared entities, so that I won\u0026rsquo;t have to check for undefined every single time I want to process a variable.)\nNext, you\u0026rsquo;ll need a Custom HTML tag in which you\u0026rsquo;ll process the IP address. The aim is to push a data layer variable \u0026ldquo;internal\u0026rdquo;: \u0026ldquo;true\u0026rdquo;, if the IP matches a given address or range.\n  Create new Custom HTML Tag\n  Add the following code within:\n  \u0026lt;script\u0026gt; var ipaddress = {{Retrieve IP}}; // Retrieves the IP from the data layer  // Comment following three lines if you want to use the IP range method  if (ipaddress == \u0026#34;111.222.333.444\u0026#34;) { dataLayer.push({\u0026#34;internal\u0026#34;: \u0026#34;true\u0026#34;}); } // To use the following IP range check, comment the previous three lines  // of code and uncomment the following lines  //  // var ipRange = ipaddress.split(\u0026#34;.\u0026#34;);  // var lastOctet = parseInt(ipRange.pop());  // if(lastOctet \u0026gt;= 1 \u0026amp;\u0026amp; lastOctet \u0026lt;= 100) {  // dataLayer.push({\u0026#34;internal\u0026#34;: \u0026#34;true\u0026#34;});  // }  dataLayer.push({\u0026#34;event\u0026#34;: \u0026#34;ipComplete\u0026#34;}); \u0026lt;/script\u0026gt;  Add Firing Rule to tag: {{event}} equals gtm.js  You have two choices in the code above. Either you do an exact match (if your office has just one IP address), or you check against a range. In the range check, I check if the last octet (111.222.333.XXX) is between 1 and 100, so remember to modify this to match the range you want to check for. And if you have multiple IP addresses you want to include in the check, just modify the if-clause to your liking. You can (and should) also use regular expression pattern matching, if the variations get more diverse.\nWhatever you do, this should push the data layer variable \u0026ldquo;internal\u0026rdquo;: \u0026ldquo;true\u0026rdquo; if the IP address matches a given pattern. Finally, you push a trigger event \u0026ldquo;ipComplete\u0026rdquo;, which is what will fire your page view tag, with which the custom dimension is also passed along.\nThe Firing Rule here is {{event}} equals gtm.js to ensure that the code is run at the earliest possible opportunity, to avoid delaying the page view call any more than necessary.\nRetrieve and process the IP with JavaScript If you can\u0026rsquo;t use PHP or server-side scripting, you can use JavaScript. Well, you can\u0026rsquo;t technically resolve the client IP using just JavaScript. You need to request the IP from an external resource such as Hostip.info or GeoPlugin. This means that this approach is a bit more suspect, because you have to trust the request endpoint to serve you with the correct data every time. Nevertheless, if you can\u0026rsquo;t retrieve the IP server-side, this is what you should try.\nI use Hostip.info in my example, but there are a number of APIs out there that you can use. I suggest you find an API that allows you to retrieve the data with an asynchronous AJAX request (you could also use an XMLHttpRequest), and which returns the data as a JSON object that you can then parse. This way your contraption won\u0026rsquo;t go up in flames, taking most of your website along in its carnival of destruction, if the endpoint chooses to change the way it distributes.\nI\u0026rsquo;ve added the AJAX call to the Custom HTML Tag where I\u0026rsquo;ll be doing the pattern matching as well.\nNOTE! This script requires that you have loaded jQuery before this script is run. So make sure the library is loaded first.\n  Create new Custom HTML Tag\n  Add the following code in the tag:\n  \u0026lt;script\u0026gt; function matchIP(ipaddress) { // Comment following three lines if you want to use the IP range method  if (ipaddress == \u0026#34;111.222.333.444\u0026#34;) { dataLayer.push({\u0026#34;internal\u0026#34;: \u0026#34;true\u0026#34;}); } // To use the following IP range check, comment the previous three lines  // of code and uncomment the following lines  //  // var ipRange = ipaddress.split(\u0026#34;.\u0026#34;);  // var lastOctet = parseInt(ipRange.pop());  // if(lastOctet \u0026gt;= 1 \u0026amp;\u0026amp; lastOctet \u0026lt;= 100) {  // dataLayer.push({\u0026#34;internal\u0026#34;: \u0026#34;true\u0026#34;});  // }  } var ipaddress = \u0026#34;\u0026#34;; try { jQuery.ajax({ type : \u0026#34;GET\u0026#34;, dataType : \u0026#34;json\u0026#34;, url : \u0026#34;http://api.hostip.info/get_json.php\u0026#34;, async : true, success : function(data) { ipaddress = data.ip; },error : function(errorData) { ipaddress = \u0026#34;none\u0026#34;; },complete : function() { matchIP(ipaddress); } }); } catch(e) { console.log(\u0026#34;Oops, something went wrong: \u0026#34; + e.message); } dataLayer.push({\u0026#34;event\u0026#34;: \u0026#34;ipComplete\u0026#34;}); \u0026lt;/script\u0026gt;  Set Firing Rule for tag {{event}} equals gtm.js  Phew!\nIn this tag, you send an asynchronous HTTP POST request to the endpoint at hostip.info. The request returns a JSON script, which you parse for the node \u0026ldquo;ip\u0026rdquo;. The value stored in this node is then stored in the data layer variable ipaddress. Finally, as soon as the request is complete, you call a function called matchIP(), where the actual parsing takes place.\nThe parsing is identical to what we did in the PHP IP retrieval script, so you can either evaluate an exact match or a range of IP addresses. With small modifications you can check versus multiple addresses and ranges, if you so choose.\nAgain, the desired end result is a new data layer variable \u0026ldquo;internal\u0026rdquo;, which should have the value \u0026ldquo;true\u0026rdquo;, if traffic is internal. It should remain undefined if traffic is not internal, or if an error crops up in your AJAX request.\nAnd again, there\u0026rsquo;s the trigger event \u0026ldquo;ipComplete\u0026rdquo;, which is the firing rule for your page view, which will be the vessel for your custom dimension.\nThe firing rule for this Custom HTML Tag is {{event}} equals gtm.js, because you want to make sure that the script is run at the earliest possible moment to avoid delaying your page view call any more than necessary.\nSend custom dimension with page view tag And, finally, we are about to reach the end of this arduous journey. It\u0026rsquo;s time to modify your page view tag, so that it includes information about the traffic type in a custom dimension.\nFirst, create a Data Layer Variable Macro for your internal traffic.\n  Macro Name: Internal IP\n  Macro Type: Data Layer Variable\n  Data Layer Variable Name: internal\n    Now we\u0026rsquo;re ready to edit the page view tag.\n  Open your page view tag\n  Go to More Settings \u0026raquo; Custom Dimensions\n  Add a new Custom Dimension, and give it the correct index number\n  In the Dimension slot, add your new data layer variable macro ({{Internal IP}})\n  Add Firing Rule: {{event}} equals ipComplete\n    That\u0026rsquo;s it! Your page view tag obediently waits for the IP check to complete. Once the check is made, if traffic was internal, the value \u0026ldquo;true\u0026rdquo; is sent as a custom dimension with the page view tag.\nNOTE! Making your page view tag wait for any scripts (especially external API calls) is a bit dangerous, since if there\u0026rsquo;s a significantly long load time, your page view might not get sent at all. You might want to leave your page view tag as it is, and send the custom dimension with a non-interaction event instead.\nConclusions I didn\u0026rsquo;t intend this post to be a be-all and end-all to All Internal Traffic Exclusion Guides on the web, so I\u0026rsquo;m a bit confused why this ended up being such a long post.\nRegardless of my verbosity, I hope this post served to highlight some of Google Tag Manager\u0026rsquo;s amazing versatility. The combination of macros, tags, and defined / undefined custom dimensions creates opportunities for really complex tagging with a really simple setup.\nWith the help of this post, you can now segment (or filter) your internal traffic in Google Analytics by looking for \u0026ldquo;true\u0026rdquo; in your Internal Traffic custom dimension.\nWhat other ways have you discovered to identify internal traffic (with or without GTM)? I\u0026rsquo;d love to add a third method to this tutorial, even though then this will really be a bloated guide.\n"
},
{
	"uri": "https://www.simoahava.com/tags/internal-traffic/",
	"title": "internal traffic",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/gtm-listener-firing-order-test/",
	"title": "GTM Listener Firing Order Test",
	"tags": ["Google Tag Manager", "listeners", "tags"],
	"description": "Quick test to see in what order Google Tag Manager&#39;s click and form triggers fire when competing for the same user action.",
	"content": "Because I was bored, I did a quick test to sort out the firing order of competing GTM listeners. If you\u0026rsquo;ve done your homework (i.e. read my article on GTM listeners), you\u0026rsquo;ll remember that GTM listeners are set up on the document node of the document object model (DOM). I wanted to test what the firing order is if you have multiple competing listeners on the same page.\nI tested with the following listeners (make sure you read up on auto-event tracking if you are completely baffled at this point):\n  Form Submit Listener - listens for a submit event on a form\n  Click Listener - listens for a click event on any element\n  Link Click Listener - listens for a click event on a link element\n  History Listener - listens for changes in browser history\n  The premise I had a very simple HTML page, with just two buttons. One was the submit button of an empty form, the other an HTML5 button element, wrapped in a link.\n  To make things more interesting, I wrapped the submit button of the form in a link as well, because I wanted to see what happens. It\u0026rsquo;s not like you\u0026rsquo;ll ever come across a silly implementation like that in real life. I hope.\nHere\u0026rsquo;s what the code looked like:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Just testing\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- GOOGLE TAG MANAGER CONTAINER WAS HERE --\u0026gt; \u0026lt;form action=\u0026#34;#form\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;?formlink\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Form test\u0026#34;/\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;a href=\u0026#34;?buttonlink\u0026#34;\u0026gt; \u0026lt;button\u0026gt;Link test\u0026lt;/button\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Very simple, right? I used query parameters in links simply because I wanted to see what happened to the {{url}} macro when different listeners fired.\nI created a Custom HTML tag for each listener, with just a simple alert dialogue. Here\u0026rsquo;s what my click tag looked like:\n\u0026lt;script\u0026gt; alert(\u0026#34;Click occurred on \u0026#34; + {{url}}); \u0026lt;/script\u0026gt;   And ditto for all the other listeners. So every time a listener fires, an alert is shown. Just by following the sequence of these alerts, I should always see what the firing order is.\nOutcome Here\u0026rsquo;s what happened on Chrome and Safari:\nClick on \u0026ldquo;Form test\u0026rdquo;   Alert: Link click occurred on (current URL)\n  Alert: Click occurred on (current URL)\n  Alert: Form submit occurred on (current URL)\n  Alert: History event occurred on (current URL + ?#form)\n  So the order is:\nLink Click -\u0026gt; Click -\u0026gt; Form Submit\nThe History Listener fires because the form transports me to the URL ?#form (it\u0026rsquo;s in the action attribute of the form element), and in Chrome and Safari this triggers a popstate event.\nClick on \u0026ldquo;Link test\u0026rdquo;   Alert: Link click occurred on (current URL)\n  Alert: Click occurred on (current URL)\n  Alert: History event occurred on (current URL + ?buttonlink)\n  And the order is:\nLink Click -\u0026gt; Click\nThe History Listener fires this time as well upon popstate, because the active history entry is changed to ?buttonlink thanks to the link\u0026rsquo;s default action.\nWith Firefox, the History Listener won\u0026rsquo;t fire upon page load, so you won\u0026rsquo;t get an alert for the history event changes. Otherwise, the sequence is the same.\nConclusions The result of this simple test is that your GTM listeners fire in the following order:\nLink Click -\u0026gt; Click -\u0026gt; Form Submit\nRemember that any other listeners you might have set up on the element nodes themselves (e.g. onClick) will be fired first. For example, if you have a pushState() call sent upon clicking a navigation link, this will change the URL (if you pass it as a parameter) before your link click listener fires. This, in turn, means that if you have an event sent to GA with gtm.linkClick as the event firing rule, you will see the URL set in pushState() as the page where the link click occurred!\nMake sure that you understand how listeners compete with each other. GMT listeners are last of the line because they\u0026rsquo;re set up on the document node, not the elements themselves.\nI haven\u0026rsquo;t come across a single case yet where having multiple GTM listeners active at the same time would cause any problems in data collection. However, it\u0026rsquo;s not inconceivable, and you should really try to keep things simple with your listener tags (among other things).\n"
},
{
	"uri": "https://www.simoahava.com/tags/tags/",
	"title": "tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-history-listener/",
	"title": "Google Tag Manager: The History Listener",
	"tags": ["Google Tag Manager", "history listener", "listeners", "tags"],
	"description": "Guide to the History Listener in Google Tag Manager. Today, this is known as the History Change trigger.",
	"content": "There\u0026rsquo;s a new listener in town! It\u0026rsquo;s a few days now since the Google Tag Manager team unleashed the History Listener, and the time has come for me to tell you what this baby can do.\nThe History Listener is designed to be used on websites where content is loaded dynamically. Typically, these websites make heavy use of AJAX (Asynchronous JavaScript and XML), which is designed for loading content in the background and serving it dynamically without having to reload the page.\n  Because I don\u0026rsquo;t want to go too deep into how a dynamic website works, this tutorial should be pretty simple. The key is to understand what events trigger the History Listener, and how you can track your page views in a website without static URLs bound to static content.\nBackground A site which loads content dynamically is usually incompatible with certain default browser events. For example, if you have a site which dynamically loads its content without any change in the URL of the page (nothing happens in the address bar), what do you think happens when a visitor navigates away from your site and then clicks the browser Back button? I can give you a hint, they won\u0026rsquo;t see whatever dynamic state the page was in just before they left.\nThis is, of course, a problem for search engines as well, since they want to index pages with unique URL addresses, which each return a static, unique piece of content. This is why URL fragments (AKA anchors AKA hashes) were introduced to solve the problems with browser history. If you loaded content clicking an \u0026ldquo;About Us\u0026rdquo; link, your URL would have looked like https://www.simoahava.com/main#aboutus.\nThis fixed the problem with browser history, since now you could detect changes in the URL fragment and serve appropriate content again, even if the user used the Back button. However, this wasn\u0026rsquo;t good enough for search engines or for Google Analytics, which have a hard time dealing with URL fragments.\nEnter the hashbang (coolest name to be ever given to anything anywhere). The hashbang is a hash followed by an exclamation mark: #!. It\u0026rsquo;s an old invention, having existed in UNIX systems to facilitate script loading. Google adopted it as a schema to signify an AJAX site with crawlable content. When search crawlers came across the hashbang sequence, they translated it to a proper URL query parameter (escaped_fragment=_the-original-fragment). A query parameter meant that the page could be crawled and indexed for search engine users to find.\nHowever, the hashbang isn\u0026rsquo;t the easiest solution to implement, it looks absolutely horrible, and it\u0026rsquo;s just not how you should treat your URLs.\nThe final chapter of the story, and the most relevant one for the History Listener, is brought about by the HTML5 standard. HTML5 introduced the window.history API, which can be used to manually manipulate browser history when loading content dynamically. You can now manually push a state into the browser history, and when someone navigates with the browser\u0026rsquo;s Back button to your page, you can look for stored states and load them in order, just like the default behavior of a static website.\nThe History Listener listens for changes in browser history. If it detects such a change event, it will tell you what happened, whether there\u0026rsquo;s a state object stored in history, and if there was a URL fragment change involved. You can use this information to send virtual page views, prevent certain tags from firing again, serve dynamic tags, and basically anything that you\u0026rsquo;d normally do when a page is loaded.\nSet up the History Listener You set up the History Listener like you would any other listener.\n  Create a new tag\n  Name it wisely\n  Set Tag Type to Event Listener \u0026gt; History Listener\n  Set firing rule to All pages ({{url}} matches regex .*)\n    And that\u0026rsquo;s it. This will set your listener up on all pages to wait for the triggers that activate it.\nYou\u0026rsquo;ll also notice that a bunch of new auto-event variables have been provided. You can use these to access properties of the history event change after the History Listener has fired. Read more about these in my Macro Guide.\nThe triggers The History Listener will activate every time one of the following occurs:\n  Call to the window.history.pushState() method\n  Call to the window.history.replaceState() method\n  A popstate event is detected\n  The URL fragment (hash) changes\n  window.history.pushState() is used to add a state into the browser history stack. pushState() takes three arguments: a state object, a title, and a URL.\nThe state object is where you store details about the content, which you can use later to serve the same content to the returning visitor. The title is just a way to name the history entry, though it can be used with popstate to set the page title. The URL is what will appear in the address bar of your browser after the pushState() csll is completed. This is especially useful, because your site visitors can share this \u0026ldquo;virtual\u0026rdquo; URL, which can be then used to serve the correct content to visitors arriving via this link.\nwindow.history.replaceState() is pretty much the same as pushState(), except that it replaces the current history state with the new state.\npopstate is what occurs every time the active history entry changes. For example, the browser\u0026rsquo;s Back and Forward buttons trigger popstate. If the history entry which is activated upon popstate was created by pushState() or replaceState(), the popstate event will have the stored state object as a property. You can use this object to serve the same content to people navigating through browser history to your AJAX site.\nNote! There are some differences as to how and when browsers send the popstate. Chrome and Safari, for example, fire a popstate with every page load (since that\u0026rsquo;s when the active history entry changes), but Firefox doesn\u0026rsquo;t. Firefox requires a history entry that was created by a pushState() or replaceState() call.\nURL fragment changes are remnants of the hashbang era, but fragments are still widely used on websites. For example, if you have internal anchor links, you\u0026rsquo;ll trigger a fragment change event every time the visitor jumps to an anchor on your page. In AJAX sites, you can listen for fragment change events, and use these to send a page view or fire a tag of your choosing.\n{{event}} equals gtm.historyChange When the History Listener is running, and any of the events described in the previous chapter occur, a gtm.historyChange is pushed into the data layer. Most of the properties in this event can be accessed by the auto-event macros, but it\u0026rsquo;s good to know what the data layer entry looks like.\nHere\u0026rsquo;s what your dataLayer object looks like after a pushState() event. Here, the state object I push is very simple. It contains just a single key-value pair, page: \u0026ldquo;About Us\u0026rdquo;.\n  Here\u0026rsquo;s what you dataLayer looks like after a replaceState() event. The parameters are the same as in pushState(). Note that I fired replaceState() just after pushState(), so you\u0026rsquo;ll see the previous state object stored in the gtm.oldHistoryState property!\n  Now, if I navigate away from the page and return with the browser Back button, a popstate event occurs with my state object neatly stored within:\n  As you can see, the old state and the new state are the same, because I navigated away from the Contact page and arrived on the same page after clicking Back in my browser. When popstate occurs, I can look into the history state object and send the correct page view for the dynamic content. In this case, I can see that the visitor returned to the Contact page.\nFinally, here\u0026rsquo;s me navigating from one URL fragment to another:\n  As you can see, I first went to the bottom of the page via URL fragment #bottom. This triggers a popstate event, with #bottom as gtm.newUrlFragment. After that, I clicked \u0026ldquo;Back to top\u0026rdquo;, which transported me to URL #backtotop, and this is reflected in the screenshot above.\nExample of use I\u0026rsquo;ll show a quick example, expanding the idea of navigating to the \u0026ldquo;Contact\u0026rdquo; page, where the page details are stored in a state object, created using pushState().\nThe premise is that when your visitor clicks a link to the \u0026ldquo;Contact\u0026rdquo; page, the contents are loaded dynamically. To account for history changes, the pushState() method is called with something like:\nvar page = {page:\u0026#34;Contact\u0026#34;}; history.pushState(page,\u0026#39;Contact\u0026#39;,\u0026#39;contact.html\u0026#39;);  This pushes a new entry into your browser history stack, with a single property within the state object. This property is the one we\u0026rsquo;ll want to access in a popstate instance, i.e. when someone returns to the contact page via a history event (browser Back, for example).\n  Make sure your History Listener is running\n  Create a new Data Layer Variable macro to identify what history event took place:\n     Create a new Data Layer Variable macro to get the page property of the new history state:     Create a new rule for your page view tag:    And that\u0026rsquo;s it. You can attach this sequence to your page view tag, for example, ensuring that the page view is sent every time someone navigates back to your Contact content via browser history (see below for how to make sure the page view properties are in order).\nThe firing rule is important. You\u0026rsquo;re looking for a gtm.historyChange event. This is pushed into the data layer when the History Listener is triggered. After that, you\u0026rsquo;ll want to make sure that the event was popstate (the macro in (2)), and that the page property of the new history state has value \u0026ldquo;Contact\u0026rdquo;.\nNow when you send a page view, the URL will be correctly contact.html (since you provide that in the pushState() call). However, your document title might be off, unless you explicitly set it in your AJAX logic. A nice way to send the page title is to use the Document Path setting in your page view tag, and populate it with the value of a Lookup Table macro, which in turn evaluates the page property again. Allow me to demonstrate:\n  That\u0026rsquo;s your page view tag. In the Document Title field, you\u0026rsquo;re retrieving the value from a Lookup Table macro, which I\u0026rsquo;ve named {{ AJAX - Page Title }}.\n  This sets the page title depending on the page property, pushed into the data layer when the history listener is fired. You could set a default value to the Lookup Table macro, but on the other hand, if it doesn\u0026rsquo;t return a value, the tag will just send whatever is found in the document.title property.\nConclusions Phew! This turned into a monster of a tutorial. But that\u0026rsquo;s to be expected. Working with AJAX and dynamic content is not easy, as there are so many things to factor in. Not only are there usability and browser compatibility issues involved, you\u0026rsquo;ll also need to take search engines into account, and you\u0026rsquo;ll need to understand how history events work.\nAs always, I highly recommend that you brush up on your JavaScript before you start working with the History Listener. There\u0026rsquo;s no damage to be done, but you don\u0026rsquo;t want to screw your tracking by not understanding how state objects work, for example. Also, it\u0026rsquo;s very likely that you\u0026rsquo;ll have to return to your AJAX setup to make sure that pushState() calls and replaceState() calls provide all the necessary data for you to use.\nUsing the History Listener on a dynamic site is so much better than resorting to Link Click Listeners. More often than not, you\u0026rsquo;ll come across event propagation problems with your listeners, since most of AJAX navigation is designed to prevent the default behavior of links. With the History Listener, you\u0026rsquo;re only listening for browser history events, so on-page conflicts won\u0026rsquo;t affect the listener\u0026rsquo;s basic functionality.\nYou\u0026rsquo;ll need to be careful and precise with your rules. Make use of all the properties of the gtm.historyChange event, because you\u0026rsquo;ll want to do different things upon pushState() and popstate, for example.\nAnd if you have any questions, drop me a mail or leave a comment below!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-playing-rules/",
	"title": "Google Tag Manager: Playing By The Rules",
	"tags": ["Google Tag Manager", "Guide", "rules"],
	"description": "Guide for Google Tag Manager&#39;s version 1 and its system of rules to fire tags. These have later been superceded by triggers.",
	"content": "There is a new version of this guide for GTM V2 here.\n(Last updated April 2014) I see Google Tag Manager\u0026rsquo;s operational model as an analogy of Montesquieu\u0026rsquo;s three-branched government theory (don\u0026rsquo;t leave just yet, I\u0026rsquo;m getting somewhere with this). We have the legislative power of tags (what should be done), the judiciary power of macros (explore the context and circumstance of each tag), and the executive power of rules (make the tag happen). Not one of these would work without the others, so checks and balances are in place as well.\nGoogle\u0026rsquo;s own documentation on firing and blocking rules is pretty clear and comprehensive, but as always, I think it could be better.\nSo here are a couple of refreshers for you concerning Google Tag Manager rules, and some special cases which have caused headaches for many.\nRules and conditions You can have many rules on a single tag, and your single rule can have many conditions. The difference between these is that if your tag has multiple rules attached to it, they operate in an either-or relationship, so it\u0026rsquo;s enough if just one of the rules is met for the tag to fire.\nNote! If you have multiple rules, operating in an either-or-relationship, your tag can fire multiple times, if all the rules match at different times. So if you have the following rules on your tag: {{event}} equals gtm.js OR {{event}} equals gtm.dom OR {{event}} equals gtm.dom, your tag will fire THREE times per page, when each of these events occur in the GTM loading process.\n  However, if your rule has multiple conditions, all of these conditions must be met at the same time for the tag to fire.\nAllow me to illustrate.\nLet\u0026rsquo;s say you have a tag that you only want to fire on your Thank you page, and you want to make sure that the entire DOM has loaded first. Maybe you have some custom variables that you want to pass along, and they\u0026rsquo;re written on the bottom of the page template.\nHere\u0026rsquo;s how not to do it:\n  In this example, you have two different firing rules set on the tag. The thing is, if you have multiple rules on a tag, it\u0026rsquo;s enough if just one of them checks out. The other doesn\u0026rsquo;t need to. So in this case, your tag will actually fire on every single page, because every page that has the GTM container snippet will also push the event gtm.dom into the data layer to signify that the DOM has loaded.\nHere\u0026rsquo;s how you should do it:\n  So here you have just one rule with two conditions. The difference is huge: when you have conditions on a rule, every single one of these conditions must be met for the rule to work.\nKEY TAKEAWAYS:\n  If you have multiple rules, it\u0026rsquo;s enough that just one of them is met for the tag to fire\n  If you have multiple rules, the tag can fire on each one, so be careful\n  If you have multiple conditions in a single rule, all conditions must be fulfilled for the rule to work\n  {{url}} and {{event}} There are two easy ways to make a tag fire as soon as possible. Either\n{{url}} matches regex .*\nor\n{{event}} equals gtm.js\nYou see, the earliest possible moment that you can query for either of these two rules is when the container is setup and macros are enabled.\nThe reason these two rules do the same thing is that GTM implicitly adds the rule {{event}} equals gtm.js to every rule that doesn\u0026rsquo;t check the {{event}} macro. This is because if there is no event evaluated, a rule such as {{url}} matches regex .* would fire every single time an event is pushed into the data layer. Due to this implicit behavior, a rule like **{{url}} matches regex .*** has actually two conditions, one which is \u0026ldquo;hidden\u0026rdquo;: **{{url}} matches regex .*** AND **{{event}} equals gtm.js**. (Thanks Brian Kuhn for this detail).\nAnother thing to observe is that\n{{event}} can only be one thing at a time.\nThis is really important. So let\u0026rsquo;s say you want to fire a tag after the DOM has loaded AND after the event \u0026ldquo;readyToFire\u0026rdquo; has been pushed to the data layer. The first thing you\u0026rsquo;d probably try is a single rule with the following two conditions:\n{{event}} equals gtm.dom\n{{event}} equals readyToFire\nThe problem here is that {{event}} is a data layer variable, and you can\u0026rsquo;t have a variable with multiple values at any given time (unless it\u0026rsquo;s an array, which {{event}} is not).\nIf you really must wait for gtm.dom first, you\u0026rsquo;ll need to make sure that the code which pushes \u0026ldquo;readyToFire\u0026rdquo; into the data layer is run after {{event}} equals gtm.dom. This way you\u0026rsquo;ll just need to add the rule {{event}} equals readyToFire in your tag, since it\u0026rsquo;s already run after the DOM has loaded.\nKEY TAKEAWAYS:\n  If your rule doesn\u0026rsquo;t check the {{event}} macro, GTM implicitly adds the condition {{event}} equals gtm.js to the rule\n  This is why {{url}} matches regex .* and **{{event}} equals gtm.js** are pretty much the same thing\n  {{event}} can only ever be one thing at any given time\n  Load order of GTM events This is run-of-the-mill stuff, but good to reiterate. When the GTM container is loaded, during the process three different events are pushed into the data layer, which you can use as triggers for your tags. The order is:\n{{event}} equals\u0026hellip;\n\u0026hellip;gtm.js\u0026hellip; \u0026gt; \u0026hellip;gtm.dom\u0026hellip; \u0026gt; \u0026hellip;gtm.load\nHere\u0026rsquo;s how they look (in order) in the data layer:\n  If you want your tags to fire on the earliest possible moment, use either {{event}} equals gtm.js or {{url}} matches regex .*.\nIf you want your tags to fire after the DOM has loaded, for example if you know you have important variables processed at the very bottom of your page template, use {{event}} equals gtm.dom.\nIf you want to wait for the window to load, meaning all initial requests have to be processed first, use {{event}} equals gtm.load.\nNote! I strongly recommend against leaving any critical tags to wait for gtm.load, since any hitches or timeouts in loading your page might lead to the tag never firing.\nKEY TAKEAWAYS:\n  The automatically created GTM events are, in order, gtm.js \u0026gt; gtm.dom \u0026gt; gtm.load\n  Use them to align your tags with their dependencies in the DOM or in external assets\n  Firing and blocking rules In GTM, you can have two kinds of rules: firing rules, which are required for a tag to fire, and blocking rules which can be used to signify when a tag must not fire.\nBlocking rules take precedence, so if you have competing conditions, the blocking rule will always win.\nFor example, in the very first chapter of this tutorial, we had a tag which only fires on the Thank You page. Now let\u0026rsquo;s say I want to exclude the Thank You page from the normal tracking tag, so that double hits are not recorded. I\u0026rsquo;ll have to add the following blocking rule to the tracking tag:\n{{url}} contains thankyou.html\nThis means that even though the tracking tag has a firing rule of {{url}} matches regex .*, it will not fire on the Thank You page, because the blocking rule denies it when {{url}} contains thankyou.html.\n  Blocking rules are an excellent way to reduce clutter in your tags. With a blocking rule, you won\u0026rsquo;t need complicated firing rules, macros or multiple tags to enact a simple IF x THEN y EXCEPT z scenario.\nKEY TAKEAWAYS\n  Firing rules tell the tag when to fire, blocking rules tell the tag when not to fire\n  If there is a conflict, the blocking rule will always win\n  Some useful rules Here\u0026rsquo;s a bunch of rules which might come in handy during your work with GTM.\n  {{url}} matches regex .* - Fire tag on all pages at the earliest possible moment\n  {{event}} equals gtm.js - Fire tag at the earliest possible moment\n  {{event}} equals gtm.dom - Fire tag after the DOM has loaded\n  {{event}} equals gtm.load - Fire tag after the page has loaded\n  {{event}} equals gtm.click - Fire tag when a GTM Click Listener records a click\n  {{event}} equals gtm.linkClick - Fire tag when a GTM Link Click Listener records a link click\n  {{event}} equals gtm.formSubmit - Fire tag when a GTM Form Submit Listener records a form submission\n  Conclusions Google Tag Manager rules are just as important as tags and macros in making your tag setup work. The problem is that rules can get really complicated really soon. When this happens, it\u0026rsquo;s really important to understand things like loading order, GTM events, firing and blocking rules, and so forth.\nThe thing about rules is that you need a good understanding of macros as well, since rules evaluate macro values on runtime. If you need a refresher on macros, don\u0026rsquo;t forget to check my macro guide.\nDo you have any other examples of trouble you\u0026rsquo;ve had with rules? Or have you come up with ingenious ways to use firing and blocking rules to simplify your tag setup?\n"
},
{
	"uri": "https://www.simoahava.com/tags/it-department/",
	"title": "it department",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/tag-management-make-redundant/",
	"title": "Tag Management Does Not Make IT Redundant",
	"tags": ["Google Tag Manager", "it department", "tag management solution"],
	"description": "Just by the virtue of adding a tag management system like GTM, you do not have the right to circumvent your organization&#39;s IT processes. Successfuly tag management incorporates the IT into the development work.",
	"content": "Here are a few quotes I found on the web regarding tag management and IT departments:\nRelief of IT department bottlenecks – once the Tag Manager is deployed, new tags can be implemented directly by Marketing with no IT department involvement. This is a huge benefit for large websites, where IT is oftentimes a bottleneck. Original text here\n IT Issues - when you use a TMS like 'Google Tag Manager' you are bypassing the IT department. If anything goes wrong, they can/will put all the blame on you. Original text here\n Most solutions that require some form of tag management will likely fall within the jurisdiction of the marketing department. By decoupling the tag management process from the IT department greater control is handed to the specifically to the (digital) marketer which is logically where its should be in relation to this process. Original text here\n Tag management saves money as marketers can swiftly make changes to vital assets without enlisting the help of the IT department. Less people and less time means less cost. Original text here\n And of course there\u0026rsquo;s the home page of the Google Tag Manager website itself:\n  The problem in all these quotes is in the rhetoric. Don\u0026rsquo;t get me wrong, all of them make a valid point, but they also do their best to antagonize IT as a slow, antiquated, process-driven, bureaucratic, revenue-hating, nerd-monster, whose sole objective is to make life difficult for marketers.\nThe all-knowing marketer   Here\u0026rsquo;s how it goes. Marketer finds Tag Management Solution. Marketer finds out s/he can \u0026ldquo;create code\u0026rdquo; using fields and a graphical user interface. Marketer questions the need to ever again consult IT, because marketer knows now how to code. Marketer learns of jQuery. Marketer copy-pastes a bunch of jQuery into a custom tag. Marketer breaks website. IT nerds laugh in glee.\nTag Management Solutions (TMS) are, as I see them, in the gray area between the marketer and the IT department. Sure, they\u0026rsquo;re self-contained solutions for manipulating tags which the solution creates, and thus there\u0026rsquo;s usually little cause for concern, if the user doesn\u0026rsquo;t stray beyond the standard features. A marketer with a basic understanding of page templates, front-end logic, and JavaScript can work magic with a TMS.\nAt the same time, these solutions inject front-end code onto the site, they manipulate DOM elements, they make use of global variables, and they are often very tightly bound with server-side logic as well (e.g. with complicated eCommerce setups).\nAn operational scale this huge for any solution cannot and must not make the solution the sole property of any one party. There are many stakeholders involved in any tag management solution setup, and the process should not be botched by some misguided preconception of ownership (\u0026ldquo;this is my solution, and you are not allowed to touch it!\u0026quot;).\nStop antagonizing IT The key is to stop thinking of IT as a bottleneck, or as something you have to fight against in your day-to-day work. There\u0026rsquo;s a reason they are process-driven and take a long time to deliberate.\nQuality assurance. A word unfamiliar to many marketers. Every large web infrastructure has a QA process in place, and it usually involves parallel environments (development, staging, live), a vigorous routine of testing (automated, unit, manual), diligent documentation, agile methods, punctual reporting, and a maintenance process.\nIf the marketer does not respect this, even I wouldn\u0026rsquo;t want them anywhere near the systems I am in charge of. Stéphane Hamel\u0026rsquo;s post Data Quality: Making It Simpler Doesn\u0026rsquo;t Mean It\u0026rsquo;s Simple touches upon this topic, and he makes many important and educational points about the inept marketer.\nThe important thing is to find common ground, to extend the IT processes to cover the TMS as well, and to work together in improving the usability, tracking, and performance of the website. This is not a one-team job, and with large websites it necessary involves IT as well.\nChange the rhetoric Here are my ten suggestions for making the deployment and use of a TMS more manageable to both the marketer and the IT department. These suggestions involve both parties.\n  Stop using antagonizing terminology (\u0026ldquo;disregard\u0026rdquo;, \u0026ldquo;circumvent\u0026rdquo;, \u0026ldquo;make redundant\u0026rdquo;), and switch to a more communicative and collaborative lingo (\u0026ldquo;complement\u0026rdquo;, \u0026ldquo;facilitate\u0026rdquo;, \u0026ldquo;cooperate\u0026rdquo;)\n  Extend quality assurance to the TMS as well, though it might be prudent to adopt a \u0026ldquo;lite\u0026rdquo; version to make things more manageable\n  Train the IT to understand how the TMS works\n  Train the marketer to understand how the IT process works\n  If necessary, make use of tag black- and whitelisting\n  Plan ahead, test and debug thoroughly, document diligently, and if you fail, make sure you fail early\n  Draft a process for going beyond standard features of the solution (DOM manipulation, using global variables, setting and reading cookies)\n  Periodically document and review changes to the website, so that your TMS won\u0026rsquo;t break if e.g. global libraries are updated\n  Run performance tests periodically on the front-end infrastructure, and fix any bottlenecks caused by the TMS\n  Keep up-to-date on the TMS development\n  BONUS: Have a beer together. Be friends.\n  As I see it, a good TMS doesn\u0026rsquo;t replace IT. It complements and facilitates their work, so that the often over-burdened department can focus on more critical things. However, a good TMS also needs a marketer who understands what the TMS is capable of, in good and in bad.\nOf course, a change in mindset isn\u0026rsquo;t warranted solely from the marketer. The IT department must become more sensitive to the fast-moving pace of the marketer\u0026rsquo;s world, where changes to tagging might have to be done on a daily basis. This necessarily requires a level of compromise in QA, or the adoption of a lighter, more agile version of the front-end development process.\nWhat are your thoughts on this dichotomy between the marketer and the IT department? Do you think a TMS should be solely in the marketer\u0026rsquo;s domain, or are you empathetic to the IT\u0026rsquo;s situation as well?\n"
},
{
	"uri": "https://www.simoahava.com/tags/tag-management-solution/",
	"title": "tag management solution",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/awesome-google-tag-manager-resources/",
	"title": "Some Awesome Google Tag Manager Resources",
	"tags": ["Google Tag Manager", "Guide", "JavaScript", "macros", "tags"],
	"description": "Here is a list of a  bunch of extremely useful Google Tag Manager resources found around the web.",
	"content": "When push comes to shove, I\u0026rsquo;m a pretty lazy guy. I enjoy nothing more than just to stretch my legs on a couch, pick up my iPad, and read what\u0026rsquo;s going on in the world. I skip the news, since they\u0026rsquo;re just full of depressing stories. Instead, I head over to my favorite Google+ communities to see what\u0026rsquo;s new in the blogosphere.\n  This approach has led me to some pretty amazing individuals, whom I follow like a suckerfish. These people have written a bunch of great articles, which have helped countless people with their Google Tag Manager installations. Or they might be really active in the online communities which people turn to for advice.\nAs you might have noticed, Google\u0026rsquo;s own documentation is often pretty scarce. That\u0026rsquo;s why so many amazing people do such an incredible job in making the often complicated details behind tag management a bit easier to comprehend.\nWithout too much further ado, I think the time is ripe to reveal my favorite Google Tag Manager resources, so that you can get on your way to becoming a Grand Tag Master.\nI\u0026rsquo;ve added a short description under each link, along with a difficulty level (my own subjective analysis). The difficulty level has nothing to do with the quality of writing. Rather, I use it to denote how difficult it is to understand the concept the article focuses on.\nIf you feel like an essential article or individual is missing from these lists, drop me a line with a link and description, and I\u0026rsquo;ll see if they belong here!\nFollow These People First and foremost, here\u0026rsquo;s my Follow These People list. I consider these people to be top contributors in Google Tag Manager related posts. I\u0026rsquo;ve chosen them partly because I really think that they write a lot of great stuff either in the forums or on their blogs, and partly because they exhibit a quality I enjoy most about a good writer: they don\u0026rsquo;t brag about themselves or their companies, nor do they exhibit (too much) self-promotion.\nSeriously, we\u0026rsquo;re all taking part in a huge knowledge transfer experiment here. Leave your ego at the door.\n  And yes, I\u0026rsquo;m aware that I\u0026rsquo;ve added myself to the picture of my favorite GTM superstars, and also that I\u0026rsquo;ve linked to a number of my own posts in the latter sections. I claim the right to do so under editorial privileges :)\nAlso, don\u0026rsquo;t forget to visit the Google Tag Manager community in Google+ on a regular basis. That\u0026rsquo;s where most of these people hang out.\nLukas Bergstrom, Product Manager for GTM at Google\nTwitter\nJulien Coquet, Senior Digital Analytics Consultant at Hub\u0026rsquo;Scan\nGoogle+, Twitter\nYehoshua Coren, CEO at Analytics Ninja\nGoogle+, Twitter\nJustin Cutroni, Analytics Evangelist at Google\nGoogle+, Twitter\nDoug Hall, Head of Internet Marketing at Conversion Works\nGoogle+, Twitter\nStéphane Hamel, Director of Innovation at Cardinal Path\nGoogle+, Twitter\nClaudia Kosny, Web Analytics Expert at Knewledge\nGoogle+\nBrian Kuhn, Software Engineer at Google\nGoogle+, Twitter\nAndré Mafei, Founder and Web Analytics Consultant at Upmize\nGoogle+\nCarmen Mardiros, Entrepreneur\nGoogle+, Twitter\nPhil Pearce, Freelancer\nGoogle+, Twitter\nJeff Sauer, President at Jeffalytics\nGoogle+, Twitter\nDaniel Waisberg, Analytics Advocate at Google\nGoogle+, Twitter\nGeneral GTM Guides These guides should help you with deploying GTM. There\u0026rsquo;s a lot of things to keep in mind when adopting a tag management system. You\u0026rsquo;d best start with Julien Coquet\u0026rsquo;s excellent slide show on what tag management is, and, more importantly, what it isn\u0026rsquo;t.\nJulien Coquet - Tag Management Is Not A Miracle Cure\nEASY - Essential reading before you go along and implement Google Tag Manager on your website.\nDaniel Waisberg - Google Tag Manager: A Step-By-Step Guide\nEASY - A basic introduction to the tool itself.\nDave Fimek - The Google Tag Manager Datalayer Explained\nEASY - A nice, simple post explaining what the dataLayer object actually is.\nDoug Hall - Google Tag Manager: Coding \u0026amp; Naming Conventions\nEASY - Best practices for Google Tag Manager deployment, naming \u0026amp; extending via custom code.\nSimo Ahava - Macro Guide For Google Tag Manager\nEASY - A guide to macro use in Google Tag Manager by yours truly.\nJeff Sauer - 5 Ways To Troubleshoot Your Google Tag Manager Installation\nEASY - Jeff goes over different ways to make sure your GTM installation is running smoothly.\nGoogle - Developer Guide - Google Tag Manager\nINTERMEDIATE - Google\u0026rsquo;s own developer guide for Google Tag Manager. Essential reading.\nKristoffer Olofsson - A Guide to Google Tag Manager for Mobile Apps\nINTERMEDIATE - Implementation guide for mobile app tracking with Google Tag Manager.\nDorcas Alexander - Unlock the Data Layer: A Non-Developer\u0026rsquo;s Guide to Google Tag Manager\nINTERMEDIATE - A nice guide to the dataLayer object in Google Tag Manager.\nPhil Pearce - Google Tag Manager Development Guide\nADVANCED - A very long and complex guide by Phil about GTM development.\nTracking Guides Tracking is why people install Google Tag Manager. There are so many different ways to track efficiently (and inefficiently) with GTM, and these guides will get you on your way to making sure you collect all the relevant data from your visitors.\nJulius Fedorovicius - 30+ Google Tag Manager Recipes\nEASY - A handy list of Google Tag Manager tracking solutions in a well-presented, easily implementable format.\nJustin Cutroni - Auto Event Tracking With Google Tag Manager\nEASY - An excellent tutorial by Justin on tracking events in GTM.\nPier-Yves C. Valade - A Guide To E-Commerce Conversion Tracking Using Google Tag Manager\nEASY - How to setup your tags and your data layer to send eCommerce transactions to GA using GTM. This example uses Magento as the backend.\nNicholas Blexrud - Google Tag Manager: Configuring Content Grouping\nEASY - Getting content grouping data sent with your hits is dead simple with Google Tag Manager.\nYehoshua Coren - Universal Analytics and Google Tag Manager\nINTERMEDIATE - Excellent slide presentation on Universal Analytics and Google Tag Manager.\nAlex Moore - Classify your Blog Posts in Analytics Using Content Groupings\nINTERMEDIATE - Another great guide for using content groupings with Google Tag Manager.\nClaudia Kosny - Google Analytics Cross-Domain Tracking with Google Tag Manager\nINTERMEDIATE - Excellent three-part guide to cross-domain tracking in Google Tag Manager.\nJim Gianoglio - Cross-Domain Tracking with Google Tag Manager\nINTERMEDIATE - Jim\u0026rsquo;s great tutorial for tracking cross-domain traffic with GTM and Google Analytics.\nSimo Ahava - Google Tag Manager: Track Social Interactions\nINTERMEDIATE - Track social interactions in Google Analytics using GTM.\nGoogle - AdWords Dynamic Remarketing - Tag Manager Help\nINTERMEDIATE - Google\u0026rsquo;s guide to deploying the AdWords remarketing tag on a website.\nSimo Ahava - Why Don\u0026rsquo;t My GTM Listeners Work?\nINTERMEDIATE - Guide to event listeners and conflicts.\nGTM Extensions This is where the real meat of GTM is. Since it\u0026rsquo;s basically run-of-the-mill client-side scripting, you can do some crazy JavaScript magic to collect data in ways you wouldn\u0026rsquo;t have thought possible. Of course, best practices must be observed, which is why you shouldn\u0026rsquo;t try your hand at these before understanding the basics of client-side programming and markup languages.\nShay Sharon - Using Google Tag Manager to Enable Visitors to Opt-Out of Being Tracked\nINTERMEDIATE - How to make your data layer persist using cookies, and how to use this to provide an opt-out of tracking for your visitors.\nSimo Ahava - Universal Analytics: Weather As A Custom Dimension\nINTERMEDIATE - Pull weather data from an external API, and send it to GA with your visit hits.\nJon Meck - Make Phone Numbers Clickable (and Trackable!) Across Mobile Devices with JavaScript\nINTERMEDIATE - Nice extension for tracking on-site phone number clicks.\nAlex Moore - How To Upgrade To Universal Analytics: A Survival Guide\nINTERMEDIATE - A great read on how you should use GTM to upgrade to Universal Analytics.\nDoug Hall - Extending Auto Event Tracking in Google tag Manager\nADVANCED - Not satisfied with the listeners that GTM provides? Create your own! Doug will show you how.\nStéphane Hamel - YouTube Video Tracking with GTM and UA: A Step-By-Step Guide\nADVANCED - Track embedded YouTube videos on your site with GTM and Universal Analytics.\nExternal Resources Finally, here\u0026rsquo;s a bunch of resources you can download or use online to make using and debugging GTM installations so much easier. I use almost all of these on a regular basis.\nWASP: Browser extension for debugging your tags.\nGA Checker: Online tool to check what tags are installed on your website pages.\nTag Inspector: A more robust tool for tag crawling on your pages.\nJSFiddle: Test your JavaScript and your markup before adding it to your site.\nFirebug: Debugger extension for Firefox.\nGoogle Analytics Debugger: Google Analytics debugger extension for Google Chrome.\nTag Assistant (by Google): On-site tag debugger extension for Google Chrome.\nCode Editor for GTM: Replaces GTM\u0026rsquo;s native editor interface with a much more useful one.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/dont-gtm-listeners-work/",
	"title": "Why Don&#39;t My GTM Listeners Work?",
	"tags": ["Google Tag Manager", "JavaScript", "listeners"],
	"description": "Tips and tricks to identify and fix problems with Google Tag Manager&#39;s triggers. Typically the issue is conflicting JavaScript running on the site.",
	"content": "Ever so often I come across a Google Tag Manager setup where GTM\u0026rsquo;s own auto-event listeners don\u0026rsquo;t perform the task they were supposed to. Listener problems seem to be a hot topic in Google+ and the Product Forums as well.\nThere may be many reasons why your listeners don\u0026rsquo;t work, but a very common trend is that you have conflicting JavaScript libraries or scripts running on your page.\nLet\u0026rsquo;s explore how listeners work before tackling the problem. You see, when you attach an event listener to an element in your Document Object Model (the collection of all elements on your page), the listener waits for the element to produce the action it is listening for.\n  With a link click listener, the handler waits for a link (\u0026lt;a/\u0026gt;) element to fire a click event.\n  With a click listener, the handler waits for any element to fire a click event.\n  With a form submit listener, the handler waits for a form (\u0026lt;form/\u0026gt;) element to fire a submit event.\n  Now this should still be easy to follow. You set up a link click listener, and when someone clicks a link, the listener pushes the event gtm.linkClick into the dataLayer object. Run-of-the-mill stuff.\nCompeting listeners Here\u0026rsquo;s the gist. If you have another listener ALSO waiting for the same event, there\u0026rsquo;s a chance that it will interrupt the event from ever reaching the GTM listener, meaning that your link click listener never fires.\nHow is this possible?\nWell, a GTM link click listener, for example, is not actually primed on every single link element on your page. Rather, it\u0026rsquo;s attached to the highest possible DOM element node: the document itself. It utilizes a JavaScript feature called event delegation. This means that when a click event occurs on a link, the event starts to bubble (yes, that\u0026rsquo;s an official term) up the DOM tree, until it reaches the topmost node. Any elements along this bubbly path with listeners attached to them will fire when the event reaches them. This is also called event propagation.\nThe picture below clarifies the difference between a listener on the link node and a listener on the document node. In the example, I have a single link element with a classic onClick=\u0026rdquo;\u0026quot; attribute. In reality, this attribute is also an event listener. It waits for a click event on this specific tag. Also, I have GTM\u0026rsquo;s own click listener primed and ready. As you can see, the onClick listener is attached to the link node itself, whereas the click listener is attached to the document node.\n  A practical example Let\u0026rsquo;s say you have a very simple page with a source code that looks something like this:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My Page\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Google Tag Manager Container here --\u0026gt; \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;http://www.google.com\u0026#34;\u0026gt;Google\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; If I now add a link click listener to my GTM container, it will be attached to the document node, which is a top-level parent to all the elements you see in the code above.\nNow, when someone clicks the link, the following happens:\n  Any listener attached to the \u0026lt;a/\u0026gt; element itself (e.g. onClick) will be fired first, because it is closest to the event occurrence\n  Any listener attached to the \u0026lt;div/\u0026gt; element will fire next, because it is the immediate parent to the element where the event occurred\n  And so on until the event bubbles up all the way to the top of the DOM tree and reaches the document node where GTM\u0026rsquo;s link click listener is waiting\n  Here\u0026rsquo;s the thing. If at any point during the event\u0026rsquo;s bubbly journey to the top of the mountain its process is halted by a conflicting script, for example, the event will never reach GTM\u0026rsquo;s listener!\nThe most common culprit is a competing jQuery listener which contains the following, ominous line of code:\nreturn false;\nThis effectively halts the progress of the event listener, and returns the action to the link element with a \u0026ldquo;don\u0026rsquo;t do what you were supposed to do in the first place\u0026rdquo; type of statement.\nA very common occurrence is when you are using jQuery to animate in-page scrolling to anchor links (i.e. smooth scrolling). I\u0026rsquo;ve seen a bunch of scripts which hijack the link click event, animate the transition to the correct part of the page, and return false;, because, naturally, they don\u0026rsquo;t want the link to perform its default action of instantly transporting you to the correct part of the page.\nWhat\u0026rsquo;s the cure? You\u0026rsquo;ll have to talk to your developers about this. Tell them that you need event propagation to work all the way up to the document node. Usually this can be done by replacing\nreturn false;\nwith\nevent.preventDefault();\nOf course, you\u0026rsquo;ll need to pass the click event to the handler function as a parameter (event in event.preventDefault();) for this to work. There might also be cross-browser concerns, so do some research before making any changes.\n.preventDefault() also prevents the default action of the link click event, but it doesn\u0026rsquo;t stop event propagation from continuing up the DOM tree. You\u0026rsquo;ll have to test it, of course, since replacing the code might disrupt the functionality of your script.\nNOTE! If you use event.preventDefault(); OR return false;, you\u0026rsquo;ll need to uncheck the Check validation option in your link click listener. If this is left checked, the listener won\u0026rsquo;t fire even if the event bubbles up all the way to the document node.\n  Conclusions Understanding event propagation and how GTM\u0026rsquo;s listeners work is one of the things you should not have to worry about when using GTM. After all, tag managers are advertised as a be-all and end-all solution to all your data collection woes with punchlines like: \u0026ldquo;Forget IT\u0026rdquo; or \u0026ldquo;No more development needed\u0026rdquo;.\nThe harsh reality is that a tag manager like GTM is just one more library in the client\u0026rsquo;s often congested resource jungle. Conflicts are bound to happen, especially when external resources like jQuery are used to add functionality to the site.\nFor one, whenever I work with a complex website, I will never, EVER, disregard the client\u0026rsquo;s development or IT unit. Rather, I will talk to them about GTM, and work together with them to come up with the best possible implementation plan, without compromising the current front-end deployment.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/macro-guide-google-tag-manager/",
	"title": "Macro Guide For Google Tag Manager",
	"tags": ["Google Tag Manager", "Guide", "macros"],
	"description": "Guide to Google Tag Manager macros. This is a comprehensive guide for macros, but has been superceded by variables in the latest Google Tag Manager version.",
	"content": "I\u0026rsquo;ve written a new Variable Guide for Google Tag Manager, which covers the new GTM UI. This guide is for the old UI.\nYou might be vaguely familiar with macros if you\u0026rsquo;ve ever used a computer. Basically, whenever you perform a complicated task with a simple gesture, or reuse complex code with a simple input mechanism, you\u0026rsquo;re using macros. Think keyboard shortcuts.\nIn Google Tag Manager, this is the essence of macros. You can do away with a lot of complexity by resorting to macros, especially if you ever find that you need the same piece of code or the same operation over and over again.\nThis guide will first take a cursory look at what macros really are, before going through the (current) list of available macros. I\u0026rsquo;ll add short examples and use cases for each, but there\u0026rsquo;s a whole lot more to be found online.\nWhat is a macro In Google Tag Manager, a macro is a construct that you can set up in the UI. However, it\u0026rsquo;s not a tag in the sense that it would be something you\u0026rsquo;d tag your page with. Rather, macros are turned into a function of the google_tag_manager object (more on this later), and they return a value when called.\n  So a macro is, in essence, a placeholder for some value that you might need in your tags. Most common uses for macros would be to:\n  Retrieve a constant string (e.g. your tracking code)\n  Retrieve a variable string (e.g. the href attribute of a clicked element)\n  Retrieve the result of some simple function (e.g. the date and time of the current visit)\n  Macros are thus used to call for and to retrieve certain values. You can surely see how they facilitate your day-to-day GTM use, can\u0026rsquo;t you? It\u0026rsquo;s much easier to just refer to a macro name rather than having to write the value-retrieving function over and over again.\n  Without macros, whenever you rewrite your hard-coded functions or variables, you have to remember to change them wherever they were used. It\u0026rsquo;s much easier to just have a macro refer to the original location.\nTechnical details DISCLAIMER: Note that the following analysis is my own and only for illustrative purposes. Don't attempt to use the DOM functions uncovered here in your own code, since they are liable to change without notice. Use macros only via the GTM UI using the syntax required by the tool. Well my skills at reverse engineering minified JavaScript code are pretty sparse, but you can uncover a lot just by going through the DOM (Document Object Model) tree of a page where macros are active. You can do this by using a JavaScript console in your browser\u0026rsquo;s developer tools. Just type window and click enter.\nFor example, if you go through the DOM, you\u0026rsquo;ll find an object called google_tag_manager:\n  Here you can see the google_tag_manager array and its contents. There\u0026rsquo;s an index for your container ID (GTM-XXXX), under which you\u0026rsquo;ll find the macro function. The function is actually defined in gtm.js, which you load in the GTM container snippet.\nThe macro function accepts a single parameter and returns something else.\nIt appears that the parameter is a numeral value which refers to a certain macro that you\u0026rsquo;ve set up in GTM. I didn\u0026rsquo;t find any way to actually identify which number returns which macro value, but just by testing you can find a lot of stuff. For example, I have a macro which stores the number of images on the page. Twiddling around with the macro function I finally found the correct index:\n  So as you can see, when you refer to a macro in your tag setup, e.g. {{number_of_images}}, it actually translates to a JavaScript function call that returns the value.\nThis is also the reason why your macros don\u0026rsquo;t accept parameters. The macro itself is a function call which only takes the index number of the macro as its parameter. Runtime evaluation is done on the spot with only elements found in the Document Object Model to work with.\nThis is why you\u0026rsquo;ll see undefined for those indices which expect a value in the data layer that hasn\u0026rsquo;t been pushed yet. As soon as the value is pushed into the data layer, you\u0026rsquo;ll find that the macro index returns the appropriate value.\nHow to use macros To use a macro, you refer to it in GTM with its name between two sets of curly brackets:\n{{macro name}}\nNote that this is case-sensitive, so {{macro name}} and {{Macro name}} refer to different things.\nYou can use macros in any GTM fields which accept a text string, e.g.:\n  On top of that (and this is really cool), you can refer to macros in your custom HTML tags:\n  Here, I check whether or not the clicked link href contained the text \u0026ldquo;.pdf\u0026rdquo;. Since it\u0026rsquo;s an auto-event macro I\u0026rsquo;m using, the macro won\u0026rsquo;t return a proper value until the auto-event variable is pushed into the data layer. So upon page load the macro {{element url}} would return undefined, but after I click a link, it would return the href attribute of the link. This means that in order for this tag to work like it should, I need a firing rule which prevents it from firing until after the auto-event variable is pushed into the data layer. Runtime evaluation, remember?\nYou can also, and this is really REALLY cool, refer to macros in other macros!\n  This returns the tag name of the first child of the clicked element.\nThere\u0026rsquo;s a small point to make about using macros and JavaScript. You can only treat a macro as a JavaScript object in the context of a script. This means that you can freely access macro properties with dot notion, e.g. {{element}}.innerText, in Custom HTML tags and Custom JavaScript macros. However, this won\u0026rsquo;t work in other GTM fields where you can use macros (e.g. Event Category), because these fields do not run JavaScript functions. So if you want to access a property of a macro in a tag field, for example, you will need to create a separate macro for this property.\n  You\u0026rsquo;ll also use macros a lot in your firing and blocking rules:\n  This rule would fire only when visits are tracked to my GA debug account.\nHere\u0026rsquo;s the key takeaway:\nUse a macro whenever you need to reuse a certain value or value-returning function.\nI won\u0026rsquo;t add a separate section for when not to use a macro, but suffice to say, if you only need a certain value once or you have a function which you know won\u0026rsquo;t be reused, don\u0026rsquo;t bother creating a macro for it. Just hard-code it into your custom HTML tag or use the value directly in your tags. A macro is always, after all, another function call, which does increase the complexity of your code.\nRules are a bit different, since you must use macros in rules. You can\u0026rsquo;t write an ad hoc function into your rule; you must have a macro which you evaluate on runtime.\nThe different macro types in GTM Here, I\u0026rsquo;ve listed all the different macro types you can create in Google Tag Manager (at the time of writing). I\u0026rsquo;ve added a short description, and an example use case for each.\nAll macros need a name, which you refer to (case-sensitive) using two sets of curly brackets, e.g. {{macro name}}.\nAlso, you can give a default value to some of the macros. It\u0026rsquo;s a good practice to use a default value, because you can use that as a blocking rule (e.g. {{macro}} equals \u0026ldquo;default\u0026rdquo;) in your tags, and you can avoid getting the pesky undefined as a return value.\nNote also that there\u0026rsquo;s a difference between macro type and macro instance. Below, I list all the different macro types you can use to create different macro instances. You can have as many instances of a macro type as you want (as long as they have different names). Macro types are provided by GTM, and you can\u0026rsquo;t edit or create new macro types.\n  When you create a new container, you get the following pre-populated macro instances. Encased in brackets is the type of macro the pre-populated macro is an instance of:\n  element (Auto-Event Variable / Element)\n  element classes (Auto-Event Variable / Element Classes)\n  element id (Auto-Event Variable / Element ID)\n  element target (Auto-Event Variable / Element Target)\n  element url (Auto-Event Variable / Element URL)\n  event (Custom Event)\n  referrer (HTTP Referrer)\n  url (URL / URL)\n  url hostname (URL / Host Name)\n  url path (URL / Path)\n  So you might be a bit confused at first, since you have these pre-populated macros which are named the same as the macro types they are instances of. But you\u0026rsquo;ll get the hang of it soon enough, don\u0026rsquo;t worry.\n1. 1st Party Cookie   DESCRIPTION\nThe 1st Party Cookie macro returns the value of the cookie whose name you indicate in the Cookie Name field. For example, if you set up a cookie named \u0026ldquo;front-page-visits\u0026rdquo; which increases by one every time the visitor visits your front page, you can set up this macro to return the value (i.e. number of visits) every time the macro is used.\nUSE CASE\nCheck my previous article on the session cookie for a nice use case for this macro.\nBasically this macro can be used to replace any code you used to have for retrieving cookie values.\n2. Auto-Event Variable   DESCRIPTION\nWhen you set up an event listener using GTM, you can use the auto-event variable macros to return the value of various attributes of the element you were listening to.\nYou need to specify the variable type of the auto-event variable you want to use. Here are the different types.\nElement: This returns the element that was clicked. Note that the macro returns an object, not a string, so the best use of it is for traversing the DOM tree of the clicked element (e.g. {{Element}}.children[0].tagName returns the tag name of the first child of the clicked element).\nElement classes: This returns the class attribute of the element that was clicked. For example, if the element was \u0026lt;div class=\u0026quot;mainBanner\u0026quot;\u0026gt;, you\u0026rsquo;d get \u0026ldquo;mainBanner\u0026rdquo; as the return value.\nElement ID: This returns the ID attribute of the element that was clicked. For example, if the element was \u0026lt;a href=\u0026quot;/home\u0026quot; id=\u0026quot;home-link\u0026quot;/\u0026gt;, the macro would return \u0026ldquo;home-link\u0026rdquo;.\nElement target: This returns the target attribute of the element that was clicked. For example, if the element was \u0026lt;a href=\u0026quot;/home\u0026quot; target=\u0026quot;blank\u0026quot;/\u0026gt;, the macro would return \u0026ldquo;blank\u0026rdquo;.\nElement text: This returns the text content (if any) of the element that was clicked. It returns either the innerText or the textContent property (depending on what browser the visitor uses).\nElement URL: This returns the href (in e.g. links) or action (in e.g. forms) attribute of the clicked (or submitted) element. For example, if the element was \u0026lt;form action=\u0026quot;/process-form\u0026quot; id=\u0026quot;myForm\u0026quot;\u0026gt;, the macro would return \u0026ldquo;/process-form\u0026rdquo;. You can also choose which URL component is returned by the Element URL macro (see the chapter on URL macros for a description of component types).\nHistory New URL Fragment: The History macros are populated by the History Listener. History New URL Fragment returns the URL fragment set in the current history state. You can use this to fire your tags (e.g. pageview) when a certain URL fragment is in the URL.\nHistory Old URL Fragment: Returns the URL fragment set in the previous history change event. For example, if you first navigate to #aboutus then to #contact, History New URL Fragment will return \u0026ldquo;aboutus\u0026rdquo; and History Old URL Fragment will return \u0026ldquo;contact\u0026rdquo;.\nHistory New State: History New State returns the state object stored in the current history entry (if any). You can access the properties of this object to set up your content and fire your dynamic tags. Note that it\u0026rsquo;s an object that is returned, so you can\u0026rsquo;t use it directly in any GTM fields. You\u0026rsquo;ll need to use some programming logic to access the properties of this object if you want to send them as strings with your tags.\nHistory Old State: The previous state stored in browser history (if any). You can use this to determine what the previous browser history state was, for example if navigation patterns determine the tags you want to fire.\nUSE CASE\nThere\u0026rsquo;s a million different things you can use auto-event variables for. The variables are most frequently used to refer to certain aspects of the element in event descriptions. You could, for example, add {{element url}} as the action of a GA event which measures link clicks on your different pages.\nCheck my tutorial on auto-event tracking to see how the different auto-event variables can be used to improve your site measurement.\n3. Constant String   DESCRIPTION\nThis macro can be used to store a string you\u0026rsquo;d use over and over again. In other words, it\u0026rsquo;s a time-saver.\nUSE CASE\nStore your GA tracking code in this, so you don\u0026rsquo;t have to remember it in every single tag. Just add the UA-XXXXXXX-X into the \u0026ldquo;Value\u0026rdquo; field, and name the macro \u0026ldquo;GA Tracking Code\u0026rdquo; or something similar. After that, whenever your tracking code is required (e.g. in tags), you can just use the macro {{GA Tracking Code}}.\n4. Container Version Number   DESCRIPTION\nThis returns your GTM container version number. If you\u0026rsquo;re in the Preview mode (as you should be whenever you\u0026rsquo;re testing), this returns the preview container version number.\nUSE CASE\nA nice way to use it is to output the version number into the console using console.log({{container version}}); in your Custom HTML tag. This way you can always check the console if you\u0026rsquo;re unsure which version you\u0026rsquo;re currently running. It\u0026rsquo;s a good way to debug your live implementation.\n5. Custom Event   DESCRIPTION\nThis macro returns the data layer \u0026ldquo;event\u0026rdquo; value of the last event that was pushed into the array. So if you\u0026rsquo;ve just executed dataLayer.push({\u0026ldquo;event\u0026rdquo;: \u0026ldquo;myEvent\u0026rdquo;});, this macro will return \u0026ldquo;myEvent\u0026rdquo;.\nUSE CASE\nThis should be one of your most familiar macros. It comes with every container setup under the name {{event}}. There are about a zillion use cases for this, most notably in firing and blocking rules. For example, if you only want a tag to fire when the event \u0026ldquo;fireNow\u0026rdquo; has been pushed into the data layer, you can add the rule {{event}} equals fireNow to your tag. Then, as soon as you execute dataLayer.push({\u0026ldquo;event\u0026rdquo;: \u0026ldquo;fireNow\u0026rdquo;}); the event macro will be populated with the value \u0026ldquo;fireNow\u0026rdquo; and your firing rule will trigger.\n6. Custom JavaScript   DESCRIPTION\nUse this macro to perform a simple JavaScript function using either ad hoc values or elements in the DOM tree. The JavaScript function has to be encased in a function structure, it has to return a value and it can\u0026rsquo;t take parameters of its own.\nUSE CASE\nYou could use it with auto-event tracking to return the value \u0026ldquo;Outbound link\u0026rdquo; for all outbound links and \u0026ldquo;In-site link\u0026rdquo; for all in-site links. You can then use these as your Event Category in the event where you send information about the link that was clicked. The function would look like this:\nfunction() { if({{element url}}.indexOf(\u0026#34;http://www.my-domain.com\u0026#34;) \u0026gt;= 0) { return \u0026#34;In-site link\u0026#34;; } else { return \u0026#34;Outbound link\u0026#34;; } }  See how I\u0026rsquo;m referring to another macro within this macro? :) \u0026ldquo;www.my-domain.com/\u0026quot; would naturally need to be replaced with your hostname.\nAnother neat trick is to use a Custom JavaScript macro to retrieve the file extension of the link that was clicked. You can use this to create more dynamic tags. Check out the tip on this Google+ post.\n7. Data Layer Variable   DESCRIPTION\nUse this macro to refer to the variables you push into the data layer. Remember, you do the push with dataLayer.push({\u0026ldquo;Variable_Name\u0026rdquo;: \u0026ldquo;Some value\u0026rdquo;});. If you set this macro to look for \u0026ldquo;Variable Name\u0026rdquo;, the return value will be \u0026ldquo;Some value\u0026rdquo;.\nWith \u0026ldquo;version 2\u0026rdquo; (now default), you can refer to nested values using dot notation. For example, if you\u0026rsquo;ve pushed dataLayer.push({\u0026ldquo;Products\u0026rdquo;: {\u0026ldquo;Product_1\u0026rdquo;: \u0026ldquo;Lawn mower\u0026rdquo;}});, by referring to Data Layer Variable Name Products.Product_1, you\u0026rsquo;ll get \u0026ldquo;Lawn mower\u0026rdquo; as the return value.\nUSE CASE\nYou can use it nicely in conjunction with auto-event tracking. When you click an element, the data layer is updated with the object gtm.element, which can be traversed like any DOM object. For example, to get the tag name of the object that was clicked, you\u0026rsquo;d create a data layer variable macro for gtm.element.tagName. Clicking an image would then return \u0026ldquo;IMG\u0026rdquo;.\n8. Debug Mode   DESCRIPTION\nThis returns \u0026ldquo;true\u0026rdquo; if you\u0026rsquo;re in Google Tag Manager Debug mode, and \u0026ldquo;false\u0026rdquo; if you aren\u0026rsquo;t.\nUSE CASE\nSee Lookup Table Macro below for a nice use case.\n9. DOM Element   DESCRIPTION\nUse this macro to refer to any element that can be found in the Document Object Model. The element needs a unique ID identifier. You can even specify the attribute whose value you want to get in return. If you don\u0026rsquo;t specify an attribute, you get the text content of the element (if any).\nUSE CASE\nLet\u0026rsquo;s say I want a certain tag to only fire on pages where the text \u0026ldquo;By Simo Ahava\u0026rdquo; can be found in the \u0026lt;div id=\u0026quot;author\u0026quot;\u0026gt; element. A good use case for just my article pages. For this to work, I\u0026rsquo;d create a DOM Element macro called \u0026ldquo;Author\u0026rdquo; with Element ID \u0026ldquo;author\u0026rdquo;. After that, I can create a firing rule {{Author}} contains \u0026ldquo;By Simo Ahava\u0026rdquo; for the tag that only fires on my article pages.\n10. HTTP Referrer   DESCRIPTION\nThis returns the HTTP referrer (i.e. the previous page URL). You can also choose which URL component is returned by the HTTP Referrer macro (see the chapter on URL macros for a description of component types).\nUSE CASE\nYou can use it to check if this is the first page of the visit or not. Create the macro and then use it like:\nif ({{referrer}}.indexOf(\u0026#34;http://www.mydomain.com\u0026#34;) \u0026gt;= 0) { alert(\u0026#34;Yay, not the landing page!\u0026#34;); } else { alert(\u0026#34;Aww, this is the landing page\u0026#34;); }  11. JavaScript Variable   DESCRIPTION\nThis returns the value of a globally defined JavaScript variable.\nYou can use dot notation to refer to nested variables, e.g. document.doctype to get the doctype declaration of the current document. (Thanks +Brian Kuhn for pointing this out).\nUSE CASE\nIf you haven\u0026rsquo;t (or can\u0026rsquo;t) push the value into the data layer (preferred), or if you can\u0026rsquo;t find the variable in the DOM, you can use this macro to refer to it. A simple use case would be if you define a global variable (e.g. var myName=\u0026quot;Simo\u0026rdquo;;) in a Custom HTML Tag, you can use this macro to get the content of this variable in other tags as well. Sure, you could just use \u0026ldquo;myName\u0026rdquo; to refer to the global variable, but reusability is the key here. If you find that you need to change the variable reference, you just need to change it once in the variable definition and once in the macro, rather than many times in all the tags where you refer to the variable.\n12. Lookup Table DESCRIPTION\nMy favorite macro, by far. Check my five-star review of the lookup table macro for specifics. The lookup table macro is basically a simple runtime evaluation, where you assign value Y to variable Z if X is something.\nUSE CASE\nHere\u0026rsquo;s a really nice use case (originally on Google+). If you use GTM Preview \u0026amp; Debug mode a lot (AND YOU SHOULD!), you might be annoyed at all the debug hits in your GA account (unless, of course, you filter yourself out). You can use a combination of Lookup Table and Debug Mode macros to make sure that debug mode hits are sent to your test account:\n  Here the macro {{GA Tracking Code}} is assigned a variable value depending on the value returned by the {{Debug Mode}} macro.\nRemember that lookup tables only accept simple runtime evaluation of \u0026ldquo;when X equals Y\u0026rdquo;, so you can\u0026rsquo;t do calculations (e.g. \u0026ldquo;when X \u0026lt; 10\u0026rdquo;).\n13. Random Number   DESCRIPTION\nReturns a random number between 0 and 2147483647.\nUSE CASE\nYou can use it to create a random hash for your URLs (for cache-busting) OR you can follow this example of using the random number to fire a tag for only a certain percentage of your visitors.\n14. URL   DESCRIPTION\nReturns the URL or a specific component thereof. The various component types are:\nURL: Returns the entire URL minus fragment (fragment begins with a #).\nProtocol: Returns the protocol of the URL address (i.e. http or https).\nHost Name: Returns the full hostname of the URL (e.g. www.google.com) or you can choose to strip the www-prefix.\nPort: Returns the port number of the hostname. If no port number is specified, the return value is either 80 or 443 (for http and https, respectively).\nPath: Returns the URI of the URL address, i.e. the structure immediately following the hostname. You can also specify \u0026ldquo;default pages\u0026rdquo;, which are stripped out of the return value if found.\nQuery: Returns the query string of the URL address (without leading question mark and possible fragment). You can also specify a query key for the macro, after which only the value of the key is returned if found in the URL.\nFragment: Returns the fragment of the URL without the leading #.\nUSE CASE\nYou could query for only email campaign traffic by creating a URL of component type Query. Set \u0026ldquo;utm_medium\u0026rdquo; as the Query Key. Then you can employ a firing rule for only those visits which came via email with {{URL Query}} equals email.\nConclusions Macros are a really powerful way of making a complicated setup simple. You can use macros to dramatically reduce the amount of code AND separate tags in your Google Tag Manager container.\nWhen using macros, a bunch of best practices should be observed:\n  Name your macros so that you\u0026rsquo;ll instantly identify them (e.g. \u0026ldquo;Name - Macro Type\u0026rdquo;)\n  Only use macros when they facilitate either coding or maintenance; don\u0026rsquo;t use them as replacements for one-off functions or variables\n  Document your macro use somewhere so that you\u0026rsquo;ll know what repercussions changes might have\n  The last is especially important in large implementations, where dozens and dozens of macros are used in dozens and dozens of tags.\nDo you have any questions about macros? Drop a comment on this post if you didn\u0026rsquo;t find what you were looking for above.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-lookup-table-macro/",
	"title": "Google Tag Manager: The Lookup Table Macro",
	"tags": ["Google Tag Manager", "lookup table", "macros"],
	"description": "Introducing the Lookup Table macro in Google Tag Manager.",
	"content": "Having just come hot of the press with my latest article on GTM and Content Grouping which, to my delight, Bounteous had written an amazing tutorial on earlier, Brian Kuhn and the amazing Google Tag Manager development team came out with another incredible new feature: The Lookup Table Macro.\nIn software engineering, a lookup table is an array which takes away a layer of complexity in runtime computation, and replaces it with a simple value assignment based on array indexing. To put it simply, a lookup table looks through an array of source values, and assigns a value to the target depending on what the source value is. Well, maybe it\u0026rsquo;s easiest to show it in an image:\n  This is probably the most simple use case for the lookup table. The range of source values is derived from the custom JavaScript macro {{post_publish_date_month}}, which returns the month when the article the visitor is browsing was published. The lookup table goes through the possible values of the source macro, and it uses these values to assign the literal month name to the target macro, {{Post publish month}}.\nNOTE! The source value (i.e. \u0026ldquo;When {{macro}} equals\u0026rdquo;) is case-sensitive!\nWhy use lookup tables So why resort to a lookup table, when you can just add simple predicate logic to your custom JavaScript, and have it return the month name directly? Well, sure, you could do that, and in most cases it would be just fine.\nHowever.\nA lookup table, just like macros in general, removes a layer of complexity from your code and replaces it with increased flexibility. Because you take away a transformation from the source (i.e. transliteration of the numerical month to the written month name), you are free to use the numerical month elsewhere in your computations.\nAlso, a lookup table is an indexing operation and not strictly a calculation, so the runtime processing power it requires is significantly reduced when compared to having your scripts do all the work. With a large setup, where lookup tables might go through a huge number of source and target operations, you\u0026rsquo;ll end up with less code, less computation, and a nice and flexible framework for value assignment in your macros.\nTo keep the operation as light as possible, you can only check for equation, e.g. \u0026ldquo;if {{target macro}} equals something, then\u0026hellip;\u0026rdquo;. I actually asked Brian about this, and he replied:\n  So if you want to see some other ways of evaluating the source value before assigning a target value, you\u0026rsquo;ll have to wait and see what the GTM team come up with. I guess it will be some combination of rule + lookup table to keep the setup as simple as possible.\nNaturally, you can refer to your lookup table macros in Custom HTML tags and Custom JavaScript macros as well, so you\u0026rsquo;ll increase the modularity of your code.\nUse cases Let\u0026rsquo;s put it this way. Any time you need to do a simple \u0026ldquo;if X is Y then Z\u0026rdquo; evaluation based on a range of values, you could do so with a lookup table. Let\u0026rsquo;s start with a simple one.\nMulti-account or multi-property container If you have a single container deployed across many Google Analytics accounts or properties, you\u0026rsquo;ll come across the problem of assigning the correct tracking code to your tags. You could do it with a unique tag for each deployment, a unique rule for each tag, and a bunch of macros and custom HTML code to check the correct account, but that will soon turn into a veritable noodle-o-rama of a setup.\nRelax. Use a lookup table:\n  Filetype defines event category This is an example of using macros in the lookup table itself. I have a custom JavaScript macro, which checks the filetype of the link that was clicked. It uses the gtm.element auto-event variables used in auto-event tracking. See how I use a macro in the target value field? That\u0026rsquo;s how flexible this is. You can create a framework or infrastructure of macros, and come up with something really complex with very simple processing.\nBe sure to follow the great Carmen Mardiros from Clear Clues to find out (hopefully soon) about applications of complex indexing logic using macro-based syntax.\n  More auto-event madness Another nice use case for auto-event tracking is to set your event parameters depending on what type of auto-event interaction took place. For example, if a link was clicked, I\u0026rsquo;d want my event action to be the URL of the clicked link. If any click occurred, I want my event action to be the tag type (i.e. DIV or IMG or SPAN etc.) of the clicked element. And if a form was submitted, I\u0026rsquo;d want my event action to be the ID of the form. Like so:\n  Then you can just use a single event tag to send your different auto-event hits, with a trigger rule like \u0026ldquo;{{event}} equals gtm.click OR {{event}} equals gtm.linkClick OR {{event}} equals gtm.formSubmit\u0026rdquo;.\nConclusions The lookup table macro is designed to help you actually create a logical infrastructure for your GTM deployment. It also increases flexibility, since you can cross-reference lookup macros in your tags and other macros. I\u0026rsquo;m a huge advocate of macros in general, because the less you hard-code into your custom tags the better.\nThere\u0026rsquo;s no limit to the number of rows in a lookup table (other than general GTM data set limitations). I know that there\u0026rsquo;s also an import feature on the roadmap, so you can import your own, pre-defined lookup tables into the system. Mapping classifications from one data set to another will be something that especially large GTM implementations will benefit from.\nFinally, I couldn\u0026rsquo;t agree more with Eric Erlebacher:\n  The GTM dev team is on a roll right now, and I hope it doesn\u0026rsquo;t stop anytime soon.\n"
},
{
	"uri": "https://www.simoahava.com/tags/content-grouping/",
	"title": "content grouping",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-content-grouping/",
	"title": "Google Tag Manager: Content Grouping",
	"tags": ["content grouping", "custom html", "google analytics", "Google Tag Manager"],
	"description": "Introducing content groupings in Universal Analytics to logically sort your content in the Google Analytics user interface.",
	"content": "Content Grouping is a nice new feature from the good folks at Google Analytics. Basically, it allows you to group your content according to a logical structure. You can create up to five Content Groupings, and you can have as many Content Groups within these groupings as you like. The difference between a Content Grouping and Content Group is hierarchy. The second is a member of the first. Read Justin Cutroni\u0026rsquo;s post on Content Groupings to get you started.\nIt\u0026rsquo;s still obviously in some kind of beta mode, and I saw a number of bugs with the feature when testing for this article. Nevertheless, it\u0026rsquo;s nice to see Content Groupings as a built-in feature of Google Tag Manager now as well.\nWhat you get In some content reports (e.g. Behavior -\u0026gt; Site content -\u0026gt; All pages), you\u0026rsquo;ll see the new Content Grouping selector among the primary dimensions:\n  Once you select an existing grouping, you can drill down in the data and compare how different groups fare among the selected grouping.\nTo be honest, I kind of wish that you could use content groupings to actually filter data on the view level. When I first heard of this feature, I instantly thought of a use case I had with a client. They have two language versions (Finnish and Swedish) running on the same domain, and no folder structure to separate the two, so Finnish URLs and Swedish URLs all mixed together. Content grouping would be a great way to have a separate view for the Swedish pages and for the Finnish pages. For now, I didn\u0026rsquo;t see content grouping as something you can filter your view data with, nor did I see it as something you can even segment your data with. It\u0026rsquo;s just a fancy way of grouping and comparing your content.\nIt didn\u0026rsquo;t take long for the amazing Google Tag Manager development team to make sure that Content Grouping is doable through the GTM UI as well. Here\u0026rsquo;s a brief walkthrough of how to do it. In my example, I created one Content Grouping for all my articles. Under this grouping, I want to measure the efficiency of my various articles by category. For this example, I\u0026rsquo;ll create Content Groups for just my SEO and Analytics articles, but I could just as well extend them to cover all my articles.\nI do have a clear URL hierarchy on my website, so I could just as well use Content Drilldown reports to achieve the same thing, but I beg of you to use your imagination here, oh good reader. There are many (much better) applications for this new feature than what I explore here as a demonstration.\nSetting up Content Groupings in GA First, you need to create the Content Grouping for your view in Google Analytics Admin.\n  Go to Admin\n  Under the View of your choice, select Content Grouping\n      Select Create New Content Grouping\n  Give a name for your Grouping\n  Choose + Enable Tracking Code (required for the GTM setup)\n  Choose an index from 1 to 5 (you can only create 5 Content Groupings!)\n     Click Done and Save  So now you\u0026rsquo;ve created a Content Grouping. Make note of the index number you chose, you\u0026rsquo;ll need it in GTM. I named my new Content Grouping imaginatively \u0026ldquo;Articles\u0026rdquo;. The other grouping in the screenshot below can be disregarded, as I don\u0026rsquo;t use it in this example.\n  Modifying your Google Tag Manager tags To send the Content Grouping data, you need to annotate a pageview tag with the Content Grouping index and group name that you want the pageview to belong to.\nIn my case, I created two new pageview tags. One that fires only on Analytics article pages ({{url path}} matches regex ^/analytic/) and one that fires only on my SEO article pages ({{url path}} matches regex ^/seo/). I naturally had to modify my \u0026ldquo;all pages\u0026rdquo; tracking tag to exclude SEO and Analytics pages, so that I don\u0026rsquo;t get double pageviews.\nTo decorate your pageview with a Content Grouping annotation, you need to do the following.\n  Edit your Pageview tag\n  Click More settings / Content Groups / Add Content Group\n  Here, you need to add the index number of your Content Grouping into the Index field, and the name you want for your Content Group in the Content Group field\n  Make sure you have a rule in place to only fire the pageview on pages which you want to have in the Content Group\n     Save tag, Preview \u0026amp; Debug  So remember, the index number refers to the Content Grouping you set up in Google Analytics Admin. Content Group is the arbitrary name of the sub-group you want to appear in your reports, when you drill down through the (maximum of 5) Content Groupings you created.\nSo Index 1 is my Articles Content Grouping, and then the Content Group is either SEO Articles or Analytics Articles, depending on the folder the page is in. Both Content Groups require a separate pageview tag, naturally.\nTest and review Finally, it\u0026rsquo;s prudent to test your setup. Always, always use the brilliant Preview \u0026amp; Debug mode of Google Tag Manager to verify your changes. If you\u0026rsquo;ve done everything correctly, you should see your new pageview firing on a page which matches the rule you set.\n  Then I use WASP and Chrome Developer Tools to verify that the data is sent correctly. In the screenshot below, you can see the correct parameter (cg1 for Content Grouping 1, and Analytics Articles for Content Group) being sent with the pageview.\n  And you saw the end result at the beginning of the post.\nConclusions It\u0026rsquo;s a nice feature, to be sure. If you don\u0026rsquo;t have a logical folder structure in your site, or if you want to thematically group content without being able to do so in any other way, Content Groupings just might save the day. They\u0026rsquo;re easy to set up, and you can add a created Content Grouping as a dimension in your favorite custom reports as well!\nWishlist / feature request list / gripes\n Fix the bugs!  I tested the above process twice, and on both times it assigned the content to wrong Groupings, even though I double-checked the Index numbers and what I had set in GTM. So for some reason, content for grouping 1 was put under grouping 2 and vice versa The \u0026ldquo;Choose Content Grouping\u0026rdquo; primary dimension appeared on a whim. Sometimes I saw nothing there, on other times I saw just Test as the name of the menu, and once I saw Page Group 1 instead of the Content Grouping name   Make it possible to filter data on the view level by Content Grouping Make it possible to create advanced segments with Content Groupings Improve the tips next to the Content Grouping selector when adding a custom grouping as primary dimension in custom reports. Some of the labels just said Error  Other than that, thanks for another cool feature!\n"
},
{
	"uri": "https://www.simoahava.com/tags/big-data/",
	"title": "big data",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/clickstream/",
	"title": "clickstream",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/context/",
	"title": "context",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/context-king-measure-everything/",
	"title": "Context Is King: Measure Everything!",
	"tags": ["analytics", "big data", "clickstream", "context"],
	"description": "Why it&#39;s difficult to decided a priori what to measure on a website, and why aggregate tracking can sometimes be beneficial.",
	"content": "Have you ever stopped to think about what the chain of events was that led you to a particular decision? Maybe not, but in web analytics it is something that should be considered. After all, there is something counter-intuitive about analytics tools such as Google Analytics, which require us to think in terms of clicks and recorded events that occur on the website, during the visit.\nThankfully, we have evolved as a species, and we no longer place too much emphasis on last click attribution. In short, last click attribution is a model where success of a particular goal is attributed to the conditions underlying the active session. So if a person visits a page via bookmark and downloads a PDF file on that page, the direct visit channel (which covers, by default, bookmarked visits) is the only one which receives the kudos for the file download. Nothing is attributed to other channels, which might have exposed the user to the brand, the product, and the file itself. Ridiculous, right? R.I.P. last click attribution!\nNow, if you read the first paragraph, then continued onto the second paragraph, and you\u0026rsquo;ve respected the basic tenets of literacy by moving onto this, the third paragraph, you might already know where I\u0026rsquo;m going with this preamble. We have entered the age of attribution modeling, where we try to right the wrongs inherent in these tools by affording a certain slice of the attribution pie to all the channels that participated in the conversion. This is vital, since it approaches a more acceptable level of realism, as it models actual human interaction with all the various channels which bring traffic to your site. Check Avinash Kaushik\u0026rsquo;s nice introduction to the topic here.\nHowever, this post is not about attribution modeling, at least in the sense we\u0026rsquo;ve come to learn by using these tools. No, this goes deeper into the murky depths of web analytics.\nClickstream context In web analytics, the clickstream is what we measure. It\u0026rsquo;s the collection of hits, events, variables, pageviews, transactions, etc. that make up the traffic of a website. Data for the clickstream is traditionally collected by adding client-side code (JavaScript, most often) to the website, tagging elements which produce added value to the analysis, and annotating each visit with custom variables and such.\nI tell you, this is a flawed approach. Are you familiar with the Observer\u0026rsquo;s Paradox, outlined by William Labov?\nThe aim of linguistic research in the community must be to find out how people talk when they are not being systematically observed; yet we can only obtain this data by systematic observation. Now transport this idea into the realm of clickstream analytics. Here\u0026rsquo;s the gist: when we tag our website, we are injecting our own, a priori hypotheses into the analysis, thus making it less an objective observation of traffic and more a filtered, premeditated peek through a keyhole. Conversely, we should be expanding our horizons to the infinite mass of possibility that each visit entails. How do we know that tagging contact form X is the best possible dimension to measure, when thinking of business goals? Actually, how do we ever know what to tag?\n  Image courtesy of minifig\nThe problem lies in context. We have absolutely no idea what brought the visitor to make a specific decision. Sure, we can look at our tags and surmise that visitor A visited landing page B via channel C and brought a conversion to goal D. Once we have enough As, Bs, Cs and Ds, we can hypothesize that this is a recurring trend. Emphasis on enough and hypothesize.\nToo few marketing professionals and analysts really question the fact that we are working with samples, and all we ever can do is hypothesize. Not respecting this point can lead to terrible decisions. Being too lazy to look beyond standard reports of a web analytics tool is another thing that\u0026rsquo;s killing creativity in analytics, and creativity is the very thing that\u0026rsquo;s called for here.\nUncovering context There\u0026rsquo;s so much buzz out there around Big Data these days. The buzz is there for a reason. We have the capacity, the processing power, and the insight to work with copious amounts of data, so all we need to do is extend this to traditional web analytics as well.\nMeasure everything! That\u0026rsquo;s the dream. Observe the clickstream as an unfiltered mesh of all the measurable actions performed by an identified visitor. No more a priori tagging, no more injecting your own notions of a successful visit into data collection, no more messing with the data before it reaches you. Server logs are already out there, with all the requests made to the server during each visit. Spice the data with information on mouse movement, scroll depth and speed, heat charts, empty clicks (i.e. clicks that didn\u0026rsquo;t result in an action, such as clicking an image you thought was a link but wasn\u0026rsquo;t), browser controls (such as clicking the back-button or adding a bookmark), etc. That\u0026rsquo;s what I call data!\nWhen we have access to the raw performance of a website, we can then use the tools to dissect it, analyze it, observe it (in real time as well), play with it, filter it, integrate it, export it, and make actual, informed decisions with it.\nBeyond the clickstream Let\u0026rsquo;s go back to the very first sentence of this post.\nHave you ever stopped to think about what the chain of events was that led you to a particular decision? A wise reader has undoubtedly already grumbled about the fact that I only focus on on-site clicks and hits in all of the above. And you are right.\nContext is not just something that can be measured on-site. There\u0026rsquo;s a world out there, if you haven\u0026rsquo;t noticed, full of stimulation. Any number of things might affect your decision to make a purchase in an eCommerce store: You might have just broken your phone and you need a new one, you might win the lottery, or there might be some nasty weather outside and you want to fly somewhere warm.\nIn traditional tagging, creativity was most often channeled to the question of \u0026ldquo;What on-site element should I tag next?\u0026rdquo;. If we measure everything in our clickstream, we can divert our creativity to the wealth of external context that underlies our on-site decisions. Want to understand why so many people are visiting your investor site at odd times? Tap into a financial API and use your stock price fluctuation as the trigger. Want to know if weather affects visitors\u0026rsquo; decisions in your eCommerce store? Annotate the visits with the weather conditions of the visitor\u0026rsquo;s location! There are so many things you can measure out there. You are only limited by your creativity and available technology to support your needs.\nWhere to next I\u0026rsquo;d love to hear if there are solutions for measuring everything out there.\nMeanwhile, there\u0026rsquo;s so much of this \u0026ldquo;external\u0026rdquo; context that you can start measuring. Take a long look through this API Directory, and see if you can find something that you want to measure.\nAnd remember this mantra: Context is King! Measure Everything!\n"
},
{
	"uri": "https://www.simoahava.com/tags/event/",
	"title": "event",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/universal-analytics-send-custom-dimension-event/",
	"title": "Universal Analytics: Send Custom Dimension With Event",
	"tags": ["custom dimension", "event", "google analytics"],
	"description": "To make the most of your Google Analytics tracking, you can send Custom Dimensions with non-interaction events simply to add session- and user-scoped custom dimensions to your data.",
	"content": "(Last updated March 2014) This post is the final chapter of a trilogy. The ultimate refinement, if you will. It all started with my foray into the murky waters of context, when I tested how weather data could be used to provide extra information about site visits. When I wrote that post, I had two trepidations: 1) does sending the API call with every single page view affect site performance negatively, and 2) does forcing the page view call to wait for the API call to complete affect the quality of visit metrics.\nI fixed (1) to my satisfaction in a later post on utilizing a custom cookie to make the API call just once per session. This meant that site performance improved, since the API call (which might take up to 500ms) is done just once per visit. Also, it has a positive effect on visit calculation, since the possibility of the page view call failing to complete due to problems in the chain of events is minimized.\nHowever, I was still not happy with having any code executed before the page view, because page views are something you don\u0026rsquo;t want to compromise due to, for example, problems with a third-party API (such as the weather API).\nSo I did what I should have been doing all along.\nCustom dimensions can be sent with events as well Here\u0026rsquo;s a little background. I have a script which polls a weather API, with the visitor\u0026rsquo;s location as the parameter. The script collects the weather conditions of said location, and passes them to a visit-scope custom dimension, which is then sent to Google Analytics.\nA custom dimension scoped for visits (or sessions) means that you just have to send it once per visit. All subsequent and previous hits (page views, events, transactions) of the visit are annotated with the dimension as well. Naturally, it\u0026rsquo;s a no-brainer to thus send the dimension just once per session, hence the need for a custom cookie (again, see this post).\nAlso, since you can send the same custom dimension with an event, why even use a page view? A non-interaction event does the same thing PLUS you can compare the number of weather events and visits from the same time period to see if there are discrepancies which indicate problems with the API call.\nAnother perk, if using GTM, is that you don\u0026rsquo;t need separate, confusing page view tags (one with the weather dimension, one without), but rather you can have your basic page view tag for tracking and an event tag for sending the weather data (which fires only when the API call is made).\nImplementation Well, I need to refer back to the code from my previous two posts. Here\u0026rsquo;s what the NEW custom HTML tag should look like (remember, you need to load jQuery and the geoPlugin service for this to work).\n\u0026lt;script\u0026gt; if (typeof({{Session alive}}) == \u0026#34;undefined\u0026#34;) { var lat = geoplugin_latitude(); var lon = geoplugin_longitude(); var weather = \u0026#34;\u0026#34;; var weatherAPI = \u0026#34;http://api.openweathermap.org/data/2.5/weather?lat=\u0026#34;+lat+\u0026#34;\u0026amp;lon=\u0026#34;+lon; $.ajax({ type : \u0026#34;POST\u0026#34;, dataType : \u0026#34;jsonp\u0026#34;, url : weatherAPI + \u0026#34;\u0026amp;units=metric\u0026amp;callback=?\u0026#34;, async : true, success : function(data) { weather = data.weather[0].main ; dataLayer.push({\u0026#34;weather\u0026#34;: weather}); },error : function(errorData) { console.log(\u0026#34;Error while getting weather data :: \u0026#34;+errorData.status); dataLayer.push({\u0026#34;weather\u0026#34;: \u0026#34;Undefined\u0026#34;}); },complete : function() { dataLayer.push({\u0026#34;event\u0026#34;: \u0026#34;weatherDone\u0026#34;}); } }); } var d = new Date(); d.setTime(d.getTime()+1800000); var expires = \u0026#34;expires=\u0026#34;+d.toGMTString(); document.cookie = \u0026#34;session=1; \u0026#34;+expires+\u0026#34;; path=/\u0026#34;; \u0026lt;/script\u0026gt; Here\u0026rsquo;s the order of events:\n  Check if a cookie named \u0026ldquo;session\u0026rdquo; exists\n  If cookie doesn\u0026rsquo;t exist, use the visitor\u0026rsquo;s location to query the weather conditions at said location\n  Save weather conditions in data layer variable \u0026ldquo;weather\u0026rdquo;\n  Push event \u0026ldquo;weatherDone\u0026rdquo; to data layer whether or not the API call is successful\n  Finally (regardless of whether the cookie existed or not) reset the \u0026ldquo;session\u0026rdquo; cookie to expire in 30 minutes\n  So no need for a second event \u0026ldquo;noWeather\u0026rdquo; for the weatherless page view. You can just have the normal page view sent with every page load, and then use \u0026ldquo;weatherDone\u0026rdquo; as the trigger rule for the event (remember to make it a non-interaction hit) which sends the \u0026ldquo;weather\u0026rdquo; variable as a custom dimension. Dead simple.\nFinally, you\u0026rsquo;ll need to set up your event tag. Remember, you need to create the custom dimension in Google Analytics\u0026rsquo; admin section (see here for a guide). Make note of the index number assigned to your new, session-level custom dimension.\nNext, create a new event tag. Now, you can have whatever you want as your event fields, but make sure that Non-Interaction Hit is set to True. In this example, I send my custom weather type and temperature (they\u0026rsquo;re data layer variable macros), but as you can see, Non-Interaction Hit is True. This is important, because then this weather event won\u0026rsquo;t affect bounce rate calculation (because it doesn\u0026rsquo;t tell anything about engagement now, does it?).\n  And here\u0026rsquo;s what the custom dimension settings look like. You set them in the end of the tag, under More settings.\n  Conclusion I think I\u0026rsquo;ve got the whole thing pinned down now. I have come up with a method to\n  poll external APIs for contextual information\n  send this information just once per visit\n  use a non-interaction event to annotate the visit hits with this dimension\n  Next steps would be to make sure the API is reliable and fast. Also, maybe some sort of timeout trigger for the API call should be in place, with a \u0026ldquo;weather\u0026rdquo;: \u0026ldquo;Undefined\u0026rdquo; value sent if the API call doesn\u0026rsquo;t complete.\n"
},
{
	"uri": "https://www.simoahava.com/analytics/universal-analytics-fire-script-just-per-session/",
	"title": "Universal Analytics: Fire Script Just Once Per Session",
	"tags": ["cookie", "google analytics", "Google Tag Manager", "JavaScript", "universal analytics"],
	"description": "Guide for creating a cookie that emulates Google Analytics sessions. The purpose is to use it to delimit specific Google Tag Manager tags to fire only once per GA session.",
	"content": "There is a new version of this post for GTM V2 here.\nWhile going over my previous post about using weather conditions to segment data in Google Analytics, I started thinking about performance issues. Since I\u0026rsquo;m using a visit-scope custom dimension, it seems futile to have it send the weather details with every single page load. The odds of the weather changing drastically during one visit are slim (unless you live in the UK), and I have yet to come up with a good reason to change my on-site behavior because the weather changed from a drizzle to a downpour.\nWith Universal Analytics, it\u0026rsquo;s not possible to mine session data from the cookie set by the service. This is because the custom client ID set by the cookie is \u0026ldquo;later used by Google Analytics servers to calculate visitor, session, and campaign data.\u0026rdquo; (emphasis mine; see the developer guide). This is relevant because I want my custom JavaScript to only fire once, at the beginning of a session (visit). I have no way to use the _ga-cookie since, as stated above, it calculates session data in the bowels of Google Analytics servers.\nTo overcome this problem, I create a custom session cookie, which mimics the default properties of the Google Analytics cookie mainly in that it expires after 30 minutes of inactivity as well.\nDo note that this is not a 100 % accurate emulation of how Google Analytics calculates sessions. The cookie doesn\u0026rsquo;t take into account changing campaign sources which, by default, always initiate a new session in Google Analytics. Preliminary research shows that the accuracy is around 95 %, though I\u0026rsquo;m getting some missed hits to the Open Weather API as well, which means that the cookie is actually even more accurate.\nHow it works I\u0026rsquo;m trying to be generic here, but I will use a real-life example from the weather script I created previously.\nHere\u0026rsquo;s how the implementation basically works:\n  When the visitor comes to the site, my script searches for a cookie named \u0026ldquo;session\u0026rdquo; using Google Tag Manager\u0026rsquo;s Session macro\n  If the cookie exists, it means that a session is still active and no code is executed but for a reset of the cookie expiration time\n  If the cookie doesn\u0026rsquo;t exist, it means that the session has expired, and in this case I run the weather code and then create the cookie\n  If you try to create a cookie which already exists, it overwrites the original cookie. This is why I can use the same cookie setup code for both (2) and (3) above.\nSince it\u0026rsquo;s a first-party cookie, and since it\u0026rsquo;s set on the root path of the site (/), there\u0026rsquo;s no risk of overwriting any other cookies (unless some other site plugin has a cookie named \u0026ldquo;session\u0026rdquo;).\nThe cookie code My original weather tracking solution fired on every single page load. This creates quite a burden on the API as well as site performance (since it might take almost half a second for the API script to fire). My new code looks like this:\n\u0026lt;script\u0026gt; if (typeof({{Session alive}}) == \u0026#34;undefined\u0026#34;) { // Weather tracking code here  } var d = new Date(); d.setTime(d.getTime()+1800000); var expires = \u0026#34;expires=\u0026#34;+d.toGMTString(); document.cookie = \u0026#34;session=1; \u0026#34;+expires+\u0026#34;; path=/\u0026#34;; \u0026lt;/script\u0026gt; As you can see, the old code is only run if the macro {{Session alive}} is undefined, i.e. the cookie doesn\u0026rsquo;t exist.\nCreating the macro is dead simple:\n  Create new macro called \u0026ldquo;Session alive\u0026rdquo;\n  Set Macro Type 1st Party Cookie\n  Set Cookie Name \u0026ldquo;session\u0026rdquo;\n    So what you do here is create a macro which returns the \u0026ldquo;session\u0026rdquo; cookie value. Whenever this macro is referenced to in code, you are actually using the cookie value.\nWhen I poll for this cookie in the code above, I\u0026rsquo;m actually only interested in whether this cookie exists or not. That is why I use the if(typeof({{Session alive}}) == \u0026ldquo;undefined\u0026rdquo;) structure. If the cookie doesn\u0026rsquo;t exist, the macro has no reference, which means that it is an undefined structure.\nFinally, regardless of whether the cookie was found or not, a cookie named \u0026ldquo;session\u0026rdquo; is created with an expiration date 30 minutes in the future (1 800 000 milliseconds later).\nPros and cons Well obviously, having the cumbersome API call fire just once per session is a victory for site performance and for \u0026ldquo;best practices\u0026rdquo; analytics. Because I already have a visit-scoped dimension, it would be counter-intuitive to send it with every single page view hit.\nThe main problems, as I see them, have to do with maintenance. If you have a custom expiration time for the GA session, you need to remember to modify the expiration date (now 1800000 milliseconds) in the \u0026ldquo;session\u0026rdquo; cookie as well, if you want it to mimic Google Analytics performance. Also, if you already have a cookie named \u0026ldquo;session\u0026rdquo; firing on your site, you need to change this one\u0026rsquo;s name.\nOne more problem was discussed in the opening paragraphs of this post. This is not a 100 % accurate emulation of Google Analytics session computation, so you will have a slight margin of error in the number of visits with weather as a dimension vs. the number of visits to your site. Feel free to explore how to make calculating a GA session even more accurate, and drop me a line if you find something interesting :)\nI would really like it if I could bind my code to the actual _ga-cookie, but as far as I know, that\u0026rsquo;s not possible.\nEDIT: See this post on sending custom dimensions with events for a smarter implementation of the idea above.\n"
},
{
	"uri": "https://www.simoahava.com/web-design/how-to-not-make-a-splash/",
	"title": "How To (Not) Make A Splash",
	"tags": ["analytics", "JavaScript", "SEO", "splash page"],
	"description": "Rant about splash pages and the how they can be done so extremely poorly.",
	"content": "A word of warning. This is not a developers\u0026rsquo; post, a guide, or a thought experiment. This is a bona fide rant. Sometimes we just need to vent.\nA couple of weeks ago, I checked one of our (inactive) client\u0026rsquo;s Google Analytics accounts I still had access to. What I saw in the acquisition report was this:\n  See how direct traffic gobbles up a great big share of organic traffic in late October?\nAs my fingers tingled with the anticipation of yet another web analytics mystery (the fuel that keeps my engines going), I was overcome with a feeling of dread. I knew for a fact that there had been a site redesign at the time (which is part of the reason I checked the account), and I had originally had some concerns with the technical implementation.\nAnyway, when one metric falls and another rises with an observable correlation, there\u0026rsquo;s usually something very wrong with site tracking. On top of that, if the fluctuation involves direct traffic, the problem is usually related to referral data being overwritten for some reason.\nWhen I went to take a look at their site, the problem became too apparent. A splash page was present. And, to make matters worse, some shoddy JavaScript reared its ugly head.\nThe problem with the splash A splash page is a screen that the visitor sees prior to entering the website. It\u0026rsquo;s been a pain for SEO and usability for a long time (see e.g. this splash page rant from 2010, or this more recent one). It introduces an unnecessary step into the conversion path, it usually caters to the wrong crowd for all the wrong reasons, and if there\u0026rsquo;s any kind of variable visibility involved (like checking for cookies or language settings), the JavaScript redirection might just be done in the wrong way.\nI know this rant is about the splash page, but I just can\u0026rsquo;t help myself. Why is JavaScript a free-for-all programming ride, where there really are no consequences for shoddy workmanship? What\u0026rsquo;s wrong with quality assurance even with major companies, where the code that gets published is riddled with performance problems, security issues, and over-complicated syntax? Global variables are being overwritten with abandon, errors are not caught and handled, references are outdated, and pushing all this crap into a library just makes the issue exponentially worse when the library is pushed to public repositories.\nIn the case of the client\u0026rsquo;s account, the splash page was used as a country / language selector. Now, I consider that to be a moderately acceptable reason for hijacking the visit, but I\u0026rsquo;m willing to bet there\u0026rsquo;s a better way to do it. Instead of imposing a selection screen, which in itself is a turn-off for anyone who just wants to get to the site, why not do an IP redirection based on geolocation, with a visible but unobtrusive way of indicating why the visitor was transported to the version they\u0026rsquo;re seeing? Maybe a small banner at the top of the page, which can be closed with a click. Of course, the banner should also have a quick link menu, with which the visitor can go to the correct version, if they were redirected wrong.\nSo what was wrong with the example? Why yes, I sidetracked a little. The problem was that upon entering the main page of the global site, a JavaScript function checked whether or not the user had a country cookie in their browser. This cookie is downloaded once the user chooses a country from the country selector, or visits a local site directly. If the cookie is not in place, they are redirected (with JavaScript again) to the splash page.\nThat\u0026rsquo;s right. A JavaScript redirection. Anyone with even the slightest knowledge of SEO and web analytics best practices is now permitted to shudder. Here\u0026rsquo;s the kicker:\nIf you do a JavaScript redirection before GA has a time to write the visit cookie, you lose the original referral information.\nYou see, because the GA tracking code doesn\u0026rsquo;t have time to load before the redirection is fired, the referral information is not written to the session cookie. This means that the first time the referral information is recorded is after the redirection, when the user enters the splash page (if it has tracking enabled). Because the original referral information is lost, the session is recorded as originating from direct traffic. Direct traffic is notorious for being the dumping ground for all sorts of undefined traffic sources. In essence, a JavaScript redirection which does not include the referral information throws away a key ingredient of understanding how your traffic channels perform.\nHow to avoid the problem If you want the redirection, your best bet is to let Google write the session cookie on the front page before the redirection. Note! If the redirection is to another domain, you will need to implement cross-domain decoration for the redirect URL!\nThe best solution, by far, is to redesign your site structure so that a splash page is not needed. Here are some ideas:\n IP redirection based on geolocation. Just remember to help those out who are redirected wrongly, or you\u0026rsquo;ll have an even bigger problem on your hands Integrate the country selector into the flow of your main site without a splash page Optimize your country sites so that people find them without having to go through your global site  "
},
{
	"uri": "https://www.simoahava.com/tags/splash-page/",
	"title": "splash page",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/categories/web-design/",
	"title": "Web design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/google-tag-manager-track-social-interactions/",
	"title": "Google Tag Manager: Track Social Interactions",
	"tags": ["custom html", "Google Tag Manager", "social interactions", "universal analytics"],
	"description": "Use social plugins&#39; own APIs to track social interactions in Universal Analytics.",
	"content": "(Last updated June 2014) Google Analytics provides us with a nifty way of tracking social interactions. With a simple plugin, you can track how many +1s and Likes your pages accumulate.\nThis guide shows you how to activate social interaction tagging with Google Tag Manager and Universal Analytics. The instructions are for Facebook Likes, Google+ +1s (now deprecated since Google Analytics tracks +1s automatically), Twitter Tweets and Pinterest Pins.\nNote that if you use a third-party API (e.g. AddThis) these tricks probably won\u0026rsquo;t work. You\u0026rsquo;ll need to use the API functions themselves to enable tracking.\nSetting up social interaction macros Well, first of all, you\u0026rsquo;ll need a bunch of macros to collect the data from your social actions.\nSo let\u0026rsquo;s start with Google Tag Manager.\nSteps:\n Create a new container version Create a new macro of the Data Layer Variable persuasion, and name it Social network Add socialNetwork as the variable name Save the macro     Create a new Data Layer Variable macro, and name it Social action Add socialAction as the variable name Save the macro Create a new Data Layer Variable macro, and name it Social target Add socialTarget as the variable name Save the macro  So now you created three important macros, all making it easier for to track social interactions via Google Tag Manager. Network will pass the name of the social network the action belongs to (e.g. Facebook), Action will pass the name of the action performed (e.g. Like), and Target will pass the name of the page the action occurred on (e.g. http://www.example.com/post1).\nSetting up the send tag For social interaction events to appear in Google Analytics, they need to be sent (d\u0026rsquo;oh again). To send a social interaction, you need to create a tag which will have the macros we just created as its arguments. This tag is then set to fire upon a social action.\n Create a new Universal Analytics tag Give it a name (such as Send social) Add your tracking ID (it\u0026rsquo;s best to store it as a macro) Choose Social as the Track Type Add your macros as the arguments (see image below for clues) Set {{event}} equals socialInt as the firing rule (we\u0026rsquo;ll come back to this soon) Save tag    Here we create a new tag which houses the arguments the social interaction will have. The firing rule is primed to wait for an event called socialInt, which we\u0026rsquo;ll send from our social action tags.\nSet up Facebook like   I will not go into how to add a Facebook like button to your page, you\u0026rsquo;ll have to do that yourself. Facebook has a great guide and tool behind this link which will help you create the necessary code.\nHowever, once you have your button in place, you need to somehow track the clicks. This is done by using a callback function. A callback is a function which is run immediately after an event has completed.\nHere\u0026rsquo;s how to create the callback function in GTM.\nSteps:\n Create a new Custom HTML tag (or use a single tag for all your social share callbacks) Give it a name (e.g. Social Shares) Add the following code:  \u0026lt;script\u0026gt; if (typeof FB !== \u0026#39;undefined\u0026#39;){ FB.Event.subscribe(\u0026#39;edge.create\u0026#39;, function(href) { dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;socialInt\u0026#39;, \u0026#39;socialNetwork\u0026#39;: \u0026#39;Facebook\u0026#39;, \u0026#39;socialAction\u0026#39;: \u0026#39;Like\u0026#39;, \u0026#39;socialTarget\u0026#39;: href }); } ); } \u0026lt;/script\u0026gt;  Set to fire after DOM has been loaded ({{event}} equals gtm.dom) Save tag  OK, so here\u0026rsquo;s your Facebook Like callback function. Upon clicking the Like button, this function is called. Before control is returned to the page, the trigger event for the Send Social tag (socialInt) is pushed into the data layer. After that, the three required arguments are passed along as well. The value for socialTarget is href, which is one of the parameters of the callback function, and it represents the URL of the page the event occurred on.\nBy using FB.Event.Subscribe, you can listen to a number of different events (e.g. message send, login). Check the available options in the Facebook dev guide.\nSet up Google +1   UPDATE: Note that this is pretty much obsolete now, since Google Analytics and Universal Analytics both track +1s automatically.\nAgain, I\u0026rsquo;m not going to tell you how to add the button to your page. It\u0026rsquo;s dead simple, and you can find a good guide here.\nWith Google+, you need to specify the callback as an argument in the button tag. With Facebook this was automated. Note that there are a number of events for which you can specify a callback: clicks, interaction starts (when someone hovers or clicks the button) and interaction ends (when someone closes the +1 bubble). I just chose the click since it\u0026rsquo;s the simplest way to track (though a tad inaccurate).\nI chose sendPlus as the callback function name. A sample button code would then be:\n\u0026lt;g:plusone href=\u0026#34;http://example.com\u0026#34; size=\u0026#34;standard\u0026#34; annotation=\u0026#34;none\u0026#34; callback=\u0026#34;sendPlus\u0026#34;\u0026gt;\u0026lt;/g:plusone\u0026gt; Note that the developers\u0026rsquo; reference has the callback as data-callback, but this seems to work as well.\nNow to set up the callback function.\n Create a new Custom HTML tag (or use a single tag for all your social callbacks) Give it a name (e.g. Google Plus One) Add the following code:  \u0026lt;script\u0026gt; function sendPlus(g) { dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;socialInt\u0026#39;, \u0026#39;socialNetwork\u0026#39;: \u0026#39;Google\u0026#39;, \u0026#39;socialAction\u0026#39;: \u0026#39;+1\u0026#39;, \u0026#39;socialTarget\u0026#39;: g.href }); }; \u0026lt;/script\u0026gt;  Set to fire on all pages ({{url}} matches RegEx .*) Save tag  It\u0026rsquo;s basically the same as with Facebook.\nThis callback function hijacks the +1 button click and sends the necessary stuff to datalayer before returning control back to the page.\nSet up tweets   To set up a Tweet callback function, you\u0026rsquo;ll need to make use of web intent events. They add additional JavaScript functionality (such as the option to declare a callback function) to your Tweet button.\nInitialize the Twitter widgets with the following code in your HEAD element:\nwindow.twttr = (function (d,s,id) { var t, js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id)) return; js=d.createElement(s); js.id=id; js.src=\u0026#34;https://platform.twitter.com/widgets.js\u0026#34;; fjs.parentNode.insertBefore(js, fjs); return window.twttr || (t = { _e: [], ready: function(f){ t._e.push(f) } }); }(document, \u0026#34;script\u0026#34;, \u0026#34;twitter-wjs\u0026#34;));  You don\u0026rsquo;t have to make any modifications to your Tweet button code (providing it\u0026rsquo;s not implemented via a third-party API, such as AddThis.\nIn your social share Custom HTML tag, add the following code:\nif (typeof twttr !== \u0026#39;undefined\u0026#39;) { twttr.ready(function (twttr) { twttr.events.bind(\u0026#39;click\u0026#39;, clickEventToAnalytics); }); } function clickEventToAnalytics() { dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;socialInt\u0026#39;, \u0026#39;socialNetwork\u0026#39;: \u0026#39;Twitter\u0026#39;, \u0026#39;socialAction\u0026#39;: \u0026#39;tweet\u0026#39;, \u0026#39;socialTarget\u0026#39;: window.location.href }); }  Here you bind the CLICK event to function clickEventToAnalytics. There are a bunch of different events at your disposal, and you can find them in the developers\u0026rsquo; documentation.\nNote how I use the global JavaScript variable window.location.href as the target this time.\nSet up Pinterest Pin It Pinterest is a bit problematic, since when you load the pinit.js script, it replaces any code you may have used in the link tags. This is problematic because Pinterest doesn\u0026rsquo;t provide any callback functions, so the social share push would need to be added directly to the link code. With pinit.js, any modifications you do are erased upon page load.\nHowever, you can get your pins tracked using auto-event tracking. With a Link Click Listener in place, create a Custom HTML Tag with the following content:\n\u0026lt;script\u0026gt; dataLayer.push({\u0026#39;event\u0026#39;: \u0026#39;socialInt\u0026#39;, \u0026#39;socialNetwork\u0026#39;: \u0026#39;Pinterest\u0026#39;, \u0026#39;socialAction\u0026#39;: \u0026#39;Pin It\u0026#39;, \u0026#39;socialTarget\u0026#39;: window.location.href}); \u0026lt;/script\u0026gt; The firing rule for this is:\n{{event}} equals gtm.linkClick\n{{element classes}} contains pin_it_button\nSo once the Pin It button is clicked (the link has \u0026ldquo;pin_it_button\u0026rdquo; in its long list of class names), this custom HTML tag pushes the event into the data layer, after which your social share event tag is fired.\nTest implementation So now you have your callback functions which provide the data, the macros which store the data, and the Universal Analytics tag which sends the data. Wonderful!\nSave your container version and publish it.\nNow go to a page on your site where the buttons can be found. Use a debugger like Firebug, or Chrome\u0026rsquo;s Developer Tools, and load the page. Pay attention to the Network tab in your debugger, and click Like or +1. You should see something happening, namely a GET request to www.google-analytics.com. Check out the headers of this request, and you should see something like this:\n  Here you can see the query parameters.\nsa is Social Action (Like)\nsn is Social Network (Facebook)\nst is Social Target (URL of the page)\nOf course, don\u0026rsquo;t forget to check Google Analytics itself (it might take some time for the data to appear).\n  "
},
{
	"uri": "https://www.simoahava.com/tags/social-interactions/",
	"title": "social interactions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/excel/",
	"title": "excel",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/universal-analytics-grab-offline-data-excel/",
	"title": "Universal Analytics: Grab Offline Data From Excel",
	"tags": ["excel", "universal analytics"],
	"description": "Script and tools to send offline data from your Excel report to Universal Analytics.",
	"content": "UPDATE 9 Oct 2013 - This works in Google Spreadsheets now as well! Feel free to make a copy of the sheet and work with the code. It\u0026rsquo;s still just a prototype, but I\u0026rsquo;m happy to see data bouncing from spreadsheets to Google Analytics.\nView the Google Spreadsheet here\nORIGINAL POST Inspired by Daniel Waisberg\u0026rsquo;s excellent post detailing the use of Google Web Forms to send data to your Analytics account, I decided to create something similar.\nIf you didn\u0026rsquo;t know already, Universal Analytics is the new version of Google Analytics, and the Measurement Protocol is its heart and soul. The protocol serves as an endpoint for raw visit data, which can be pushed from any service, application, or interface equipped to send HTTP requests. In this post, I\u0026rsquo;ll introduce you to a prototype of a Microsoft Excel extension I\u0026rsquo;ve been working on. With the extension, you can push data from Excel back to your account. By using carefully labelled events and by extracting the client ID from the online visit, you can combine offline and online data together in Google Analytics.\nWithout further ado, I give you: the journey of the coupon.\nThe premise Let\u0026rsquo;s say we have a traditional brick-and-mortar store, which wants to award its regular customers with something special. So they send an e-mail with a link to a page, where the user can download a special coupon, with which they can get a pack of coffee for 5 euros. Incredible deal!\nSo the user goes to the page and sees this on the phone (of course they used a mobile device!).\n  Yes, I know it looks like phlegm, but like I said, this is a prototype!\nWhen they press the button, this happens:\n  And, as the button is pressed, this happens in Google Analytics:\n  So what happened?\nWhen the user clicks the button, they receive the coupon code. In reality, the coupon code is the user\u0026rsquo;s client ID, which is a variable Universal Analytics uses to identify the client. In essence, all events, transactions, pageviews etc. done by this client ID are always registered to the same unique visitor. Because it\u0026rsquo;s so easy to get (just a simple function), I chose the variable to anchor the data.\nAfter the button is clicked, an event is sent to Google Analytics with the label get-coupon. The action is always the coupon code (the client ID), which can then be used to categorize the events nicely.\nTrip to the store So now the user has the coupon code on their mobile phone. Next, she heads on down to the store.\nShe chooses two cans of Coffee Java (a delicious brand), which each cost just 5 euros each, thanks to the coupon.\nShe goes to the checkout, gives the coupon, thanks, and leaves.\nWhen registering the sale, the cashier types down the coupon code the customer showed.\nAt the end of the day, the cashier fills the obligatory Microsoft Excel worksheet with the transaction details for the day.\nI have no idea what these systems look like in reality, but in my wicked dreams I saw something like this:\n  Here you have basic information, which can be exported from any backend system. We have the transaction details for each transaction (just one for the day, how sad!). Under each transaction are the items that were purchased in that one transaction.\nEach transaction can have a coupon code attached to it.\nThere\u0026rsquo;s also the basic information required for Universal Analytics (namely, the tracking ID).\nUpon checking the details and clicking the Big Blue Button, these three message boxes pop up one after another:\n      I\u0026rsquo;ve actually bound the button to a custom macro, which sends the HTTP POST request to Google Analytics. First, an event is sent, to link the coupon use to the coupon download that was processed earlier. Next, the transaction details are sent, and finally, the item is pushed through the API to Google Analytics.\nThe end result First of all, we can see that the coupon code has both get-coupon and use-coupon events attached to it:\n  Also, we can see that the transaction was registered as well:\n  And finally, with a custom report, we can display a number of things. First, each coupon is associated with just one unique visitor, so the client ID really works! Also, we can quickly see whether some coupons were used in transactions by looking at the Revenue column:\n  Conclusions This was, and I hope I made it clear, just a prototype. But it shows the power of the Measurement Protocol. You can send visit data (pageviews, events, transactions, you name it) from almost any digital source equipped with some scripting language which supports sending HTTP POST requests!\nUsing the client ID has its pros and cons. Basically, with the one device, only one coupon can ever be downloaded and used, since the client ID is always the same. But the user only has to switch to another device, and a new client ID is generated. So some other checks need to be in place to make sure that the system is not abused.\nAlso, and this is a major problem with the Measurement Protocol:\nYou can\u0026rsquo;t attach a custom timestamp to hits!\nThis means that in order to preserve at least some level of accuracy, the events should be sent from Microsoft Excel at the end of the day the transaction occurred. This is the only way to make sure that the transactions are registered on the correct date. I see this as a major shortcoming, and I hope a timestamping feature is introduced in a future version.\n(Queue Time will not do, as it can only introduce a time lag which stretches no further than four hour past midnight of the day when the event is sent)\nI\u0026rsquo;m looking forward to expanding the features of the Excel extension. It should be possible to send heaps of transaction data with one button press, as now the macro only supports one item, one transaction, and one coupon event.\n"
},
{
	"uri": "https://www.simoahava.com/tags/troubleshoot/",
	"title": "troubleshoot",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/troubleshoot-google-analytics-9-step-program/",
	"title": "Troubleshoot Google Analytics: 9-Step Program",
	"tags": ["google analytics", "Guide", "troubleshoot"],
	"description": "A step-by-step process for troubleshooting Google Analytics implementations.",
	"content": "I\u0026rsquo;ve had such a blast in the Google+ Google Analytics community. Not only are the people super-duper-nice, but I have the wonderful opportunity of helping people with their Google Analytics problems without feeling obligated to invoice them or ask for compensation for my troubles. I do it because I love to help, because I feel like I have a lot to share with the community, and because I\u0026rsquo;ve always believed that the more knowledge you share the more you accumulate.\nInspired by the sheer range of problems people have with Google Analytics, which is, in reality, a very complex tool, I decided to write a general guide where I reveal some of my own methods on how to troubleshoot Google Analytics. I almost always follow these steps, and if you give this data rehab program a try, chances are you\u0026rsquo;ll be more secure with your data in the future.\n  Before we begin Remember a few things:\n1. GA displays symptoms, not actual problems.\nWeb analytics tools measure data in your marketing channels. They don\u0026rsquo;t create data, nor do they feed anything back into the site. This means that when your boss comes storming into your room screaming “What the f*** is the problem with this report?” you\u0026rsquo;ll need to tell the boss that the report just visualizes the problem, and you need time to get to the bottom of it.\n2. Every account is unique, every problem is unique\nI\u0026rsquo;ve tried to be as comprehensive as possible, while still being as generic as possible. The thing is that most problems, if not purely technical in nature, have something to do with the business area you operate in. Fluctuations in your market segment, media exposure, new product launch, viral content are all possible causes behind the things you see in your reports. In those cases, remember to approach the data as the business expert. Don\u0026rsquo;t just trust any crazy blogger who gives you tips on how to fix everything.\n3. Be careful with your data.\nTROUBLESHOOT GOOGLE ANALYTICS ASSESS\nStep 1: Evaluate the problem Step 2: Gather data\nACT\nStep 3: Cover your bases Step 4: Test and release\nVALIDATE\nStep 5: Check the DOM Step 6: Check requests Step 7: Check Real-Time\nMONITOR\nStep 8: Assess Step 9: Optimize\nBONUS Step 10: When all else fails\nASSESS During assessment, you gather as much data on the problem and its symptoms as possible. Be thorough, and make sure you look at the problem from all angles, not just the ones you\u0026rsquo;re used to.\nStep 1: Evaluate the problem\nFirst thing you have to do is narrow the problem down.\n  In this example, conversions fell dramatically in July, and they never got back up. You already know the symptoms, so start following the chain backwards. For example, if your conversion rate is down,\n  Look at the channels which drive conversions\n  When looking at the channels, focus on engagement and visitor behavior: has something changed in how people use your site at the same time the conversion rate has gone down?\n  Once you know the behavior, look at traffic sources\n  Finally you should be able to draw a line from traffic sources, through behavior, all the way to conversions\n  So if the symptom is a gradual change in your data, make sure you understand the visitor journey completely.\nHowever, if the symptoms are more drastic (traffic cut off completely, for example), there\u0026rsquo;s bound to be some technical issues involved.\nAll in all, try to narrow the issue down so that you can describe it with one sentence, e.g.\nThe symptom is that conversion rate for Goal X has fallen dramatically while traffic has stayed the same Here are a couple of leads to follow:\nVery little or no incoming data at all\n  Troubleshoot your tracking code\n  Make sure you migrated to Universal Analytics properly\n  Check if there\u0026rsquo;s a bug in Google Analytics\n  Determine if you\u0026rsquo;re accidentally running traditional Google Analytics code (search for “_gaq”) with Universal Analytics in place (search for “analytics.js”, though this doesn\u0026rsquo;t apply with Google Tag Manager)\n  Abnormal data\n  Determine if it\u0026rsquo;s a sampling issue\n  Identify bounce rate problems with this helpful article\n  If the problem is in your organic search traffic, make sure you\u0026rsquo;re not being penalized\n  Check if it\u0026rsquo;s a trend question, and people are just searching less for your services these days\n  Step 2: Gather data\nOnce you have an understanding of where the problem might be, start gathering data.\nNote that if the problem is minor and easily identified, you can pretty much skip this step.\nBy data gathering I mean the following:\n  Write down the specific date/time the symptoms first occurred\n  Take screenshots or write down the reports which display the symptoms\n  Take screenshots or write down the (relevant) reports which do NOT display the symptoms\n  Make sure you have a step-by-step guide of how to display the symptoms in Google Analytics – this is invaluable in convincing management\n  Gather intel about changes in your environment.\n  Remember, you have to link the symptoms to a problem. You can\u0026rsquo;t just look at a plummeting conversion rate and expect to fix it in the Google Analytics UI (though sometimes the problem is in your Google Analytics settings).\nIf some metric has suddenly dropped, for example, you should identify who the stakeholder is.\nIf the problem is in your tracking code implementation, you need to contact whomever manages your page templates.\nIf the problem is that your servers are unresponsive, you need to contact your IT management.\nIf the problem is that you\u0026rsquo;ve been penalized, you need to contact your SEO consultant. And so on.\nLook for clues and changes in your environment that could be causing the data problem. Without instigating a witch hunt, try to find whoever or whatever is responsible for the problem.\nNote that sometimes (especially when the symptom is a gradual shift in a metric), the problem might be a combination of factors. The more problems there are, the more focus you need.\nTo apply this knowledge to the problem above (falling conversion rate), my data intel would look like this:\nThe symptoms began to appear in early **July 2013**. Since visit metrics are stable (see screenshot), the conversion rate hasn't fallen to zero (see screenshot), and we're still receiving conversions along every other marketing channel, the problem is probably in the fact that pageviews for this goal completion are not being sent in every visit to the goal completion destination   ACT When you\u0026rsquo;ve reached this point, you know the symptoms, you know the problem, and you\u0026rsquo;ve gathered enough data to warrant action.\nWhen making changes to your code, your site, your environment, or your Google Analytics account, just remember one thing: a single error can be fatal. Think of yourself as a neurosurgeon operating on Stephen Hawking\u0026rsquo;s brain. You really, really, don\u0026rsquo;t want to make a mistake.\nStep 3: Cover your bases\nBefore you do anything drastic, make sure your data is secure.\n  Backup your page templates and JavaScript code\n  Make sure you have an unfiltered Analytics profile (a few good words on Google Analytics profiles)\n  It\u0026rsquo;s so incredibly important to have a raw Analytics profile where all data is secure. If you make the beginner mistake of creating a filtered profile, with no master profile to get back to, you\u0026rsquo;re losing potentially heaps and heaps of data.\nRemember to back up your templates and code before you make any changes. Of course, if problems have cropped up because of faulty code, you should look at previous revisions (so you should already have backups of your code to roll back to).\nBecause some data might take from hours (e.g. new pageviews) to days (e.g. eCommerce) to collect, it\u0026rsquo;s important to have a clear implementation plan.\nI always, always, record every change I make to JavaScript code with version control. Sometimes it\u0026rsquo;s overkill, but I always get a fuzzy feeling of accomplishment afterwards.\nGoogle Tag Manager automatically saves versions of your containers, which is another brilliant reason to start using it.\nIn this stage, I had determined that the problem was indeed with JavaScript. A JavaScript redirection was set to fire upon page load, which meant that sometimes it fired before the pageview was sent. I wrote new code to make sure the pageview is fired first. Step 4: Test and release\nAnother hugely important thing.\nTo test the change perfectly, you need:\n  A staging environment with a copy of your site templates the way they are in the live environment\n  A web property in Google Analytics dedicated solely to gathering data from the staging environment\n  Check Daniel Weisberg\u0026rsquo;s guide to building a bulletproof Analytics implementation for ideas.\nIf the change is something that needs to be tested on real data, you might want to shift to a higher gear and just take the change live instantly. In this case, you need to place extra focus on steps 5–7.\nWhen testing, validate the test with the instructions in the following steps. Make sure the changes you made are actually registered in Analytics.\nNote that with some problems, only time will tell if the change was for the better. That\u0026rsquo;s why it\u0026rsquo;s even more important to have a raw profile to fall back to if you happened to mess up your implementation.\nI would advise going through the following checks with any new code:\n  Check that your code is good with JSFiddle\n  Check that your HTML is valid with W3C Validator\n  Download the Google Analytics Debugger extension for Chrome and use it to debug the implementation\n  When testing, make sure you do what your visitors would do. Your code should have enough try…catch blocks to make sure that errors don\u0026rsquo;t decimate your site, but you really need to follow actual use cases when testing the implementation.\nIf testing a sales funnel, for example, remember to drop out from each stage of the funnel. This way you can see if the funnel has been correctly configured.\nWhen testing a new event, test it with multiple browsers and devices. There are so many browser and device incompatibilities, and you need to be aware of these when analyzing your data.\nHere I tested the JavaScript with a number of browsers and a number of devices. I tried to make it as performance-intensive as I could, with multiple page loads concurrently. None of the tests failed, so I was happy with the results When you\u0026rsquo;re all done with your testing, and you\u0026rsquo;re happy with the results, you can publish the changes to your live environment. Just remember to update the tracking code from the test property to the actual live property.\nVALIDATE During testing and immediately after you\u0026rsquo;ve updated your live environment, you need to make sure that the changes are being recorded in Google Analytics.\nDue to some delay in data refresh, you might not see anything for some time in the standard reports. This is why knowing how to use debugging tools and Real-Time reports is really important.\nYou can find a nice list of debugging tools on the Google Developer pages.\nStep 5: Check requests\nThe first thing to do is go the page where the implemented code changes are running. Use a network debugger to monitor the requests that are sent to the server when loading the page. Firebug, Chrome Developer Tools and IE Developer Tools all have tabs for Network debugging.\nYou might have to reload the page for data to appear in the Network tab.\nWhen looking at the requests, try to locate any requests for GIF files that are done to Google. You can identify these by looking for the domain “google-analytics.com” in the path or domain information.\nIf you see no requests, then the tracking code isn\u0026rsquo;t working and you need to go right back to your code.\nFor each page, you need to find one request which includes the pageview, and one request for every event that you fire on the page.\nYou can identify what type of request you\u0026rsquo;re dealing with by looking at the parameters sent with the request. So first choose the request and then click on Params (Firefox) or Headers (Chrome). With IE it looks like there\u0026rsquo;s no default option to view parameters, so you have to look at the URL call instead.\nIn the image below you\u0026rsquo;ll see what a Universal Analytics pageview in Chrome vs. a Universal Analytics event in Firebug look like.\n  Here are the GIF parameters for Google Analytics and for Universal Analytics.\nStep 6: Check the DOM\nThe DOM, or the Document Object Model, represents all the objects that can be accessed client-side in your web page. This means that if some data is stored in a variable (for example to be sent to Google Analytics), you\u0026rsquo;ll find it in the DOM.\nCheck the DOM to verify that your custom functions and variables work. So if data is sent to Google, but it\u0026rsquo;s not right or the strings are empty, check your DOM to validate at which point you mess things up. Use also the Console tab in your developer tools to look for JavaScript errors.\nI really recommend using Firebug, since it has a very intuitive UI for DOM and console inspection.\nHere\u0026rsquo;s an example of what the DOM would look for a couple of custom Universal Analytics variables, which have been pushed into the data layer:\n  Familiarize yourself with the DOM profile of your pages, so you\u0026rsquo;ll know where to look for information in the future.\nStep 7: Check Real-Time\nOne of the incredible features of Google Analytics are the Real-Time reports. Seriously, I could spend hours just looking at the traffic come and go.\nReal-Time reports show data as it passes through the Google Analytics API. You\u0026rsquo;ll see visits, visitors, events and so forth, all in a nice graphical interface with a number of segments you can play around with.\nSo if the GIF requests are passed correctly, and if the DOM is healthy, open Real-Time in the GA interface to make sure that data is actually being collected, labelled, and processed correctly.\n  Test it by going on different pages of the site with different browsers. Go to pages where there used to be problems, and check that pageviews and events are fired correctly.\nUse Real-Time until you\u0026rsquo;re bored, and then use it some more. It\u0026rsquo;s a really powerful debugging tool.\nMONITOR Monitoring is crucial if the change you\u0026rsquo;ve implemented has a gradual effect.\nSometimes the changes you make are realized only after enough traffic passes through your site. This might mean that you don\u0026rsquo;t know for days, or even weeks, whether or not the change was for the better.\nThis is why you have to monitor your site effectively, especially in the days following the update.\nStep 8: Assess (again)\nGo back to the reports you gathered in the first stage of the rehab program. Follow the step-by-step guide of reproducing the symptoms, and verify whether or not the symptoms have subsided.\nIdentify, locate, and follow any collateral symptoms that might arise due to the changes you made to fix the previous problems. Make sure you have enough evidence to support the fact that the changes you made were necessary, and any further fluctuation in the data is the result of the update you did. It might be that what you did was effectively a lose-lose situation, where you just chose the lesser of two evils.\nSo when monitoring the data in the few weeks following the changes, look for\n  Traces of the symptoms you fixed\n  Related metrics, dimensions, graphs, data that could have been affected by the change\n  Be prepared to act on your findings, since the relationship between traffic on your website and your Analytics account can be very fragile. Smallest changes in the system might have big ripple effects on the quality of the data you collect.\nStep 9: Optimize\nEspecially after writing new code or making changes in the templates, you might need to go back to your code and optimize it.\nTake a look at load times and site performance. Did they change for the worse after your implementation? Are you losing traffic data because you don\u0026rsquo;t understand (a)synchronicity of JavaScript?\nEspecially if you\u0026rsquo;re chaining events or making stuff happen before the pageview is called, you might see problems in the future. If the pageview is not sent, you might be seeing “ghost” visitors, who add to conversions, but who don\u0026rsquo;t count towards visits. This will have an adverse effect on your conversion rate.\nSimilarly, if you\u0026rsquo;re chaining events, you might come across a situation where the previous event hasn\u0026rsquo;t completed once the next event is already firing. This is a sure-fire way to get events with empty parameters.\nI would seriously recommend you to familiarize yourself with Google Tag Manager. It handles event chaining like a charm, and debugging, testing, and implementing code changes is really simple and handy.\nBONUS Step 10: When all else fails\nIf all else fails, contact me and I\u0026rsquo;ll help you.\n  My Google+ page\n  My Twitter account\n  My LinkedIn profile\n  My e-mail: simo (at) simoahava.com\n  Seriously, I love to help. As long as we\u0026rsquo;re just talking about simple consultation (helping with the steps in this guide, for example), I\u0026rsquo;m sure I can help. If we\u0026rsquo;re talking about bigger stuff (such as new implementations, writing custom code, managing your accounts, etc.), you\u0026rsquo;ll have to contact me through my agency, NetBooster Finland.\nAlso, remember to check the wonderful Google Analytics Google+ group for help as well.\nConclusion\nTo sum it all up, here are the key action points in my 9-step data rehab program:\n  Google Analytics shows symptoms of a problem, not the problem itself\n  Make sure you have enough data gathered before taking action (for benchmarking as well)\n  Have a staging environment handy, where you can test development work\n  During testing and after you go live, make sure you spend a lot of time validating the update\n  Keep monitoring for a period of time after the update\n  Google Analytics is a wonderful, complex tool, with so many opportunities for analysing your online presence. However, due to the complexity, there\u0026rsquo;s a very high probability that you\u0026rsquo;ll come across problems with your Google Analytics implementation.\nJust be methodical in your approach, make sure you record what you\u0026rsquo;re doing (for further use), Google furiously if you come across any problems, and you should be fine.\nAnd if you just have to remember one thing:\nDon\u0026rsquo;t forget to have a raw data profile for each of your Google Analytics web properties!\n"
},
{
	"uri": "https://www.simoahava.com/analytics/auto-event-tracking-google-tag-manager/",
	"title": "Auto-Event Tracking In Google Tag Manager",
	"tags": ["auto-event tracking", "Google Tag Manager", "listeners", "macros", "tags", "universal analytics"],
	"description": "Introducing the amazing auto-event tracking feature in Google Tag Manager.",
	"content": "There is a new version of this post for GTM V2 here.\nThe Google Analytics Summit came and went, and thanks to the Live Stream, everyone could participate. We were treated to a rapid-fire selection of Google Analytics\u0026rsquo; new features, and this post sheds light on one of these in particular: automated event tracking in Google Tag Manager.\nAuto-event tracking introduces a nice feature, which does what tag managers ought to do: it provides functionality without HTML template editing. This isn\u0026rsquo;t always a good thing, since automation is usually generic and only works for a couple of viable scenarios, but especially for these generic use cases, this new feature is a great addition to GTM\u0026rsquo;s already impressive feature list.\nAuto-event tracking has four different types of event listeners you can create:\n Click listener Link Click listener Timer listener Form Submit listener  A listener is a function which operates in the background. When creating the listener, you tell what operations it should wait for, and once these operations take place, the listener activates and fires any code within.\nIn this short tutorial, I take a look at the first three listeners. I\u0026rsquo;ll return to Form Submit listeners as soon as I have a functioning form I can work with.\nNote that all these tutorials use Universal Analytics, but it\u0026rsquo;s easy to do the same in Google Analytics (the only difference is which tag you use to send the event to your account).\nTimer Listener This is the easiest, so I\u0026rsquo;ll start with it. If you\u0026rsquo;ve read my previous posts, it makes the whole concept of \u0026ldquo;Dwell time\u0026rdquo; a whole lot simpler, without having to employ custom HTML tags (the whole point of auto-event tracking).\nWhat it does is set off a timer for X milliseconds. After the timer reaches the end, an event is pushed to the data layer, which you can then use as a firing rule for your Analytics event.\nSteps:\n Create a new Tag with the following settings:     Create a new Universal Analytics tag with the following settings; remember to add your tracking code to the \u0026ldquo;Tracking ID\u0026rdquo; field, and if you want the event to count as a hit, set \u0026ldquo;Non-interaction\u0026rdquo; as False:     Make sure you have a rule in place to fire this UA tag:     Save container version Publish container  Here you create a Timer Listener, which starts the countdown upon DOM load. As soon as the timer hits 30 seconds, the event gtm.timer is pushed to the data layer.\nThe Universal Analytics tag you created is set to launch as soon as the event gtm.timer is pushed to the data layer, so as soon as the timer goes off, the event is sent to your Analytics account.\nAnd no custom HTML editing was involved. Just some tags and rules.\nRemember to check that the implementation works by looking, for example, at the Network tab in Firebug:\n  Link Click listener This is a bit more complex than the Timer listener, but it\u0026rsquo;s still much easier than what you had to do before with custom HTML code.\nThe scenario here is that I have a \u0026ldquo;Back to top\u0026rdquo; link on my site, and I want to track its clicks. This way I\u0026rsquo;ll know a) do people actually read my pages to the very bottom and b) do they have an urge to get quickly back to the top of the page.\nThis feature makes use of the Auto-Event Variable, which is essentially a macro that can be used to refer to, for example, the DOM element where a click occurred.\nSteps:\n Create a new tag which listens to link clicks on your site:     Create a macro which identifies all element IDs on your site (by using the Auto-Event Variable):     Create a new Universal Analytics tag     Make sure you have the correct rule in place:     Save container version Publish container  Here you first create the link click listener. When set up to fire on all pages, it listens to all link clicks throughout your site. As soon as a link click occurs, it pushes the gtm.linkClick event into the data layer.\nIn the Analytics tag, the important part is the firing rule. See how you\u0026rsquo;re waiting both for the gtm.linkClick event and for the element ID macro to match a certain DOM element? This is to prevent the event firing when all links are clicked. Instead, now it identifies the DOM element ID (using the Auto-Event Variable of the macro) where the click event occurred (#backtotoplink).\nSo now you have an event sent each time the Back to top link is clicked. Remember, again, to check the Network data:\n  Click listener This is pretty much the same as the previous tutorial, but the crucial difference is that the Click listener listens to all click events on your page.\nThe scenario is classic landing page optimization: I have a (fictional) blog home page, where article headings are just plain text, not actual links to the articles themselves. I want to employ the click listener to check how many people try to click the heading in vain.\nSteps:\n Create a new click listener tag:     Create a macro for all classes (we use this to identify the headings):     Create a rule which waits for a click to the .title DOM element:     Create a new Universal Analytics tag:     Save container version Publish container  Here the important thing is to create a rule which waits for the gtm.click event (which means that a click has occurred, thanks to the Click listener) and which requires that the click occur on a DOM element with the class of title, which happens to be the class of the headings on the home page.\nCheck the implementation in Firebug:\n  Auto-event tracking conclusions Well it\u0026rsquo;s a nice feature, that\u0026rsquo;s for sure.\nAt the moment, implementing listeners requires the following general steps:\n Create a tag which acts as the listener and pushes the appropriate gtm event into the data layer as soon as the operation occurs Create a tag which fires as soon as the gtm event occurs and sends the data to Analytics  If you want to be more specific, i.e. wait for clicks on specific DOM elements, you need to create a macro which binds the auto-event variable in the data layer.\nI\u0026rsquo;d also like to see the auto-event variables as default macros (similar to {{url}}) in the system.\nAuto-event tracking is a good addition to Google Tag Manager, and it removes a lot of hassle with custom HTML code. However, it\u0026rsquo;s no a be-all and end-all solution, and there\u0026rsquo;s still a lot of manual work involved if you want to do anything more complicated (cross-event dependencies, complex chaining of events and so forth).\n"
},
{
	"uri": "https://www.simoahava.com/tags/bounce-rate/",
	"title": "bounce rate",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/track-adjusted-bounce-rate-universal-analytics/",
	"title": "Track Adjusted Bounce Rate In Universal Analytics",
	"tags": ["bounce rate", "custom html", "Google Tag Manager", "universal analytics"],
	"description": "Guide for tracking adjusted bounce rate in Google Analytics. This solution takes into account the time spent on the page and the amount scrolled.",
	"content": "So here we are again. Universal Analytics and Google Tag Manager, the dynamic duo, ready to strike again.\nFirst, remember to check my previous two tips for UA and GTM use in custom scenarios:\n Weather as a custom dimension Tracking page load time  In this post, I visit the idea of adjusted bounce rate, which I came across a year ago in the Google Analytics blog.\nAdjusted bounce rate basically refers to tweaking the traditional bounce rate collection method (single engagement hits / total visits) so that visits which only included a single page view would not count towards a bounce, as long as they met some qualitative requirements.\nBy the way, check this great post on bounce rate by Yehoshua Coren if you need a refresher on what the term means.\nFor these custom events, I use the generic event container I created in my previous post.\nThe easy method: visit duration This is the easiest to implement. It\u0026rsquo;s also the one in the Google Analytics blog post I referred to above.\nWhat it does is fire a setTimeout() method as soon as the page has loaded. If the timer runs out (the time is 30 seconds in this script), an interaction event is fired, meaning the visit is not counted as a bounce.\nThe end result is this:\n  And here\u0026rsquo;s how to do it.\nSteps:\n Create a new custom tag called \u0026ldquo;Dwell time\u0026rdquo; Set Tag Type as Custom HTML tag Add the following code in the HTML field:  \u0026lt;script\u0026gt; setTimeout(\u0026#34;dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39;: \u0026#39;NoBounce\u0026#39;, \u0026#39;eventAction\u0026#39;: \u0026#39;Over 30 seconds\u0026#39; })\u0026#34;, 30000); \u0026lt;/script\u0026gt;  Add {{event}} equals gtm.dom as the firing rule Save tag Save container and publish  See, I told you it was easy. What happens here is that after the DOM has loaded (the firing rule), a timer starts. If the user stays on the page when the timer goes off, the event is fired. Remember, you need the generic event container for this to work. So remember to check my previous post for instructions how to build it (it\u0026rsquo;s easy, I promise!).\nThe intermediate method: measure scrolling This method was inspired by a Google+ post I came across by Avinash Kaushik, where he detailed a script written by Nick Mihailovski. This script is used here extensively, with just the event call in a different format (to support UA and GTM).\nWhat happens here is that the event listener waits for a scroll event (so you actually scroll the page down), and fires the no-bounce event after that. Interesting! The statement is that if you scroll, you read, and if you read, you\u0026rsquo;re engaged (with the content).\nSteps:\n Create a new tag \u0026ldquo;Scroll the page\u0026rdquo; Set Tag Type as Custom HTML tag Add the following code in the HTML field  \u0026lt;script\u0026gt; window.addEventListener ? window.addEventListener(\u0026#39;scroll\u0026#39;, testScroll, false) : window.attachEvent(\u0026#39;onScroll\u0026#39;, testScroll); var scrollCount = 0; function testScroll() { ++scrollCount; if (scrollCount == 2) { dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39;: \u0026#39;NoBounce\u0026#39;, \u0026#39;eventAction\u0026#39;: \u0026#39;Scrolled the page\u0026#39; }); } } \u0026lt;/script\u0026gt;  Set {{event}} equals gtm.dom as the firing rule Save tag Save container and publish  And you\u0026rsquo;re done. So what the script does is measure if a scroll event occurs during your page view. If it does, the event is fired. Note that it won\u0026rsquo;t fire the event with every subsequent scroll, so you don\u0026rsquo;t have to worry about clogging your 500 events per session quotas.\nThis isn\u0026rsquo;t fool-proof, of course. It just checks whether the user scrolls. What this DOES prevent is the miscalculation of page visit duration if the user just opens the page in a separate tab and leaves it be. Now you actually need interaction, albeit a very minimal one, to produce an engagement and neutralize the bounce.\nThe advanced method: page load AND scroll So what about measuring whether there was a scroll event and the visit duration on the page was over 30 seconds? Wouldn\u0026rsquo;t that be an even better way to calculate engagement? I think so. So here\u0026rsquo;s what you do. First, make sure the two tags you just created are not active any more (otherwise you\u0026rsquo;ll be sending multiple bounce-wrecking events).\nSteps:\n Create new tag \u0026ldquo;Dwell and scroll\u0026rdquo; Set Tag Type as Custom HTML Tag Add the following code in the HTML field:  \u0026lt;script\u0026gt; (function() { var visitTookTime = false; var didScroll = false; var bounceSent = false; var scrollCount = 0; setTimeout(timeElapsed, 30000); window.addEventListener ? window.addEventListener(\u0026#39;scroll\u0026#39;, testScroll, false) : window.attachEvent(\u0026#39;onScroll\u0026#39;, testScroll); function testScroll() { ++scrollCount; if (scrollCount == 2) { didScroll = true }; sendNoBounce(); } function timeElapsed() { visitTookTime = true; sendNoBounce(); } function sendNoBounce() { if ((didScroll) \u0026amp;\u0026amp; (visitTookTime) \u0026amp;\u0026amp; !(bounceSent)) { bounceSent = true; dataLayer.push({ \u0026#39;event\u0026#39;: \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39;: \u0026#39;NoBounce\u0026#39;, \u0026#39;eventAction\u0026#39;: \u0026#39;Time spent and page scrolled\u0026#39; }); } } })(); \u0026lt;/script\u0026gt;  Set {{event}} equals gtm.dom as the firing rule Save tag Save container version and publish  Here the timer starts first. As soon as it hits 30 seconds, it calls the sendBounce() method. This method checks if the user has also scrolled, and if they have, the event is fired. Note that I also make sure that the event is sent only once with the boolean variable bounceSent.\nWhen the user scrolls, the same method is called and the same check is made.\nSo there are four different scenarios resulting from this script:\n The user doesn\u0026rsquo;t scroll, but stays on the page for 0-to-infinite seconds, and the event is not fired (visit is a bounce) The user scrolls, but the timer hasn\u0026rsquo;t gone off, and the event is not fired (visit is a bounce) The timer goes off, and the user has already scrolled, and the event is fired (visit is not a bounce) The user scrolls, and the timer goes off, and the event is fired (visit is not a bounce)  A much healthier way of calculating adjusted bounce rate, in my opinion.\nConclusion The way you measure bounce rate should always be in relation to the goals you set for a page or for your site. If engagement is important, remember to add clear calls-to-action, so that you don\u0026rsquo;t have to resort to artificial adjustments like those depicted in this post.\nHowever, for a simple blog like this, measuring engagement by a combination of visit time and scrolling interaction is probably a pretty good way of getting a more realistic metric for tracking visit quality.\nAn even more advanced (and qualitative) method would be to see just where the user scrolls to. Is it to the end of the post or just the first paragraph? In other words, does the user read or just skim. That\u0026rsquo;s a crucial question, and I might just return to the issue in a later post.\n"
},
{
	"uri": "https://www.simoahava.com/tags/page-load-time/",
	"title": "page load time",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/analytics/page-load-time-universal-analytics/",
	"title": "Page Load Time In Universal Analytics",
	"tags": ["custom html", "Google Tag Manager", "page load time", "universal analytics"],
	"description": "Send all page load timings into Google Analytics with events. This way you&#39;ll avoid sampling issues with automatic page sample collection.",
	"content": "In Google Analytics, you can monitor your site speed and get a decent overview of what pages are contributing positively and negatively to site speed. The problem with page load time metric is that it\u0026rsquo;s an average based on a sample. You can modify the sample rate with setSiteSpeedSampleRate(), but for me that\u0026rsquo;s not bloody well good enough.\n(UPDATE 28.3.2014: This post is still valid, but an implementation with User Timings is a much smarter way to measure actual page load time. I\u0026rsquo;ll be doing a short update at some point in the near future.)\nSo in this post I continue exploring the beautiful combination that is Universal Analytics and Google Tag Manager. Be sure to check out my earlier post on using Universal Analytics to send weather as a custom dimension.\nThe end result This is what you get:\n  What you see is an event, sent along with each page view, telling you how long it took to load the page. The reason I\u0026rsquo;m using events, and not, say, custom dimensions, is just convenience. With events, I have a nice set of standrad reports, which means I don\u0026rsquo;t have to mess around with custom reports.\nI would have liked to use absolute values (in milliseconds) as a custom metric, but the problem is that you can\u0026rsquo;t calculate with custom metrics, so recording all-important averages would be impossible. This is why I chose the scheme of converting the milliseconds to a string which approximates the page load time.\nDISCLAIMER: As with Google Analytics site speed, this whole thing only works if the visitor\u0026rsquo;s browser supports the Navigation Timing (window.performance.timing) API.\nCreate some macros in GTM We\u0026rsquo;re soon creating a generic event processor, so create some macros for it.\nSteps:\n Create a macro \u0026ldquo;Event category\u0026rdquo;, with Macro Type: Data Layer Variable and Data Layer Variable Name: eventCategory     Create a macro \u0026ldquo;Event action\u0026rdquo;, with Macro Type: Data Layer Variable and Data Layer Variable Name: eventAction Create a macro \u0026ldquo;Non-interaction\u0026rdquo;, with Macro Type: Data Layer Variable and Data Layer Variable Name: nonInteraction  You could create macros for Event label and Event value as well (which would make this even more generic), but for this project I only need these.\nBy creating these macros, you\u0026rsquo;re making it possible to send variable data through the data layer into the tag, whose contents are eventually passed on to Google Analytics.\nCreate a generic event container This is nice. I can just create a generic event processor, which uses the macros I just created, and use it in the future for all events I want to send to GA.\n Create a new tag called \u0026ldquo;Send event\u0026rdquo; Choose Universal Analytics (beta) as the Tag Type Add you GA tracking ID to the appropriate slot Choose Event as the Track Type Add {{Event category}} as the macro for, duh, Category Add {{Event action}} as the macro for, that\u0026rsquo;s right, Action Add {{Non-interaction}} as the macro for Non-interaction hit Create a firing rule \u0026ldquo;GA Event\u0026rdquo; with the condition \u0026ldquo;{{event}} equals GAEvent\u0026rdquo; Save your new tag    So here you create a container for all your events. Now you have your macros and your container. In the future, to send a new event, you just need to push an event named \u0026ldquo;GAEvent\u0026rdquo; to the data layer, which fires the tag you just created. And that\u0026rsquo;s what we\u0026rsquo;re doing next.\nCoding the page load time script  Create a new tag \u0026ldquo;Page load time\u0026rdquo; Set Custom HTML Tag as the Tag Type Add the following code within the HTML field:  \u0026lt;script\u0026gt; var perfData = window.performance.timing; var pageLoadTime = perfData.domComplete - perfData.navigationStart; var loadTime = \u0026#34;\u0026#34;; if (pageLoadTime \u0026lt; 1000) { loadTime = \u0026#34;0-1 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 2000) { loadTime = \u0026#34;1-2 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 3000) { loadTime = \u0026#34;2-3 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 4000) { loadTime = \u0026#34;3-4 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 5000) { loadTime = \u0026#34;4-5 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 6000) { loadTime = \u0026#34;5-6 seconds\u0026#34;; } else if (pageLoadTime \u0026lt; 10000) { loadTime = \u0026#34;6-10 seconds\u0026#34;; } else { loadTime = \u0026#34;10+ seconds\u0026#34;; } dataLayer.push ({ \u0026#39;event\u0026#39;: \u0026#39;GAEvent\u0026#39;, \u0026#39;eventCategory\u0026#39;: \u0026#39;Page Load Time\u0026#39;, \u0026#39;eventAction\u0026#39;: loadTime, \u0026#39;nonInteraction\u0026#39;: 1 }); \u0026lt;/script\u0026gt;  Add a firing rule which waits for the page to load: {{event}} matches gtm.load Save the new tag  What happens here is that after the document has loaded, the script counts the time from the moment the rendering of the page began to the moment the DOM was completely loaded. I tried having loadEventEnd as the end point, but for some reason the tag was fired before this was reached, so I got ridiculous results.\nFor more information on the Navigation Timing API, be sure to check Navigation Timing Overview.\nOnce I get the time (in milliseconds), I use quick and dirty if\u0026hellip;else statement to approximate the load time to a string.\nFinally, I push the data in the dataLayer. In the push, I signify GAEvent as the event name, which was the trigger for the Send Event tag to fire. It\u0026rsquo;s important to signify this event as a non-interaction event (nonInteraction: 1), since otherwise you\u0026rsquo;d start getting 0% bounce rate on all your pages.\nSave the container version and publish And that\u0026rsquo;s all there is to it. To sum up, here\u0026rsquo;s what happens:\n When the visitor starts loading a page, the load time is calculated This information is passed into a generic event container, with category and action names in place to make sure the event is recorded clearly in Analytics The container tag is fired as soon as the data is passed, since the trigger is the event name (which is passed with the data) You can verify that you did things correctly by looking at the Network log in Firebug:    Final words This might seem a bit overkill (tracking every single load time of every single page load), but I like the idea of looking at page load time on a more detailed level. Now I can use the Event reports to see what browsers were used with the slowest visits, what countries had the lowest page load times, etc. It\u0026rsquo;s all data, baby.\nSure, I know, by upping the sampling rate you can get similar results in the Site Speed report, but hey, it\u0026rsquo;s more fun when you can code the stuff yourself!\nI\u0026rsquo;m still waiting for the possibility to start doing calculations with custom metrics. It would be so cool to see what the average page load time was during a set day, with a certain browser, with visits from a specific country, etc.\n"
},
{
	"uri": "https://www.simoahava.com/web-development/universal-analytics-weather-custom-dimension/",
	"title": "Universal Analytics: Weather As A Custom Dimension",
	"tags": ["api", "custom html", "google analytics", "Google Tag Manager", "JavaScript", "universal analytics", "Web development"],
	"description": "Use Universal Analytics and some free (or cheap) APIs to collect data on weather conditions when the user visited the site.",
	"content": "There is a new version of this post for GTM V2 here.\n[Last updated June 2014] I\u0026rsquo;ve fallen in love with Universal Analytics and Google Tag Manager. Together they form an incredibly powerful tool for marketing professionals. In most cases, I no longer need to post recommendations to my client for yet another page template revision, since with the tag manager in place, I can just add custom code via the admin panel. Add to that the power of Universal Analytics with its ultra-sensitive Measurement Protocol, and the ability to craft custom dimensions and metrics, and voila! I\u0026rsquo;m in a happy place.\nIn this post, I take you through a short JavaScript dev journey of utilizing weather data as a custom dimension in your site\u0026rsquo;s Analytics. You only need to have Universal Analytics and Google Tag Manager installed on your site.\nThe end result This is what you get:\n  As you can see, I\u0026rsquo;ve created a custom report which shows visits, pageviews and conversion rates (Pages / Visit \u0026gt; 2) for various weather conditions. This report unequivocally, with a plain-as-day-without-a-doubt display of causality, proves that when it rains, people are more likely to convert. I should do more targeting to British audiences\u0026hellip;\nCreate a new tag in GTM  Name the tag \u0026ldquo;Weather API\u0026rdquo; Choose Custom HTML Tag as the tag type Enter the following code into the HTML field (Note! You need to load jQuery for this to work! See here for more information):  \u0026lt;script\u0026gt; var lat = geoplugin_latitude(); var lon = geoplugin_longitude(); var weather = \u0026#34;\u0026#34;; var weatherAPI = \u0026#34;http://api.openweathermap.org/data/2.5/weather?lat=\u0026#34;+lat+\u0026#34;\u0026amp;lon=\u0026#34;+lon; $.ajax({ type : \u0026#34;POST\u0026#34;, dataType : \u0026#34;jsonp\u0026#34;, url : weatherAPI+\u0026#34;\u0026amp;units=metric\u0026amp;callback=?\u0026#34;, async : true, success : function(data) { weather = data.weather[0].main; dataLayer.push({\u0026#34;weather\u0026#34;: weather}); }, error : function(errorData) { console.log(\u0026#34;Error while getting weather data :: \u0026#34;+errorData.status); }, complete : function() { dataLayer.push({\u0026#34;event\u0026#34;: \u0026#34;weatherDone\u0026#34;}); } }); \u0026lt;/script\u0026gt;  Add rule to fire tag on every page ({{url}} matches RegEx .*)  Here\u0026rsquo;s what\u0026rsquo;s going on.\nFirst, a couple of external JavaScript functions are called to retrieve the longitude and latitude of the visitor by using their IP addresses. I\u0026rsquo;m using the free geoPlugin service. You can load it on your site by adding \u0026lt;script src=\u0026quot;http://www.geoplugin.net/javascript.gp\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; in your template.\nRemember to load this script BEFORE the weather API runs. It\u0026rsquo;s best to put it in the \u0026lt;head/\u0026gt; of your page template. If you want, you can also put the script load as the first line of the custom HTML tag you just created.\nNext, I use the API of OpenWeatherMap to retrieve the weather data for the latitude and longitude I got in the previous step.\nIn the AJAX call, I make an asynchronous call to the API, requesting the data in JSONP (since it originates from a different domain than mine). If the call is successful, I look for the /weather/main/ node, as it has a nice, short description of the weather in the area (e.g. \u0026ldquo;Clear\u0026rdquo;). I then push the string into the data layer.\nFinally, whether the API call is a success or not, I push an event \u0026ldquo;weatherDone\u0026rdquo; into the data layer. This is used to make sure the API call is done before the Universal Analytics tag is fired (see below).\nCreate new custom dimension in GA  Go to the admin panel of your Google Analytics site Under Property, choose Custom Definitions / Custom Dimensions     Create a new Custom Dimension with the name \u0026ldquo;Weather\u0026rdquo; Scope the new dimension to Session Set the dimension as Active Click Save Make note of the index of the new dimension  Here you create a new dimension in Google Analytics and set it active. You need to make note of the dimension index, since you will be referring to this in a short while.\nCreate new macro in GTM  In Google Tag Manager, go to your container and click New Macro     Name the macro \u0026ldquo;Weather\u0026rdquo; Set Macro Type as Data Layer Variable Type \u0026ldquo;weather\u0026rdquo; in Data Layer Variable Name Click Save  You create the macro to access the weather string you pushed into the data layer a couple of steps ago.\nEdit your GA tag (NOTE! I suggest you take a look at this post for details on sending the weather data using a non-interaction event instead. This way your precious pageviews will never be compromised if the weather script fails to work.)\nSteps:\n Go to the tag that tracks your pageviews and sends them to GA Choose More Settings Choose Custom Dimensions Add the index number of your new dimension in the Index field In the Dimension field, click {{+macro}} and choose the macro you just created     Edit the firing rule and add condition {{event}} equals weatherDone     Save the tag Create a new container version Publish the new container version  Here you send a new custom dimension with the pageview, and it gets its content from the data layer variable you pushed into the data layer during the weather API call. Since the calls are asynchronous, you\u0026rsquo;ll need to make sure the UA tag is fired only after the weather API call has been made. This is done by waiting for the event weatherDone to be pushed into the data layer.\nMake sure everything works  Visit your site Take a look at the requests your site sends by using Firebug or something similar    Enjoy I created a new custom report in GA, with visits, pageviews and conversion rate as metrics, and my new custom dimension as the dimension.\nNote that it might take some time for the new dimension to start pulling in data. If Firebug tells you that the weather data is sent along with your pageview data, you\u0026rsquo;re fine.\nThis could be done in so many different ways, but I chose JavaScript simply for this quick prototype. I\u0026rsquo;m pretty pleased with the result, and even though I make a couple of external calls, page load speed is not an issue. There\u0026rsquo;s always a risk with asynchronous scripting that the user is quick enough to interact with the site before the pageview is sent, but I don\u0026rsquo;t think that will be an issue with this light-weight scripting.\nI can think of a number of cool applications for this weather API, but I\u0026rsquo;ll leave those to your imagination.\nGoogle Tag Manager and Universal Analytics do a terrific job of providing high value for marketing professionals. I love the fact that you don\u0026rsquo;t need to mess with the page template, and you can test and preview your tags as much as you want before publishing them.\nEDIT: I edited the firing rules so that the UA tag occurs only after the weather API call has completed.\nEDIT II: Note that you need to load jQuery for this to work! The $.ajax call is a jQuery function! So either load the jQuery resource in the \u0026lt;head/\u0026gt; of your page method (so that it gets loaded before the GTM container) OR have the following as the first line of the custom HTML tag:\n\u0026lt;script src=\u0026#34;//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; EDIT III: Be sure to check my post on how to make sure the weather API is polled just once per visit. This will improve site performance, and decrease the burden on the external API.\nEDIT IV: Finally, see this post on sending custom dimensions with events rather than pageviews for the optimal version of the code above.\n"
},
{
	"uri": "https://www.simoahava.com/tags/marketing/",
	"title": "marketing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/pitch/",
	"title": "pitch",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/sales/",
	"title": "sales",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/seo-for-sales-making-the-pitch/",
	"title": "SEO For Sales: Making The Pitch",
	"tags": ["marketing", "pitch", "sales", "SEO"],
	"description": "Helping sales teams understand search engine optimization.",
	"content": "My days as a freewheeling, unrestrained, happy-go-lucky, gung-ho city cowboy are over. As of 31 August 2013, I\u0026rsquo;m a married man, and for the rest of my life, I will dedicate myself to optimizing my relationship to my beautiful wife. The old ball-and-chain. But before I bury myself in the bosom of our blessèd alliance, I\u0026rsquo;ll continue to blog about the dark and murky undergrowth of digital marketing that SEO is. Specifically, this post will be all about SEO for sales: how to prepare yourself for the questions your prospective client should/will be asking.\n  First, a lesson:\nDon\u0026rsquo;t sell SEO\nThat\u0026rsquo;s right! You can quote me on that. If you\u0026rsquo;re focusing on solely on SEO for sales, your chance of making the sale will diminish. Or even if you make the sale, you\u0026rsquo;re red-lighting a huge sales potential.\nYour client is not really interested in SEO, even if they seem to explicitly communicate so to you. Your client is interested in results, which might include more conversions, more brand awareness, more social media exposure, better content, a well-sculpted marketing strategy, more revenue, etc. Their goal is not to increase their visibility in organic search engine rankings, which is, as you might know, what SEO has traditionally aimed at. SEO should never be the goal of the sale (or the project), it should be one of the steps you take in helping the client understand and, eventually, reach their business goals.\nHaving said that, there are a number of questions the client should ask if you are far enough in the sales process to actually introduce the methodologies you have hand-picked in making the client\u0026rsquo;s dreams come true. I\u0026rsquo;ve listed a number of these questions here. Most are based on experience, some on common sense, and some on the faint hope that someday, someone will ask them from me in a sales meeting.\nWhat is it you actually do? Wow, straight to the point, huh? Well, when this question is asked, I like to draw the \u0026ldquo;marketing general\u0026rdquo; card. I do a lot of things in SEO, but my first and foremost task is to increase or improve the online presence of my client. This might include a whole slew of traditional SEO \u0026ldquo;tactics\u0026rdquo;, as sometimes the whole online presence of a client revolves around a website built on some nasty ass templates. Sometimes the client seems to have everything under control, with a strong website driving lots of quality traffic, a powerful social media presence, and a high-tuned understanding of digital marketing. In cases like this I sell maintenance work: \u0026ldquo;Let me take some of that burden away from you, so that you can focus on developing your business\u0026rdquo;. There\u0026rsquo;s nothing I enjoy more than taking control of a well-oiled machine, bringing my own expertise to the mix, and making sure that the wheels keep turning.\nCan you make me number one on the search engine results page with keyword X? No, I can\u0026rsquo;t! And no, it\u0026rsquo;s not because I\u0026rsquo;m bad at my job, it\u0026rsquo;s because the search engine results page (SERP) as it used to be is no more. Searches are personalized, localized, embellished, enriched, and made as flexible as possible without hurting the relevance of the results. This is a great thing for the business owners, as they have a chance of reaching an even wider audience. This is a terrible thing for SEO micro-managers, since keyword rankings mean next to nothing these days. Actually, it\u0026rsquo;s actually a good thing, since keyword rankings were a horrible KPI to begin with. Seriously, who cares about rankings when you can actually look at the traffic the keywords are driving to your site?\nHow can you do keyword research when you\u0026rsquo;re not an expert in our business? I might not be an expert in your business, but I want to be. And I have you to help me. Seriously, with your business knowledge and my ultimate SEO skills, together we\u0026rsquo;re the dynamic duo of the digital world. Let\u0026rsquo;s rock, baby! Seriously, keyword research is all about search intent, relevance, search volume, data analysis, search trends, blah blah blah. My job is to get to know your business, so that I can use that knowledge in the keyword research to find the right keyword, which drives the right visitor to the right landing page on your site.\nHow will I benefit from our partnership? Excellent question! This ties me back to the first rule (you still remember it, right?). If you don\u0026rsquo;t know them by now, this is where you dig out the goals your client has in mind. If these goals are too superficial, make them work a little to really find out why they need your help. It\u0026rsquo;s never about traffic, it\u0026rsquo;s never about organic search results, it\u0026rsquo;s never about brand awareness. There\u0026rsquo;s always a process involved, and the client is implicitly asking you to be part of it. Make sure this process is articulated, after which it is much easier to explain why you suggest SEO for this particular phase. Remember, don\u0026rsquo;t just talk about SEO. Tie your project in with an entire online marketing ecosystem, opening the door for all your other products, projects, and processes, which might complement your SEO work.\nGive this question a lot of thought, and plan ahead. If they don\u0026rsquo;t flat out ask this question, they will be thinking about it. So make sure to drive the point through. The main benefit your client will receive is your knowledge and your skills. You are there to drive business, to create or improve a marketing plan, to strategize, to increase revenue or brand awareness, etc. You\u0026rsquo;re not just looking for keywords and optimizing templates. Remember that, young padawan.\nWhat\u0026rsquo;s wrong with our current SEO? Ouch. Tread lightly, my friend. Be diplomatic and constructive. When pointing out flaws, don\u0026rsquo;t be superficial (\u0026ldquo;you\u0026rsquo;re missing a title here and a header there\u0026rdquo;), but make the client understand how they\u0026rsquo;re actually missing out on revenue because of the combined mass of mistakes weighing their website down. This is what the client wants to hear. Or well, they don\u0026rsquo;t want to hear it, but they have to hear it. As a SEO, your goal is to build the client\u0026rsquo;s business. If their current SEO work has been done shoddily, it\u0026rsquo;s your job as a consultant to rectify the situation.\nWhat\u0026rsquo;s the ROI of hiring you? Again, tread lightly. You know how much you cost, you have the means to estimate the increase in traffic, conversions, revenue, etc. But remember the elusive goal I was talking about? Sometimes return of investment is incredibly difficult to calculate. For example, if you do social media work to a client in an obscure business area with no previous exposure in social media, you\u0026rsquo;ll have a hard time estimating the worth of a single new fan, let alone a hundred. In these cases, you must help the client arrive at the right figures. Give them your numbers, how much do you cost as a consultant? How much does the client have to employ other stakeholders? After this, start looking at the obvious benefits: more traffic is more potential. Increase potential with targeted social media advertising, conversion optimization, viral content, etc. Make it clear that positive ROI is a given, since with SEO it most often is. Let the client evaluate the worth of a single, committed fan belonging to the target audience. Then multiply that by the number of new relationships you know you can deliver with your SEO magic, and you have all you need to estimate the ROI.\n  Thank you? No, thank YOU! When thinking about SEO for sales, it\u0026rsquo;s important to remember the golden rule: it\u0026rsquo;s not about marketing. It\u0026rsquo;s not about concepts, it\u0026rsquo;s not about products, it\u0026rsquo;s not about SEO. It\u0026rsquo;s about delivering the goods, understanding the client, and building the client\u0026rsquo;s business.\n"
},
{
	"uri": "https://www.simoahava.com/digital-marketing/an-optimized-haiku/",
	"title": "An Optimized Haiku",
	"tags": ["Poetry"],
	"description": "Haiku celebrating search engine optimization and digital marketing.",
	"content": "Dear client, search engine\nOptimization for you?\nOr perhaps - donuts?\nThe Web, I\u0026rsquo;m afraid,\nIs a pretty sticky maze.\nAnd you\u0026rsquo;re the trapped fly.\nWe tell you we\u0026rsquo;ll help,\nBut you will not believe us.\nYou are very wise.\nEverything we do,\nEverything we claim we do,\nYou can do yourself.\nBut if you let us\nApproach you humbly, gently,\nAs a team, we\u0026rsquo;ll rock.\nWe\u0026rsquo;ll rock Google, check!\nWe\u0026rsquo;ll rock the blogosphere, check!\nWe\u0026rsquo;ll rock Bing! (Who cares?)\nWe\u0026rsquo;ll take your ugly,\nAntediluvian site,\nAnd sex it all up.\nHowever, you must,\nReally must, believe in us,\nAnd let us help you.\nMarketing Partners,\nHow cool does that sound, my friend?\nBetter than donuts.\nPerhaps in the end\nYou\u0026rsquo;ll say \u0026ldquo;I love you\u0026rdquo;, and then,\nThen we optimize.\nourselves\n"
},
{
	"uri": "https://www.simoahava.com/tags/poetry/",
	"title": "Poetry",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/content-management/after-the-click-conversion-time/",
	"title": "After The Click: Conversion Time",
	"tags": ["content management", "Conversion", "ppc", "SEO"],
	"description": "For measurement and optimization, just collecting clicks is not enough. Having a viable conversion target is of paramount importance.",
	"content": "So you\u0026rsquo;ve spent a whopping amount of money on paid search, SEO, Facebook ads, competitions, link building, and traditional marketing. You\u0026rsquo;re seeing a crazy influx of visitors to your site. Just as you\u0026rsquo;re about to call your boss to accept the promotion, you notice a worrying trend: your new visitors are barely making an impression on conversions from online traffic. What\u0026rsquo;s up with that? All your high-paid consultants told you that a high return of investment is guaranteed, since The Internet is da bomb, everything is cheap, and percentages are always in your favor!\nWell nuh-uh!\nTake a moment, if you will, to step into the visitor\u0026rsquo;s shoes: \u0026ldquo;Gee, I want to know where I can find a new lightbulb for my stove overhead lamp. Let\u0026rsquo;s Google it. \u0026lsquo;Buy stove lamp online\u0026rsquo;. Ok, here\u0026rsquo;s a bunch of colorful results, let\u0026rsquo;s click the top one and see what happens. Oh, cool, a flashin\u0026hellip;MY EYES!! MY EYES!! IT BURNS, OH IT BURNS!!!\u0026rdquo;.\n  See what happened there? You had the top result for an excellent, conversion-prone, long tail search phrase, but since your landing page looked like a half-hearted Picasso imitation, picked up from the tail end of a digestive system, you just scared this \u0026ldquo;guaranteed\u0026rdquo; customer away.\nIt\u0026rsquo;s easy to fool yourself into thinking that getting people to come to your site is all that matters. Afer all, Total Visits is a clearly defined metric, easy to set up and evaluate, and you can go a long way without any qualitative analyses.\nI blame the digital marketing industry for enforcing this misconception. There\u0026rsquo;s a lot of unhealthy stagnation going on, at least in Finland, and not only do we have to make our customers understand the holistic approach required to nurture an online presence, we also need to lobby for more follow-through, more long-term planning, more strategy, more involvement, and more commitment.\nFor starters, we need to reevaluate:\nOn-page optimization Keywords in all the right places, check. Images with descriptive and keyword-rich ALT texts, check. A coherent and well-planned information architecture, check. Top quality content, check. A cute cat video just waiting to go viral, check!\nGood, you\u0026rsquo;ve done everything your SEO guy told you to do. Now step outside your cocoon and think of the visitor. They arrive on your site through organic search by using a keyword which implicitly or explicitly outlines their information need. Now,\nDoes this page satisfy that need?\nDo you provide a clear call-to-action?\nDoes this page introduce new needs, some that the customer would be prone to subscribe to given the opportunity?\nThink about it. You want to convert the visitor into a customer. This means that you need to go all Hansel and Gretel. Give the visitors a trail to follow with a clear, delicious goal in the end. (It\u0026rsquo;s up to you and your business whether there\u0026rsquo;s a cannibalistic witch inside).\nThe clearer the path, the better your chances at conversion.\nOutreach marketing Another pitfall is over-promotion. Not only is it worth less if you get referrals or mentions in completely unrelated forums, it will hurt your conversions if you try to coax a population whose interests are, by default, as far removed from your offering as can be.\nNote that there\u0026rsquo;s nothing wrong in trying to penetrate a new interest group. It\u0026rsquo;s just that the connection needs to be established before you reach out to uncharted waters.\nIt\u0026rsquo;s best to focus on related sites, forums, and channels. Google values referrals from related authority sources. Why not focus solely on them? It\u0026rsquo;s bound to help your conversion rates if your referral traffic already has an idea what you are all about.\nDon\u0026rsquo;t cling to the past I\u0026rsquo;ve come across some cases where a desperate need to rank high with a set of keywords resulted in an even more desperate need to rank high with said keywords after the product was discontinued. Not cool.\nThis means that the search engine result would promise one thing, but the landing page would dismiss this promise with \u0026ldquo;We don\u0026rsquo;t have what you\u0026rsquo;re looking for anymore, but now we have THIS!\u0026rdquo;. And the visitor is flabbergasted since they actually only wanted the item promised to them in the search engine results.\nThere are other ways to retain this traffic, and most of them have to do with your marketing strategy in general. Before you pull the plug, you\u0026rsquo;d sure better have an idea of how to cater to those loyal customers who loved the product. In other words, you need a transition plan.\nWhatever you do, don\u0026rsquo;t retain a landing page just for the sake of traffic. A broken promise is a poor way to begin a customer relationship.\nMake sure you have a plan B For conversions, your website is just one, albeit significant, channel. Even if you can\u0026rsquo;t commit to turning your website into a super-charged conversion machine, you need to make sure that its role in assisting conversions is not diminished.\nPerhaps you have a social media channel, where each person who likes your brand is valued as much as any conversion on your website. In this case, make sure that social media is omnipresent throughout your website.\nPerhaps you haven\u0026rsquo;t jumped on the eShop bandwagon, and you still trust ye olde storefront to bring the customers in. Well, make sure your website gives your store the proper introduction, with sufficient targeting of your core group (with storefronts you\u0026rsquo;re bound to have a core group) to ensure that people actually locate it and walk through the door.\nOr perhaps your website is just a small cog in the machine of a huge, faceless organization, and the website solely exists to drive traffic away from your competition. In that case, bless your black heart, you little devil you.\nA promise is a promise is a promise I\u0026rsquo;m sorry for this sappy idealism, but I\u0026rsquo;m about to get married in two weeks, and I\u0026rsquo;m feeling all soft and fluffy inside.\nIf you want to convert, you have to keep your promises.\n(Read The READY Conversion Optimization Framework for a nice method of evaluating how well you deliver on your promises.)\nYou can\u0026rsquo;t base your business on the off chance that someone makes a purchase even though they\u0026rsquo;re disgusted by the manner in which you conduct your online marketing. Conversions are driven by satisfaction, and satisfaction is regulated by the efficiency of your marketing message.\nIf you sell stove lamps online, and if you\u0026rsquo;ve manufactured your landing page to target visitors who search for \u0026ldquo;buy stove lamp online\u0026rdquo;, providing that the purchase process has been optimized, you\u0026rsquo;ll be sure to arrive at a high conversion rate.\nNext time you\u0026rsquo;re browsing the web, using search engines \u0026lsquo;n stuff, take a look at the landing pages you are directed to. How many actually have the content you were expecting to find? Depending on your information need, the answer might vary from \u0026ldquo;none\u0026rdquo; to \u0026ldquo;some\u0026rdquo;. Hardly ever is it \u0026ldquo;all\u0026rdquo;.\n"
},
{
	"uri": "https://www.simoahava.com/categories/content-management/",
	"title": "Content management",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/content-management/",
	"title": "content management",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/conversion/",
	"title": "Conversion",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/ppc/",
	"title": "ppc",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/cms/",
	"title": "CMS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/website-redesign/",
	"title": "website redesign",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/website-redesign-with-seo/",
	"title": "Website Redesign With SEO And Common Sense",
	"tags": ["CMS", "SEO", "website redesign"],
	"description": "Tips for redesigning a website, especially from the search engine optimization point-of-view.",
	"content": "So it\u0026rsquo;s been a while, I know. I\u0026rsquo;ve been enjoying my summer vacation, either swimming in the lush blue waters of Finland\u0026rsquo;s lakes or in the murky, greenish, (only slightly toxic) chemistry lab reject also known as the Baltic Sea. I\u0026rsquo;ve also had the pleasure of playing golf only to realize I\u0026rsquo;m roughly at the same skill level I was at when I first started.\n  I blame the fact I\u0026rsquo;ve played less than ever before due to some unexpected patellofemoral pain in my right knee (yes, I know it\u0026rsquo;s called the Runner\u0026rsquo;s Knee, but if I know words like patellofemoral, I will use them). However, now it\u0026rsquo;s time to OPTIMIZE again! And what better way to start this new season of invigorating blog posts than to tackle one of my favorite subjects: website redesign.\nFirst, let\u0026rsquo;s explore a tangent. You know (or maybe you don\u0026rsquo;t) how before every regular season of NBA basketball, there\u0026rsquo;s a ridiculous amount of trading, signing, waiving and skimming to make sure each team has the perfect lineup for the upcoming season? Sometimes the changes are really inconspicuous, like what the San Antonio Spurs have done. Just a few role players here and there to strengthen an already world-class unit. Sometimes the changes are much bigger, like Celtics trading away almost every single star in their team (coach included) and leaving Rajon Rondo feeling betrayed.\nIt\u0026rsquo;s the same with website redesign. Sometimes the redesign has a singular purpose: to update the URL structure, to change the visuals, to redo the content. And sometimes the website is on the receiving end of a complete, and often painful, overhaul.\n  The problem with the latter path is the same as with a complete revamp of a basketball team: you really have no idea how well the pieces fit together in the overall scheme of things. By changing everything, you\u0026rsquo;re taking a risk, since if something goes awry, you\u0026rsquo;ll have a hard time figuring out which of the changes you implemented is to blame.\nOops, I already uncovered my first tip, so let\u0026rsquo;s go to another patented Top 5 list.\nTop 5 things to consider in a website redesign 1. Choose a focus As I said, it\u0026rsquo;s better to choose a focus and roll out the redesign in stages. Search engines are impatient, and for some reason they are as comfortable with change as the typical Facebook user. If your redesign has a technical focus, don\u0026rsquo;t do anything else during the reindexing of your site. In other words, if you\n  Redo the URL structure\n  Migrate to a new domain\n  you\u0026rsquo;ll want to do these separate from new content creation. Why? Because it takes time for the new pages to be indexed in the search engines. If you add new content to the mix, you\u0026rsquo;ll risk endangering your entire web presence by not having any current pages in the index, and the pages that are indexed don\u0026rsquo;t respond to the keywords you used to rank with.\nIf your website redesign comprises new content creation or new visuals (including conversion optimization), you\u0026rsquo;ll want to do these separate of any technical upgrades you\u0026rsquo;ve been thinking of.\n2. Choose a CMS Choose your content management system wisely. At the very least, it should have the features of a modern CMS, with special focus on SEO capabilities. There simply is no excuse for any CMS to not be technically equipped for proper web design and development. And with proper I mean search engine friendly.\nMake sure your CMS caters to your needs. You don\u0026rsquo;t need to spend tens of thousands of dollars on expensive CMS licenses if WordPress is good enough for you.\n3. Think mobile In the recent years, both Finland\u0026rsquo;s largest online reseller of technology goods (www.verkkokauppa.com) and the largest online auction portal (www.huuto.net) have redesigned their websites. You want to know what sucks? Neither has been optimized for mobile visitors!\nIt\u0026rsquo;s no use trying to wave the \u0026ldquo;but no one makes mobile purchases in Finland\u0026rdquo; card any longer. That simply isn\u0026rsquo;t an excuse. A recent study showed that 10% of the respondents have made purchases online with their mobile devices. This figure would be larger if the websites would actually provide the means for mobile shopping.\nSo go responsive, create a dedicated mobile site, or create a mobile app for your website. It\u0026rsquo;s better to be prepared for the mobile storm than to be left deaf, dumb and blind in the onslaught of the I-told-you-so\u0026rsquo;s I\u0026rsquo;ll be whacking your way in a year or two.\n4. Do things the SEO way This, I\u0026rsquo;m biased enough to say, is the most important thing to consider.\nMake sure your website redesign is done with minimal impact on your site\u0026rsquo;s search engine friendliness.\nFeel free to read and re-read that sentence again and again. To make sure your redesign is done the SEO way, you\u0026rsquo;ll need to do (at least) the following:\n  Make note of your \u0026ldquo;before\u0026rdquo; state (indexed pages, PageRank, domain authority, etc.)\n  Create a 301 strategy to make sure that all traffic to your old pages is redirected correctly to your new pages\n  Use web analytics to identify your most important content and make sure it is retained in the website redesign\n  Update your sitemap.xml to make sure the search engine index is updated without delay\n  Place extra focus on the first few weeks after the transition: find and fix any crawl errors without delay, make sure your 301 strategy is working, and react to any glitches in the indexing process with haste.\n  It\u0026rsquo;s so easy to botch the redesign with poorly implemented SEO-proofing, but I\u0026rsquo;ve seen it happen so many times I\u0026rsquo;m beyond cynicism.\nDon\u0026rsquo;t go for a CMS or IT supplier who won\u0026rsquo;t let you SEO-proof your website. The loss in organic search rankings and visibility can be devastating to your business.\n5. Choose a professional who can help you This is a given. There\u0026rsquo;s SO much work involved in a redesign that it\u0026rsquo;s futile for one person or one team to tackle all the aspects, even with a smallish site. Don\u0026rsquo;t hesitate to delegate your workload to professionals and consultants who can do the work for you.\nOn the other hand, the science behind the appropriate steps you need to take is sound, so it\u0026rsquo;s not difficult to find the necessary guides and instructions online. However, I know for a fact that SEO, for example, is something that just isn\u0026rsquo;t considered enough when redesigning a website. It\u0026rsquo;s easy to overlook especially if your company has very little web presence to begin with. Nevertheless, I\u0026rsquo;ve explained the virtues of SEO elsewhere, so I won\u0026rsquo;t go there now.\nConclusion A website redesign is a good thing, and you should do it every couple of years. However, don\u0026rsquo;t botch it so that you lose all credibility you ever had online. Chop the project down to a manageable size, and please, please, PLEASE:\n  Don\u0026rsquo;t use Flash.\n"
},
{
	"uri": "https://www.simoahava.com/tags/learning/",
	"title": "Learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/technique/",
	"title": "Technique",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/learning-seo-is-good-for-you/",
	"title": "Why Learning SEO Is (Probably) Good For You",
	"tags": ["Learning", "SEO", "Technique"],
	"description": "Argument for why learning about search engine optimization is good for anyone working in digital.",
	"content": "Before I begin, it is important to note that SEO, as any other facet of web design, covers both technique and purpose. Technique in the sense that SEO comprises a number of de facto guidelines (and accompanying tools) which help improve the search engine friendliness of your website. With purpose I mean the elusive concept of setting goals, and how to pursue them. Both are incredibly important aspects of learning SEO, and it can be argued that one cannot exist without the other.\nIn this post, I take a stand on why it\u0026rsquo;s important for all those working with web design to learn and understand the basics of search engine optimized web design. I sincerely believe that search (in one form or another) is and will be the reigning paradigm of the Internet, mainly due to the fact that the sheer amount of online content has surpassed any other means of gaining access to it. This means that as a whole, and for the good of information retrieval, tuning existing web content to be more search-accessible should be a benefit to all, especially in the near future.\n  Learning SEO techniques Most of SEO, as it stands today, is based on guesswork. That\u0026rsquo;s what it boils down to. Search engines want to promote the democratic nature of the web by keeping all parties equally misinformed about their machinations. Just imagine what would happen if search preference was afforded directly to the highest bidder? In the long run, search would be rendered useless, as money could be used to buy undeserved relevance in search results.\nThe fact that it\u0026rsquo;s guesswork doesn\u0026rsquo;t change the equally important fact that we actually know a lot about search engines. This is thanks to a vibrant community, vigorous testing, and some helpful engineers who treat us with factual knowledge, if we\u0026rsquo;ve been good.\nThat being said, there are a number of techniques, whose adoption into your everyday content management will definitely lead to good results. I\u0026rsquo;ve covered some of these techniques in an earlier post on combining accessibility and SEO requirements, and I also touched the subject in my SEO in a nutshell post.\nIf you grasp the basic ideas behind keywords, HTML templates, meta data, and web crawling, you should be well on your way to creating good, accessible, and healthy web content.\nUnderstanding the purpose of SEO But technique is not, and should not be, enough. Doing SEO for the sake of SEO will probably result in you making progress with keyword rankings, but are you also able to increase the number of conversions? Is your brand achieving more popularity? Do you have adequate presence in social media? After you create your content, do you have any idea how to market it? What is there on your website that should be found? Is your online presence justified?\nI can\u0026rsquo;t stand the idea of just selling organic search improvement. A savvy client will understand that it\u0026rsquo;s not enough, and if they don\u0026rsquo;t, it takes a scrupulous consultant to capitalize on this lack of understanding. What about the quality of visits? Does SEO maximize the potential of landing pages? Is keyword research taken beyond volume, to relevance and beyond?\nI pose all these questions because I know how this business works. SEO is an incredibly easy thing to sell, and, I\u0026rsquo;m sorry to say, most buyers are incredibly easy to fool.\nAnd this is why I\u0026rsquo;m calling for increased understanding of the purpose of SEO. Developing content strategy, for example, is actually really easy if the clients themselves see the benefit of creating good and optimized content that goes beyond \u0026ldquo;traditional SEO\u0026rdquo; deep into the realm of audience design and creativity.\nWhat to demand from SEO As a client, you should ask the difficult questions right from the get-go. Remember to go beyond the concepts and marketing jargon. Make sure you are not spoon-fed with age-old SEO tactics, which might increase visitor levels, but do nothing for the quality of these visits.\nIn the first meetings, ask at least the following questions:\n Have you understood what our business is about? How do you determine the relevance of your keyword research? How will you make sure our site\u0026rsquo;s link profile is kept clean and relevant? How will you assist us in truly making quality content and marketing it? Will you train me to use all necessary tools so that I don\u0026rsquo;t have to buy consultancy every time I want to update our site?  The last is the most important. I really REALLY believe in knowledge transfer, and I see SEO professionals as obligated to educate the masses. Doing things the right way is and should be a common undertaking, not restricted to just a few professionals.\nLearning SEO is good for you, trust me. It\u0026rsquo;s a question of doing web design and content management in a certain, unobtrusive manner.\nThis manner maximizes search engine friendliness and usability of your site content.\nAnd it makes the Internet just a little less chaotic.\n"
},
{
	"uri": "https://www.simoahava.com/tags/digital-marketing/",
	"title": "Digital marketing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/digital-marketing/its-not-about-marketing/",
	"title": "It&#39;s Not About Marketing",
	"tags": ["Digital marketing", "Guide", "SEO", "Tips"],
	"description": "As it turns out, marketing is successful when it&#39;s not about marketing.",
	"content": "In a recent post, I took a short foray into the world of clumsy analogies by comparing the team work qualities (and necessities) of basketball and digital marketing. In an even earlier post, I made the claim that the single most important facet of content strategy is audience design. Well, now is the time to pull these two threads together all trilogy-like. After this, you can hail me as the Stieg Larsson of marketing.\nLet me start by repeating one of the greatest quotes to come out of professional basketball: \u0026ldquo;[The secret of basketball is that] it\u0026rsquo;s not about basketball.\u0026rdquo; This from the mouth of one of the most controversial team leaders in basketball history, but also one whose loyalty and team skills just can\u0026rsquo;t be disputed.\nIt\u0026rsquo;s not about basketball.\nThis simple quote means that as soon as players learn to disregard individual achievements, statistic-driven egoism, and isolation in the court, they will learn to look past the concepts of the game and learn to focus on the broader context of winning as a team.\nThis is what successful marketing is all about, too.\nMarketing concepts The problem is simple, and in no way unique to marketing: we sell concepts, acronyms, statistical lift, productized ideas, and clinical processes. Benefits are usually introduced as wispy afterthoughts with little regard to the eccentricities of the client in question.\nWhat\u0026rsquo;s worse is that our selling points, take SEO for example, are loaded with history. This history dictates the whats, whys, and how-tos of SEO marketing. A great many of us try to change this by choosing a different approach to SEO, but maybe the problem was never with the concept behind the acronym, but the fact that we tend to treat our products as solutions.\nWe proclaim to be experts, and therefore we take the floor during sales pitches. Well yes, we are professionals, we probably know the theory inside out, and we have a vast body of experience to back us up. But I tell you this: in marketing, we should acknowledge that the client\u0026rsquo;s business expertise is what everything else should revolve around.\nAfter all, it is their domain, their kitchen, their niche we are penetrating with our fancy marketing jargon. We are bold enough to claim that we can help them achieve a number of goals, but in the end are we really sure that these goals are even relevant? As a matter of fact, do we really know who and what we\u0026rsquo;re dealing with, before we start drafting our proposals?\nThe plan After this disjointed hodgepodge of an introduction, let me introduce you to my STOP-list. These are commandments which I believe help me (as a marketing professional) make better use of my client\u0026rsquo;s business knowledge. The client is, after all, at the receiving end of this professionalism.\nStop confusing products with solutions SEO is not a solution. SEM is not a solution. Graphical design is not a solution. Most probably the things you call solutions are not solutions. These are all tools which help you reach the solution. Solution has become synonymous with product (and vice versa) to the point where no one really understands the difference between the two.\nStop selling blindly Don\u0026rsquo;t start your pitch until you\u0026rsquo;ve heard everything about the client\u0026rsquo;s problem. If they\u0026rsquo;re not forthcoming or if they\u0026rsquo;re unable to articulate the problem, help them! And be altruistic about it. You don\u0026rsquo;t have to sell your help, because by finding the need together, you\u0026rsquo;ll form a bond with the client that will reflect upon the success of your possible cooperation. Remember, you must identify a need to focus the pitch. A pitch without an established need is worth nothing.\nStop dictating, start discussing I really believe in this one even though it might seem detrimental to my business (which, I guarantee you, it\u0026rsquo;s not!). I see my duty as educating the client. I teach them how to use the same tools as I do, I teach them the theory behind SEO, I help them understand how they are the biggest difference-makers when it comes to managing their organic search rankings. This makes the client happier, more satisfied, and more understanding of my efforts.\nStop the monotony Just because you\u0026rsquo;ve worked with similar clients before, don\u0026rsquo;t believe for a second that you can just carbon copy a process and be done with it. Each case requires a unique approach, as each business need is unique. Don\u0026rsquo;t jump to conclusions during sales, or while the project is on-going, or, most importantly, when presenting your findings. Be empathetic, find the relevance in your work, and be prepared to communicate it to your client in words that they undestand. The more you talk about their business and the less you talk about yours, the better.\nStop enforcing your recommendations Consulting is a touchy business. After all, you\u0026rsquo;re hired to consult the client. That is, you observe, you recommend, and you track. If your client says no to your recommendations, don\u0026rsquo;t have a fit, they probably have a good reason for it. The larger the client, the more complex their change management process. If this happens, it is your duty as a marketing professional to come up with something else.\nThe human factor I guess my motivation for writing this post stems from the fact that I believe there\u0026rsquo;s a huge amount of untapped potential in digital marketing.\nIn the end, it\u0026rsquo;s all about the human factor. We are proud, selfish beasts, who battle for control and refuse to compromise.\nAnd so, our greatest battle in self-development is not against the unstoppable influx of new information, nor is it against the ever changing business needs of our clients. No, the greatest battle is fought within, against our own flaws and the restrictions we impose upon ourselves.\nLuckily, these flaws are usually enhanced by stagnated processes, expired concepts, and poorly designed products. This can be remedied.\nBe flexible and make your processes flexible, so that they can accommodate the diverse spectrum of you clients\u0026rsquo; business requirements. This way you will be able to take your professionalism to another level.\n"
},
{
	"uri": "https://www.simoahava.com/tags/basketball/",
	"title": "basketball",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/creative/",
	"title": "creative",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/digital-marketing/how-is-digital-marketing-like-basketball/",
	"title": "How Is Digital Marketing Like Basketball?",
	"tags": ["basketball", "creative", "Digital marketing", "SEO"],
	"description": "Drawing the obvious parallel between a digital marketing team and a professional basketball team.",
	"content": "So I was watching the 2013 NBA Eastern Conference Finals game 7 between Miami Heat and Indiana Pacers. While making note of the dozens of different ways that the lackluster Pacers were taken to the cleaners by the dominant Miami team (read: LeBron James), I started churning a funky thought in my head. This beautiful, wonderful, exciting, adrenaline-pumping, superstar-studded, tattoo-galore of a game must be an analogy of something. Something equally thought-provoking, exhilarating and life-changing. Oh! I know! Digital marketing!\n  There has to be something in common between the helter-skelter world of marketing and the organized chaos that basketball – at times – seems to be. Now, I\u0026rsquo;m making this up as I go along, so just bear with me. I\u0026rsquo;m sure you\u0026rsquo;ll either know more about basketball or more about digital marketing once I\u0026rsquo;m through, and even if not, at least you\u0026rsquo;ll have suffered with me through another unbearably hot summer evening.\nAnd just as a quick disclaimer: I apologize for the half-Freakonomics, half-Lewis-Carroll, sensationalist post title. It\u0026rsquo;s just how I roll today.\nThe premise To add some substance to my argument, my premise is simple: digital marketing is team work, where points are awarded for blasting through competition in an attempt to reach the goals we set for our clients. I know, I know, you could substitute \u0026ldquo;Digital marketing\u0026rdquo; with any other B2B or B2C business, but my motive here is as simple as my premise: Digital marketing is team work. It is team work. It is team work. Every single word in this sentence should be emphasized. There is no i in team (although this has been disproved, and there\u0026rsquo;s also \u0026lsquo;me\u0026rsquo;).\nBasketball, as you may know, is all about team work as well. The five-man-unit must play together like a seamless group in order to blast through competition in an attempt to finish the game with a higher score than the other team. A good team is one that comprises of individuals with spectacular talent, all hell-bent on winning the championship trophy. A perfect team is one that transcends the concept of individuality, and becomes more of a unit where the only thing that stands out is the team itself.\nHow\u0026rsquo;s that for a premise? Flimsy, generic and completely unsurprising. Which is why I have to go deeper. Much deeper.\nThe coach vs. the account manager I\u0026rsquo;m not going to the GM/CEO-level simply because that\u0026rsquo;s way too deep, and because that\u0026rsquo;s a world I (want to) know little about.\nHowever, a coach in basketball is very much like the account manager in digital marketing. In a well-oiled organization, the coach/account manager calls the plays. They analyze the competition and choose the team that will work on any specific goal. Usually these goals are tailored against the competition in the field. In basketball, if the other team goes big then your team goes big. In marketing, if your competition is ad agencies, you want to bring out the visually talented nerds. If your competition is Google\u0026rsquo;s algorithms, you want to bring out the SEOs. If your competition is your client\u0026rsquo;s competition, you want to bring out the big guns from all departments to find out where and how your client can stand out.\nCoaches and account managers are really important in the grand scheme of things, but in the end they should stay on the sidelines and delegate the brunt of the work to the specialists. Naturally, the world is full of coaches who meddle in the on-court action, and there are plenty of account managers who do all the work by themselves.\nAnd this is OK. Well, actually, basketball coaches shouldn\u0026rsquo;t be the go-to guys in a choked up offense, but they should participate in the action. Maybe call the referee some dirty names after a poor foul call. Maybe inconspicuously trip the other team\u0026rsquo;s players. Similarly, account managers should take an interest in the on-going projects. After all, it is their role to keep the client updated and happy.\nBut seriously. With a team bursting with talent, it\u0026rsquo;s all about delegation. Let the specialists do their magic.\nThe back court vs. the creative department The back court is the most important function of a basketball team. You can argue with me all you want, but I will not budge. Similarly, th__e creative department (especially the content managers) is the lifeline of a digital marketing organization.\n  The back court are the guards. The ball handlers. The play makers. The distributors. The sharpshooters. The fast breakers. A well-organized offense begins (the dribble, the pass or the shot) and ends (the transition to defense) with the back court. Just think of the following names and you\u0026rsquo;ll agree with me: Stockton \u0026amp; Hornacek, Isiah \u0026amp; Dumars, Magic \u0026amp; Scott, Curry \u0026amp; Thompson. Even Jordan \u0026amp; Pippen (if you stretch your imagination a little). And then the individuals like Kobe Bryant, Allen Iverson, Steve Nash, etc.\nThe content managers are the king-makers. The miracle factory. The fantastic N (where N = number of content managers). The visionaries. The illuminators. The fashionistas. They are tasked with coming up with the coolest campaigns in the world. With the viral videos. With the keywords that bring in the millions. With the content that wows, ooh-aahs, makes you cry and laugh at the same time, and makes you feel insignificant and god-like all at once. They are the gears that make the machine turn.\n\u0026ldquo;Creative department?! Hah!\u0026rdquo; I hear you scoff. Well mark my words. Even if your digital marketing team doesn\u0026rsquo;t have a designated creative department with its Don-Draper-esque content creators, you probably have someone or some people who fit the description. They are the go-to guys who get the project rolling with a crazy idea that everyone kind of fears but kind of loves at the same time.\n  If you don\u0026rsquo;t have people like this, you\u0026rsquo;ll want to, because they make the difference between a poor campaign and a good one. Wait, just like a solid back court is the difference between a poor team (e.g. 2012 Bobcats) versus a good team (e.g. San Antonio Spurs). See how it\u0026rsquo;s all working out for my analogy?\nThe forwards vs. the SEOs, the SEMs and all other specialists And then you have the forwards. These guys are the ones who, at least since the early 80s, have received most stars on the basketball walk of fame. Larry Bird, Charles Barkley, Michael Jordan (when he felt like playing a forward), Kobe Bryant (same thing), Kevin Durant, LeBron James, Kevin Love, and so on and so forth. They are the players who have the most potential to dazzle, as their roles require ridiculous amounts of athleticism (they have to juggle fluently between various roles in both ends of the court), some very specific skills (sharpshooting, post-up playing, hustling for loose balls, rebounding, defensive rotation), and chameleon-like adaption. They are often quite single-minded and thus poor as play makers, but they make up for it with pure, unadulterated skill in their preferred roles.\nTheir counterparts in the digital marketing world are the specialists of the marketing team. Do you need someone to hitch up the client\u0026rsquo;s website in organic results? Get the SEO. Do you need someone to inject the SERP with exact ad impressions? Call the SEM. Do you need someone to design some kick-ass campaigns? Go wake up the AD. Do you see where I\u0026rsquo;m going with this?\nSimilarly, in basketball, If you need someone to pick the team up and make the decisive play, you get Kobe or post-2011-LeBron or pre-2000-Jordan. If you need someone to grab the rebound and initiate a fast break, you get Rodman or Moses.\nIf you need someone for something because you are sure that that someone doing that something will take your project or your game to the next level, you choose that someone from this category.\nThe center vs. sales This was a difficult category, as I had no idea what would be the ideal digital marketing counterpart for the big men in the basketball game. But the more I thought about it, the more I thought of the sales people I\u0026rsquo;ve had the fortune of working with.\nA center dominates below the basket. In both ends of the court. They push away the other team, they make room for the driving forwards, they make all-essential screens for the back court, they tip the loose balls in, they grab the rebounds, and they swat away the pitiful shots the opposition makes.\nThe sales people dominate the end zones as well. In both ends of the project pipeline. They push away the competitors, they negotiate more work for the role-players, they make sure the content managers know what the opportunities are, they ensure continuation after the project is wrapping up, they turn whispers into leads and leads into new sales, and they do this by assuring the client that this digital marketing team is the only one that matters. There\u0026rsquo;s some overlap with account managers here, as you may notice, but sales people can be specialists as well.\nIf your sales department stands out in your company, there\u0026rsquo;s probably something wrong. You don\u0026rsquo;t want a super-efficient sales unit if your production isn\u0026rsquo;t up to par. You want equilibrium in both ends of the sales funnel.\nAnd, of course, if your center stands out in your basketball team, you\u0026rsquo;re also looking at difficulties in the near future. First of all, they usually have a very limited zone of action. Due to their height and mass, they are hardly ever good for dribbling or shooting the ball (there are exceptions). Also, because of the physical nature of their position, they are very injury-prone and thus poor candidates for picking the team up and carrying them to the championship.\nThe rest Well I know that there are so many more people involved in digital marketing, and I know I\u0026rsquo;ve insulted at least, well, no one, by not including them.\nI\u0026rsquo;d love to say that programmers are the most important people in the team, but no. They\u0026rsquo;re not on the court, they work off-screen. They lay the foundation for the team to operate. They draft the plays, they maintain them, they create new versions of them, and they teach the others how to use them. They are specialists, with specialized functions in the organization, but they are hardly the movers and shakers of a marketing team.\nI\u0026rsquo;d love to say that the analysts, the consultants, and the other managers are important, but I really don\u0026rsquo;t know what these job titles are so I won\u0026rsquo;t go there. There\u0026rsquo;s no such things as just \u0026ldquo;an analyst\u0026rdquo; or just \u0026ldquo;a consultant\u0026rdquo;, so I\u0026rsquo;m assuming that they are the same people as the ones I mentioned before. What\u0026rsquo;s funny is that the job market is overflowing with fancy but nonsensical titles like these.\nSo is digital marketing like basketball? I\u0026rsquo;m happy with how things ended. Sure, I\u0026rsquo;m being awfully generic and the analogy is stretched at times, but you know what, a basketball team really is like a digital marketing team! They are both built around pure talent, they have very specific goals, for which equally specific tactics and strategies have been devised, and they both bleed and sweat to make sure these goals are achieved (and surpassed). Everybody has a function, and everyone has the potential to make or break every single game/project.\nAt the same time, everybody is part of a unit, a team, a whole. Remember Isiah Thomas and the secret of basketball? This is what teamwork is all about.\nNBA players and marketing professionals, they\u0026rsquo;re all stars. When you drop your ego and work with your colleagues to surpass all expectations – that\u0026rsquo;s when you become superstars.\n"
},
{
	"uri": "https://www.simoahava.com/tags/audit/",
	"title": "Audit",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/score-card/",
	"title": "Score card",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/seo-report/",
	"title": "SEO report",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/what-makes-a-good-seo-report/",
	"title": "What Makes A Good SEO Report?",
	"tags": ["Audit", "Score card", "SEO", "SEO report"],
	"description": "Identifying features of a good search engine optimization report.",
	"content": "I recently started rewriting some of the templates we use for SEO reports at my company. I first thought that the task would be a simple one. Just rewrite all the SEO stuff to match the latest trends, add more diagrams, charts, and graphs, and make it more personal by increasing the number of client-specific sections. However, soon I started to question my motives (as I usually do when doing something independently). Are these reports really necessary? Who reads them? What use is the information within? Should I require that my clients fix all the issues I reported or are they just recommendations? Are SEO reports the final product of a project, or do they mark the beginning of a hopefully successful post-audit journey for the client? The questions just kept coming, and in the end I was pretty certain that I had uncovered at least some insight into the business of reporting.\nThe biggest revelation I came across was the most obvious one.\nA good report is really important.\nA good report turns data into prose, problems into opportunities, errors into fixes, troughs into peaks, and most importantly, it makes everything seem manageable. A good report doesn’t just suggest corrections, it educates and facilitates. Finally, it’s also my calling card. A good SEO report tells more about my professionalism than the results within.\nThis post contains a number questions you might want to ask yourself if you’re either a) the one who has to write the report or b) the one who has to read it. The questions might seem almost self-evident, but hey, simple revelations are usually the ones that have the biggest impact.\nAnd yes, I know there are all sorts of SEO reports out there: keyword research reports, content analysis reports, comprehensive site audits, link profile analyses etc. I’m being intentionally generic here, and the following questions should be asked regardless of the report you’re dealing with.\nFor whom is the SEO report for? A fundamental question, and one that is most often forgotten.\nI used the concept of audience design in an earlier post on content strategy. The same credo applies here: know your audience! Or preferably, know your audiences. It’s very likely that your report is written for a number of target groups: marketing directors, content managers, ICT services, art directors, etc. The likelihood increases the more complex the report is.\nStart with this question, and revisit it every time you start writing a new section. Make it obvious from the content that you’re targeting a specific audience. This way the client can distribute the report to the right recipients independently without you having to act as a mediator.\nWhat are the most pressing issues? This is something that every SEO report should begin with. Essentially you’re taking the entire body of the report and congesting it into a hit list of 5–10 most important issues which make the biggest impact with the least amount of work.\nClients will really benefit from this. It’s such a huge thing to see that there’s something you can do right away to make most of the problems go away. This is not only reassuring but also revelatory, as most clients don’t understand why traffic has plummeted or why certain keywords have lost their ranking.\nJust make sure you’re not misleading the client. It’s perfectly OK to skip this step by noting that the issues within the report require more time and effort than can be expressed in a digested format. Honesty is always valued more than any quick fix lists that don’t deliver.\nHow did the website score? I don’t like score cards which score each analyzed section from 1 to 10, since the scale is hardly ever scientifically founded, and the grades are just very, very subjective.\nI tend to go for the traffic lights: red if something is critical and requires immediate attention, yellow if something is wrong but not catastrophic, and green if something is OK. This is usually enough, and it’s nicer to look at than just a matrix of labels and grades.\n  Just remember that it’s ok to have lots of greens (or 9s and 10s). A pass is a result as well, and you shouldn’t be a harbinger of doom just because you want justify your fee.\nRemember to start each section with a look at the respective row in the score card. This provides more context for each section, and it also gives a powerful visual cue to the reader of the gravity of each problem you address.\nWhy recommend this? Why suggest that? This is something again that you should return to in every single section of the report. Don’t just write about the problem and the fix. This is your chance to educate! If you add a proper introduction to each section, you not only provide the foundation for each fix you’re recommending, you’re also telling the client how to avert the problem in the future.\nThis is in many ways the meat and bone of the report.\nStart with a description of the problem along with its symptoms. Do it academically: transition from the general to the specific. Provide the SEO context for each problem, so that the reader understands your motivation for suggesting the corrections. Then provide the details of how the problem manifests in the site you are auditing. Remember that you’ve been hired to fix problems. You can’t fix a problem without identifying its source.\nSimilarly, you shouldn’t report a cure without explaining how you came about it. You have to provide context for all the concepts, because part of your job is to consult and educate your reader.\nI know it sounds noble and highbrow, but remember why you (hopefully) got into SEO in the first place? You wanted to make the Internet a better place, right? RIGHT? I know that years of fighting Google’s algorithms has made you cynical and battle-scarred, but that’s no reason to abandon your principles.\nDoes this look good? Okay, I left the most important for last. Just kidding. But seriously, it’s important to make sure the report looks like a real SEO report. Don’t just stitch things together in a hesitant email that you send to the client in the last minute. Put some time and effort into it. This is how my ideal template looks like:\n  Cover page\n  Introduction (a personal note from you to the client)\n  Table of contents\n  Hit list of the most pressing issues\n  Score card\n  Report itself\n  Conclusion\n  Appendices\n  Presentation is important, because you don’t want to have to explain each section again and again just because of sloppy output. On the other hand, you do want to do some explaining, which brings me to the last question.\nAre you prepared to present the report? Don’t just send the report and expect that your work is done. Remember that you might be talking about a 100-page manual, detailing the heart and core of years and years of SEO research, and unraveling the infinitely complex machinations of a huge website with equally huge problems.\nCall the client an hour after sending the report. Go over the table of contents, the score card, and the hit list. If the report is large, arrange a workshop for all stakeholders and target groups.\nMake sure the message sinks in. Your report is a testament to the hard work you’ve put in. And remember: Internet. Make it a better place (for you and for me and the entire human rac\u0026hellip; sorry).\n"
},
{
	"uri": "https://www.simoahava.com/tags/content-strategy/",
	"title": "Content strategy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/seo-in-a-nutshell-and-some-tips/",
	"title": "SEO In A Nutshell (And Some Tips)",
	"tags": ["Content strategy", "SEO"],
	"description": "All-around usable tips for search engine optimization.",
	"content": "\u0026ldquo;Oh no, not another \u0026lsquo;SEO in a nutshell\u0026rsquo; post!\u0026rdquo; I hear you scream. Oh yes! And to make matters worse, I\u0026rsquo;m actually calling this SEO In A Nutshell just out of spite! But why, oh why, must I litter the otherwise so clean and orderly forum that Internet is with yet another here\u0026rsquo;s-what-something-is-in-case-you-ever-cared-post? I promise, I\u0026rsquo;m only doing this for selfish reasons. I\u0026rsquo;m not trying to buy myself into the major league by posting about something that everyone else is posting about. I\u0026rsquo;m not trying to impress those whom I work with by telling them I\u0026rsquo;m good at something I\u0026rsquo;m supposed to be good at. And I\u0026rsquo;m definitely not trying to impress you, my cynical reader, since I know you are so difficult to please.\nNo, I actually honestly believe that at this point to approach SEO with any other agenda than that of introspection and self-enlightenment would be fruitless. So that\u0026rsquo;s it. I\u0026rsquo;m writing this to get my thoughts straight and really look at what SEO is today. It\u0026rsquo;s not the same it was five years ago, hell, it\u0026rsquo;s not the same it was last week.\nSEO in a nutshell \u0026hellip; in a nutshell Well, as you might now, SEO stands for search engine optimization, or if referring to the person who does it, search engine optimizer. At least, it used to. Nowadays, partly thanks to the efforts of a community hyped up about the age-old, Bill Gatesian Content is King mantra, it\u0026rsquo;s also referred to lovingly as search experience optimization. It sounds nice, but they changed the wrong word. Optimization is too close to manipulation for my taste. I\u0026rsquo;d love it if someone could come up with a better alternative. Something to do with facilitation. Or assistance. Or community, comprehension, camraderie.\nSo SEO is search engine optimization, that\u0026rsquo;s how far I got. Great!\nTraditionally, they way SEOs approach a website (note, traditionally), is via a three-pronged attack:\n1) Technical optimization - where we make sure that the titles and metas are there, that robots.txt and XML sitemaps are in place, that all redirections are kosher etc. This is what we do to ensure that the search engines can index the site and its pages properly. Indexing means that the page can be searched for.\n2) Semantic (or keyword) optimization - where we partake in an endless tug-of-war between the popularity of a keyword versus its competition. Keywords are search queries that your preferred customers use. If you own a webstore that sells skis, you want the site to be found by people who are looking for winter sports equipment and not hammocks. This is achieved by priming your content to make use of these keywords as inconspicuously as possible.\n3) External (or link profile or inbound marketing) optimization - where we try to build the hype around the site, because we know that parts (1) and (2) are simply not enough to lure people in. You see, it matters where you appear in the search engine results page (or SERP). It\u0026rsquo;s crazy, but apparently if you\u0026rsquo;re not in the top 10, you\u0026rsquo;re screwed. And you get there by increasing your site\u0026rsquo;s popularity. And what is a better sign of popularity than people linking to your site?\nSo think of it like this:\nWithout technical optimization, the search engines won\u0026rsquo;t know your page exists.\nWithout semantic analysis, the right crowd won\u0026rsquo;t know your page exists.\nWithout link profiling, no one will know your page exists.\nAnd this is traditional SEO at its best. Doing stuff with the page template, doing stuff with keywords, and doing stuff with links.\nBut that\u0026rsquo;s not enough, is it? Hell no it isn\u0026rsquo;t! If you\u0026rsquo;re approached by SEOs who promise you the moon and stars by doing nothing but the above, you\u0026rsquo;re in for an unpleasant treat. The problem with the traditional approach is that somewhere along the way someone forgot that humans use search engines (and some very well educated apes).\nThat\u0026rsquo;s the beauty of search experience optimization as the new SEO. You\u0026rsquo;re not creating your website for search engines, you\u0026rsquo;re creating it for users! That\u0026rsquo;s what Google\u0026rsquo;s been telling you to do all along! Forget tactics, forget dirty schemes to undermine Google\u0026rsquo;s algorithms, forget buying links from the black market, just focus on content. That\u0026rsquo;s all there should be to it.\nSure, in a perfect world. Even at its easiest, creating content is bloody difficult. Some people have a talent for it, but even they have to stay on top of the latest trends and fashions of the Web. The thing about content is that you never know what really works until you try it. It\u0026rsquo;s like trying to create a hit song (not that I know anything about it). Often times the most unpredictable B-side of a failed pop tune turns into the biggest hit the artist has ever had. And that\u0026rsquo;s without trying to market it at all! Many times things that go viral in the Internet are accompanied by a bewildered eccentric who thought it would be cool to combine cat videos with pop songs in a random mayhem of laughter-inducing proc(r)a(s)tination.\nBut that\u0026rsquo;s what SEOs have to do. They have to help the customer market their content. They have to know what\u0026rsquo;s cool and what isn\u0026rsquo;t. And they have to know the right channels to promote the content in.\nSo add step number (4) to the list as an all-encompassing feature of SEO work: content strategy. It\u0026rsquo;s the most important thing by a landslide, as none of the other aspects works in the long run without a decent content strategy to back it up.\nAnd that\u0026rsquo;s all there is to it. In a nutshell, SEO is all about creating magical content to lure, charm, and convert your site visitors into loyal fans. It\u0026rsquo;s all about the buzz, the viral videos, and the annoying memes. It\u0026rsquo;s about being unique in a forum where being unique is almost impossible. It\u0026rsquo;s about finding the critical point between hype and saturation, and hanging in there for as long as you can.\nI heard there were going to be some tips? OK, but just two this time.\n1) Did you know that Google uses pixels to determine title length in SERPs? That\u0026rsquo;s right. The traditional approach in SEO has been to limit page title length to just under 70 (and over 50) characters in length. However, much longer titles are OK as long as they\u0026rsquo;re below a certain width in pixels. So what\u0026rsquo;s the length? Well, there\u0026rsquo;s some debate about that, but it appears it\u0026rsquo;s somewhere in the vicinity of 500 pixels.\n2) You can extrapolate your keyword data in Analytics to uncover your (not provided) results. Some time ago, Google hid all search queries from Google Analytics that have been made by signed in (Google) users. This is a pain for reporting SEO success, as (not provided) results can dominate the rankings. Here\u0026rsquo;s a nice tip by AJ Kohn to use the distribution of other keywords to extrapolate the distribution of (not provided) keywords.\n"
},
{
	"uri": "https://www.simoahava.com/tags/google/",
	"title": "google",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/penguin/",
	"title": "penguin",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/penguin-2-0-googles-next-major-update/",
	"title": "Penguin 2.0: Google&#39;s Next Major Update",
	"tags": ["google", "penguin", "SEO"],
	"description": "A brief introduction to the Penguin 2.0 Google search algorithm update.",
	"content": "When Matt Cutts speaks, the world listens. We reach out our hands to catch even the tiniest morsels that make up the bread that fills the basket that is Google. We hush in anticipation, as we know that we are about to be revealed another piece of the puzzle that is Google\u0026rsquo;s search algorithms. We want to know these dark, esoteric, technological secrets because a) as humans we are genetically coded to abhor secrets and shadow-talk, and b) as SEOs we are competing in a business where only the first place is rewarded. And now Matt Cutts has spoken a glorious 7 minutes, 24 seconds on Penguin 2.0, the next version of Google\u0026rsquo;s spam-blasting incarnation.\n  Before you pack your survival kit and head for the fox hole you dug out in the back yard, don\u0026rsquo;t fret. If you\u0026rsquo;ve been doing things by the book, you should be OK. Well, you never know with Google, but that\u0026rsquo;s what we\u0026rsquo;ve been promised. Remember, Google only wants you to do a quality website. There\u0026rsquo;s no magic to it, you just have to create good content for people who might be genuinely interested in reading it. Doing things all clean like this, your site\u0026rsquo;s link profile should have accumulated organically, without you taking part in any dirty link-scavenging schemes (black hat SEO, as the industry calls it).\nHowever, if you\u0026rsquo;ve been buying links, or if you\u0026rsquo;ve accumulated links from directories or services which exist in the periphery of good taste, you might be in trouble. You should always try to aim for links which come from domains that relate to your business. Getting links from authority sites that have direct relevance to your own is a win-win situation in SEO. You get the important link you need, and you increase the authority of the hub that linked to you, thus increasing the power of your connection.\nIf your links come from disreputable sites, or sites with low authority, you might be considered a prime candidate to target with spam-based demotion in Penguin 2.0.\nPenguin 2.0 speculation Remember that this is all still speculative. Matt Cutts can\u0026rsquo;t and won\u0026rsquo;t go into detail, because that\u0026rsquo;s just how things roll in the Googleplex. They want us to find out for ourselves, and thus keep the search engine industry blooming. However, he does have a pretty good idea on how things might turn up in Penguin 2.0, and here\u0026rsquo;s its current status:\n1. Advertorials Google are looking to punish advertorials whose sole reason is to pass PageRank. Advertorials are basically legitimate looking texts or articles, but they contain paid links back to the website where the commission originated from. Google wants to target these page links, as they do not want to permit PageRank flowing from articles whose link value is bought, and not earned through authority and relevance. In Penguin 2.0, advertorials need to be conspicuous. The readers must be aware that the text is an advert, or otherwise penalties might kick in.\n2. Spammy queries Another thing Penguin 2.0 will target are queries that are traditionally contested by spammers. Google wants to clean these queries up and provide results that are actually relevant. This is, of course, good news to porn hunters, as the adult industry is possibly one of the most spammed niches of the web. All in all, the new update will take a strong stand against link spamming and link spammers.\n3. Webmaster tools Google wants to help webmasters with better tools. One of these new tools will have to do with hacked site detection and with remedies to battle hacking. If a webmaster finds out that their site has been hacked, Google provides the tools to report this and fix the situation.\n4. Authority counts! Now\u0026rsquo;s the time to look at your link profile and start hounding for links coming from authorities in your field of business. Google is looking to increase the visibility and PageRank of authority sites. What this means is that if you are recognized (by Google\u0026rsquo;s algorithms, of course) to be an authority in your field of business or information, your site will gain momentum in the organic ranking of pages. Naturally, all sites who benefit from your generous links will enjoy this change as well.\n5. Cluster-proofing Penguin 2.0 will make changes to how we see clustered results in search queries. Have you ever noticed that when you go past page 3 or 4 in Google\u0026rsquo;s search results, you\u0026rsquo;ll find a bunch of results coming from the same host domain? This is host clustering, and it basically means that a single domain is over-represented in search results when going deeper in. The planned algorithm change will activate so that if you\u0026rsquo;ve already seen results from a domain, it will be less likely that you\u0026rsquo;ll see more hits from that domain further on down the chain.\nWhat goes around, comes around Even though this is all just speculation, and nothing is official yet, we know that Matt Cutts doesn\u0026rsquo;t waste his breath on trivialities. So my advice is this:\nLook at your link profile and clean it up if necessary. Get those suspicious looking, low-authority domains away from there, and start looking for authority websites that might spread you with link honey.\nKeep creating quality content. Nothing is more effective than doing what you should be doing. Stop looking for ways to manipulate the search engines, and start looking for ways to coexist with them. Penguin 2.0 might sound like a fluffy toy or something to do with Linux, but it will surely cause some mayhem, especially with sites boasting vast link diversity.\n"
},
{
	"uri": "https://www.simoahava.com/seo/checklist-for-optimizing-web-design/",
	"title": "Checklist For Optimizing Web Design",
	"tags": ["SEO", "WCAG", "Web design"],
	"description": "Handy checklist for comparing site accessibility, web development best practices, and search engine optimization preferences-",
	"content": "In this post, I propose that a combination of valid, accessible, and search engine friendly markup is the perfect recipe for optimal web design.\nFor markup to be valid, it needs to conform to the guidelines laid out by the \u0026ldquo;governing body\u0026rdquo; of HTML standardization: the World Wide Web Consortium, or W3C. While the Internet anxiously waits for HTML5 to shift in status from candidate to recommendation, we\u0026rsquo;re stuck with ye olde HTML 4.01 standard (est. 1999) as laid out by W3C. Naturally, HTML5 is already widely supported by all the major browsers, and it can (and should) be incorporated in web design without hesitation.\nAccessibility here means that the markup follows the recommendations in the Web Content Accessibility Guidelines (WCAG). WCAG is developed by the Web Accessilibity Initiative (WAI), also engineered by W3C. The guidelines, now matured to their 2.0 version (since 2008), provide a list of steps to consider during web design. These steps aim to make the pages accessible for users with disabilities, and also for different platforms and user agents such as mobile devices.\nFinally, search engine friendly markup follows the best practices identified by the enormous SEO community, all working to uncover the best ways to make friends with the elusive search engines that govern our daily life on the web. There is a seemingly endless supply of good SEO guides for optimizing web design, but if the topic is new to you, you might as well start with SEOmoz\u0026rsquo;s Beginner\u0026rsquo;s Guide To SEO.\nOptimal markup is good for your web design On our way to finding the best practices, i.e. the perfect combination of recommendations, let\u0026rsquo;s get one thing straight. It is a self-evident, nature-given, inalienable fact that your markup must be 100% valid. Even though the HTML standard is just a recommendation, meaning that web browsers can steer around your poor markup, it\u0026rsquo;s no reason to be lax about it.\nFor one thing, invalid markup is a sign of sloppiness. Web designers should take pride in creating markup that is clear, readable, and passes validation tests. Also, valid markup is always a requirement for accessibility and SEO. There is no such markup that is search engine friendly or accessible and not valid at the same time. So if your markup is valid, you are well on your way to optimizing it for WCAG and SEO as well.\n  Finding the optimal markup for W3C, WCAG and SEO is the holy grail of web design\nWhether you want to promote your site\u0026rsquo;s accessibility or search engine friendliness is up to you. Most of the things you do for SEO will apply for WCAG as well, and vice versa. Note that there are elements prohibited in one but accepted in the other, and there are elements which have relevance only for one but not the other. In your web design efforts, I recommend that you pay heed to the rules and make note of the recommendations. You can always try to make your website as accessible, search engine friendly, and valid as possible, but this post looks to find the minimal strategy required to find the holy grail of optimal markup.\nThe guidelines What follows is essentially a checklist for your web design needs. I have chopped it up so that first you see the recommendations that are shared by SEO and WCAG. Following that are the recommendations that are SEO- or WCAG-specific. A green bulb means that the item is a requirement. A yellow bulb means that the item is a recommendation (or has some other significance). A red bulb means that the item is prohibited, and a dash means that the item has no (significant) relevance. Note that W3C validation criteria are not listed, as I mentioned above that regardless of recommendations or guidelines, make sure that your HTML is always 100% valid.\nFor the WCAG guidelines, I use the recommendations in WebAIM\u0026rsquo;s WCAG 2.0 checklist. Kevin Vertommen has provided a wonderful SEO checklist for web designers, which I refer to here as well. Note here also that because especially SEO best practices are quite subjective and usually require context to perform well, I have used my judgment on what is most often the best practice for each recommendation. This means that you should take it with a grain of salt, and treat it as a requirement only if it\u0026rsquo;s relevant for your particular SEO case.\nDownload the entire chart in PDF format (Comparison chart for SEO and WCAG guidelines.pdf).\n  In the snippet above I only included those recommendations that apply to both SEO and WCAG guidelines, in good or in bad. The PDF contains the entire chart. Most of the recommendations put forth by WCAG and SEO are relevant to one but not the other. As you can see from the image above, I identified only three recommendations as possibly harmful, and they were only harmful to SEO. They are easily avoided, as you should really try to do without frames, and null ALT text simply has no justification. If you have images that serve no content purpose, make them background elements by using CSS.\nBecause SEO is the only thing that stands to be harmed if you follow all guidelines diligently, it would imply that you should make following SEO guidelines a priority. So when writing markup 1) make sure it is valid, 2) make sure your page conforms to all SEO best practices, and only then 3) make sure your page follows WCAG recommendations. Just be wary of using frames and null ALT text, and you\u0026rsquo;ll do fine.\nThe PDF is not comprehensive, but it does serve as a nice \u0026ldquo;checklist\u0026rdquo; for checking if your site adheres to accessibility and search optimization guidelines. It is not a bad habit to familiarize yourself with these guidelines and to make them part of your web design routine. Once you ensure that your HTML is valid, accessible, and search engine friendly, you can rest assured that you are doing your part in making the Internet a better place.\n"
},
{
	"uri": "https://www.simoahava.com/tags/wcag/",
	"title": "WCAG",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/web-design/",
	"title": "Web design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/audience-design/",
	"title": "Audience design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/content-management-system/",
	"title": "Content management system",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/content-strategy-know-your-audience/",
	"title": "Content Strategy: Know Your Audience",
	"tags": ["Audience design", "CMS", "Content management system", "Content strategy", "Search engine optimization"],
	"description": "Content strategy is the key to any successful search engine optimization effort.",
	"content": "For years and years, the one constant in the chaotic world of SEO has been a simple statement: Content is King. This statement has been the cornerstone of content strategy in SEO projects, and its validity has hardly been contested. This is not due to lack of trying. Many posts in the blogosphere have taken an opposing stand (Heidi Cohen: format is king, Carl Ocab: marketing is king, Bernadette Coleman: trust is king). The message of the opposition is this: content cannot be king, as it is only a small part of the audience experience.\n  I agree, and I also think the metaphor is a poor one. Being king implies sovereignty. Kings inherit their power through lineage (or usurpation), and they rule supreme (well, they used to, at least). The problem with this metaphor, in terms of content strategy, is that this devalues all the other, hugely important aspects of creating a successful website. In SEO, all parts of the optimization process come together in a well-developed content strategy:\nYou need the technical groundwork to make sure the site is crawlable. Without crawled pages, your site\u0026rsquo;s standing in organic search results will be compromised. As long as you have access to a modern CMS, you won\u0026rsquo;t need to worry about this too much.\nYou need a strong keyword analysis to make sure that the right audience arrives at your site. You need to understand your audience\u0026rsquo;s needs in order to convert them to followers/fans/revenue/whatever your goals are.\nYou need jaw-dropping design to make sure that the initial impression your site gives is the one that draws your visitors in.\nYou need a good outreach plan to build hype around your site. Your marketing department must be up-to-speed with the latest online trends and fashions.\nAnd finally, you need incredible content to make sure that the audience who does arrive at your site doesn\u0026rsquo;t leave. Ever. You want your site to be the last one they look at before they go to sleep, the one they dream about in their feverish dreams, and the first thing they think of when they wake up in the morning.\nYou can focus your content strategy in many ways (see e.g James Agate\u0026rsquo;s wonderfully informative post A Guide to Producing World-Class Infographics), but the basics are always the same, and I will introduce them next.\nRight content strategy for the right audience I like to approach content strategy with the same tools and methods I used when I was a product owner in an agile development team. Everything began with a vision statement. Every product, every release, every increment, and every feature needed to adhere to this vision religiously.\nimilarly, when I think about what content to create for a single page, for example, I think of a single content statement which governs everything the page should be about. Really, it must be simple. A good content statement contains the the core message of the page, the general target group, the differentiating factor, and a goal statement. For example:\nThis page tells all about MyToolCompany's new Power Drill 2000. This information is of most interest to home owners, building crews and DIY hobbyists. This page stands out because the product is introduced in a memorable and easily accessible way. The goal of this page is to get leads and sales contacts. After I have my content statement, I can start thinking about the target groups more carefully. This aspect of content strategy requires a lot of niche knowledge about the business. If a SEO consultant has been hired to work on the content, target group analysis should be developed as a team effort.\nI call this audience design, and it should provide you with the keywords you need to work on semantic optimization. Look at search trends to make early bird calls, and home in on your competition\u0026rsquo;s tactics.\nAnother really good method of implementing audience design is to draft user stories and scenarios. Think of a few user profiles who might be typical visitors to your site, and write scenarios for them: \u0026ldquo;As a user type, I want to find out about page content so that I can statement of need or action\u0026rdquo;. Once you have a number of these, you can start writing your content.\nThe point behind all this labor is simple. Your content strategy will ensure high quality content if you ask the following questions after each paragraph you write:\n  Is this paragraph in line with the content statement? If there is any content that cannot directly be related to the content statement, discard the text. You don\u0026rsquo;t want to confuse your visitors by littering your pages with irrelevant or anecdotal content.\n  Does this paragraph cater to the user scenarios you wrote? Step into the shoes of the user profiles you created. Read through the text as if you were a visitor on the site. Does it work for you? Do you feel attached to the content of the paragraph, or does it leave you cold?\n  Does this paragraph cover just one issue? Or do you ramble on? Focusing your writing is a good habit and increases readibility a lot. Each paragraph should cover just one issue related to the topic of the page, and each page should have just one topic.\n  As long as each paragraph you write scores well on the questions above, you should be well on your way to creating amazing content.\nThe most important thing is to really, REALLY, suck up to your audience. Your content strategy is not king-like. Its power is not self-derived, it doesn\u0026rsquo;t have the means to dictate, and it can\u0026rsquo;t overrule the masses. It\u0026rsquo;s more like a democratically elected president: It has a lot of power, but the minute the people are dissatisfied is the minute they abandon the fruit of all your hard work.\nSo content really isn\u0026rsquo;t king? That\u0026rsquo;s right it isn\u0026rsquo;t. Content is not and should not be the sole reason your site is successful. And here\u0026rsquo;s the kicker: your content strategy should not just create quality content. Your website design should not just create beautiful and accessible visuals. Your forms, plugins and widgets should not just provide fluent interaction with your site visitors. No.\nEverything you do should be geared towards eliciting an emotional response from your guest.\nRead that again. You are competing against a gazillion of similar sites in the web. You must know who your visitors are, and you must model your content strategy after their needs.\nIn other words: Remember your audience and they will remember you.\n"
},
{
	"uri": "https://www.simoahava.com/tags/search-engine-optimization/",
	"title": "Search engine optimization",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/modern-cms/",
	"title": "Modern CMS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/content-management/modern-cms-top-5-features/",
	"title": "Modern CMS: Top 5 Features",
	"tags": ["CMS", "Content management system", "Modern CMS", "Modular architecture", "OWASP", "Security"],
	"description": "Listing top 5 features that any modern content management system (CMS) should have.",
	"content": "You are right now enjoying the fruits of a very popular content management system, or CMS, whether you know it or not. This blog is published via WordPress, a modern CMS if there ever was one. In fact, if you add a comment to this post with the form below (hint, hint), you will be participating in content creation, using tools that come out-of-the-box in this particular platform.\n  But what makes a modern CMS? Is WordPress the perfect choice for you or your business? Well, I\u0026rsquo;m not going to review the products on the market, so I\u0026rsquo;ll leave the second question to you or your IT manager. However, I will take a stand on the first question, as I look at what makes a modern content management system. What are the must-have features it should boast, and what can you live without? My word is not doctrine, but I do have a long history of working with CMS\u0026rsquo;s and with clients who only want the best.\nThe top 5 must-have features of any modern CMS 5. Modularity\nUnless you have the kind of money to pay for a tailored solution created from scratch just for you and your needs, you\u0026rsquo;re going to want a modular platform. Actually, even if you\u0026rsquo;re shopping around for a customized solution you\u0026rsquo;ll still want it to be modular. The reason is simple. No matter how far-sighted you think you are, your needs will change and the requirements you attach to a CMS will change as well. Lack of modularity leads to a number of problems, with cumbersome change management at the top of the list.\n  Furthermore, if the platform is modular, development will be more cost-efficient. An able version management plan for the core as well as the modules ensures that only the necessary changes are implemented in each release cycle. Trust me, you will be thankful when a new core package doesn\u0026rsquo;t automatically result in the quick and merciless incapacitation of your installation.\nIn addition to architectural modularity, where the core features and the components are separated in the code, support for dedicated and third-party plugins, widgets, apps etc. is a huge plus. To have an entire community working on the development is what a modern CMS is all about. Which brings me to the next key feature:\n4. Open source\nThis could be easily debated, as there are a number of hugely popular proprietary CMS\u0026rsquo;s out there, such as Microsoft SharePoint and EPiServer. The problem with these solutions is that their development is governed by the company who owns the software. This means that the community\u0026rsquo;s impact is diminished, since the company can usually support only a singular product vision. Furthermore, SharePoint, for example, is so much more than just a CMS. It\u0026rsquo;s a full-blown web platform for enterprises, and its development is surely not geared to making it the best web publishing solution around.\n  Open source software takes the development away from a single point-of-origin and brings the power to the community. The mark of a modern CMS is to stay up-to-date, fresh and vitalized. A vibrant user community, where everyone partakes in the design, development and support of the platform can ensure just that.\nOpen source does have its own issues, ranging from the significant (lack of a proper support process, exaggerated focus on template design) to the insignificant (unpredictability of community-driven development, uncertain platform life cycle), but the pros of having an open-source platform outweigh the cons. But as I said earlier, whether or not this point is a key feature of a CMS is debatable. What I do believe with absolute certainty is that community- and user-driven development have a far more significant impact on defining what a modern CMS is than development that is hidden behind the interfaces of a closed source application.\n3. Control over the content\nIn many ways, this is the single most important feature of any modern CMS, and will most probably affect your decision the most. There is simply no excuse for a software solution which doesn\u0026rsquo;t provide all the necessary tools for advanced content manipulation. Here are some of the features the CMS must be able to tackle:\n  Quick publishing - if you are a blogger or if you update the news section of your website, you will want it to be as painless as possible. Having to click 5+ times just to get your modifications published is a turn-off\n  Access to the HTML source of the template - if you are denied the possibility to edit the title, the meta tags or other markup in your website, you are in a jam, as these all have a huge impact on the accessibility of your website\n  Editing the URL structure - another huge thing for accessibility and SEO, because being stuck with a garbled URL structure or one riddled with confusing parameters will make it difficult for users and search engines to identify important content\n  Separation of content and design - you might think that you\u0026rsquo;d do fine with a visual, WYSIWYG editor, but trust me: you will not want the design to dictate what content you can and should publish. Rather, it should always be the other way around, where the design adapts to whatever content you feed into it.\n  2. Customizability\nThis steps somewhat into the same territory which we already covered in point #5. Where modularity covers the architectural aspects of a modern CMS, customizability has to do with how you want the platform to suit your needs. Plugins, widgets, apps, components, controls etc. exist to provide you with a number of ways to customize your solution. You can shop around for the perfect addition to your core application. If you can\u0026rsquo;t find what you\u0026rsquo;re looking for, you can always create (or pay for the creation of) the extension.\nYou need to be able to stay on top of how the software works. You should be able to choose the features that are active, and hide or deactivate the rest. You should be able to choose the theme, design, skin or template of your site, and you should be able to modify or remove it if you wish. You must be able to secure single pages or sections of your website behind restricted access, and it should be possible to edit and mass edit any resources, pages or assets within the site.\nIn short: if there\u0026rsquo;s something you can\u0026rsquo;t modify in the platform, there must be a really good reason for it. Not the other way around.\n1. Security\nThis is a no-brainer. Any modern CMS worth its salt must be secure. OWASP has listed the top 10 security concerns for 2013, and you\u0026rsquo;d be surprised how many CMS deployments I\u0026rsquo;ve come across where these concerns have surfaced.\nNaturally, especially with open source software, security problems crop up more often than not. Since users can customize the application to their liking, it doesn\u0026rsquo;t require too much imagination to see how lack of proper quality control might lead to breaches in the system.\nOnce you have your platform set up with all its extensions, customizations and most of the content, it would be a good idea to order a security audit of the solution. It\u0026rsquo;s too easy to overlook injection flaws and data exposure problems, among others.\nSo what is the best solution for you? Well, your business needs dictate which platform you should get. These five modern CMS features exist in a number of excellent software solutions out there. If you also count in the proprietary software which have all but feature #4, the list is long indeed. Most likely you already have a CMS in mind if you are reading this. I hope that these five requirements help you in your decision-making.\n"
},
{
	"uri": "https://www.simoahava.com/tags/modular-architecture/",
	"title": "Modular architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/owasp/",
	"title": "OWASP",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/google-trends/",
	"title": "Google trends",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/tags/search-trends/",
	"title": "Search trends",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.simoahava.com/seo/search-trends-and-what-they-reveal/",
	"title": "Search Trends And What They Reveal",
	"tags": ["Google trends", "Search trends"],
	"description": "Research search trends to get a pulse of the world.",
	"content": "I\u0026rsquo;m hooked on Google Trends. For example, it probably won\u0026rsquo;t astound you to learn that whenever search trends peak for flu symptoms, there\u0026rsquo;s a similar peak for vaccine.\n  (See also how at some points vaccine comes first and only then do flu symptoms arise. Conspiracy theorists, the ball is in your court!)\nProbing this particular case further, I looked at the search trends for swine flu and vaccine. The former peaks in April 2009 and, to my surprise, vaccine actually declines over the following two months. However, it skyrockets during the third peak of the swine flu searches. What does this tell us? I\u0026rsquo;d like to speculate that people have become resilient to rumors, needing factual confirmation before rushing to the vaccine line. However, one thing is sure: This is a clear incentive for pharmacies and health care stations. What better time to promote their goods than the minute search trends for flu activity begin to gain statistical momentum?\n  By the way, like me, Google has tapped into Trends as a resource for identifying flu trends (see Google Flu Trends).\nSearch trends, correlations and conclusions I\u0026rsquo;m a HUGE fan of Freakonomics by Steven Levitt and Stephen J. Dubner. It\u0026rsquo;s a book on economics, correlations, causality, data mining and other boring-sounding topics, but the authors have managed to spin a yarn so delightful that it makes for a very entertaining read. Freakonomics studies trends and how they seem to correlate. They draw conclusions that might at times stray beyond the scope of credibility, but it all appears kosher enough to a layman (which is probably why it\u0026rsquo;s a bestseller). Anyway, once you get really into populist economics like this book, you start to get paranoid.\nYou start seeing correlations everywhere.\nFor example, between 2004-2013 there\u0026rsquo;s a clear decline in the search trends for war, which is a good thing, I guess. I like to think that it\u0026rsquo;s an indication of the fact that war is not the foremost thing in people\u0026rsquo;s minds. On the other hand, there\u0026rsquo;s a clear rise in queries for what to do, the motto of a very bored generation. It would be so easy to make a tale out of these two distinct search trends. Maybe it would be something like \u0026ldquo;there\u0026rsquo;s no war going on, what should we do now?\u0026quot;; an unhealthy, completely irresponsible statement (and factually wrong) coupled with a very popular information query (e.g. what to do in Berlin?), coming together in a correlation which might mislead the misinformed.\n  This is, of course, completely ungrounded conjecture. But seemingly unrelated correlations can be a breeding ground for some pretty amazing conclusions, as can be seen in Freakonomics. War and what to do might be an example of search trends having correlation only in the wildest imaginations of blog authors. Conversely, a study was done on the search volumes for debt, and it showed that search trends for debt were a very clear indicator of market fluctuations, thus showing that studying search queries has concrete applications.\nHow are we doing, really? What do search trends tell about the state of humanity? Can you really draw conclusions on the basis of what people type into the small input box day in, day out? I think you can, up to a point. First, you need a hypothesis before you start shuffling through the data. Just throwing in terms at random provides optimal manure for misguided conclusions based on your first interesting findings (see the comparison between war and what to do above). Second, a good quantitative analysis can go only so far without a qualitative component. In addition to crunching up the numbers, think of how the statistics relate to events around the globe during the peaks and troughs in the search volume. Think of why people did those very searches in those very months.\nOnce you accept the fact that you have no control over the quality of your informants, that your case study has little to do with ideal, clinical laboratory conditions, and that you are looking at search trends not absolutes (people do a lot more searches than just the ones you look at), you might see something of interest. Maybe this turns into a lucrative chance for you, especially if you identify something that has market value. Or maybe you\u0026rsquo;ve found something that tells you more about human nature than a single Superbowl commercial ever did.\nOr maybe you forget all about statistical significance and quality, and you see something like this, and it makes you smile just a little bit:\n  "
},
{
	"uri": "https://www.simoahava.com/archives/",
	"title": "Archives",
	"tags": [],
	"description": "",
	"content": ""
}]
