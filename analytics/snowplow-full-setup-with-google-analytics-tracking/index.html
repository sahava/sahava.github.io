

  
    
  


  





  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta name="google-site-verification" content="xQwWI2KUfP9LbFKhw2CVLFtrMY6Czrla7L3PD2aBolA" />


<script type="application/ld+json">
{
  "@context": "http://schema.org", 
  "@type": "BlogPosting",
  "headline": "Snowplow: Full Setup With Google Analytics Tracking | Simo Ahava's blog",
  "image": "https:\/\/www.simoahava.com\/images\/2018\/02\/snowplow-etl-emr.jpg",
  "editor": "Simo Ahava",
  "publisher": {
    "@type": "Organization",
    "name": "Simo Ahava's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/www.simoahava.com\/images\/simo.png",
      "height": "393",
      "width": "407"
    }
  },
  "url": "https:\/\/www.simoahava.com\/analytics\/snowplow-full-setup-with-google-analytics-tracking\/",
  "datePublished": "2018-02-07T09:00:37\u002b02:00",
  "dateModified": "2018-02-07T09:00:37\u002b02:00",
  "description": "Step-by-step process for setting up a Snowplow analytics pipeline with data pulled in from your Google Analytics trackers.",
  "author": {
    "@type": "Person",
    "name": "Simo Ahava"
  }
}
</script>


    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Snowplow: Full Setup With Google Analytics Tracking | Simo Ahava's blog</title>
    
    <script>
      window.dataLayer=window.dataLayer||[];window.dataLayer.push({pageType: 'article', environment:  null });
    </script>
    

    
    
    <script src="https://www.simoahava.com/x-js/lazyload.iife.min.js"></script>

    

    
    <script>
      (function(S,i,m,o){
        window[m]=window[m]||[];
        window[m].push({'gtm.start':new Date().getTime(),event:'gtm.js'});
        var f=S.getElementsByTagName(i)[0],j=S.createElement(i),dl=m!='dataLayer'?'&l='+m:'';
        j.async=true;
        j.src='https://www.googletagmanager.com/gtm.js?id='+o+dl;
        f.parentNode.insertBefore(j,f);
      })(document,'script','dataLayer','GTM-PZ7GMV9');
    </script>
    
    <meta name="author" content="Simo Ahava">
    <meta name="keywords" content="">

    <link rel="icon" href="https://www.simoahava.com/images/favicon.ico">
    
    <link rel="amphtml" href="https://www.simoahava.com/amp/analytics/snowplow-full-setup-with-google-analytics-tracking/">
    
    <meta name="description" content="Step-by-step process for setting up a Snowplow analytics pipeline with data pulled in from your Google Analytics trackers.">
    <meta property="og:description" content="Step-by-step process for setting up a Snowplow analytics pipeline with data pulled in from your Google Analytics trackers.">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Snowplow: Full Setup With Google Analytics Tracking">
    <meta property="og:url" content="https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
    <meta property="og:site_name" content="Simo Ahava&#39;s blog">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Snowplow: Full Setup With Google Analytics Tracking">
    <meta name="twitter:description" content="Step-by-step process for setting up a Snowplow analytics pipeline with data pulled in from your Google Analytics trackers.">
    
      <meta name="twitter:creator" content="@SimoAhava">
      
    

    
    

    
      <meta property="og:image" content="https://www.simoahava.com/images/simo.png">
    

    
      <meta name="twitter:image" content="https://www.simoahava.com/images/2018/02/snowplow-etl-emr.jpg">  
      <meta property="og:image" content="https://www.simoahava.com/images/2018/02/snowplow-etl-emr.jpg">
    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    
    
    
    <link rel="stylesheet" href="https://www.simoahava.com/css/style.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://www.simoahava.com/">Simo Ahava&#39;s blog</a>
  </div>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://www.simoahava.com/">
          <img class="sidebar-profile-picture" src="https://www.simoahava.com/images/simo.png" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Simo Ahava</h4>
        
          <h5 class="sidebar-profile-bio">Husband | Father | Analytics developer<br>simo (at) simoahava.com</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/categories/">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/tags/">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/categories/gtm-tips/">
    
      <i class="sidebar-button-icon fa fa-lg fa-magic"></i>
      
      <span class="sidebar-button-desc">GTM Tips</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/about-simo-ahava/">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/tools/">
    
      <i class="sidebar-button-icon fa fa-lg fa-wrench"></i>
      
      <span class="sidebar-button-desc">Tools</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/blog-statistics/">
    
      <i class="sidebar-button-icon fa fa-lg fa-bar-chart"></i>
      
      <span class="sidebar-button-desc">Blog statistics</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/sahava" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/upcoming-talks/">
    
      <i class="sidebar-button-icon fa fa-lg fa-bullhorn"></i>
      
      <span class="sidebar-button-desc">Upcoming talks</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/custom-templates/">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Templates</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.simoahava.com/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post">
	  <form id="search" action="https://www.simoahava.com/search/">
  <input name="q" type="text" class="form-control input--xlarge" placeholder="Search blog..." autocomplete="off">
  </form>

          
          
            <div class="post-header main-content-wrap text-left">

  

    <h1>
      Snowplow: Full Setup With Google Analytics Tracking
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time datetime="2018-02-07T09:00:37&#43;02:00">
        
  February 7, 2018

      </time>
      
      
  
  
    <span>in</span>
    
      <a class="category-link" href="https://www.simoahava.com/categories/analytics">Analytics</a>
    
  


      | <a href="https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/#commento">Comments</a>
      
  </div>


</div>

          
          <div class="post-content markdown">
            <div class="main-content-wrap">
              <p>A <a href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">recent guide</a> of mine introduced the <a href="https://snowplowanalytics.com/blog/2018/01/25/snowplow-r99-carnac-with-google-analytics-support/">Google Analytics adapter</a> in <a href="https://snowplowanalytics.com/">Snowplow</a>. The idea was that you can duplicate the <a href="https://analytics.google.com/">Google Analytics</a> requests sent via <a href="https://tagmanager.google.com/">Google Tag Manager</a> and dispatch them to your Snowplow analytics pipeline, too. The pipeline then takes care of these duplicated requests, using the new adapter to automatically align the hits with their corresponding data tables, ready for data modeling and analysis.</p>
<p>While testing the new adapter, I implemented a Snowplow pipeline from scratch for parsing data from my own website. This was the first time I&rsquo;d done the whole process from end-to-end myself, so I thought it might be prudent to document the process for the benefit of others who might want to take a jab at Snowplow but are intimidated by the moving parts.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/snowplow-etl-emr.jpg" title="Snowplow ETL in process">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/snowplow-etl-emr.jpg#ZgotmplZ" alt="Snowplow ETL in process" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1919 467'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>And make no mistake. There are <strong>plenty</strong> of moving parts. Snowplow leverages a number of <a href="https://aws.amazon.com/">Amazon Web Services</a> components, in addition to a whole host of utilities of its own. It&rsquo;s not like setting up Google Analytics, which is, at its most basic, a fairly rudimentary plug-and-play affair.</p>



<div class="figure " >
  
    <a href="https://www.simoahava.com/images/2018/02/ga-schema.jpg" title="Image source: https://goo.gl/D2f3xi">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/ga-schema.jpg#ZgotmplZ" alt="Image source: https://goo.gl/D2f3xi" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 690 317'%3E%3C/svg%3E">
  
    </a>
  
   
    <span class="caption">Image source: https://goo.gl/D2f3xi</span>
  
</div>


<p>Take this article with a grain of salt. It definitely does <strong>not</strong> describe the most efficient or cost-effective way to do things, but it should help you get started with Snowplow, ending up with a data store full of usable data for modeling and analysis. As such, it&rsquo;s not necessarily a <strong>guide</strong> rather than a description of the steps I took, in good and bad.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/sql-workbench-query.jpg" title="SQL Workbench query against page view hits">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/sql-workbench-query.jpg#ZgotmplZ" alt="SQL Workbench query against page view hits" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1433 297'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p><strong>WARNING:</strong> If you DO follow this guide step-by-step (which makes me very happy), please do note that there will be costs involved. For example, my current, very light-weight setup, is costing me a couple of dollars a day to maintain, with most costs incurred by running the collector on a virtual machine in Amazon&rsquo;s cloud. Just keep this in mind when working with AWS. It&rsquo;s unlikely to be totally free, even if you have the free tier for your account.</p>
<div id="toc-container"><h1 id="table-of-contents">Table of Contents</h1> <input type="checkbox" id="show"><label for="show" id="showbtn"><span class="show">&nbsp;[+show]</span><span class="hide">&nbsp;[–hide]</span></label></h1><nav id="TableOfContents">
  <ul>
    <li><a href="#to-start-with">To start with</a></li>
    <li><a href="#why-snowplow">Why Snowplow?</a></li>
    <li><a href="#what-we-are-going-to-build">What we are going to build</a></li>
    <li><a href="#step-0-register-on-aws-and-setup-iam-roles">Step 0: Register on AWS and setup IAM roles</a>
      <ul>
        <li><a href="#what-you-need-for-this-step">What you need for this step</a></li>
        <li><a href="#register-on-amr">Register on AMR</a></li>
        <li><a href="#create-an-identity-and-account-management-iam-user">Create an Identity and Account Management (IAM) user</a></li>
        <li><a href="#what-you-should-have-after-this-step">What you should have after this step</a></li>
      </ul>
    </li>
    <li><a href="#step-1-the-clojure-collector">Step 1: The Clojure collector</a>
      <ul>
        <li><a href="#what-you-need-for-this-step-1">What you need for this step</a></li>
        <li><a href="#getting-started">Getting started</a></li>
        <li><a href="#setting-up-the-clojure-collector">Setting up the Clojure collector</a></li>
        <li><a href="#enable-logging-to-s3">Enable logging to S3</a></li>
        <li><a href="#set-up-the-load-balancer">Set up the load balancer</a></li>
        <li><a href="#route-traffic-from-your-custom-domain-name-to-the-load-balancer">Route traffic from your custom domain name to the load balancer</a></li>
        <li><a href="#setting-up-https-for-the-collector">Setting up HTTPS for the collector</a></li>
        <li><a href="#switch-the-load-balancer-to-support-https">Switch the load balancer to support HTTPS</a></li>
        <li><a href="#all-done">All done!</a></li>
        <li><a href="#what-you-should-have-after-this-step-1">What you should have after this step</a></li>
      </ul>
    </li>
    <li><a href="#step-2-the-tracker">Step 2: The tracker</a></li>
    <li><a href="#step-25-test-the-tracker-and-collector">Step 2.5: Test the tracker and collector</a></li>
    <li><a href="#step-3-configure-the-etl-process">Step 3: Configure the ETL process</a>
      <ul>
        <li><a href="#what-you-need-for-this-step-2">What you need for this step</a></li>
        <li><a href="#getting-started-1">Getting started</a></li>
        <li><a href="#download-the-necessary-files">Download the necessary files</a></li>
        <li><a href="#create-an-ec2-key-pair">Create an EC2 key pair</a></li>
        <li><a href="#create-the-s3-buckets">Create the S3 buckets</a></li>
        <li><a href="#prepare-for-configuring-the-emretlrunner">Prepare for configuring the EmrEtlRunner</a></li>
        <li><a href="#configure-emretlrunner">Configure EmrEtlRunner</a></li>
        <li><a href="#what-you-should-have-after-this-step-2">What you should have after this step</a></li>
      </ul>
    </li>
    <li><a href="#step-4-load-the-data-into-redshift">Step 4: Load the data into Redshift</a>
      <ul>
        <li><a href="#what-you-need-for-this-step-3">What you need for this step</a></li>
        <li><a href="#getting-started-2">Getting started</a></li>
        <li><a href="#create-the-cluster">Create the cluster</a></li>
        <li><a href="#configure-the-cluster-and-connect-to-it">Configure the cluster and connect to it</a></li>
        <li><a href="#create-the-database-tables">Create the database tables</a></li>
        <li><a href="#create-the-database-users">Create the database users</a></li>
        <li><a href="#create-new-iam-role-for-database-loader">Create new IAM role for database loader</a></li>
        <li><a href="#edit-the-redshift-target-configuration">Edit the Redshift target configuration</a></li>
        <li><a href="#re-run-emretlrunner-through-the-whole-process">Re-run EmrEtlRunner through the whole process</a></li>
        <li><a href="#test-it">Test it</a></li>
      </ul>
    </li>
    <li><a href="#wrapping-it-all-up">Wrapping it all up</a></li>
  </ul>
</nav></div>
<h2 id="to-start-with">To start with</h2>
<p>I ended up needing the following things to make the pipeline work:</p>
<ul>
<li>
<p>The Linux/Unix command line (handily accessible via the Terminal application of Mac OS X).</p>
</li>
<li>
<p><a href="https://git-scm.com/">Git</a> client - not strictly necessary but it makes life easier to clone the Snowplow repo and work with it locally.</p>
</li>
<li>
<p>A new <a href="https://aws.amazon.com/">Amazon Web Services</a> account with the introductory free tier (first 12 months).</p>
</li>
<li>
<p>A credit card - even with the free tier the pipeline is not free.</p>
</li>
<li>
<p>A <strong>domain name</strong> of my own (I used gtmtools.com) whose DNS records I can modify.</p>
</li>
<li>
<p>A Google Analytics tag running through Google Tag Manager.</p>
</li>
<li>
<p>A lot of time.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/command-line.jpg" title="The OS X command line">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/command-line.jpg#ZgotmplZ" alt="The OS X command line" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1453 470'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>The bullets concerning money and custom domain name might be a turn-off to some.</p>
<p>You might be able to set up the pipeline without a domain name by using some combination of Amazon CloudFront and Route 53 with Amazon&rsquo;s own SSL certificates, but I didn&rsquo;t explore this option.</p>
<p>And yes, this whole thing is going to cost money. As I wrote in the beginning, I didn&rsquo;t follow the most cost-effective path. But even if I did, it would still cost a dollar or something per day to keep this up and running. If this is enough of a red flag for you, then take a look at what <a href="https://snowplowanalytics.com/products/snowplow-insights/">managed solutions</a> Snowplow is offering. This article is for the engineers out there who want to try building the whole thing from scratch.</p>
<h2 id="why-snowplow">Why Snowplow?</h2>
<p>Why do this exercise at all? Why even look towards Snowplow? The transition from the pre-built, top-down world of Google Analytics to the anarchy represented by Snowplow&rsquo;s agnostic approach to data processing can be daunting.</p>
<p>Let me be frank: Snowplow is not for everyone. Even though the company itself offers managed solutions, making it as turnkey as it can get, it&rsquo;s still <strong>you</strong> building an <strong>analytics pipeline</strong> to suit <strong>your</strong> organization&rsquo;s needs. This involves asking very difficult questions, such as:</p>
<ul>
<li>
<p>What is an &ldquo;event&rdquo;?</p>
</li>
<li>
<p>What constitutes a &ldquo;session&rdquo;?</p>
</li>
<li>
<p>Who owns the data?</p>
</li>
<li>
<p>What&rsquo;s the ROI of data analytics?</p>
</li>
<li>
<p>How should conversions be attributed?</p>
</li>
<li>
<p>How should I measure users across domains and devices?</p>
</li>
</ul>
<p>If you&rsquo;ve never asked one of these (or similar) questions before, you might not want to look at Snowplow or any other custom-built setup yet. These are questions that inevitably surface at some point when using tools that give you very few configuration options.</p>
<p>At this point I think I should add a disclaimer. This article is not <strong>Google Analytics versus Snowplow</strong>. There&rsquo;s no reason to bring one down to highlight the benefits of the other. Both GA and Snowplow have their place in the world of analytics, and having one is not predicated on the absence of the other.</p>
<p>The whole idea behind the Google Analytics plugin, for example, is that you can duplicate tracking to both GA and to Snowplow. You might want to reserve GA tracking for marketing and advertising attribution, as Google&rsquo;s backend integrations are still unmatched by other platforms. You can then run Snowplow to collect this same data so that you&rsquo;ll have access to an unsampled, raw, relational database you can use to enrich and join with your other data sets.</p>
<p><strong>Snowplow is NOT a Google Analytics killer</strong>. They&rsquo;re more like cousins fighting together for the honor of the same family line, but occasionally quarreling over the inheritance of a common, recently deceased relative.</p>
<h2 id="what-we-are-going-to-build">What we are going to build</h2>
<p>Here&rsquo;s a diagram of what we&rsquo;re hopefully going to build in this article:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/snowplow-process.jpg" title="Snowplow pipeline">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/snowplow-process.jpg#ZgotmplZ" alt="Snowplow pipeline" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1392 1029'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>I wonder why no one&rsquo;s hired me as a designer yet&hellip;</p>
<p>The process will cover the following steps.</p>
<ol>
<li>
<p>The website will duplicate the payloads sent to Google Analytics, and send them to a collector written with <a href="https://clojure.org/">Clojure</a>.</p>
</li>
<li>
<p>The collector runs as a web service on AWS <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">Elastic Beanstalk</a>, to which traffic is routed and secured with SSL from my custom domain name using AWS <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html">Route 53</a>.</p>
</li>
<li>
<p>The log data from the collector is stored in AWS <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html">S3</a>.</p>
</li>
<li>
<p>A utility is periodically executed on my local machine, which runs an ETL (<strong>e</strong>xtract, <strong>t</strong>ransform, <strong>l</strong>oad) process using AWS <a href="https://docs.aws.amazon.com/emr/latest/APIReference/Welcome.html">EMR</a> to enrich and &ldquo;shred&rdquo; the data in S3.</p>
</li>
<li>
<p>The same utility finally stores the processed data files into relational tables in AWS <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html">Redshift</a>.</p>
</li>
</ol>
<p>So the process ends with a relational database that has all your collected data populated periodically.</p>
<h2 id="step-0-register-on-aws-and-setup-iam-roles">Step 0: Register on AWS and setup IAM roles</h2>
<h3 id="what-you-need-for-this-step">What you need for this step</h3>
<ol>
<li>You&rsquo;ll just need a credit card to <a href="https://portal.aws.amazon.com/billing/signup#/start">register on AWS</a>. You&rsquo;ll get the benefits of a <a href="https://aws.amazon.com/free/">free tier</a>, but you&rsquo;ll still need to enable billing.</li>
</ol>
<h3 id="register-on-amr">Register on AMR</h3>
<p>The very first thing to do is register on Amazon Web Services and setup an IAM (Identity and Access Management) User that will run the whole setup.</p>
<p>So browse to <a href="https://aws.amazon.com/,">https://aws.amazon.com/,</a> and select the option to create a free account.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/create-free-account-aws.jpg" title="Create free AWS account">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/create-free-account-aws.jpg#ZgotmplZ" alt="Create free AWS account" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1466 388'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>The free account gives you access to the <a href="https://aws.amazon.com/free/">free tier</a> of services, some of which will help keep costs down for this pipeline, too.</p>
<h3 id="create-an-identity-and-account-management-iam-user">Create an Identity and Account Management (IAM) user</h3>
<p>Once you&rsquo;ve created the account, you can do the first important thing in setting up the pipeline: create an IAM User. We&rsquo;ll be following Snowplow&rsquo;s own excellent <a href="https://github.com/snowplow/snowplow/wiki/Setup-IAM-permissions-for-users-installing-Snowplow">IAM setup guide</a> for these steps.</p>
<p>In the <strong>Services</strong> menu, select IAM from the long list of items.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/services-menu.jpg" title="Services / IAM">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/services-menu.jpg#ZgotmplZ" alt="Services / IAM" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1232 337'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>In the left-hand menu, select <strong>Groups</strong>.</p>
</li>
<li>
<p>Click the <strong>Create New Group</strong> in the view that opens.</p>
</li>
<li>
<p>Name the group <code>snowplow-setup</code>.</p>
</li>
<li>
<p>Skip the Attach Policy step for now by clicking the <strong>Next Step</strong> button.</p>
</li>
<li>
<p>Click <strong>Create Group</strong>.</p>
</li>
</ul>
<p>Now in the left-hand menu, select <strong>Policies</strong>.</p>
<ul>
<li>
<p>Click <strong>Create Policy</strong>.</p>
</li>
<li>
<p>Select the <strong>JSON</strong> tab, and replace the contents with the following:</p>
</li>
</ul>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#1e90ff;font-weight:bold">&#34;Version&#34;</span>: <span style="color:#a50">&#34;2012-10-17&#34;</span>,
  <span style="color:#1e90ff;font-weight:bold">&#34;Statement&#34;</span>: [
    {
      <span style="color:#1e90ff;font-weight:bold">&#34;Effect&#34;</span>: <span style="color:#a50">&#34;Allow&#34;</span>,
      <span style="color:#1e90ff;font-weight:bold">&#34;Action&#34;</span>: [
        <span style="color:#a50">&#34;acm:*&#34;</span>,
        <span style="color:#a50">&#34;autoscaling:*&#34;</span>,
        <span style="color:#a50">&#34;aws-marketplace:ViewSubscriptions&#34;</span>,
        <span style="color:#a50">&#34;aws-marketplace:Subscribe&#34;</span>,
        <span style="color:#a50">&#34;aws-marketplace:Unsubscribe&#34;</span>,
        <span style="color:#a50">&#34;cloudformation:*&#34;</span>,
        <span style="color:#a50">&#34;cloudfront:*&#34;</span>,
        <span style="color:#a50">&#34;cloudwatch:*&#34;</span>,
        <span style="color:#a50">&#34;ec2:*&#34;</span>,
        <span style="color:#a50">&#34;elasticbeanstalk:*&#34;</span>,
        <span style="color:#a50">&#34;elasticloadbalancing:*&#34;</span>,
        <span style="color:#a50">&#34;elasticmapreduce:*&#34;</span>,
        <span style="color:#a50">&#34;es:*&#34;</span>,
        <span style="color:#a50">&#34;iam:*&#34;</span>,
        <span style="color:#a50">&#34;rds:*&#34;</span>,
        <span style="color:#a50">&#34;redshift:*&#34;</span>,
        <span style="color:#a50">&#34;s3:*&#34;</span>,
        <span style="color:#a50">&#34;sns:*&#34;</span>
      ],
      <span style="color:#1e90ff;font-weight:bold">&#34;Resource&#34;</span>: <span style="color:#a50">&#34;*&#34;</span>
    }
  ]
}</code></pre></div>
<ul>
<li>
<p>Next, click <strong>Review Policy</strong>.</p>
</li>
<li>
<p>Name the policy <code>snowplow-setup-policy-infrastructure</code>.</p>
</li>
<li>
<p>Finally, click <strong>Create Policy</strong>.</p>
</li>
</ul>
<p>Now go back to <strong>Groups</strong> from the left-hand menu, and click the <strong>snowplow-setup</strong> group name to open its settings.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/iam-group.jpg" title="IAM Group list">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/iam-group.jpg#ZgotmplZ" alt="IAM Group list" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 911 325'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Switch to the <strong>Permissions</strong> tab and click <strong>Attach Policy</strong>.</p>
</li>
<li>
<p>From the list that opens, select <strong>snowplow-setup-policy-infrastructure</strong> and click <strong>Attach Policy</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/attach-policy.jpg" title="Attach custom policy">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/attach-policy.jpg#ZgotmplZ" alt="Attach custom policy" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 855 395'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Now select <strong>Users</strong> from the left-hand menu, and click the <strong>Add user</strong> button.</p>
<ul>
<li>
<p>Name the user <code>snowplow-setup</code>.</p>
</li>
<li>
<p>Check the box next to <strong>Programmatic access</strong>.</p>
</li>
<li>
<p>Click <strong>Next: Permissions</strong>.</p>
</li>
<li>
<p>With <strong>Add user to group</strong> selected, check the box next to <strong>snowplow-setup</strong>, and click the <strong>Next: Review</strong> button at the bottom of the page.</p>
</li>
<li>
<p>Finally, click <strong>Create user</strong>.</p>
</li>
</ul>
<p>The following screen will show you a success message, and your user with an <strong>Access Key ID</strong> and <strong>Secret Access Key</strong> (click Show to see it) available. At this point, <strong>copy both of these somewhere safe</strong>. You will need them soon, and once you leave this screen you will not be able to see the secret key anymore. You can also download them as a CSV file by clicking the <strong>Download .csv</strong> button.</p>
<p>You have now created a user with which you will set up your entire pipeline. Once everything is set up, you will create a new user with fewer permissions, who will take care of running and managing the pipeline.</p>
<p>Congratulations, step 0 complete!</p>
<h3 id="what-you-should-have-after-this-step">What you should have after this step</h3>
<ol>
<li>
<p>You should have successfully registered a new account on AWS.</p>
</li>
<li>
<p>You should have a new IAM user named <code>snowplow-setup</code> with all the access privileges distributed by the custom policy you created.</p>
</li>
</ol>
<h2 id="step-1-the-clojure-collector">Step 1: The Clojure collector</h2>
<h3 id="what-you-need-for-this-step-1">What you need for this step</h3>
<ol>
<li>A custom domain name and access to its DNS configuration</li>
</ol>
<h3 id="getting-started">Getting started</h3>
<p>This is going to be one of the more difficult steps to do, since there&rsquo;s no generic guide for some of the things that need to be done with the collector endpoint. If you want, you can look into the <a href="https://github.com/snowplow/snowplow/wiki/Setting-up-the-Cloudfront-collector">CloudFront Collector</a> instead, because that runs directly on top of S3 without needing a web service to collect the hits. However, it doesn&rsquo;t support the Google Analytics hit duplicator, which is why in this article we&rsquo;ll use the <a href="https://github.com/snowplow/snowplow/wiki/Setting-up-the-Clojure-collector">Clojure collector</a>.</p>
<p>The Clojure collector is basically a web endpoint to which you will log the requests from your site. The endpoint is a scalable web service running on Apache Tomcat, which is hosted on AWS&rsquo; Elastic Beanstalk. The collector has been configured to automatically log the Tomcat access logs directly into AWS S3 storage, meaning whenever your site sends a request to the collector, it logs this request as an access log entry, ready for the ETL process that comes soon after.</p>
<p>But let&rsquo;s not get ahead of ourselves.</p>
<h3 id="setting-up-the-clojure-collector">Setting up the Clojure collector</h3>
<p>The first thing you&rsquo;ll need to do is download the Clojure collector WAR file and store it locally in a temporary location (such as your Downloads folder).</p>
<p>You can download the binary by following <a href="https://github.com/snowplow/snowplow/wiki/Hosted-assets#21-clojure-collector-resources">this</a> link. It should lead you to a file that looks something like <code>clojure-collector-1.X.X-standalone.war</code>.</p>
<p>Once you&rsquo;ve downloaded it, you can set up the <strong>Elastic Beanstalk</strong> application.</p>
<p>In AWS, open the <strong>Services</strong> menu again, and select <strong>Elastic Beanstalk</strong>.</p>
<p>At this point, you&rsquo;ll also want to make sure that any AWS services you use in this pipeline are located in the same region. There are differences to what services are supported in each region. I built the pipeline in <strong>EU (Ireland)</strong>, and the Snowplow guide itself uses <strong>US West (Oregon)</strong>. Click the region menu and choose the region you want to use.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/region-selection.jpg" title="Select AWS region">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/region-selection.jpg#ZgotmplZ" alt="Select AWS region" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 879 405'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Next, click the <strong>Create New Application</strong> link in the top-right corner of the page (just below the region selector).</p>
</li>
<li>
<p>Give the application a descriptive name (e.g. <code>Snowplow Clojure Collector</code>) and description (e.g. <code>I love Kamaka HF-3</code>), and then click <strong>Next</strong>.</p>
</li>
<li>
<p>In the New Environment view, select <strong>Create web server</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/beanstalk-new-web-server.jpg" title="New Elastic Beanstalk web server">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/beanstalk-new-web-server.jpg#ZgotmplZ" alt="New Elastic Beanstalk web server" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1270 481'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>In the Environment Type view, set the following:</li>
</ul>
<blockquote>
<p><strong>Predefined configuration</strong>: Tomcat<br>
<strong>Environment type</strong>: Single instance</p>
</blockquote>
<ul>
<li>
<p>Then, click <strong>Next</strong>.</p>
</li>
<li>
<p>In the Application Version view, select <strong>Upload your own</strong>, click <strong>Choose file</strong>, and find the WAR file you downloaded earlier in this chapter. Click <strong>Next</strong> to upload the file.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/upload-war-file.jpg" title="Upload WAR file">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/upload-war-file.jpg#ZgotmplZ" alt="Upload WAR file" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 838 315'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>In the Environment Info view, you&rsquo;ll need to set the <strong>Environment name</strong>, which is then used to generate the <strong>Environment URL</strong> (which, in turn, will be the endpoint URL receiving the collector requests).</p>
</li>
<li>
<p>Remember to click <strong>Check availability</strong> for the URL to make sure someone hasn&rsquo;t grabbed it yet. Click <strong>Next</strong> once you&rsquo;re done.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/environment-name-and-url.jpg" title="Environment name and URL">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/environment-name-and-url.jpg#ZgotmplZ" alt="Environment name and URL" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 804 329'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>In Additional Resources, you can leave both options unchecked for now, and click <strong>Next</strong>.</p>
</li>
<li>
<p>In Configuration Details, select <strong>m1.small</strong> as the instance type. You can leave all the other options to their default settings. Click <strong>Next</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/configuration-details.jpg" title="Configuration details">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/configuration-details.jpg#ZgotmplZ" alt="Configuration details" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 977 381'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>No need to add any Environment Tags, so click <strong>Next</strong> again.</p>
</li>
<li>
<p>In the Permissions view, by clicking <strong>Next</strong>, AWS assigns default roles to Instance profile and Service role, so that&rsquo;s fine.</p>
</li>
<li>
<p>Finally, you can take a quick look at what you&rsquo;ve done in the Review view, before clicking the <strong>Launch</strong> button.</p>
</li>
</ul>
<p>At this point, you&rsquo;ll see that AWS is firing up your environment, where the Clojure collector WAR file will start running the instant the environment has been created.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/clojure-collector-starting-up.jpg" title="Clojure collector starting up">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/clojure-collector-starting-up.jpg#ZgotmplZ" alt="Clojure collector starting up" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1709 614'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Once the environment is up and running, you can copy the URL from the top of the view, paste it into the address bar of your browser, and add the path <code>/i</code> to its end, so it ends up something like:</p>
<p><code>http://simoahava-snowplow-collector.eu-west-1.elasticbeanstalk.com/i</code></p>
<p>If the collector is running correctly, you should see a pixel in the center of the screen. By clicking the right mouse button and choosing Inspect (in the Google Chrome browser), you should now find a cookie named <code>sp</code> in the Applications tab. If you do, it means the collector is working correctly.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/clojure-test.jpg" title="Testing the clojure collector">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/clojure-test.jpg#ZgotmplZ" alt="Testing the clojure collector" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1335 891'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Congratulations! You&rsquo;ve set up the collector.</p>
<p>However, we&rsquo;re not done here yet.</p>
<h3 id="enable-logging-to-s3">Enable logging to S3</h3>
<p>Automatically logging the Tomcat access logs to S3 storage is absolutely crucial for this whole pipeline. The batch process looks for these logs when sorting out the data into queriable chunks. Access logs are your typical web server logs, detailing all the HTTP requests made to the endpoint, with request headers and payloads included.</p>
<p>To enable logging, you&rsquo;ll need to edit the Elastic Beanstalk application you just created. So, once the endpoint is up and running, you can open it by clicking the application name while in the <strong>Elastic Beanstalk</strong> service front page.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/clojure-collector-application.jpg" title="Clojure collector application">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/clojure-collector-application.jpg#ZgotmplZ" alt="Clojure collector application" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1158 658'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Next, select <strong>Configuration</strong> from the left-hand menu.</p>
</li>
<li>
<p>Click the cogwheel in the box titled <strong>Software Configuration</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/software-configuration.jpg" title="Software configuration">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/software-configuration.jpg#ZgotmplZ" alt="Software configuration" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 838 451'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>Under <strong>Log Options</strong>, check the box next to <strong>Enable log file rotation to Amazon S3. If checked, service logs are published to S3</strong>.</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/enable-log-rotation.jpg" title="Enable log rotation">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/enable-log-rotation.jpg#ZgotmplZ" alt="Enable log rotation" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 921 235'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>This is a crucial step, because it will store all the access logs from your endpoint requests to S3, ready for ETL.</p>
<p>Click <strong>Apply</strong> to apply the change.</p>
<h3 id="set-up-the-load-balancer">Set up the load balancer</h3>
<p>Next, we need to configure the Elastic Beanstalk environment for SSL.</p>
<p>Before you do anything else, you’ll need to switch from a single instance to a load-balancing, auto-scaling environment in Elastic Beanstalk. This is necessary for securing the traffic between your domain name and the Clojure collector.</p>
<p>In the AWS <strong>Services</strong> menu, select <strong>Elastic Beanstalk</strong>, and then click your application name in the view that opens.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/clojure-collector-application.jpg" title="Clojure collector application">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/clojure-collector-application.jpg#ZgotmplZ" alt="Clojure collector application" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1158 658'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>In the next view, select <strong>Configuration</strong> in the left-hand menu.</p>
</li>
<li>
<p>Click the cogwheel in the box titled <strong>Scaling</strong>.</p>
</li>
<li>
<p>From the <strong>Environment type</strong> menu, select <strong>Load balancing, auto scaling</strong>, and then click the <strong>Apply</strong> button, and <strong>Save</strong> in the next view.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/change-environment-type.jpg" title="Change environment type">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/change-environment-type.jpg#ZgotmplZ" alt="Change environment type" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1404 380'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>You now have set up the load balancer.</p>
<h3 id="route-traffic-from-your-custom-domain-name-to-the-load-balancer">Route traffic from your custom domain name to the load balancer</h3>
<p>Next, we&rsquo;ll get started on routing traffic from your custom domain name to this load balancer.</p>
<ul>
<li>
<p>Open the <strong>Services</strong> menu in the AWS console, and select <strong>Route 53</strong> from the list.</p>
</li>
<li>
<p>In the view that opens, click <strong>Create Hosted Zone</strong>.</p>
</li>
<li>
<p>Set the <strong>Domain Name</strong> to the domain whose DNS records you want to delegate to Amazon. I chose <strong>collector.gtmtools.com</strong>. Leave <strong>Type</strong> as <strong>Public Hosted Zone</strong>, and click <strong>Create</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/route-53-hosted-zone.jpg" title="Configure hosted zone in Route 53">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/route-53-hosted-zone.jpg#ZgotmplZ" alt="Configure hosted zone in Route 53" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1050 412'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>In the view that opens, you&rsquo;ll see the settings for your Hosted Zone. Make note of the four NS records AWS has assigned to your domain name. You&rsquo;ll need these in the next step.</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/hosted-zone-name-servers.jpg" title="Hosted Zone name servers">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/hosted-zone-name-servers.jpg#ZgotmplZ" alt="Hosted Zone name servers" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1271 297'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Next, you&rsquo;ll need to go wherever it is you manage your DNS records. I use <a href="https://www.godaddy.com/">GoDaddy</a>.</p>
</li>
<li>
<p>You need to add the four NS addresses in the AWS Hosted Zone as <strong>NS</strong> records in the DNS settings of your domain name. This is what the modifications would look like in my GoDaddy control panel:</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/godaddy-ns-records.jpg" title="GoDaddy NS records">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/godaddy-ns-records.jpg#ZgotmplZ" alt="GoDaddy NS records" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1170 344'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>As you can see, there are four <strong>NS</strong> records with host <strong>collector</strong> (for collector.gtmtools.com), each pointing to one of the four corresponding NS addresses in the AWS Hosted Zone. I set the TTL to the shortest possible GoDaddy allows, which is 600 seconds. That means that within 10 minutes, the Hosted Zone name servers should respond to <strong>collector.gtmtools.com</strong>.</p>
<p>You can test this with a service such as <a href="https://dig.whois.com.au/dig/,">https://dig.whois.com.au/dig/,</a> or any similar service that lets you check DNS records. Once the DNS settings are updated, you can increase the TTL to something more sensible, such as 1 hour, or even 1 day.</p>
<p>Now that you&rsquo;ve set up your custom domain name to point to your Route 53 Hosted Zone, there&rsquo;s just one step missing. You&rsquo;ll need to create an <strong>Alias</strong> record in the Hosted Zone, which points to your load balancer. That way when typing the URL <strong>collector.gtmtools.com</strong> into the browser address bar, the DNS record first directs it to your Hosted Zone, where a new A record shuffles the traffic to your load balanced Clojure collector endpoint. Phew!</p>
<ul>
<li>
<p>So, in the Hosted Zone you&rsquo;ve created, click <strong>Create Record Set</strong>.</p>
</li>
<li>
<p>In the overlay that opens, leave the <strong>Name</strong> empty, since you want to apply the name to the root domain of the NS (collector.gtmtools.com in my case). Keep <strong>A - IPv4 address</strong> as the Type, and select <strong>Yes</strong> for <strong>Alias</strong>.</p>
</li>
<li>
<p>When you click the <strong>Target</strong> field, a drop-down list should appear, and your load balancer should be selectable under the <strong>ELB Classic load balancers</strong> heading. Select that, and then click <strong>Create</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/a-record-set.jpg" title="A Record Set">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/a-record-set.jpg#ZgotmplZ" alt="A Record Set" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 418 547'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Now if you visit <code>http://collector.gtmtools.com/i</code>, you should see the same pixel response as you got when visiting the Clojure collector endpoint directly. So your domain name routing works!</p>
<p>But we&rsquo;re STILL not done here.</p>
<h3 id="setting-up-https-for-the-collector">Setting up HTTPS for the collector</h3>
<p>To make sure the collector is secured with HTTPS, you will need to generate a (free) AWS SSL certificate for it, and apply it to the Load Balancer. Luckily this is easy to do now that we&rsquo;re working with Route 53.</p>
<ul>
<li>
<p>The first thing to do is generate the SSL certificate. In <strong>Services</strong>, find and select the <strong>AWS Certificate Manager</strong>. Click <strong>Get started</strong> to, well, get started.</p>
</li>
<li>
<p>Type the domain name you want to apply the certificate to in the relevant field. I wrote <strong>collector.gtmtools.com</strong>.</p>
</li>
<li>
<p>Click <strong>Next</strong> when ready.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/apply-certificate.jpg" title="Apply AWS certificate">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/apply-certificate.jpg#ZgotmplZ" alt="Apply AWS certificate" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1351 609'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>In the next step you need to choose a validation method. Since we&rsquo;ve delegated DNS of collector.gtmtools.com to Route 53, I chose <strong>DNS Validation</strong> without hesitation.</p>
</li>
<li>
<p>Then click <strong>Review</strong>, and then <strong>Confirm and request</strong>.</p>
</li>
<li>
<p>Validation is done in the next view. With Route 53, this is really easy. Just click <strong>Create record in Route 53</strong>, and <strong>Create</strong> in the overlay that opens. Amazon takes care of validation for you!</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/validate-certificate.jpg" title="Validate SSL certificate">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/validate-certificate.jpg#ZgotmplZ" alt="Validate SSL certificate" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1781 552'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>After clicking Create, you should see a <strong>Success</strong> message, and you can click the <strong>Continue</strong> button in the bottom of the screen. It might take up to 30 minutes for the certificate to validate, so go grab a cup of coffee or something! We still have one more step left&hellip;</p>
<h3 id="switch-the-load-balancer-to-support-https">Switch the load balancer to support HTTPS</h3>
<p>You&rsquo;ll still need to switch your load balancer to listen for secure requests, too.</p>
<ul>
<li>
<p>In <strong>Services</strong>, open <strong>Elastic Beanstalk</strong>, click your application name in the view that opens, and finally click <strong>Configuration</strong> in the left-hand menu. You should be able to do this stuff in your sleep by now!</p>
</li>
<li>
<p>Next, scroll down to the box titled <strong>Load Balancing</strong> and click the cogwheel in it.</p>
</li>
<li>
<p>In the view that opens, set <strong>Secure listener port</strong> to <strong>443</strong>, and select the SSL certificate you just generated from the menu next to <strong>SSL Certificate ID</strong>. Click <strong>Apply</strong> when ready.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/load-balancer-settings.jpg" title="HTTPS for load balancer">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/load-balancer-settings.jpg#ZgotmplZ" alt="HTTPS for load balancer" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1035 455'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<h3 id="all-done">All done!</h3>
<p>At this point, you might also want to take a look at <a href="https://github.com/snowplow/snowplow/wiki/Configuring-the-Clojure-collector">this Snowplow guide</a> for configuring the collector further (e.g. applying proper scaling settings to the load balancer).</p>
<p>The process above might seem convoluted, but there&rsquo;s a certain practical logic to it all. And once you have the whole pipeline up and running, it will be easier to understand how things proceed from the S3 storage onwards.</p>
<p>Setting up the custom domain name is a bit of chore, though. But if you use Route 53, most of the things are either automated for you or taken care of with the click of a button.</p>
<h3 id="what-you-should-have-after-this-step-1">What you should have after this step</h3>
<ol>
<li>
<p>The Clojure collector application running in an Elastic Beanstalk environment.</p>
</li>
<li>
<p>Your own custom domain name pointing at the router configured in Amazon Route 53.</p>
</li>
<li>
<p>An SSL secured load balancer, to which Route 53 diverts traffic to your custom domain name.</p>
</li>
<li>
<p>Automatic logging of the collector Tomcat logs to S3. The bucket name is something like <strong>elasticbeanstalk-region-id</strong>, and you can find it by clicking <strong>Services</strong> in the AWS navigation and choosing <strong>S3</strong>. The logs are pushed hourly.</p>
</li>
</ol>
<h2 id="step-2-the-tracker">Step 2: The tracker</h2>
<p>You&rsquo;ll need to configure a tracker to collect data to the S3 storage.</p>
<p>This is really easy, since you&rsquo;re of course using Google Analytics, and tracking data to it using Google Tag Manager tags.</p>
<p>Navigate to my <a href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">recent guide</a> on setting up the duplicator, do what it says, and you&rsquo;ll be set. Remember to change the <code>endpoint</code> variable in the Custom JavaScript variable to the domain name you set up in the previous chapter (<code>https://collector.gtmtools.com/</code> in my case).</p>
<h2 id="step-25-test-the-tracker-and-collector">Step 2.5: Test the tracker and collector</h2>
<p>Once you&rsquo;ve installed the GA duplicator, you can test to see if the logs are being stored in S3 properly.</p>
<p>If the duplicator is doing its job correctly, you can open the <strong>Network</strong> tab in your browser&rsquo;s developer tools, and look for requests to your collector endpoint. You should see POST requests for each GA call, with the payload of the request in the POST body:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/network-developer-tools.jpg" title="Network tab in developer tools">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/network-developer-tools.jpg#ZgotmplZ" alt="Network tab in developer tools" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1476 383'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>If you don&rsquo;t see these requests, it means you&rsquo;ve misconfigured the duplicator somehow, and you should re-read the <a href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">instructions</a>.</p>
<p>If you see the requests but there&rsquo;s an error in the HTTP response, you&rsquo;ll need to check the process outlined in the previous two chapters again.</p>
<p>At something like 10 minutes past each hour, the Clojure collector running in Elastic Beanstalk will dump all the Tomcat access logs to S3. You should check that they are being stored, because the whole batch process hinges on the presence of these logs.</p>
<p>In S3, there will be a bucket prefixed with <strong>elasticbeanstalk-region-id</strong>. Within that bucket, browse to folder <strong>resources / environments / logs / publish / (some ID) / (some ID)</strong>. In other words, within the <strong>publish</strong> folder will be a folder named something like <strong>e-ab12cd23ef</strong>, and within that will be a folder named something like <strong>i-1234567890</strong>. Within that final folder will be all your logs in gzip format.</p>
<p>Look for the ones named <strong>_var_log_tomcat8_rotated_localhost_access_log.txt123456789.gz</strong>, as these are the logs that the ETL process will use to build the data tables.</p>
<p>If you open one of those logs, you should find a bunch of GET and POST entries. Look for POST entries where the endpoint is <code>/com.google.analytics/v1</code>, and the HTTP status code is <code>200</code>. If you see these, it means that the Clojure collector is almost certainly doing its job. The entry will contain a bunch of interesting information, such as the IP address of the visitor, the User-Agent string of the browser, and a <em>base64</em> encoded string with the payload. If you decode this string, you should see the full payload of your Google Analytics hit as a query string.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/post-request-from-logs.jpg" title="POST request from Tomcat access logs">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/post-request-from-logs.jpg#ZgotmplZ" alt="POST request from Tomcat access logs" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 827 205'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<h2 id="step-3-configure-the-etl-process">Step 3: Configure the ETL process</h2>
<h3 id="what-you-need-for-this-step-2">What you need for this step</h3>
<ol>
<li>
<p>A collector running and dumping the logs in an S3 bucket.</p>
</li>
<li>
<p>Access Key ID and Secret Access Key for the IAM user you created in <a href="#step-0-register-on-aws-and-setup-iam-roles">Step 0</a>.</p>
</li>
</ol>
<h3 id="getting-started-1">Getting started</h3>
<p>This step <strong>should</strong> be pretty straightforward, at least more so than the previous one.</p>
<p>The process does a number of things, and you&rsquo;ll want to check out <a href="https://github.com/snowplow/snowplow/wiki/Batch-pipeline-steps">this page</a> for more info.</p>
<p>But, in short, here are the main steps operated by the AWS Elastic MapReduce (EMR) service.</p>
<ol>
<li>
<p>The Tomcat logs are cleaned up so that they can be parsed more easily. Irrelevant log entries are discarded.</p>
</li>
<li>
<p>Custom enrichments are applied to the data, if you so wish. Enrichments include things like geolocation from IP addresses, or adding <a href="https://www.simoahava.com/analytics/send-weather-data-to-google-analytics-in-gtm-v2/">weather</a> information to the data set.</p>
</li>
<li>
<p>The enriched data is then <em>shredded</em>, or split into more atomic data sets, each corresponding with a hit that validates against a given schema. For example, if you are collecting data with the Google Analytics setup outlined in my <a href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">guide</a>, these hits would be automatically shredded into data sets for Page Views, Events, and other Google Analytics tables, ready for transportation to a relational database.</p>
</li>
<li>
<p>Finally, the data is copied from the shredded sets to a database created in Amazon Redshift.</p>
</li>
</ol>
<p>We&rsquo;ll go over these steps in detail next. It&rsquo;s important to understand that each step of the ETL process leaves a trace in S3 buckets you&rsquo;ll build along the way. This means that even if you choose to apply the current process to your raw logs, you can rerun your entire log history, if you&rsquo;ve decided to keep the files, with new enrichments and shredding schemas later on. All the Tomcat logs are archived, too, so you&rsquo;ll always be able to start the entire process from scratch, using all your historical data, if you wish!</p>
<p>The way we&rsquo;ll work in this process is run a Java application name <strong>EmrEtlRunner</strong> from your local machine. This application initiates and runs the ETL process using Amazon&rsquo;s Elastic MapReduce. At some point, you might want to upgrade your setup, and have <strong>EmrEtlRunner</strong> execute in an AWS EC2 instance (basically a virtual machine in Amazon&rsquo;s cloud). That way you could schedule it to run, say, every 60 minutes, and then forget about it.</p>
<h3 id="download-the-necessary-files">Download the necessary files</h3>
<p>The ETL runner is a Unix application you can download from <a href="http://dl.bintray.com/snowplow/snowplow-generic/">this</a> link. To grab the latest version, look for the file that begins with <code>snowplow_emr_rXX</code>, where XX is the highest number you can find. At the time of writing, the latest binary is <code>snowplow_emr_r97_knossos.zip</code>.</p>
<ul>
<li>
<p>Download this ZIP file, and copy the <code>snowplow-emr-etl-runner</code> Unix executable into a new folder on your hard drive. This folder will be your base of operations.</p>
</li>
<li>
<p>At this point, you&rsquo;ll want to also clone the <a href="https://github.com/snowplow/snowplow">Snowplow Github repo</a> in that folder, because it has all the config file templates and SQL files you&rsquo;ll need later on.</p>
</li>
<li>
<p>So browse to the directory to where you copied the <code>snowplow-emr-etl-runner</code> file, and run the following command:</p>
</li>
</ul>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/snowplow/snowplow.git</code></pre></div>
<p>If you don&rsquo;t have Git installed, now would be a good time to <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">do it</a>.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/git-clone.jpg" title="Git clone the Snowplow repo">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/git-clone.jpg#ZgotmplZ" alt="Git clone the Snowplow repo" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1015 217'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Now you should have both the <code>snowplow-emr-etl-runner</code> file and the <strong>snowplow</strong> folder in the same directory.</p>
</li>
<li>
<p>Next, create a new folder named <code>config</code>, and in that, a new folder named <code>targets</code>.</p>
</li>
<li>
<p>Then, perform the following copy operations:</p>
<ol>
<li>
<p>Copy <code>snowplow/3-enrich/emr-etl-runner/config/config.yml.sample</code> to <code>config/config.yml</code>.</p>
</li>
<li>
<p>Copy <code>snowplow/3-enrich/config/iglu_resolver.json</code> to <code>config/iglu_resolver.json</code>.</p>
</li>
<li>
<p>Copy <code>snowplow/4-storage/config/targets/redshift.json</code> to <code>config/targets/redshift.json</code>.</p>
</li>
</ol>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/copy-config-files.jpg" title="Copy config files">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/copy-config-files.jpg#ZgotmplZ" alt="Copy config files" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 617 104'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>In the end, you should end up with a folder and file structure like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">.
|-- snowplow-emr-etl-runner
|-- snowplow
|   |-- -SNOWPLOW GIT REPO HERE-
|-- config
|   |-- iglu_resolver.json
|   |-- config.yml
|   |-- targets
|       |-- redshift.json </code></pre></div>
<h3 id="create-an-ec2-key-pair">Create an EC2 key pair</h3>
<p>At this point, you&rsquo;ll also need to create a private key pair in Amazon EC2. The ETL process will run on virtual machines in the Amazon cloud, and these machines are powered by Amazon EC2. For the runner to have full privileges to create and manage these machines, you will need to provide it with access control rights, and that&rsquo;s what the key pair is for.</p>
<ul>
<li>
<p>In AWS, select <strong>Services</strong> from the top navigation, and click on <strong>EC2</strong>. In the left-hand menu, browse down to <strong>Key Pairs</strong>, and click the link.</p>
</li>
<li>
<p>At this point, make sure you are in the Region where you&rsquo;ll be running all the proceeding jobs. For consistency&rsquo;s sake, it makes sense to stay in the same Region you&rsquo;ve been in all along. Remember, you can choose the Region from the top navigation.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/region-selection.jpg" title="Region selection">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/region-selection.jpg#ZgotmplZ" alt="Region selection" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 879 405'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Once you&rsquo;ve made sure you&rsquo;re in the correct Region, click <strong>Create Key Pair</strong>.</p>
</li>
<li>
<p>Give the key pair a name you&rsquo;ll remember. My key pair is named <code>simoahava</code>.</p>
</li>
<li>
<p>Once you&rsquo;re done, you&rsquo;ll see your new key pair in the list, and the browser automatically downloads the file <code>&lt;key pair name&gt;.pem</code> to your computer.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/create-key-pair.jpg" title="Create EC2 key pair">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/create-key-pair.jpg#ZgotmplZ" alt="Create EC2 key pair" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 647 194'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<h3 id="create-the-s3-buckets">Create the S3 buckets</h3>
<p>At this time, you&rsquo;ll need to create a bunch of buckets (storage locations) in Amazon S3. These will be used by the batch process to manage all your data files through various stages of the ETL process.</p>
<p>You will need buckets for the following:</p>
<ul>
<li>
<p><code>:raw:in</code> - you already have this, actually. It&rsquo;s the <strong>elasticbeanstalk-region-id</strong> created by the Clojure collector running in Elastic Beanstalk.</p>
</li>
<li>
<p><code>:processing</code> - intermediate bucket for the log files before they are enriched.</p>
</li>
<li>
<p><code>:archive</code> - you&rsquo;ll need three different archive buckets: <code>:raw</code> (for the raw log files), <code>:enriched</code> (for the enriched files), <code>:shredded</code> (for the shredded files).</p>
</li>
<li>
<p><code>:enriched</code> - you&rsquo;ll need two buckets for enriched data: <code>:good</code> (for data sets successfully enriched), <code>:bad</code> (for those that failed enrich).</p>
</li>
<li>
<p><code>:shredded</code> - you&rsquo;ll likewise need two buckets for shredded data: <code>:good</code> (for data sets successfully shredded), <code>:bad</code> (for those that failed shredding).</p>
</li>
<li>
<p><code>:log</code> - a bucket for logs produced by the ETL process.</p>
</li>
</ul>
<p>To create these buckets, head on over to S3 by clicking <strong>Services</strong> in the AWS top navigation, and choosing <strong>S3</strong>.</p>
<p>You should already have your <code>:raw:in</code> bucket here, it&rsquo;s the one whose name starts with <strong>elasticbeanstalk-</strong>.</p>
<p>Let&rsquo;s start with creating a new bucket that will contain all the &ldquo;sub-buckets&rdquo; for ETL.</p>
<p>Click <strong>+Create bucket</strong>, and name the bucket something like <strong>simoahava-snowplow-data</strong>. The bucket name must be unique across all of S3, so you can&rsquo;t just name it <strong>snowplow</strong>. Click <strong>Next</strong> a couple of times and then finally <strong>Create bucket</strong> to create this root bucket.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/create-root-bucket.jpg" title="create root bucket in S3">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/create-root-bucket.jpg#ZgotmplZ" alt="create root bucket in S3" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 722 514'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Now click the new bucket name to open the bucket. You should see a screen like this:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/empty-bucket.jpg" title="Empty S3 bucket">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/empty-bucket.jpg#ZgotmplZ" alt="Empty S3 bucket" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1807 754'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Click <strong>+ Create folder</strong>, and create the following three folders into this empty bucket:</p>
<ol>
<li>
<p><strong>archive</strong></p>
</li>
<li>
<p><strong>enriched</strong></p>
</li>
<li>
<p><strong>shredded</strong></p>
</li>
</ol>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/s3-create-folder.jpg" title="Create folder in S3">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/s3-create-folder.jpg#ZgotmplZ" alt="Create folder in S3" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1033 447'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Then, in <strong>archive</strong>, create the following three folders:</p>
<ol>
<li>
<p><strong>raw</strong></p>
</li>
<li>
<p><strong>enriched</strong></p>
</li>
<li>
<p><strong>shredded</strong></p>
</li>
</ol>
<p>Next, in both <strong>enriched</strong> and <strong>shredded</strong>, create the following two folders:</p>
<ol>
<li>
<p><strong>good</strong></p>
</li>
<li>
<p><strong>bad</strong></p>
</li>
</ol>
<p>Thus, you should end up with a bucket that has the following structure:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">.
|-- elasticbeanstalk-region-id
|-- simoahava-snowplow-data
|   |-- archive
|   |   |-- raw
|   |   |-- enriched
|   |   |-- shredded
|   |-- encriched
|   |   |-- good
|   |   |-- bad
|   |-- shredded
|   |   |-- good
|   |   |-- bad</code></pre></div>
<p>Finally, create one more bucket in the root of S3 named something like <strong>simoahava-snowplow-log</strong>. You&rsquo;ll use this for the logs produced by the batch process.</p>
<h3 id="prepare-for-configuring-the-emretlrunner">Prepare for configuring the EmrEtlRunner</h3>
<p>Now you&rsquo;ll need to configure the EmrEtlRunner. You&rsquo;ll use the <code>config.yml</code> file you copied from the Snowplow repo to the <code>config/</code> folder. For the config, you&rsquo;ll need the following things:</p>
<ol>
<li>
<p>The Access Key ID and Secret Access Key for the <code>snowplow-setup</code> user you created all the way back in <a href="#step-0-register-on-aws-and-setup-iam-roles">Step 0</a>. If you didn&rsquo;t save these, you can generate a new Access Key through AWS <strong>IAM</strong>.</p>
</li>
<li>
<p>You will need to download and install the <a href="https://aws.amazon.com/cli/">AWS Command Line Interface</a>. You can use the <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-install-macos.html">official guide</a> to install it with Python/pip, but if you&rsquo;re running Mac OS X, I recommend using <a href="https://brew.sh/">Homebrew</a> instead. Once you&rsquo;ve installed Homebrew, you just need to run <code>brew install awscli</code> to install the AWS client.</p>
</li>
</ol>
<p>Once you&rsquo;ve installed <code>awscli</code>, you need to run <code>aws configure</code> in your terminal, and do what it instructs you to do. You&rsquo;ll need your <strong>Access Key ID</strong> and <strong>Secret Access Key</strong> handy, as well as the region name (e.g. <code>eu-west-1</code>) where you&rsquo;ll be running your EC2 (again, I recommend to use the same region for all parts of this pipeline process).</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ aws configure
AWS Access Key ID: &lt;enter your IAM user Access Key ID here&gt;
AWS Secret Access Key: &lt;enter you IAM user Secret Access Key here&gt;
Default region name: &lt;enter the region name, e.g. eu-west-1 here&gt;
Default output format: &lt;just press enter&gt;</code></pre></div>
<p>This is what it looked like when I ran <code>aws configure</code>.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/aws-configure.jpg" title="aws configure">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/aws-configure.jpg#ZgotmplZ" alt="aws configure" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 548 142'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>After running <code>aws configure</code>, the next command you&rsquo;ll need to run is <code>aws emr create-default-roles</code>. This will create default roles for the EmrEtlRunner, so that it can perform the necessary tasks in EC2 for you.</p>
<p>Once you&rsquo;ve done these steps (remember to still keep your Access Key ID and Secret Access Key close by), you&rsquo;re ready to configure EmrEtlRunner!</p>
<h3 id="configure-emretlrunner">Configure EmrEtlRunner</h3>
<p><strong>EmrEtlRunner</strong> is the name of the utility you downloaded earlier, with the filename <code>snowplow-emr-etl-runner</code>.</p>
<p>EmrEtlRunner does a LOT of things. See <a href="https://github.com/snowplow/snowplow/wiki/Batch-pipeline-steps#dataflow-diagram">this diagram</a> to see an overview of the process. At this point, we&rsquo;ll do all the steps except for step 13, <strong>rdb_load</strong>. That&rsquo;s the step where the enriched and shredded data are copied into a relational database. We&rsquo;ll take care of that in the next step.</p>
<p>Anyway, EmrEtlRunner is operated by <code>config.yml</code>, which you&rsquo;ve copied into the <code>config/</code> directory. I&rsquo;ll show you the config I use, and highlight the parts you&rsquo;ll need to change.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">aws:
  access_key_id: AKIAIBAWU2NAYME55123
  secret_access_key: iEmruXM7dSbOemQy63FhRjzhSboisP5TcJlj9123
  s3:
    region: eu-west-1
    buckets:
      assets: s3://snowplow-hosted-assets
      jsonpath_assets:
      log: s3://simoahava-snowplow-log
      raw:
        in:
          - s3://elasticbeanstalk-eu-west-1-375284143851/resources/environments/logs/publish/e-f4pdn8dtsg
        processing: s3://simoahava-snowplow-data/processing
        archive: s3://simoahava-snowplow-data/archive/raw
      enriched:
        good: s3://simoahava-snowplow-data/enriched/good
        bad: s3://simoahava-snowplow-data/enriched/bad
        errors: 
        archive: s3://simoahava-snowplow-data/archive/enriched
      shredded:
        good: s3://simoahava-snowplow-data/shredded/good
        bad: s3://simoahava-snowplow-data/shredded/bad
        errors:
        archive: s3://simoahava-snowplow-data/archive/shredded
  emr:
    ami_version: 5.9.0
    region: eu-west-1
    jobflow_role: EMR_EC2_DefaultRole
    service_role: EMR_DefaultRole
    placement:
    ec2_subnet_id: subnet-d6e91a9e
    ec2_key_name: simoahava
    bootstrap: []
    software:
      hbase:
      lingual:
    jobflow:
      job_name: Snowplow ETL
      master_instance_type: m1.medium
      core_instance_count: 2
      core_instance_type: m1.medium
      core_instance_ebs:
        volume_size: 100
        volume_type: &#34;gp2&#34;
        volume_iops: 400
        ebs_optimized: false
      task_instance_count: 0
      task_instance_type: m1.medium
      task_instance_bid: 0.015
    bootstrap_failure_tries: 3
    configuration:
      yarn-site:
        yarn.resourcemanager.am.max-attempts: &#34;1&#34;
      spark:
        maximizeResourceAllocation: &#34;true&#34;
    additional_info:
collectors:
  format: clj-tomcat
enrich:
  versions:
    spark_enrich: 1.12.0
  continue_on_unexpected_error: false
  output_compression: NONE
storage:
  versions:
    rdb_loader: 0.14.0
    rdb_shredder: 0.13.0
    hadoop_elasticsearch: 0.1.0
monitoring:
  tags: {}
  logging:
    level: DEBUG</code></pre></div>
<p>The keys you need to edit are listed next, with a comment on how to edit them. All the keys not listed below you can leave with their default values. I really recommend you read through the <a href="https://github.com/snowplow/snowplow/wiki/Common-configuration">configuration documentation</a> for ideas on how to modify the rest of the keys to make your setup more powerful.</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>:aws:access_key_id</code></td>
<td>Copy-paste the Access Key ID of your IAM user here.</td>
</tr>
<tr>
<td><code>:aws:secret_access_key</code></td>
<td>Copy-paste the Secret Access Key of your IAM user here.</td>
</tr>
<tr>
<td><code>:aws:s3:region</code></td>
<td>Set this to the region where your S3 buckets are located in.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:log</code></td>
<td>Change this to the name of the S3 bucket you created for the ETL logs.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:raw:in</code></td>
<td>This is the bucket where the Tomcat logs are automatically pushed to. Do <strong>not</strong> include the last folder in the path, because this might change with an auto-scaling environment. <strong>Note!</strong> Keep the hyphen in the beginning of the line as in the config file example!</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:raw:processing</code></td>
<td>Set this to the respective processing bucket.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:raw:archive</code></td>
<td>Set this to the archive bucket for raw data.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:enriched:good</code></td>
<td>Set this to the enriched/good bucket.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:enriched:bad</code></td>
<td>Set this to the enriched/bad bucket.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:enriched:errors</code></td>
<td>Leave this empty.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:enriched:archive</code></td>
<td>Set this to the archive bucket for enriched data.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:shredded:good</code></td>
<td>Set this to the shredded/good bucket.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:shredded:bad</code></td>
<td>Set this to the shredded/bad bucket.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:shredded:errors</code></td>
<td>Leave this empty.</td>
</tr>
<tr>
<td><code>:aws:s3:buckets:shredded:archive</code></td>
<td>Set this to the archive bucket for shredded data.</td>
</tr>
<tr>
<td><code>:aws:emr:region</code></td>
<td>This should be the region where the EC2 job will run.</td>
</tr>
<tr>
<td><code>:aws:emr:placement</code></td>
<td>Leave this empty.</td>
</tr>
<tr>
<td><code>:aws:emr:ec2_subnet_id</code></td>
<td>The subnet ID of the Virtual Private Cloud the job will run in. You can use the same subnet ID used by the EC2 instance running your collector.</td>
</tr>
<tr>
<td><code>:aws:emr:ec2_key_name</code></td>
<td>The name of the EC2 Key Pair you created earlier.</td>
</tr>
<tr>
<td><code>:collectors:format</code></td>
<td>Set this to <strong>clj-tomcat</strong>.</td>
</tr>
<tr>
<td><code>:monitoring:snowplow</code></td>
<td>Remove this key and all its children (<code>:method</code>, <code>:app_id</code>, and <code>:collector</code>).</td>
</tr>
</tbody>
</table>
<p>Just two things to point out.</p>
<p>First, when copying the <code>:aws:s3:buckets:raw:in</code> path, do not copy the last folder name. This is the instance ID. With an auto-scaling environment set for the collector, there might be multiple instance folders in this bucket. If you only name one folder, you&rsquo;ll risk missing out on a lot of data.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/s3-raw-in-bucket.jpg" title="Path to the raw:in bucket">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/s3-raw-in-bucket.jpg#ZgotmplZ" alt="Path to the raw:in bucket" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2704 892'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>You can get the <code>:aws:emr:ec2_subnet_id</code> by opening the <strong>Services</strong> menu in AWS and clicking <strong>EC2</strong>. Click the link titled <strong>Running Instances</strong> (there should be 1 running instance - your collector). Scroll down the <strong>Description</strong> tab contents until you find the <strong>Subnet ID</strong> entry. Copy-paste that into the <code>aws:emr:ec2_subnet_id</code> field.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/subnet-id.jpg" title="EC2 subnet ID">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/subnet-id.jpg#ZgotmplZ" alt="EC2 subnet ID" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2878 1072'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>If you&rsquo;ve followed all the steps in this chapter, you should now be set.</p>
<p>You can verify everything works by running the following command in the directory where the <code>snowplow-emr-etl-runner</code> executable is, and where the <code>config</code> folder is located.</p>
<p><code>./snowplow-emr-etl-runner run -c config/config.yml -r config/iglu_resolver.json</code></p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/emr-etl-runner.jpg" title="Successfully ran that emr-etl-runner">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/emr-etl-runner.jpg#ZgotmplZ" alt="Successfully ran that emr-etl-runner" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1482 202'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>You can also follow the process in real-time by opening the <strong>Services</strong> menu in AWS and clicking <strong>EMR</strong>. There, you should see the job named <strong>Snowplow ETL</strong>. By clicking it, you can see all the steps it is running through. If the process ends in an error, you can also debug quite handily via this view, since you can see the exact step where the process failed.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/aws-emr-view.jpg" title="AWS EMR steps">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/aws-emr-view.jpg#ZgotmplZ" alt="AWS EMR steps" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2878 998'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Once the ETL has successfully completed, you can check your S3 buckets again. Your Snowplow data buckets should now contain a lot of new stuff. The folder with the interesting data is <strong>archive</strong> / <strong>shredded</strong>. This is where the good shredded datasets are copied to, and corresponds to what would have been copied to the relational database had you set this up in this step.</p>
<p>Anyway, with the ETL process up and running, just one more step remains in this monster of a guide: setting up AWS Redshift as the relational database where you&rsquo;ll be warehousing your analytics data!</p>
<h3 id="what-you-should-have-after-this-step-2">What you should have after this step</h3>
<ol>
<li>
<p>The <code>snowplow-emr-etl-runner</code> executable configured with your <code>config.yml</code> file.</p>
</li>
<li>
<p>New buckets in S3 to store all the files created by the batch process.</p>
</li>
<li>
<p>The ETL job running without errors all the way to completion, enriching and shredding the raw Tomcat logs into relevant S3 buckets.</p>
</li>
</ol>
<h2 id="step-4-load-the-data-into-redshift">Step 4: Load the data into Redshift</h2>
<h3 id="what-you-need-for-this-step-3">What you need for this step</h3>
<ol>
<li>
<p>The ETL process configured and available to run at your whim.</p>
</li>
<li>
<p>Shredded files being stored in the <strong>archive</strong> / <strong>shredded</strong> S3 bucket.</p>
</li>
<li>
<p>An SQL query client. I recommend <a href="http://www.sql-workbench.net/">SQL Workbench/J</a>, which is free. That&rsquo;s the one I&rsquo;ll be using in this guide.</p>
</li>
</ol>
<h3 id="getting-started-2">Getting started</h3>
<p>In this final step of this tutorial, we&rsquo;ll load the shredded data into Redshift tables. Redshift is a data warehouse service offered by AWS. We&rsquo;ll use it to build a relational database, where each table stores the information shredded from the Tomcat logs in a format easy to query with SQL. By the way, if you&rsquo;re unfamiliar with SQL, look no further than this great <a href="https://www.codecademy.com/learn/learn-sql">Codecademy course</a> to get you started with the query language!</p>
<p>The steps you&rsquo;ll take in this chapter are roughly these:</p>
<ol>
<li>
<p>Create a new cluster and database in Redshift.</p>
</li>
<li>
<p>Add users and all the necessary tables to the database.</p>
</li>
<li>
<p>Configure the EmrEtlRunner to automatically load the shredded data into Redshift tables.</p>
</li>
</ol>
<p>Once you&rsquo;re done, each time you run EmrEtlRunner, the tables will be populated with the parsed tracker data. You can then run SQL queries against this data, and use it to proceed with the two remaining steps (not covered in this guide) of the Snowplow pipeline: <a href="https://github.com/snowplow/snowplow/wiki/getting-started-with-data-modeling">data modeling</a> and <a href="https://github.com/snowplow/snowplow/wiki/getting-started-analyzing-snowplow-data">analysis</a>.</p>
<h3 id="create-the-cluster">Create the cluster</h3>
<p>In AWS, select <strong>Services</strong> from the top navigation and choose the <strong>Amazon Redshift</strong> service.</p>
<p>Again, double-check that you are in the correct region (the same one where you&rsquo;ve been working on all along, or, at the very least, the one where your S3 logs are). Then click the <strong>Launch cluster</strong> button.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/redshift-launch-cluster.jpg" title="Launch Redshift cluster">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/redshift-launch-cluster.jpg#ZgotmplZ" alt="Launch Redshift cluster" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1918 790'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Give the cluster an identifier. I named my cluster <code>simoahava</code>. Give a name to the database, too. The name I chose was <code>snowplow</code>.</p>
<p>Keep the database port at its default value (<strong>5439</strong>).</p>
<p>Add a username and password to your master user. This is the user you&rsquo;ll initially log in with, and it&rsquo;s the one you&rsquo;ll create the rest of the users and all the necessary tables with. Remember to write these down somewhere - you&rsquo;ll need them in just a bit.</p>
<p>Click <strong>Continue</strong> when ready.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/cluster-details.jpg" title="Redshift Cluster details">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/cluster-details.jpg#ZgotmplZ" alt="Redshift Cluster details" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2252 1114'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>In the next view, leave the two options at their default values. Node type should be <strong>dc2.large</strong>, and Cluster type should be <strong>Single Node</strong> (with <code>1</code> as the number of compute nodes used). Click <strong>Continue</strong> when ready.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/node-configuration.jpg" title="Configure cluster node">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/node-configuration.jpg#ZgotmplZ" alt="Configure cluster node" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2190 1294'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>In the Additional Configuration view, you can leave most of the options at their default values. For the <strong>VPC security group</strong>, select the <strong>default</strong> group. The settings should thus be something like these:</p>
<p><strong>Cluster parameter group</strong>: default-redshift-1.0<br>
<strong>Encrypt database</strong>: None<br>
<strong>Choose a VPC</strong>: Default VPC (&hellip;)<br>
<strong>Cluster subnet group</strong>: default<br>
<strong>Publicly accessible</strong>: Yes<br>
<strong>Choose a public IP address</strong>: No<br>
<strong>Enhanced VPC Routing</strong>: No<br>
<strong>Availability zone</strong>: No Preference<br>
<strong>VPC security groups</strong>: default (sg-&hellip;)<br>
<strong>Create CloudWatch Alarm</strong>: No<br>
<strong>Available roles</strong>: No selection</p>
<p>Once done, click <strong>Continue</strong>.</p>
<p>You can double-check your settings, and then just click <strong>Launch cluster</strong>.</p>
<p>The cluster will take some minutes to launch. You can check the status of the cluster in the Redshift dashboard.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/launching-cluster.jpg" title="Launching Redshift cluster">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/launching-cluster.jpg#ZgotmplZ" alt="Launching Redshift cluster" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2838 454'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Once the cluster has been launched, you are ready to log in and configure it!</p>
<h3 id="configure-the-cluster-and-connect-to-it">Configure the cluster and connect to it</h3>
<p>The first thing you&rsquo;ll need to do is make sure the cluster accepts incoming connections from your local machine.</p>
<p>So after clicking <strong>Services</strong> in the AWS top navigation and choosing <strong>Amazon Redshift</strong>, go to <strong>Clusters</strong> and then click the cluster name in the dashboard.</p>
<p>Under <strong>Cluster Properties</strong>, click the link to the <strong>VPC security group</strong> (should be named something like <code>default (sg-1234abcd)</code>).</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/vpc-security-group.jpg" title="VPC Security Group">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/vpc-security-group.jpg#ZgotmplZ" alt="VPC Security Group" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 748 502'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>You should be transported to the <strong>EC2</strong> dashboard, and <strong>Security Groups</strong> should be active in the navigation menu.</p>
<p>In the bottom of the screen, the settings for the security group you clicked should be visible.</p>
<p>Select the <strong>Inbound</strong> tab, and make sure it shows a <strong>TCP</strong> connection with Port Range <strong>5439</strong> and <strong>0.0.0.0/0</strong> as the Source. This means that all incoming TCP connections are permitted (you can change this to a more stricter policy later on).</p>
<p>If the values are different, you can <strong>Edit</strong> the Inbound rule to match these.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/edit-security-group.jpg" title="Edit security group">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/edit-security-group.jpg#ZgotmplZ" alt="Edit security group" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1912 837'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Now it&rsquo;s time to connect to the cluster. Go back to <strong>Amazon Redshift</strong>, and open your cluster settings as before. Copy the link to the cluster from the top of the settings list.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/redshift-cluster-url.jpg" title="Redshift cluster address">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/redshift-cluster-url.jpg#ZgotmplZ" alt="Redshift cluster address" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 727 190'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Next, open the SQL query tool of your choice. I&rsquo;m using <a href="http://www.sql-workbench.net/">SQL Workbench/J</a>. Select <strong>File</strong> / <strong>Connect Window</strong>, and create a new connection with the following settings changed from defaults:</p>
<p><strong>Driver</strong>: Amazon Redshift (com.amazon.redshift.jdbc.Driver)<br>
<strong>URL</strong>: jdbc:redshift://cluster_url:cluster_port/database_name<br>
<strong>Username</strong>: master_username<br>
<strong>Password</strong>: master_password<br>
<strong>Autocommit</strong>: Check</p>
<p>In <strong>URL</strong>, copy-paste the Redshift URL with port after the colon and database name after the slash.</p>
<p>As <strong>Username</strong> and <strong>Password</strong>, add the master username and master password you chose when creating the cluster.</p>
<p>Make sure <strong>Autocommit</strong> is checked. These are settings I have:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/connect-cluster-settings.jpg" title="Cluster connection settings">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/connect-cluster-settings.jpg#ZgotmplZ" alt="Cluster connection settings" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 978 385'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Once done, you can click <strong>OK</strong>, and the tool will connect to your cluster and database.</p>
<p>Once connected, you can feed the command <code>SELECT current_database();</code> and click the <strong>Execute</strong> button to check if everything works. This is what you should see:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/test-sql.jpg" title="Test SQL query">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/test-sql.jpg#ZgotmplZ" alt="Test SQL query" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 980 419'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>If the query returns the name of the database, you&rsquo;re good to go!</p>
<h3 id="create-the-database-tables">Create the database tables</h3>
<p>First, we&rsquo;ll need to create the tables that will store the Google Analytics tracker data within them. The tables are loaded as <code>.sql</code> files, and these files contain DDL (data-definition language) constructions that build all the necessary schemas and tables.</p>
<p>For this, you&rsquo;ll need access to the schema <code>.sql</code> files, which you&rsquo;ll find in the following locations within the snowplow Git repo:</p>
<ul>
<li>
<p><a href="https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/atomic-def.sql">snowplow/4-storage/redshift-storage/sql/atomic-def.sql</a></p>
</li>
<li>
<p><a href="https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/manifest-def.sql">snowplow/4-storage/redshift-storage/sql/manifest-def.sql</a></p>
</li>
</ul>
<p>Load <code>atomic-def.sql</code> first, and run the file in your SQL query tool while connected to your Redshift database. You should see a message that the schema <code>atomic</code> and table <code>atomic.events</code> were created successfully.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/atomic-def-run.jpg" title="atomic-def.sql run successfully">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/atomic-def-run.jpg#ZgotmplZ" alt="atomic-def.sql run successfully" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1540 600'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Next, run <code>manifest-def.sql</code> while connected to the database. You should see a message that the table <code>atomic.manifest</code> was created successfully.</p>
<p>Now you need to load all the DDLs for the Google Analytics schemas. If you don&rsquo;t create these tables, then the ETL process will run into an error, where the utility tries to copy shredded events into non-existent tables.</p>
<p>You can find all the required <code>.sql</code> files in the following three directories:</p>
<ul>
<li>
<p><a href="https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics">https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics</a></p>
</li>
<li>
<p><a href="https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.enhanced-ecommerce">https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.enhanced-ecommerce</a></p>
</li>
<li>
<p><a href="https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.measurement-protocol">https://github.com/snowplow/iglu-central/tree/master/sql/com.google.analytics.measurement-protocol</a></p>
</li>
</ul>
<p>You need to load all the <code>.sql</code> files in these three directories and run them while connected to your database. This will create a whole bunch of tables you&rsquo;ll need if you want to collect Google Analytics tracker data.</p>
<p>It might be easiest to clone the <strong>iglu-central</strong> repo, and then load the <code>.sql</code> files into your query tool from the local directories.</p>
<p>Once you&rsquo;re done, you should be able to run the following SQL query and see a list of all the tables you just created as a result (should be 40 in total):</p>
<p><code>SELECT * FROM pg_tables WHERE schemaname='atomic';</code></p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/all-tables-created.jpg" title="All tables created">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/all-tables-created.jpg#ZgotmplZ" alt="All tables created" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2008 880'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<h3 id="create-the-database-users">Create the database users</h3>
<p>Next thing we&rsquo;ll do is create three users:</p>
<ul>
<li>
<p><code>storageloader</code>, who will be in charge of the ETL process.</p>
</li>
<li>
<p><code>power_user</code>, who will have admin privileges, so you no longer have to log in with the master credentials.</p>
</li>
<li>
<p><code>read_only</code>, who can query data and create his/her own tables.</p>
</li>
</ul>
<p>Make sure you&rsquo;re still connected to the database, and copy-paste the following SQL queries into the query window. For each <code>$password</code>, change it to a proper password, and make sure you write these user + password combinations down somewhere.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00a">CREATE</span> <span style="color:#00a">USER</span> storageloader PASSWORD <span style="color:#a50">&#39;$password&#39;</span>;
<span style="color:#00a">GRANT</span> <span style="color:#00a">USAGE</span> <span style="color:#00a">ON</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> storageloader;
<span style="color:#00a">GRANT</span> <span style="color:#00a">INSERT</span> <span style="color:#00a">ON</span> <span style="color:#00a">ALL</span> TABLES <span style="color:#00a">IN</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> storageloader;

<span style="color:#00a">CREATE</span> <span style="color:#00a">USER</span> read_only PASSWORD <span style="color:#a50">&#39;$password&#39;</span>;
<span style="color:#00a">GRANT</span> <span style="color:#00a">USAGE</span> <span style="color:#00a">ON</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> read_only;
<span style="color:#00a">GRANT</span> <span style="color:#00a">SELECT</span> <span style="color:#00a">ON</span> <span style="color:#00a">ALL</span> TABLES <span style="color:#00a">IN</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> read_only;
<span style="color:#00a">CREATE</span> <span style="color:#00a">SCHEMA</span> scratchpad;
<span style="color:#00a">GRANT</span> <span style="color:#00a">ALL</span> <span style="color:#00a">ON</span> <span style="color:#00a">SCHEMA</span> scratchpad <span style="color:#00a">TO</span> read_only;

<span style="color:#00a">CREATE</span> <span style="color:#00a">USER</span> power_user PASSWORD <span style="color:#a50">&#39;$password&#39;</span>;
<span style="color:#00a">GRANT</span> <span style="color:#00a">ALL</span> <span style="color:#00a">ON</span> <span style="color:#00a">DATABASE</span> snowplow <span style="color:#00a">TO</span> power_user;
<span style="color:#00a">GRANT</span> <span style="color:#00a">ALL</span> <span style="color:#00a">ON</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> power_user;
<span style="color:#00a">GRANT</span> <span style="color:#00a">ALL</span> <span style="color:#00a">ON</span> <span style="color:#00a">ALL</span> TABLES <span style="color:#00a">IN</span> <span style="color:#00a">SCHEMA</span> <span style="color:#00a">atomic</span> <span style="color:#00a">TO</span> power_user;</code></pre></div>
<p>Again, remember to change the three <code>$password</code> values to proper SQL user passwords.</p>
<p>If all goes well, you should see 12 &ldquo;COMMAND executed successfully&rdquo; statements.</p>
<p>Finally, you need to grant ownership of all tables in schema <code>atomic</code> to <strong>storageloader</strong>, because this user will need to run some commands (specifically, <code>vacuum</code> and <code>analyze</code>) that only table owners can run.</p>
<p>So, first run the following query in the database.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00a">SELECT</span> <span style="color:#a50">&#39;ALTER TABLE atomic.&#39;</span> || tablename ||<span style="color:#a50">&#39; OWNER TO storageloader;&#39;</span>
<span style="color:#00a">FROM</span> pg_tables <span style="color:#00a">WHERE</span> schemaname=<span style="color:#a50">&#39;atomic&#39;</span> <span style="color:#00a">AND</span> <span style="color:#00a">NOT</span> tableowner=<span style="color:#a50">&#39;storageloader&#39;</span>;</code></pre></div>
<p>In the query results, you should see a bunch of <code>ALTER TABLE atomic.* OWNER TO storageloader;</code> queries. Copy all of these, and paste them into the statement field as new queries. Then run the statements.</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/change-owner.jpg" title="Change ownership of all tables">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/change-owner.jpg#ZgotmplZ" alt="Change ownership of all tables" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2080 1148'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Now, if you run <code>SELECT * FROM pg_tables WHERE schemaname='atomic' AND tableowner='storageloader';</code>, you should see all the tables in the atomic schema as a result.</p>
<p>You have successfully created the users and the tables in the database. All that&rsquo;s left is to configure the EmrEtlRunner to execute the final step of the ETL process, where the <code>storageloader</code> user copies all the data from the shredded files into the corresponding Redshift tables.</p>
<h3 id="create-new-iam-role-for-database-loader">Create new IAM role for database loader</h3>
<p>The EmrEtlRunner will copy the files to Redshift using a utility called RDB Loader (Relational Database Loader). For this tool to work with sufficient privileges, you&rsquo;ll need to create a new <strong>IAM Role</strong>, which grants the Redshift cluster read-only access to your S3 buckets.</p>
<ul>
<li>
<p>So, in AWS, click <strong>Services</strong> and select <strong>IAM</strong>.</p>
</li>
<li>
<p>Select <strong>Roles</strong> from the left-hand navigation. Click the <strong>Create role</strong> button.</p>
</li>
<li>
<p>In the <strong>Select type of trusted entity</strong> view, keep the default <strong>AWS Service</strong> selected, and choose <strong>Redshift</strong> from the list of services. In the <strong>Select your use case</strong> list, choose <strong>Redshift - Customizable</strong>, and then click <strong>Next: Permissions</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/iam-role-redshift.jpg" title="Grant Redshift permissions to IAM role">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/iam-role-redshift.jpg#ZgotmplZ" alt="Grant Redshift permissions to IAM role" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1522 1454'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>In the next view, find the policy named <strong>AmazonS3ReadOnlyAccess</strong>, and check the box next to it. Click <strong>Next: Review</strong>.</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/amazon-s3-read-only.jpg" title="AmazonS3ReadOnlyAccess rights">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/amazon-s3-read-only.jpg#ZgotmplZ" alt="AmazonS3ReadOnlyAccess rights" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1996 1220'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Name the role something useful, such as <code>RedshiftS3Access</code> and click <strong>Create Role</strong> when ready.</p>
</li>
<li>
<p>You should be back in the list of roles. Click the newly created <code>RedshiftS3Access</code> role to see its configuration. Copy the value in the <strong>Role ARN</strong> field to the clipboard. You&rsquo;ll need it very soon.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/role-arn.jpg" title="Role ARN for redshift IAM role">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/role-arn.jpg#ZgotmplZ" alt="Role ARN for redshift IAM role" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2458 950'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>
<p>Finally, select <strong>Services</strong> from AWS top navigation and choose the <strong>Amazon Redshift</strong> service. Click <strong>Clusters</strong> in the left-hand navigation to see the list of running clusters.</p>
</li>
<li>
<p>Check the box next to your Snowplow cluster, and click <strong>Manage IAM Roles</strong>.</p>
</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/manage-iam-roles.jpg" title="Manage IAM Roles">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/manage-iam-roles.jpg#ZgotmplZ" alt="Manage IAM Roles" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 2246 672'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<ul>
<li>In the <strong>Available roles</strong> list, choose the role you just created, and then click <strong>Apply changes</strong> to apply the role to your cluster.</li>
</ul>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/apply-role.jpg" title="Apply IAM role">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/apply-role.jpg#ZgotmplZ" alt="Apply IAM role" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1212 630'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>The Cluster Status should change to <strong>modifying</strong>. Once it&rsquo;s done, the status will change to <strong>available</strong>, and you can check if the role you assigned is labeled as <strong>in-sync</strong> by clicking <strong>Manage IAM Roles</strong> again.</p>
<h3 id="edit-the-redshift-target-configuration">Edit the Redshift target configuration</h3>
<p>If you copied all necessary files back in <a href="#download-the-necessary-files">Step 3</a>, your project <code>config/</code> directory should include a <code>targets/</code> folder with the file <code>redshift.json</code> in it. If you don&rsquo;t have it, go back to Step 3 and make sure you copy the <code>redshift.json</code> template to the correct folder.</p>
<p>Once you&rsquo;ve found the template, open it for editing, and make sure it looks something like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">{<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#00a">&#34;schema&#34;: </span><span style="color:#a50">&#34;iglu:com.snowplowanalytics.snowplow.storage/redshift_config/jsonschema/2-1-0&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">    </span><span style="color:#00a">&#34;data&#34;: </span>{<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;name&#34;: </span><span style="color:#a50">&#34;AWS Redshift enriched events storage&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;host&#34;: </span><span style="color:#a50">&#34;simoahava.coyhone1deuh.eu-west-1.redshift.amazonaws.com&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;database&#34;: </span><span style="color:#a50">&#34;snowplow&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;port&#34;: </span><span style="color:#099">5439</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;sslMode&#34;: </span><span style="color:#a50">&#34;DISABLE&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;username&#34;: </span><span style="color:#a50">&#34;storageloader&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;password&#34;: </span><span style="color:#a50">&#34;...&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;roleArn&#34;: </span><span style="color:#a50">&#34;arn:aws:iam::375284143851:role/RedshiftS3Access&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;schema&#34;: </span><span style="color:#a50">&#34;atomic&#34;</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;maxError&#34;: </span><span style="color:#099">1</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;compRows&#34;: </span><span style="color:#099">20000</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;sshTunnel&#34;: </span><span style="color:#00a">null</span>,<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#00a">&#34;purpose&#34;: </span><span style="color:#a50">&#34;ENRICHED_EVENTS&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">    </span>}<span style="color:#bbb">
</span><span style="color:#bbb"></span>}</code></pre></div>
<p>Here are the fields you need to edit:</p>
<ul>
<li><strong>host</strong>: The URL of your Redshift cluster</li>
<li><strong>database</strong>: The database name</li>
<li><strong>username</strong>: storageloader</li>
<li><strong>password</strong>: storageloader password</li>
<li><strong>roleArn</strong>: The Role ARN of the IAM role you created in the previous step</li>
</ul>
<p>All the other options you can leave with their default values.</p>
<h3 id="re-run-emretlrunner-through-the-whole-process">Re-run EmrEtlRunner through the whole process</h3>
<p>Now that you&rsquo;ve configured <strong>everything</strong>, you&rsquo;re ready to run the EmrEtlRunner with all the steps in the ETL process included. This means <strong>enrichment</strong> of the log data, <strong>shredding</strong> of the log data to atomic datasets, and <strong>loading</strong> these datasets into your Redshift tables.</p>
<p>The command you&rsquo;ll need to run in the root of your project folder (where the <code>snowplow-emr-etl-runner</code> executable is) is this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./snowplow-emr-etl-runner run -c config/config.yml -r config/iglu_resolver.json -t config/targets</code></pre></div>
<p>This command will process all the data in the <code>:raw:in</code> bucket (the one with all your Tomcat logs), and proceed to extract and transform them, and finally load them into your Redshift tables. The process will take a while, so go grab a coffee. Remember that you can check the status of the job by browsing to <strong>EMR</strong> via the AWS <strong>Services</strong> navigation.</p>
<p>Once complete, you should see something like this in the command line:</p>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/emr-etl-complete.jpg" title="EmrEtlRunner finished">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/emr-etl-complete.jpg#ZgotmplZ" alt="EmrEtlRunner finished" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1722 338'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<h3 id="test-it">Test it</h3>
<p>Now you should be able to login to the database using the new <code>read_only</code> user. If you run the following query, it should return a list of timestamps and events for each Client ID visiting your site.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00a">SELECT</span> u.root_tstamp, u.client_id, h.<span style="color:#00a">type</span>
<span style="color:#00a">FROM</span> <span style="color:#00a">atomic</span>.com_google_analytics_measurement_protocol_user_1 <span style="color:#00a">AS</span> u 
<span style="color:#00a">JOIN</span> <span style="color:#00a">atomic</span>.com_google_analytics_measurement_protocol_hit_1 <span style="color:#00a">AS</span> h
<span style="color:#00a">ON</span> u.root_id = h.root_id
<span style="color:#00a">ORDER</span> <span style="color:#00a">BY</span> root_tstamp <span style="color:#00a">ASC</span></code></pre></div>



<div class="figure nocaption" >
  
    <a href="https://www.simoahava.com/images/2018/02/sql-example-query.jpg" title="SQL Example Query">
  
    <img class="fig-img lazy" data-src="https://www.simoahava.com/images/2018/02/sql-example-query.jpg#ZgotmplZ" alt="SQL Example Query" src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1648 836'%3E%3C/svg%3E">
  
    </a>
  
  
</div>


<p>Considering how much time you have probably put into making everything work (if following this guide diligently), I really hope it all works correctly.</p>
<h2 id="wrapping-it-all-up">Wrapping it all up</h2>
<p>By following this guide, you should be able to set up an end-to-end Snowplow batch pipeline.</p>
<ol>
<li>
<p>Google Tag Manager duplicates the payloads sent to Google Analytics, and sends these to your Amazon endpoint, using a custom domain name whose DNS records you have delegated to AWS.</p>
</li>
<li>
<p>The endpoint is a collector which logs all the HTTP requests to Tomcat logs, and stores them in an S3 bucket.</p>
</li>
<li>
<p>An ETL process is then run, enriching the stored data, and shredding it to atomic datasets, again stored in S3.</p>
</li>
<li>
<p>Finally, the ETL runner copies these datasets into tables you&rsquo;ve set up in a new relational database running on an AWS Redshift cluster.</p>
</li>
</ol>
<p>There are <strong>SO</strong> many moving parts here, that it&rsquo;s possible you&rsquo;ll get something wrong at some point. Just try to patiently walk through the steps in this guide to see if you&rsquo;ve missed anything.</p>
<p>Feel free to ask questions in the comments, and maybe I or my readers will be able to help you along.</p>
<p>You can also join the discussions in Snowplow&rsquo;s <a href="https://discourse.snowplowanalytics.com/">Discourse site</a> - I&rsquo;m certain the folks there are more than happy to help you if you run into trouble setting up the pipeline.</p>
<p>Do note also that the setup outlined in this guide is very rudimentary. There are many ways you can, and should, optimize the process, such as:</p>
<ol>
<li>
<p>Add SSL support to your Redshift cluster.</p>
</li>
<li>
<p>Scale the instances (collector and the ETL process) correctly to account for peaks in traffic and dataset size.</p>
</li>
<li>
<p>Move the EmrEtlRunner to AWS, too. There&rsquo;s no need to run it on your local machine.</p>
</li>
<li>
<p>Schedule the EmrEtlRunner to run (at least) once a day, so that your database is refreshed with new data periodically.</p>
</li>
</ol>
<p>Good luck!</p>

              

            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://www.simoahava.com/tags/google-tag-manager/">google tag manager</a>

  <a class="tag tag--primary tag--small" href="https://www.simoahava.com/tags/snowplow/">snowplow</a>

  <a class="tag tag--primary tag--small" href="https://www.simoahava.com/tags/amazon/">amazon</a>

  <a class="tag tag--primary tag--small" href="https://www.simoahava.com/tags/aws/">aws</a>

                  </div>
                
              
            
            
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default" href="https://www.simoahava.com/analytics/fix-missing-page-view-broken-triggers/">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default" href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=Snowplow:%20Full%20Setup%20With%20Google%20Analytics%20Tracking%20https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/%20via%20@SimoAhava">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.linkedin.com/cws/share?url=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
              <i class="fa fa-linkedin"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#commento_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#">
          <i class="fa fa-search"></i>
        </a>
      </li>
    </ul>
  </div>


            
              <span id="commento_thread"></span>
<div id="commento"></div>
<script defer src="https://cdn.commento.io/js/commento.js"></script>

            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Simo Ahava. All Rights Reserved
  </span>
</footer>

      </div>
      
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default" href="https://www.simoahava.com/analytics/fix-missing-page-view-broken-triggers/">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default" href="https://www.simoahava.com/analytics/automatically-fork-google-analytics-hits-snowplow/">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=Snowplow:%20Full%20Setup%20With%20Google%20Analytics%20Tracking%20https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/%20via%20@SimoAhava">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.linkedin.com/cws/share?url=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
              <i class="fa fa-linkedin"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#commento_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#">
          <i class="fa fa-search"></i>
        </a>
      </li>
    </ul>
  </div>


      </div>
      
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=Snowplow:%20Full%20Setup%20With%20Google%20Analytics%20Tracking%20https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/%20via%20@SimoAhava">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.linkedin.com/cws/share?url=https://www.simoahava.com/analytics/snowplow-full-setup-with-google-analytics-tracking/">
          <i class="fa fa-linkedin"></i><span>Share on LinkedIn</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>

    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.simoahava.com/images/simo.png" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Simo Ahava</h4>
    
      <div id="about-card-bio">Husband | Father | Analytics developer<br>simo (at) simoahava.com</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Senior Data Advocate at Reaktor
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Finland
      </div>
    
  </div>
</div>

    
  
    
    <div id="cover" style="background-image:url('https://www.simoahava.com/images/maisema.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>




<script src="https://www.simoahava.com/js/script.js"></script>




<script src="https://cdn.commento.io/js/count.js"></script>


    <script>
      var mylazyload = new LazyLoad({elements_selector: '.lazy'});
      </script>

    
  </body>
</html>

