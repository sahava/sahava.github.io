<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Google Cloud on Simo Ahava's blog</title><link>https://www.simoahava.com/amp/categories/google-cloud/</link><description>Recent content in Google Cloud on Simo Ahava's blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 15 Jun 2020 07:00:00 +0300</lastBuildDate><atom:link href="https://www.simoahava.com/amp/categories/google-cloud/index.xml" rel="self" type="application/rss+xml"/><item><title>Cookie Audit With Google BigQuery</title><link>https://www.simoahava.com/amp/google-cloud/cookie-audit-with-google-bigquery/</link><pubDate>Mon, 15 Jun 2020 07:00:00 +0300</pubDate><guid>https://www.simoahava.com/amp/google-cloud/cookie-audit-with-google-bigquery/</guid><description>&lt;p>On New Year&amp;rsquo;s Eve 2018, I published an article which instructed &lt;a href="https://www.simoahava.com/amp/google-cloud/scrape-domain-and-write-results-to-bigquery/">how to scrape pages of a site and write the results into Google BigQuery&lt;/a>. I considered it to be a cool way to build your own web scraper, as it utilized the power and scale of the Google Cloud platform combined with the flexibility of a &lt;a href="https://github.com/yujiosaka/headless-chrome-crawler">headless crawler&lt;/a> built on top of &lt;a href="https://github.com/puppeteer/puppeteer">Puppeteer&lt;/a>.&lt;/p>
&lt;p>In today&amp;rsquo;s article, I&amp;rsquo;m revisiting this solution in order to share with you its latest version, which includes a feature that you might find extremely useful when &lt;a href="https://ico.org.uk/for-organisations/guide-to-pecr/guidance-on-the-use-of-cookies-and-similar-technologies/how-do-we-comply-with-the-cookie-rules/">auditing the cookies&lt;/a> that are dropped on your site.&lt;/p></description></item><item><title>Create A Cookie Rewrite Web Service Using The Google Cloud Platform</title><link>https://www.simoahava.com/amp/google-cloud/create-cookie-rewrite-web-service-google-cloud/</link><pubDate>Wed, 12 Jun 2019 07:00:00 +0300</pubDate><guid>https://www.simoahava.com/amp/google-cloud/create-cookie-rewrite-web-service-google-cloud/</guid><description>&lt;blockquote>
&lt;p>&lt;strong>Last updated 11 September 2020&lt;/strong>: Added &lt;a href="#step-4-map-the-custom-domain-to-the-endpoint">important note&lt;/a> about how the custom domain should be mapped with A/AAAA DNS records rather than a CNAME record.&lt;/p>
&lt;/blockquote>
&lt;p>Ah, Safari&amp;rsquo;s &lt;a href="https://webkit.org/blog/category/privacy/">Intelligent Tracking Prevention&lt;/a> - the gift that &lt;a href="https://www.simoahava.com/amp/analytics/itp-2-1-and-web-analytics/">keeps on giving&lt;/a>. Having almost milked this cow for all it&amp;rsquo;s worth, I was sure there would be little need to revisit the topic. Maybe, I thought, it would be better to just sit back and watch the world burn.&lt;/p></description></item><item><title>Scrape The URLs Of A Domain And Write The Results To BigQuery</title><link>https://www.simoahava.com/amp/google-cloud/scrape-domain-and-write-results-to-bigquery/</link><pubDate>Mon, 31 Dec 2018 11:14:06 +0200</pubDate><guid>https://www.simoahava.com/amp/google-cloud/scrape-domain-and-write-results-to-bigquery/</guid><description>&lt;p>In my intense love affair with the &lt;a href="https://cloud.google.com/">Google Cloud Platform&lt;/a>, I&amp;rsquo;ve never felt more inspired to write content and try things out. After starting with a &lt;a href="https://www.simoahava.com/amp/analytics/install-snowplow-on-the-google-cloud-platform/">Snowplow Analytics setup guide&lt;/a>, and continuing with a &lt;a href="https://www.simoahava.com/amp/google-cloud/lighthouse-bigquery-google-cloud-platform/">Lighthouse audit automation tutorial&lt;/a>, I&amp;rsquo;m going to show you yet another cool thing you can do with GCP.&lt;/p>


 &lt;a href="https://www.simoahava.com/amp/images/2018/12/bigquery-status-codes.jpg" title="BigQuery status codes">
 
 &lt;amp-img src="https://www.simoahava.com/amp/images/2018/12/bigquery-status-codes.jpg" layout="responsive" alt="BigQuery status codes" height="501" width="981">&lt;/amp-img>
 
 &lt;/a>
 

&lt;p>In this guide, I&amp;rsquo;ll show you how to use an &lt;a href="https://github.com/yujiosaka/headless-chrome-crawler">open-source web crawler&lt;/a> running in a &lt;a href="https://cloud.google.com/compute/">Google Compute Engine&lt;/a> virtual machine (VM) instance to scrape all the internal and external links of a given domain, and write the results into a &lt;a href="https://cloud.google.com/bigquery/">BigQuery table&lt;/a>. With this setup, you can audit and monitor the links in any website, looking for bad status codes or missing titles, and fix them to improve your site&amp;rsquo;s logical architecture.&lt;/p></description></item><item><title>Audit Multiple Sites With Lighthouse And Write Results To BigQuery</title><link>https://www.simoahava.com/amp/google-cloud/lighthouse-bigquery-google-cloud-platform/</link><pubDate>Tue, 18 Dec 2018 09:03:08 +0200</pubDate><guid>https://www.simoahava.com/amp/google-cloud/lighthouse-bigquery-google-cloud-platform/</guid><description>&lt;p>&lt;a href="https://cloud.google.com/">Google Cloud Platform&lt;/a> is very, very cool. It&amp;rsquo;s a fully capable, enterprise-grade, scalable cloud ecosystem which lets even total novices get started with building their first cloud applications. I wrote a long guide for installing &lt;a href="https://www.simoahava.com/amp/analytics/install-snowplow-on-the-google-cloud-platform/">Snowplow on the GCP&lt;/a>, and you might want to read that if you want to see how you can build your own analytics tool using some nifty open-source modules.&lt;/p>
&lt;p>But this guide will not be about &lt;a href="https://snowplowanalytics.com/">Snowplow&lt;/a>. Rather, it will tackle Google&amp;rsquo;s own open-source performance audit tool: &lt;a href="https://developers.google.com/web/tools/lighthouse/">Lighthouse&lt;/a>.&lt;/p></description></item></channel></rss>